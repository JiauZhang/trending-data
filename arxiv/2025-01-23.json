[
    {
        "id": "1",
        "title": "The ELEVATE-AI LLMs Framework: An Evaluation Framework for Use of Large Language Models in HEOR: an ISPOR Working Group Report",
        "author": [
            "Rachael L. Fleurence",
            "Dalia Dawoud",
            "Jiang Bian",
            "Mitchell K. Higashi",
            "Xiaoyan Wang",
            "Hua Xu",
            "Jagpreet Chhatwal",
            "Turgay Ayer"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12394",
        "abstract": "Introduction. Generative Artificial Intelligence, particularly large language models (LLMs), offers transformative potential for Health Economics and Outcomes Research (HEOR). However, evaluating the quality, transparency, and rigor of LLM-assisted research lacks standardized guidance. This article introduces the ELEVATE AI LLMs framework and checklist, designed to support researchers and reviewers in assessing LLM use in HEOR.\nMethods. The ELEVATE AI LLMs framework was developed through a targeted review of existing guidelines and evaluation frameworks. The framework comprises ten evaluation domains, including model characteristics, accuracy, comprehensiveness, and fairness. The accompanying checklist operationalizes the framework. To validate the framework, we applied it to two published studies, demonstrating its usability across different HEOR tasks.\nResults. The ELEVATE AI LLMs framework provides a comprehensive structure for evaluating LLM-assisted research, while the checklist facilitates practical application. Validation of the framework and checklist on studies of systematic literature reviews and health economic modeling highlighted their ability to identify strengths and gaps in reporting.\nLimitations. While the ELEVATE AI LLMs framework provides robust guidance, its broader generalizability and applicability to diverse HEOR tasks require further empirical testing. Additionally, several metrics adapted from computer science need further validation in HEOR contexts.\nConclusion. The ELEVATE AI LLMs framework and checklist fill a critical gap in HEOR by offering structured guidance for evaluating LLM-assisted research. By promoting transparency, accuracy, and reproducibility, they aim to standardize and improve the integration of LLMs into HEOR, ensuring their outputs meet the field's rigorous standards.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "2",
        "title": "FinSphere: A Conversational Stock Analysis Agent Equipped with Quantitative Tools based on Real-Time Database",
        "author": [
            "Shijie Han",
            "Changhai Zhou",
            "Yiqing Shen",
            "Tianning Sun",
            "Yuhua Zhou",
            "Xiaoxia Wang",
            "Zhixiao Yang",
            "Jingshu Zhang",
            "Hongguang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12399",
        "abstract": "Current financial Large Language Models (LLMs) struggle with two critical limitations: a lack of depth in stock analysis, which impedes their ability to generate professional-grade insights, and the absence of objective evaluation metrics to assess the quality of stock analysis reports. To address these challenges, this paper introduces FinSphere, a conversational stock analysis agent, along with three major contributions: (1) Stocksis, a dataset curated by industry experts to enhance LLMs' stock analysis capabilities, (2) AnalyScore, a systematic evaluation framework for assessing stock analysis quality, and (3) FinSphere, an AI agent that can generate high-quality stock analysis reports in response to user queries. Experiments demonstrate that FinSphere achieves superior performance compared to both general and domain-specific LLMs, as well as existing agent-based systems, even when they are enhanced with real-time data access and few-shot guidance. The integrated framework, which combines real-time data feeds, quantitative tools, and an instruction-tuned LLM, yields substantial improvements in both analytical quality and practical applicability for real-world stock analysis.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "3",
        "title": "Scopes of Alignment",
        "author": [
            "Kush R. Varshney",
            "Zahra Ashktorab",
            "Djallel Bouneffouf",
            "Matthew Riemer",
            "Justin D. Weisz"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12405",
        "abstract": "Much of the research focus on AI alignment seeks to align large language models and other foundation models to the context-less and generic values of helpfulness, harmlessness, and honesty. Frontier model providers also strive to align their models with these values. In this paper, we motivate why we need to move beyond such a limited conception and propose three dimensions for doing so. The first scope of alignment is competence: knowledge, skills, or behaviors the model must possess to be useful for its intended purpose. The second scope of alignment is transience: either semantic or episodic depending on the context of use. The third scope of alignment is audience: either mass, public, small-group, or dyadic. At the end of the paper, we use the proposed framework to position some technologies and workflows that go beyond prevailing notions of alignment.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "4",
        "title": "The Streaming Batch Model for Efficient and Fault-Tolerant Heterogeneous Execution",
        "author": [
            "Frank Sifei Luan",
            "Ziming Mao",
            "Ron Yifeng Wang",
            "Charlotte Lin",
            "Amog Kamsetty",
            "Hao Chen",
            "Cheng Su",
            "Balaji Veeramani",
            "Scott Lee",
            "SangBin Cho",
            "Clark Zinzow",
            "Eric Liang",
            "Ion Stoica",
            "Stephanie Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12407",
        "abstract": "While ML model training and inference are both GPU-intensive, CPU-based data processing is often the bottleneck. Distributed data processing systems based on the batch or stream processing models assume homogeneous resource requirements. They excel at CPU-based computation but either under-utilize heterogeneous resources or impose high overheads on failure and reconfiguration. We introduce the streaming batch model, a hybrid of the two models that enables efficient and fault-tolerant heterogeneous execution. The key idea is to execute one partition at a time to allow lineage-based recovery with dynamic resource allocation. This enables memory-efficient pipelining across heterogeneous resources, similar to stream processing, but also offers the elasticity and fault tolerance properties of batch processing. We present Ray Data, an implementation of the streaming batch model that improves throughput on heterogeneous batch inference pipelines by 3--8$\\times$ compared to traditional batch and stream processing systems. When training Stable Diffusion, Ray Data matches the throughput of single-node ML data loaders while additionally leveraging distributed heterogeneous clusters to further improve training throughput by 31%.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "5",
        "title": "Consolidating TinyML Lifecycle with Large Language Models: Reality, Illusion, or Opportunity?",
        "author": [
            "Guanghan Wu",
            "Sasu Tarkoma",
            "Roberto Morabito"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12420",
        "abstract": "The evolving requirements of Internet of Things (IoT) applications are driving an increasing shift toward bringing intelligence to the edge, enabling real-time insights and decision-making within resource-constrained environments. Tiny Machine Learning (TinyML) has emerged as a key enabler of this evolution, facilitating the deployment of ML models on devices such as microcontrollers and embedded systems. However, the complexity of managing the TinyML lifecycle, including stages such as data processing, model optimization and conversion, and device deployment, presents significant challenges and often requires substantial human intervention. Motivated by these challenges, we began exploring whether Large Language Models (LLMs) could help automate and streamline the TinyML lifecycle. We developed a framework that leverages the natural language processing (NLP) and code generation capabilities of LLMs to reduce development time and lower the barriers to entry for TinyML deployment. Through a case study involving a computer vision classification model, we demonstrate the framework's ability to automate key stages of the TinyML lifecycle. Our findings suggest that LLM-powered automation holds potential for improving the lifecycle development process and adapting to diverse requirements. However, while this approach shows promise, there remain obstacles and limitations, particularly in achieving fully automated solutions. This paper sheds light on both the challenges and opportunities of integrating LLMs into TinyML workflows, providing insights into the path forward for efficient, AI-assisted embedded system development.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "6",
        "title": "CroMe: Multimodal Fake News Detection using Cross-Modal Tri-Transformer and Metric Learning",
        "author": [
            "Eunjee Choi",
            "Junhyun Ahn",
            "XinYu Piao",
            "Jong-Kook Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12422",
        "abstract": "Multimodal Fake News Detection has received increasing attention recently. Existing methods rely on independently encoded unimodal data and overlook the advantages of capturing intra-modality relationships and integrating inter-modal similarities using advanced techniques. To address these issues, Cross-Modal Tri-Transformer and Metric Learning for Multimodal Fake News Detection (CroMe) is proposed. CroMe utilizes Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (BLIP2) as encoders to capture detailed text, image and combined image-text representations. The metric learning module employs a proxy anchor method to capture intra-modality relationships while the feature fusion module uses a Cross-Modal and Tri-Transformer for effective integration. The final fake news detector processes the fused features through a classifier to predict the authenticity of the content. Experiments on datasets show that CroMe excels in multimodal fake news detection.",
        "tags": [
            "Detection",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "7",
        "title": "FREYR: A Framework for Recognizing and Executing Your Requests",
        "author": [
            "Roberto Gallotta",
            "Antonios Liapis",
            "Georgios N. Yannakakis"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12423",
        "abstract": "Large language models excel as conversational agents, but their capabilities can be further extended through tool usage, i.e.: executable code, to enhance response accuracy or address specialized domains. Current approaches to enable tool usage often rely on model-specific prompting or fine-tuning a model for function-calling instructions. Both approaches have notable limitations, including reduced adaptability to unseen tools and high resource requirements. This paper introduces FREYR, a streamlined framework that modularizes the tool usage process into separate steps. Through this decomposition, we show that FREYR achieves superior performance compared to conventional tool usage methods. We evaluate FREYR on a set of real-world test cases specific for video game design and compare it against traditional tool usage as provided by the Ollama API.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "8",
        "title": "SplitQuant: Layer Splitting for Low-Bit Neural Network Quantization",
        "author": [
            "Jaewoo Song",
            "Fangzhen Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12428",
        "abstract": "Quantization for deep neural networks (DNNs) is the process of mapping the parameter values of DNNs from original data types to other data types of lower precision to reduce model sizes and make inference faster. Quantization often maps different original values to a single quantized value because the range of the original values is larger than the range of the quantized values. This leads to the degradation of the accuracy of the quantized DNNs. Outliers are a main cause of the degradation of quantization resolution because they enlarge the range of original values. To solve the problem, the percentile method is often used to clip outliers. However, clipping the outliers has another problem of removing the important and strong signals in the DNNs. This paper proposes SplitQuant to keep the outliers and improve the quantization resolution at the same time. SplitQuant narrows down the range of the original values and mitigates the effect of outliers by splitting each quantizable layer into three mathematically equivalent layers and applies different scaling factors. Especially, weights and biases are clustered into lower, middle and upper clusters for optimized split. By preprocessing DNNs with SplitQuant, quantization algorithms can achieve better results. SplitQuant was applied on two BERT-Tiny models and improved the accuracy of INT2 quantization by 3.3%p and 2.1%p, achieving accuracies comparable to those of the original FP32 models.",
        "tags": [
            "BERT",
            "CLIP"
        ]
    },
    {
        "id": "9",
        "title": "SCFCRC: Simultaneously Counteract Feature Camouflage and Relation Camouflage for Fraud Detection",
        "author": [
            "Xiaocheng Zhang",
            "Zhuangzhuang Ye",
            "GuoPing Zhao",
            "Jianing Wang",
            "Xiaohong Su"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12430",
        "abstract": "In fraud detection, fraudsters often interact with many benign users, camouflaging their features or relations to hide themselves. Most existing work concentrates solely on either feature camouflage or relation camouflage, or decoupling feature learning and relation learning to avoid the two camouflage from affecting each other. However, this inadvertently neglects the valuable information derived from features or relations, which could mutually enhance their adversarial camouflage strategies. In response to this gap, we propose SCFCRC, a Transformer-based fraud detector that Simultaneously Counteract Feature Camouflage and Relation Camouflage. SCFCRC consists of two components: Feature Camouflage Filter and Relation Camouflage Refiner. The feature camouflage filter utilizes pseudo labels generated through label propagation to train the filter and uses contrastive learning that combines instance-wise and prototype-wise to improve the quality of features. The relation camouflage refiner uses Mixture-of-Experts(MoE) network to disassemble the multi-relations graph into multiple substructures and divide and conquer them to mitigate the degradation of detection performance caused by relation camouflage. Furthermore, we introduce a regularization method for MoE to enhance the robustness of the model. Extensive experiments on two fraud detection benchmark datasets demonstrate that our method outperforms state-of-the-art baselines.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "10",
        "title": "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation",
        "author": [
            "Dongsheng Zhu",
            "Weixian Shi",
            "Zhengliang Shi",
            "Zhaochun Ren",
            "Shuaiqiang Wang",
            "Lingyong Yan",
            "Dawei Yin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12432",
        "abstract": "Although current Large Language Models (LLMs) exhibit impressive capabilities, performing complex real-world tasks still requires tool learning. Mainstream methods, such as CoT/ReAct, rely on step-by-step tool invocation to interact with external environments, but they are limited in perceptual scope and lack adequate task-planning capability. To address these limitations, other studies introduce the first Search-based Decision Tree (DFSDT), which still suffers from the high computational cost. In this paper, we introduce a novel parallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama). First, we transform traditional tree-based tool search paths into Directed Acyclic Graph (DAG) structure, generating a high-quality parallel tool invocation dataset. The DTA-Llama is then trained on the dataset to learn to iteratively divide the current task into several parallel tool invocation sub-tasks and aggregate the invocation results to decide the next actions. Furthermore, we introduce an efficient inference framework inspired by the Process/Threads mechanism when applying the DTA-Llama to practical tasks. Experimental results show that our approach substantially enhances task performance while reducing token consumption and inference time. Llama2-7B, using our method, is comparable to the official parallel function calling method of GPT-3.5. The relevant code, dataset, and model weights are available at https://corn0205.github.io/",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "11",
        "title": "Deploying Privacy Guardrails for LLMs: A Comparative Analysis of Real-World Applications",
        "author": [
            "Shubhi Asthana",
            "Bing Zhang",
            "Ruchi Mahindru",
            "Chad DeLuca",
            "Anna Lisa Gentile",
            "Sandeep Gopisetty"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12456",
        "abstract": "The adoption of Large Language Models (LLMs) has revolutionized AI applications but poses significant challenges in safeguarding user privacy. Ensuring compliance with privacy regulations such as GDPR and CCPA while addressing nuanced privacy risks requires robust and scalable frameworks. This paper presents a detailed study of OneShield Privacy Guard, a framework designed to mitigate privacy risks in user inputs and LLM outputs across enterprise and open-source settings. We analyze two real-world deployments:(1) a multilingual privacy-preserving system integrated with Data and Model Factory, focusing on enterprise-scale data governance; and (2) PR Insights, an open-source repository emphasizing automated triaging and community-driven refinements. In Deployment 1, OneShield achieved a 0.95 F1 score in detecting sensitive entities like dates, names, and phone numbers across 26 languages, outperforming state-of-the-art tool such as StarPII and Presidio by up to 12\\%. Deployment 2, with an average F1 score of 0.86, reduced manual effort by over 300 hours in three months, accurately flagging 8.25\\% of 1,256 pull requests for privacy risks with enhanced context sensitivity. These results demonstrate OneShield's adaptability and efficacy in diverse environments, offering actionable insights for context-aware entity recognition, automated compliance, and ethical AI adoption. This work advances privacy-preserving frameworks, supporting user trust and compliance across operational contexts.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "12",
        "title": "Empowering AIOps: Leveraging Large Language Models for IT Operations ManagementOperations Management",
        "author": [
            "Arthur Vitui",
            "Tse-Hsun Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12461",
        "abstract": "The integration of Artificial Intelligence (AI) into IT Operations Management (ITOM), commonly referred to as AIOps, offers substantial potential for automating workflows, enhancing efficiency, and supporting informed decision-making. However, implementing AI within IT operations is not without its challenges, including issues related to data quality, the complexity of IT environments, and skill gaps within teams. The advent of Large Language Models (LLMs) presents an opportunity to address some of these challenges, particularly through their advanced natural language understanding capabilities. These features enable organizations to process and analyze vast amounts of unstructured data, such as system logs, incident reports, and technical documentation. This ability aligns with the motivation behind our research, where we aim to integrate traditional predictive machine learning models with generative AI technologies like LLMs. By combining these approaches, we propose innovative methods to tackle persistent challenges in AIOps and enhance the capabilities of IT operations management.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "13",
        "title": "Adaptive PII Mitigation Framework for Large Language Models",
        "author": [
            "Shubhi Asthana",
            "Ruchi Mahindru",
            "Bing Zhang",
            "Jorge Sanz"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12465",
        "abstract": "Artificial Intelligence (AI) faces growing challenges from evolving data protection laws and enforcement practices worldwide. Regulations like GDPR and CCPA impose strict compliance requirements on Machine Learning (ML) models, especially concerning personal data use. These laws grant individuals rights such as data correction and deletion, complicating the training and deployment of Large Language Models (LLMs) that rely on extensive datasets. Public data availability does not guarantee its lawful use for ML, amplifying these challenges.\nThis paper introduces an adaptive system for mitigating risk of Personally Identifiable Information (PII) and Sensitive Personal Information (SPI) in LLMs. It dynamically aligns with diverse regulatory frameworks and integrates seamlessly into Governance, Risk, and Compliance (GRC) systems. The system uses advanced NLP techniques, context-aware analysis, and policy-driven masking to ensure regulatory compliance.\nBenchmarks highlight the system's effectiveness, with an F1 score of 0.95 for Passport Numbers, outperforming tools like Microsoft Presidio (0.33) and Amazon Comprehend (0.54). In human evaluations, the system achieved an average user trust score of 4.6/5, with participants acknowledging its accuracy and transparency. Observations demonstrate stricter anonymization under GDPR compared to CCPA, which permits pseudonymization and user opt-outs. These results validate the system as a scalable and robust solution for enterprise privacy compliance.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "14",
        "title": "An Empirical Characterization of Outages and Incidents in Public Services for Large Language Models",
        "author": [
            "Xiaoyu Chu",
            "Sacheendra Talluri",
            "Qingxian Lu",
            "Alexandru Iosup"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12469",
        "abstract": "People and businesses increasingly rely on public LLM services, such as ChatGPT, DALLE, and Claude. Understanding their outages, and particularly measuring their failure-recovery processes, is becoming a stringent problem. However, only limited studies exist in this emerging area. Addressing this problem, in this work we conduct an empirical characterization of outages and failure-recovery in public LLM services. We collect and prepare datasets for 8 commonly used LLM services across 3 major LLM providers, including market-leads OpenAI and Anthropic. We conduct a detailed analysis of failure recovery statistical properties, temporal patterns, co-occurrence, and the impact range of outage-causing incidents. We make over 10 observations, among which: (1) Failures in OpenAI's ChatGPT take longer to resolve but occur less frequently than those in Anthropic's Claude;(2) OpenAI and Anthropic service failures exhibit strong weekly and monthly periodicity; and (3) OpenAI services offer better failure-isolation than Anthropic services. Our research explains LLM failure characteristics and thus enables optimization in building and using LLM systems. FAIR data and code are publicly available on https://zenodo.org/records/14018219 and https://github.com/atlarge-research/llm-service-analysis.",
        "tags": [
            "ChatGPT",
            "Large Language Models"
        ]
    },
    {
        "id": "15",
        "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
        "author": [
            "Tian Jin",
            "Ahmed Imtiaz Humayun",
            "Utku Evci",
            "Suvinay Subramanian",
            "Amir Yazdanbakhsh",
            "Dan Alistarh",
            "Gintare Karolina Dziugaite"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12486",
        "abstract": "Pruning eliminates unnecessary parameters in neural networks; it offers a promising solution to the growing computational demands of large language models (LLMs). While many focus on post-training pruning, sparse pre-training--which combines pruning and pre-training into a single phase--provides a simpler alternative. In this work, we present the first systematic exploration of optimal sparse pre-training configurations for LLMs through an examination of 80 unique pruning schedules across different sparsity levels and training durations. We find that initiating pruning at 25% of total training compute and concluding at 75% achieves near-optimal final evaluation loss. These findings provide valuable insights for efficient and effective sparse pre-training of LLMs. Furthermore, we propose a new scaling law that modifies the Chinchilla scaling law to use the average parameter count over pre-training. Through empirical and theoretical validation, we demonstrate that this modified scaling law accurately models evaluation loss for both sparsely and densely pre-trained LLMs, unifying scaling laws across pre-training paradigms. Our findings indicate that while sparse pre-training achieves the same final model quality as dense pre-training for equivalent compute budgets, it provides substantial benefits through reduced model size, enabling significant potential computational savings during inference.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "16",
        "title": "A Basis for Human Responsibility in Artificial Intelligence Computation",
        "author": [
            "Vincenzo Calderonio"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12498",
        "abstract": "Recent advancements in artificial intelligence have reopened the question about the boundaries of AI autonomy, particularly in discussions around artificial general intelligence (AGI) and its potential to act independently across varied purposes. This paper explores these boundaries through the analysis of the Alignment Research Center experiment on GPT-4 and introduces the Start Button Problem, a thought experiment that examines the origins and limits of AI autonomy. By examining the thought experiment and its counterarguments will be enlightened how in the need for human activation and purpose definition lies the AI's inherent dependency on human-initiated actions, challenging the assumption of AI as an agent. Finally, the paper addresses the implications of this dependency on human responsibility, questioning the measure of the extension of human responsibility when using AI systems.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "17",
        "title": "An Empirically-grounded tool for Automatic Prompt Linting and Repair: A Case Study on Bias, Vulnerability, and Optimization in Developer Prompts",
        "author": [
            "Dhia Elhaq Rzig",
            "Dhruba Jyoti Paul",
            "Kaiser Pister",
            "Jordan Henkel",
            "Foyzul Hassan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12521",
        "abstract": "The tidal wave of advancements in Large Language Models (LLMs) has led to their swift integration into application-level logic. Many software systems now use prompts to interact with these black-box models, combining natural language with dynamic values interpolated at runtime, to perform tasks ranging from sentiment analysis to question answering. Due to the programmatic and structured natural language aspects of these prompts, we refer to them as Developer Prompts. Unlike traditional software artifacts, Dev Prompts blend natural language instructions with artificial languages such as programming and markup languages, thus requiring specialized tools for analysis, distinct from classical software evaluation methods.\nIn response to this need, we introduce PromptDoctor, a tool explicitly designed to detect and correct issues of Dev Prompts. PromptDoctor identifies and addresses problems related to bias, vulnerability, and sub-optimal performance in Dev Prompts, helping mitigate their possible harms. In our analysis of 2,173 Dev Prompts, selected as a representative sample of 40,573 Dev Prompts, we found that 3.46% contained one or more forms of bias, 10.75% were vulnerable to prompt injection attacks. Additionally, 3,310 were amenable to automated prompt optimization. To address these issues, we applied PromptDoctor to the flawed Dev Prompts we discovered. PromptDoctor de-biased 68.29% of the biased Dev Prompts, hardened 41.81% of the vulnerable Dev Prompts, and improved the performance of 37.1% sub-optimal Dev Prompts. Finally, we developed a PromptDoctor VSCode extension, enabling developers to easily enhance Dev Prompts in their existing development workflows. The data and source code for this work are available at",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "18",
        "title": "Comparative Approaches to Sentiment Analysis Using Datasets in Major European and Arabic Languages",
        "author": [
            "Mikhail Krasitskii",
            "Olga Kolesnikova",
            "Liliana Chanona Hernandez",
            "Grigori Sidorov",
            "Alexander Gelbukh"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12540",
        "abstract": "This study explores transformer-based models such as BERT, mBERT, and XLM-R for multi-lingual sentiment analysis across diverse linguistic structures. Key contributions include the identification of XLM-R superior adaptability in morphologically complex languages, achieving accuracy levels above 88%. The work highlights fine-tuning strategies and emphasizes their significance for improving sentiment classification in underrepresented languages.",
        "tags": [
            "BERT",
            "Transformer"
        ]
    },
    {
        "id": "19",
        "title": "Human-like conceptual representations emerge from language prediction",
        "author": [
            "Ningyu Xu",
            "Qi Zhang",
            "Chao Du",
            "Qiang Luo",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Menghan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12547",
        "abstract": "Recent advances in large language models (LLMs) provide a new opportunity to address the long-standing question of how concepts are represented and organized in the mind, which is central to unravelling the nature of human cognition. Here, we reframed the classic reverse dictionary task to simulate human concept inference in context and investigated the emergence of human-like conceptual representations within LLMs. We found that LLMs were able to infer concepts from definitional descriptions and construct representation spaces that converge towards a shared, context-independent structure. These representations effectively predicted human behavioural judgments and aligned well with neural activity patterns in the human brain, offering evidence for biological plausibility. These findings demonstrate that human-like conceptual representations and organization can naturally emerge from language prediction, even without real-world grounding. Our work supports the view that LLMs serve as valuable tools for understanding complex human cognition and paves the way for better alignment between artificial and human intelligence.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "20",
        "title": "Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review",
        "author": [
            "Rock Yuren Pang",
            "Hope Schroeder",
            "Kynnedy Simone Smith",
            "Solon Barocas",
            "Ziang Xiao",
            "Emily Tseng",
            "Danielle Bragg"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12557",
        "abstract": "Large language models (LLMs) have been positioned to revolutionize HCI, by reshaping not only the interfaces, design patterns, and sociotechnical systems that we study, but also the research practices we use. To-date, however, there has been little understanding of LLMs' uptake in HCI. We address this gap via a systematic literature review of 153 CHI papers from 2020-24 that engage with LLMs. We taxonomize: (1) domains where LLMs are applied; (2) roles of LLMs in HCI projects; (3) contribution types; and (4) acknowledged limitations and risks. We find LLM work in 10 diverse domains, primarily via empirical and artifact contributions. Authors use LLMs in five distinct roles, including as research tools or simulated users. Still, authors often raise validity and reproducibility concerns, and overwhelmingly study closed models. We outline opportunities to improve HCI research with and on LLMs, and provide guiding questions for researchers to consider the validity and appropriateness of LLM-related work.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "21",
        "title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
        "author": [
            "Haotian Luo",
            "Li Shen",
            "Haiying He",
            "Yibo Wang",
            "Shiwei Liu",
            "Wei Li",
            "Naiqiang Tan",
            "Xiaochun Cao",
            "Dacheng Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12570",
        "abstract": "Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning process leads to a substantial increase in inference time. A pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLM's baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing a novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner",
        "tags": [
            "LLMs",
            "RL"
        ]
    },
    {
        "id": "22",
        "title": "D-LoRa: a Distributed Parameter Adaptation Scheme for LoRa Network",
        "author": [
            "Ruiqi Wang",
            "Tongyu Song",
            "Jing Ren",
            "Xiong Wang",
            "Shizhong Xu",
            "Sheng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12589",
        "abstract": "The deployment of LoRa networks necessitates joint performance optimization, including packet delivery rate, energy efficiency, and throughput. Additionally, multiple LoRa parameters for packet transmission must be dynamically configured to tailor the performance metrics prioritization across varying channel environments. Because of the coupling relationship between LoRa parameters and metrics, existing works have opted to focus on certain parameters or specific metrics to circumvent the intricate coupling relationship, leading to limited adaptability. Therefore, we propose D-LoRa, a distributed parameter adaptation scheme, based on reinforcement learning towards network performance. We decompose the joint performance optimization problem into multiple independent Multi-Armed Bandit (MAB) problems with different reward functions. We have also built a comprehensive analytical model for the LoRa network that considers path loss, quasi-orthogonality of spreading factor, and packet collision. Experimental results show that our scheme can increase packet delivery rate by up to 28.8% and demonstrates superior adaptability across different performance metrics.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "23",
        "title": "Adapting OpenAI's CLIP Model for Few-Shot Image Inspection in Manufacturing Quality Control: An Expository Case Study with Multiple Application Examples",
        "author": [
            "Fadel M. Megahed",
            "Ying-Ju Chen",
            "Bianca Maria Colosimo",
            "Marco Luigi Giuseppe Grasso",
            "L. Allison Jones-Farmer",
            "Sven Knoth",
            "Hongyue Sun",
            "Inez Zwetsloot"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12596",
        "abstract": "This expository paper introduces a simplified approach to image-based quality inspection in manufacturing using OpenAI's CLIP (Contrastive Language-Image Pretraining) model adapted for few-shot learning. While CLIP has demonstrated impressive capabilities in general computer vision tasks, its direct application to manufacturing inspection presents challenges due to the domain gap between its training data and industrial applications. We evaluate CLIP's effectiveness through five case studies: metallic pan surface inspection, 3D printing extrusion profile analysis, stochastic textured surface evaluation, automotive assembly inspection, and microstructure image classification. Our results show that CLIP can achieve high classification accuracy with relatively small learning sets (50-100 examples per class) for single-component and texture-based applications. However, the performance degrades with complex multi-component scenes. We provide a practical implementation framework that enables quality engineers to quickly assess CLIP's suitability for their specific applications before pursuing more complex solutions. This work establishes CLIP-based few-shot learning as an effective baseline approach that balances implementation simplicity with robust performance, demonstrated in several manufacturing quality control applications.",
        "tags": [
            "3D",
            "CLIP"
        ]
    },
    {
        "id": "24",
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
        "author": [
            "Kimi Team",
            "Angang Du",
            "Bofei Gao",
            "Bowei Xing",
            "Changjiu Jiang",
            "Cheng Chen",
            "Cheng Li",
            "Chenjun Xiao",
            "Chenzhuang Du",
            "Chonghua Liao",
            "Chuning Tang",
            "Congcong Wang",
            "Dehao Zhang",
            "Enming Yuan",
            "Enzhe Lu",
            "Fengxiang Tang",
            "Flood Sung",
            "Guangda Wei",
            "Guokun Lai",
            "Haiqing Guo",
            "Han Zhu",
            "Hao Ding",
            "Hao Hu",
            "Hao Yang",
            "Hao Zhang",
            "Haotian Yao",
            "Haotian Zhao",
            "Haoyu Lu",
            "Haoze Li",
            "Haozhen Yu",
            "Hongcheng Gao",
            "Huabin Zheng",
            "Huan Yuan",
            "Jia Chen",
            "Jianhang Guo",
            "Jianlin Su",
            "Jianzhou Wang",
            "Jie Zhao",
            "Jin Zhang",
            "Jingyuan Liu",
            "Junjie Yan",
            "Junyan Wu",
            "Lidong Shi",
            "Ling Ye",
            "Longhui Yu",
            "Mengnan Dong",
            "Neo Zhang",
            "Ningchen Ma",
            "Qiwei Pan",
            "Qucheng Gong",
            "Shaowei Liu",
            "Shengling Ma",
            "Shupeng Wei",
            "Sihan Cao",
            "Siying Huang",
            "Tao Jiang",
            "Weihao Gao",
            "Weimin Xiong",
            "Weiran He",
            "Weixiao Huang",
            "Wenhao Wu",
            "Wenyang He",
            "Xianghui Wei",
            "Xianqing Jia",
            "Xingzhe Wu",
            "Xinran Xu",
            "Xinxing Zu",
            "Xinyu Zhou",
            "Xuehai Pan",
            "Y. Charles",
            "Yang Li",
            "Yangyang Hu",
            "Yangyang Liu",
            "Yanru Chen",
            "Yejie Wang",
            "Yibo Liu",
            "Yidao Qin",
            "Yifeng Liu",
            "Ying Yang",
            "Yiping Bao",
            "Yulun Du",
            "Yuxin Wu",
            "Yuzhi Wang",
            "Zaida Zhou",
            "Zhaoji Wang",
            "Zhaowei Li",
            "Zhen Zhu",
            "Zheng Zhang",
            "Zhexu Wang",
            "Zhilin Yang",
            "Zhiqi Huang",
            "Zihao Huang",
            "Ziyao Xu",
            "Zonghan Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12599",
        "abstract": "Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models",
            "RL"
        ]
    },
    {
        "id": "25",
        "title": "T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation",
        "author": [
            "Lijun Li",
            "Zhelun Shi",
            "Xuhao Hu",
            "Bowen Dong",
            "Yiran Qin",
            "Xihui Liu",
            "Lu Sheng",
            "Jing Shao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12612",
        "abstract": "Text-to-image (T2I) models have rapidly advanced, enabling the generation of high-quality images from text prompts across various domains. However, these models present notable safety concerns, including the risk of generating harmful, biased, or private content. Current research on assessing T2I safety remains in its early stages. While some efforts have been made to evaluate models on specific safety dimensions, many critical risks remain unexplored. To address this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I models across three key domains: toxicity, fairness, and bias. We build a detailed hierarchy of 12 tasks and 44 categories based on these three domains, and meticulously collect 70K corresponding prompts. Based on this taxonomy and prompt set, we build a large-scale T2I dataset with 68K manually annotated images and train an evaluator capable of detecting critical risks that previous work has failed to identify, including risks that even ultra-large proprietary models like GPTs cannot correctly detect. We evaluate 12 prominent diffusion models on T2ISafety and reveal several concerns including persistent issues with racial fairness, a tendency to generate toxic content, and significant variation in privacy protection across the models, even with defense methods like concept erasing. Data and evaluator are released under https://github.com/adwardlee/t2i_safety.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "26",
        "title": "Deep Learning-Based Identification of Inconsistent Method Names: How Far Are We?",
        "author": [
            "Taiming Wang",
            "Yuxia Zhang",
            "Lin Jiang",
            "Yi Tang",
            "Guangjie Li",
            "Hui Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12617",
        "abstract": "Concise and meaningful method names are crucial for program comprehension and maintenance. However, method names may become inconsistent with their corresponding implementations, causing confusion and errors. Several deep learning (DL)-based approaches have been proposed to identify such inconsistencies, with initial evaluations showing promising results. However, these evaluations typically use a balanced dataset, where the number of inconsistent and consistent names are equal. This setup, along with flawed dataset construction, leads to false positives, making reported performance less reliable in real-world scenarios, where most method names are consistent. In this paper, we present an empirical study that evaluates state-of-the-art DL-based methods for identifying inconsistent method names. We create a new benchmark by combining automatic identification from commit histories and manual developer inspections, reducing false positives. We evaluate five representative DL approaches (one retrieval-based and four generation-based) on this benchmark. Our results show that performance drops substantially when moving from the balanced dataset to the new benchmark. We further conduct quantitative and qualitative analyses to understand the strengths and weaknesses of the approaches. Retrieval-based methods perform well on simple methods and those with popular name sub-tokens but fail due to inefficient representation techniques. Generation-based methods struggle with inaccurate similarity calculations and immature name generation. Based on these findings, we propose improvements using contrastive learning and large language models (LLMs). Our study suggests that significant improvements are needed before these DL approaches can be effectively applied to real-world software systems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "27",
        "title": "Distillation Quantification for Large Language Models",
        "author": [
            "Sunbowen Lee",
            "Junting Zhou",
            "Chang Ao",
            "Kaige Li",
            "Xinrun Du",
            "Sirui He",
            "Jiaheng Liu",
            "Min Yang",
            "Zhoufutu Wen",
            "Shiwen Ni"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12619",
        "abstract": "Model distillation is a technique for transferring knowledge from large language models (LLMs) to smaller ones, aiming to create resource-efficient yet high-performing models. However, excessive distillation can lead to homogenization, reducing diversity among models and impairing their ability to robustly handle complex or novel tasks. These limitations underscore the need to systematically quantify the distillation process and its impact. In this work, we propose a framework to evaluate and quantify model distillation. Our method addresses two key aspects: (1) Identifying identity cognition contradictions to assess discrepancies in how models perceive and represent identity-related information, and (2) Analyzing multi-granularity response similarities across models to measure the extent of homogenization. Experimental results demonstrate two key insights: (1) Well-known closed-source and open-source LLMs usually exhibit high distillation degrees, except for Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees compared to aligned LLMs. By offering a systematic approach to improve the transparency of LLM data distillation, we call for LLMs with more independent development and more transparent technical reports to improve LLMs' robustness and safety. The code and data are available under https://github.com/Aegis1863/LLMs-Distillation-Quantification.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "28",
        "title": "Towards Robust Multi-tab Website Fingerprinting",
        "author": [
            "Xinhao Deng",
            "Xiyuan Zhao",
            "Qilei Yin",
            "Zhuotao Liu",
            "Qi Li",
            "Mingwei Xu",
            "Ke Xu",
            "Jianping Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12622",
        "abstract": "Website fingerprinting enables an eavesdropper to determine which websites a user is visiting over an encrypted connection. State-of-the-art website fingerprinting (WF) attacks have demonstrated effectiveness even against Tor-protected network traffic. However, existing WF attacks have critical limitations on accurately identifying websites in multi-tab browsing sessions, where the holistic pattern of individual websites is no longer preserved, and the number of tabs opened by a client is unknown a priori. In this paper, we propose ARES, a novel WF framework natively designed for multi-tab WF attacks. ARES formulates the multi-tab attack as a multi-label classification problem and solves it using the novel Transformer-based models. Specifically, ARES extracts local patterns based on multi-level traffic aggregation features and utilizes the improved self-attention mechanism to analyze the correlations between these local patterns, effectively identifying websites. We implement a prototype of ARES and extensively evaluate its effectiveness using our large-scale datasets collected over multiple months. The experimental results illustrate that ARES achieves optimal performance in several realistic scenarios. Further, ARES remains robust even against various WF defenses.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "29",
        "title": "TeD-Loc: Text Distillation for Weakly Supervised Object Localization",
        "author": [
            "Shakeeb Murtaza",
            "Soufiane Belharbi",
            "Marco Pedersoli",
            "Eric Granger"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12632",
        "abstract": "Weakly supervised object localization (WSOL) using classification models trained with only image-class labels remains an important challenge in computer vision. Given their reliance on classification objectives, traditional WSOL methods like class activation mapping focus on the most discriminative object parts, often missing the full spatial extent. In contrast, recent WSOL methods based on vision-language models like CLIP require ground truth classes or external classifiers to produce a localization map, limiting their deployment in downstream tasks. Moreover, methods like GenPromp attempt to address these issues but introduce considerable complexity due to their reliance on conditional denoising processes and intricate prompt learning. This paper introduces Text Distillation for Localization (TeD-Loc), an approach that directly distills knowledge from CLIP text embeddings into the model backbone and produces patch-level localization. Multiple instance learning of these image patches allows for accurate localization and classification using one model without requiring external classifiers. Such integration of textual and visual modalities addresses the longstanding challenge of achieving accurate localization and classification concurrently, as WSOL methods in the literature typically converge at different epochs. Extensive experiments show that leveraging text embeddings and localization cues provides a cost-effective WSOL model. TeD-Loc improves Top-1 LOC accuracy over state-of-the-art models by about 5% on both CUB and ILSVRC datasets, while significantly reducing computational complexity compared to GenPromp.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "30",
        "title": "DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet Transform",
        "author": [
            "Hung Nguyen",
            "Blark Runfa Li",
            "Truong Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12637",
        "abstract": "Neural Radiance Fields (NeRF) has achieved superior performance in novel view synthesis and 3D scene representation, but its practical applications are hindered by slow convergence and reliance on dense training views. To this end, we present DWTNeRF, a unified framework based on Instant-NGP's fast-training hash encoding. It is coupled with regularization terms designed for few-shot NeRF, which operates on sparse training views. Our DWTNeRF includes a novel Discrete Wavelet loss that allows explicit prioritization of low frequencies directly in the training objective, reducing few-shot NeRF's overfitting on high frequencies in earlier training stages. We additionally introduce a model-based approach, based on multi-head attention, that is compatible with INGP-based models, which are sensitive to architectural changes. On the 3-shot LLFF benchmark, DWTNeRF outperforms Vanilla NeRF by 15.07% in PSNR, 24.45% in SSIM and 36.30% in LPIPS. Our approach encourages a re-thinking of current few-shot approaches for INGP-based models.",
        "tags": [
            "3D",
            "NeRF"
        ]
    },
    {
        "id": "31",
        "title": "Structure-preserving parametric finite element methods for anisotropic surface diffusion flow with minimal deformation formulation",
        "author": [
            "Yihang Guo",
            "Meng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12638",
        "abstract": "High mesh quality plays a crucial role in maintaining the stability of solutions in geometric flow problems. Duan and Li [Duan & Li, SIAM J. Sci. Comput. 46 (1) (2024) A587-A608] applied the minimal deformation (MD) formulation to propose an artificial tangential velocity determined by harmonic mapping to improve mesh quality. In this work, we extend the method to anisotropic surface diffusion flows, which, similar to isotropic curvature flow, also preserves excellent mesh quality. Furthermore, developing a numerical algorithm for the flow with MD formulation that guarantees volume conservation and energy stability remains a challenging task. We, in this paper, successfully construct several structure-preserving algorithms, including first-order and high-order temporal discretization methods. Extensive numerical experiments show that our methods effectively preserve mesh quality for anisotropic SDFs, ensuring high-order temporal accuracy, volume conservation or/and energy stability.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "32",
        "title": "Training Data Attribution (TDA): Examining Its Adoption & Use Cases",
        "author": [
            "Deric Cheng",
            "Juhan Bae",
            "Justin Bullock",
            "David Kristofferson"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12642",
        "abstract": "This report investigates Training Data Attribution (TDA) and its potential importance to and tractability for reducing extreme risks from AI. First, we discuss the plausibility and amount of effort it would take to bring existing TDA research efforts from their current state, to an efficient and accurate tool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss the numerous research benefits AI labs will expect to see from using such TDA tooling. Then, we discuss a key outstanding bottleneck that would limit such TDA tooling from being accessible publicly: AI labs' willingness to disclose their training data. We suggest ways AI labs may work around these limitations, and discuss the willingness of governments to mandate such access. Assuming that AI labs willingly provide access to TDA inference, we then discuss what high-level societal benefits you might see. We list and discuss a series of policies and systems that may be enabled by TDA. Finally, we present an evaluation of TDA's potential impact on mitigating large-scale risks from AI systems.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "33",
        "title": "Can masking background and object reduce static bias for zero-shot action recognition?",
        "author": [
            "Takumi Fukuzawa",
            "Kensho Hara",
            "Hirokatsu Kataoka",
            "Toru Tamaki"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12681",
        "abstract": "In this paper, we address the issue of static bias in zero-shot action recognition. Action recognition models need to represent the action itself, not the appearance. However, some fully-supervised works show that models often rely on static appearances, such as the background and objects, rather than human actions. This issue, known as static bias, has not been investigated for zero-shot. Although CLIP-based zero-shot models are now common, it remains unclear if they sufficiently focus on human actions, as CLIP primarily captures appearance features related to languages. In this paper, we investigate the influence of static bias in zero-shot action recognition with CLIP-based models. Our approach involves masking backgrounds, objects, and people differently during training and validation. Experiments with masking background show that models depend on background bias as their performance decreases for Kinetics400. However, for Mimetics, which has a weak background bias, masking the background leads to improved performance even if the background is masked during validation. Furthermore, masking both the background and objects in different colors improves performance for SSv2, which has a strong object bias. These results suggest that masking the background or objects during training prevents models from overly depending on static bias and makes them focus more on human action.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "34",
        "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
        "author": [
            "Yifan Yu",
            "Yu Gan",
            "Lily Tasi",
            "Nikhil Sarda",
            "Jiaming Shen",
            "Yanqi Zhou",
            "Arvind Krishnamurthy",
            "Fan Lai",
            "Henry M. Levy",
            "David Culler"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12689",
        "abstract": "Large language models (LLMs) have excelled in various applications, yet serving them at scale is challenging due to their substantial resource demands and high latency. Our real-world studies reveal that over 60% of user requests to LLMs have semantically similar counterparts, suggesting the potential for knowledge sharing among requests. However, naively caching and reusing past responses leads to large quality degradation. In this paper, we introduce EchoLM, an in-context caching system that leverages historical requests as examples to guide response generation, enabling selective offloading of requests to more efficient LLMs. However, enabling this real-time knowledge transfer leads to intricate tradeoffs between response quality, latency, and system throughput at scale. For a new request, EchoLM identifies similar, high-utility examples and efficiently prepends them to the input for better response. At scale, EchoLM adaptively routes requests to LLMs of varying capabilities, accounting for response quality and serving loads. EchoLM employs a cost-aware cache replay mechanism to improve example quality and coverage offline, maximizing cache utility and runtime efficiency. Evaluations on millions of open-source requests demonstrate that EchoLM has a throughput improvement of 1.4-5.9x while reducing latency by 28-71% without hurting response quality on average.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "35",
        "title": "Combining Knowledge Graph and LLMs for Enhanced Zero-shot Visual Question Answering",
        "author": [
            "Qian Tao",
            "Xiaoyang Fan",
            "Yong Xu",
            "Xingquan Zhu",
            "Yufei Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12697",
        "abstract": "Zero-shot visual question answering (ZS-VQA), an emerged critical research area, intends to answer visual questions without providing training samples. Existing research in ZS-VQA has proposed to leverage knowledge graphs or large language models (LLMs), respectively, as external information sources to help VQA model comprehend images and questions. However, LLMs often struggle in accurately interpreting specific question meanings. Meanwhile, although knowledge graph has rich entity relationships, it is challenging to effectively connect entities to individual image content for visual question answers. In this paper, we propose a novel design to combine knowledge graph and LLMs for zero-shot visual question answer. Our approach uses LLMs' powerful understanding capabilities to accurately interpret image content through a strategic question search mechanism. Meanwhile, the knowledge graph is used to expand and connect users' queries to the image content for better visual question answering. An optimization algorithm is further used to determine the optimal weights for the loss functions derived from different information sources, towards a globally optimal set of candidate answers. Experimental results on two benchmark datasets demonstrate that our model achieves state-of-the-art (SOTA) performance. Both source code and benchmark data will be released for public access.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "36",
        "title": "Training Dialogue Systems by AI Feedback for Improving Overall Dialogue Impression",
        "author": [
            "Kai Yoshida",
            "Masahiro Mizukami",
            "Seiya Kawano",
            "Canasai Kruengkrai",
            "Hiroaki Sugiyama",
            "Koichiro Yoshino"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12698",
        "abstract": "To improve user engagement during conversations with dialogue systems, we must improve individual dialogue responses and dialogue impressions such as consistency, personality, and empathy throughout the entire dialogue. While such dialogue systems have been developing rapidly with the help of large language models (LLMs), reinforcement learning from AI feedback (RLAIF) has attracted attention to align LLM-based dialogue models for such dialogue impressions. In RLAIF, a reward model based on another LLM is used to create a training signal for an LLM-based dialogue model using zero-shot/few-shot prompting techniques. However, evaluating an entire dialogue only by prompting LLMs is challenging. In this study, the supervised fine-tuning (SFT) of LLMs prepared reward models corresponding to 12 metrics related to the impression of the entire dialogue for evaluating dialogue responses. We tuned our dialogue models using the reward model signals as feedback to improve the impression of the system. The results of automatic and human evaluations showed that tuning the dialogue model using our reward model corresponding to dialogue impression improved the evaluation of individual metrics and the naturalness of the dialogue response.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "37",
        "title": "Paradigm-Based Automatic HDL Code Generation Using LLMs",
        "author": [
            "Wenhao Sun",
            "Bing Li",
            "Grace Li Zhang",
            "Xunzhao Yin",
            "Cheng Zhuo",
            "Ulf Schlichtmann"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12702",
        "abstract": "While large language models (LLMs) have demonstrated the ability to generate hardware description language (HDL) code for digital circuits, they still face the hallucination problem, which can result in the generation of incorrect HDL code or misinterpretation of specifications. In this work, we introduce a human-expert-inspired method to mitigate the hallucination of LLMs and enhance their performance in HDL code generation. We begin by constructing specialized paradigm blocks that consist of several steps designed to divide and conquer generation tasks, mirroring the design methodology of human experts. These steps include information extraction, human-like design flows, and the integration of external tools. LLMs are then instructed to classify the type of circuit in order to match it with the appropriate paradigm block and execute the block to generate the HDL codes. Additionally, we propose a two-phase workflow for multi-round generation, aimed at effectively improving the testbench pass rate of the generated HDL codes within a limited number of generation and verification rounds. Experimental results demonstrate that our method significantly enhances the functional correctness of the generated Verilog code",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "38",
        "title": "A Call for Critically Rethinking and Reforming Data Analysis in Empirical Software Engineering",
        "author": [
            "Matteo Esposito",
            "Mikel Robredo",
            "Murali Sridharan",
            "Guilherme Horta Travassos",
            "Rafael PeÃ±aloza",
            "Valentina Lenarduzzi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12728",
        "abstract": "Context: Empirical Software Engineering (ESE) drives innovation in SE through qualitative and quantitative studies. However, concerns about the correct application of empirical methodologies have existed since the 2006 Dagstuhl seminar on SE. Objective: To analyze three decades of SE research, identify mistakes in statistical methods, and evaluate experts' ability to detect and address these issues. Methods: We conducted a literature survey of ~27,000 empirical studies, using LLMs to classify statistical methodologies as adequate or inadequate. Additionally, we selected 30 primary studies and held a workshop with 33 ESE experts to assess their ability to identify and resolve statistical issues. Results: Significant statistical issues were found in the primary studies, and experts showed limited ability to detect and correct these methodological problems, raising concerns about the broader ESE community's proficiency in this area. Conclusions. Despite our study's eventual limitations, its results shed light on recurring issues from promoting information copy-and-paste from past authors' works and the continuous publication of inadequate approaches that promote dubious results and jeopardize the spread of the correct statistical strategies among researchers. Besides, it justifies further investigation into empirical rigor in software engineering to expose these recurring issues and establish a framework for reassessing our field's foundation of statistical methodology application. Therefore, this work calls for critically rethinking and reforming data analysis in empirical software engineering, paving the way for our work soon.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "39",
        "title": "GRAMA: Adaptive Graph Autoregressive Moving Average Models",
        "author": [
            "Moshe Eliasof",
            "Alessio Gravina",
            "Andrea Ceni",
            "Claudio Gallicchio",
            "Davide Bacciu",
            "Carola-Bibiane SchÃ¶nlieb"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12732",
        "abstract": "Graph State Space Models (SSMs) have recently been introduced to enhance Graph Neural Networks (GNNs) in modeling long-range interactions. Despite their success, existing methods either compromise on permutation equivariance or limit their focus to pairwise interactions rather than sequences. Building on the connection between Autoregressive Moving Average (ARMA) and SSM, in this paper, we introduce GRAMA, a Graph Adaptive method based on a learnable Autoregressive Moving Average (ARMA) framework that addresses these limitations. By transforming from static to sequential graph data, GRAMA leverages the strengths of the ARMA framework, while preserving permutation equivariance. Moreover, GRAMA incorporates a selective attention mechanism for dynamic learning of ARMA coefficients, enabling efficient and flexible long-range information propagation. We also establish theoretical connections between GRAMA and Selective SSMs, providing insights into its ability to capture long-range dependencies. Extensive experiments on 14 synthetic and real-world datasets demonstrate that GRAMA consistently outperforms backbone models and performs competitively with state-of-the-art methods.",
        "tags": [
            "SSMs",
            "State Space Models"
        ]
    },
    {
        "id": "40",
        "title": "Online Preference Alignment for Language Models via Count-based Exploration",
        "author": [
            "Chenjia Bai",
            "Yang Zhang",
            "Shuang Qiu",
            "Qiaosheng Zhang",
            "Kang Xu",
            "Xuelong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12735",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage, and the resulting reward model is hard to generalize in out-of-distribution responses. Thus, online RLHF is more desirable to empower the LLM to explore outside the support of the initial dataset by iteratively collecting the prompt-response pairs. In this paper, we study the fundamental problem in online RLHF, i.e. \\emph{how to explore} for LLM. We give a theoretical motivation in linear reward assumption to show that an optimistic reward with an upper confidence bound (UCB) term leads to a provably efficient RLHF policy. Then, we reformulate our objective to direct preference optimization with an exploration term, where the UCB-term can be converted to a count-based exploration bonus. We further propose a practical algorithm, named \\emph{Count-based Online Preference Optimization (COPO)}, which leverages a simple coin-flip counting module to estimate the pseudo-count of a prompt-response pair in previously collected data. COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The results on instruction-following and standard academic benchmarks show that COPO significantly increases performance.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "41",
        "title": "Grid-based Submap Joining: An Efficient Algorithm for Simultaneously Optimizing Global Occupancy Map and Local Submap Frames",
        "author": [
            "Yingyu Wang",
            "Liang Zhao",
            "Shoudong Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12764",
        "abstract": "Optimizing robot poses and the map simultaneously has been shown to provide more accurate SLAM results. However, for non-feature based SLAM approaches, directly optimizing all the robot poses and the whole map will greatly increase the computational cost, making SLAM problems difficult to solve in large-scale environments. To solve the 2D non-feature based SLAM problem in large-scale environments more accurately and efficiently, we propose the grid-based submap joining method. Specifically, we first formulate the 2D grid-based submap joining problem as a non-linear least squares (NLLS) form to optimize the global occupancy map and local submap frames simultaneously. We then prove that in solving the NLLS problem using Gauss-Newton (GN) method, the increments of the poses in each iteration are independent of the occupancy values of the global occupancy map. Based on this property, we propose a poseonly GN algorithm equivalent to full GN method to solve the NLLS problem. The proposed submap joining algorithm is very efficient due to the independent property and the pose-only solution. Evaluations using simulations and publicly available practical 2D laser datasets confirm the outperformance of our proposed method compared to the state-of-the-art methods in terms of efficiency and accuracy, as well as the ability to solve the grid-based SLAM problem in very large-scale environments.",
        "tags": [
            "Robot",
            "SLAM"
        ]
    },
    {
        "id": "42",
        "title": "NExtLong: Toward Effective Long-Context Training without Long Documents",
        "author": [
            "Chaochen Gao",
            "Xing Wu",
            "Zijia Lin",
            "Debing Zhang",
            "Songlin Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12766",
        "abstract": "Large language models (LLMs) with extended context windows have made significant strides yet remain a challenge due to the scarcity of long documents. Existing methods tend to synthesize long-context data but lack a clear mechanism to reinforce the long-range dependency modeling. To address this limitation, we propose NExtLong, a novel framework for synthesizing long-context data through Negative document Extension. NExtLong decomposes a document into multiple meta-chunks and extends the context by interleaving hard negative distractors retrieved from pretraining corpora. This approach compels the model to discriminate long-range dependent context from distracting content, enhancing its ability to model long-range dependencies. Extensive experiments demonstrate that NExtLong achieves significant performance improvements on the HELMET and RULER benchmarks compared to existing long-context synthesis approaches and leading models, which are trained on non-synthetic long documents. These findings highlight NExtLong's ability to reduce reliance on non-synthetic long documents, making it an effective framework for developing advanced long-context LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "43",
        "title": "LLMs as Repositories of Factual Knowledge: Limitations and Solutions",
        "author": [
            "Seyed Mahed Mousavi",
            "Simone Alghisi",
            "Giuseppe Riccardi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12774",
        "abstract": "LLMs' sources of knowledge are data snapshots containing factual information about entities collected at different timestamps and from different media types (e.g. wikis, social media, etc.). Such unstructured knowledge is subject to change due to updates through time from past to present. Equally important are the inconsistencies and inaccuracies occurring in different information sources. Consequently, the model's knowledge about an entity may be perturbed while training over the sequence of snapshots or at inference time, resulting in inconsistent and inaccurate model performance. In this work, we study the appropriateness of Large Language Models (LLMs) as repositories of factual knowledge. We consider twenty-four state-of-the-art LLMs that are either closed-, partially (weights), or fully (weight and training data) open-source. We evaluate their reliability in responding to time-sensitive factual questions in terms of accuracy and consistency when prompts are perturbed. We further evaluate the effectiveness of state-of-the-art methods to improve LLMs' accuracy and consistency. We then propose \"ENtity-Aware Fine-tuning\" (ENAF), a soft neurosymbolic approach aimed at providing a structured representation of entities during fine-tuning to improve the model's performance.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "44",
        "title": "Regularization, Semi-supervision, and Supervision for a Plausible Attention-Based Explanation",
        "author": [
            "Duc Hau Nguyen",
            "Cyrielle Mallart",
            "Guillaume Gravier",
            "Pascale SÃ©billot"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12775",
        "abstract": "Attention mechanism is contributing to the majority of recent advances in machine learning for natural language processing. Additionally, it results in an attention map that shows the proportional influence of each input in its decision. Empirical studies postulate that attention maps can be provided as an explanation for model output. However, it is still questionable to ask whether this explanation helps regular people to understand and accept the model output (the plausibility of the explanation). Recent studies show that attention weights in the RNN encoders are hardly plausible because they spread on input tokens. We thus propose 3 additional constraints to the learning objective function to improve the plausibility of the attention map: regularization to increase the attention weight sparsity, semi-supervision to supervise the map by a heuristic and supervision by human annotation. Results show that all techniques can improve the attention map plausibility at some level. We also observe that specific instructions for human annotation might have a negative effect on classification performance. Beyond the attention map, the result of experiments on text classification tasks also shows that no matter how the constraint brings the gain, the contextualization layer plays a crucial role in finding the right space for finding plausible tokens.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "45",
        "title": "Revisit Self-Debugging with Self-Generated Tests for Code Generation",
        "author": [
            "Xiancai Chen",
            "Zhengwei Tao",
            "Kechi Zhang",
            "Changzhi Zhou",
            "Wanli Gu",
            "Yuanpeng He",
            "Mengdi Zhang",
            "Xunliang Cai",
            "Haiyan Zhao",
            "Zhi Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12793",
        "abstract": "Large language models (LLMs) have shown significant advancements in code generation, but still face challenges on tasks beyond their basic capabilities. Recently, the notion of self-debugging has been proposed to boost the performance of code generation by leveraging execution feedback from tests. Despite its promise, the availability of high-quality tests in real-world scenarios is limited. In this context, self-debugging with self-generated tests is a promising solution but lacks a full exploration of its limitations and practical potential. Therefore, we investigate its efficacy on diverse programming problems. To deepen our understanding, we propose two distinct paradigms for the process: post-execution and in-execution self-debugging. Within the scope of self-contained Python programming tasks, we find that post-execution self-debugging struggles on basic problems but shows potential for improvement on competitive ones, due to the bias introduced by self-generated tests. On the other hand, in-execution self-debugging enables LLMs to mitigate the bias by solely leveraging intermediate states during execution, thereby enhancing code generation.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "46",
        "title": "Certified Guidance for Planning with Deep Generative Models",
        "author": [
            "Francesco Giacomarra",
            "Mehran Hosseini",
            "Nicola Paoletti",
            "Francesca Cairoli"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12815",
        "abstract": "Deep generative models, such as generative adversarial networks and diffusion models, have recently emerged as powerful tools for planning tasks and behavior synthesis in autonomous systems. Various guidance strategies have been introduced to steer the generative process toward outputs that are more likely to satisfy the planning objectives. These strategies avoid the need for model retraining but do not provide any guarantee that the generated outputs will satisfy the desired planning objectives. To address this limitation, we introduce certified guidance, an approach that modifies a generative model, without retraining it, into a new model guaranteed to satisfy a given specification with probability one. We focus on Signal Temporal Logic specifications, which are rich enough to describe nontrivial planning tasks. Our approach leverages neural network verification techniques to systematically explore the latent spaces of the generative models, identifying latent regions that are certifiably correct with respect to the STL property of interest. We evaluate the effectiveness of our method on four planning benchmarks using GANs and diffusion models. Our results confirm that certified guidance produces generative models that are always correct, unlike existing guidance methods that are not certified.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "47",
        "title": "Nonlinear reduction strategies for data compression: a comprehensive comparison from diffusion to advection problems",
        "author": [
            "Isabella Carla Gonnella",
            "Federico Pichi",
            "Gianluigi Rozza"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12816",
        "abstract": "This work presents an overview of several nonlinear reduction strategies for data compression from various research fields, and a comparison of their performance when applied to problems characterized by diffusion and/or advection terms. We aim to create a common framework by unifying the notation referring to a common two-stage pipeline. At the same time, we underline their main differences and objectives by highlighting the diverse choices made for each stage. We test the considered approaches on three test cases belonging to the family of Advection-Diffusion problems, also focusing on the pure Advection and pure Diffusion benchmarks, studying their reducibility while varying the latent dimension. Finally, we interpret the numerical results under the lens of the discussed theoretical considerations, offering a comprehensive landscape for nonlinear reduction methods for general Advection-Diffusion dynamics.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "48",
        "title": "Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek",
        "author": [
            "John Pavlopoulos",
            "Juli Bakagianni",
            "Kanella Pouli",
            "Maria Gavriilidou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12826",
        "abstract": "Natural Language Processing (NLP) for lesser-resourced languages faces persistent challenges, including limited datasets, inherited biases from high-resource languages, and the need for domain-specific solutions. This study addresses these gaps for Modern Greek through three key contributions. First, we evaluate the performance of open-source (Llama-70b) and closed-source (GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset availability, revealing task-specific strengths, weaknesses, and parity in their performance. Second, we expand the scope of Greek NLP by reframing Authorship Attribution as a tool to assess potential data usage by LLMs in pre-training, with high 0-shot accuracy suggesting ethical implications for data provenance. Third, we showcase a legal NLP case study, where a Summarize, Translate, and Embed (STE) methodology outperforms the traditional TF-IDF approach for clustering \\emph{long} legal texts. Together, these contributions provide a roadmap to advance NLP in lesser-resourced languages, bridging gaps in model evaluation, task innovation, and real-world impact.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "49",
        "title": "A transformer-based deep q learning approach for dynamic load balancing in software-defined networks",
        "author": [
            "Evans Tetteh Owusu",
            "Kwame Agyemang-Prempeh Agyekum",
            "Marinah Benneh",
            "Pius Ayorna",
            "Justice Owusu Agyemang",
            "George Nii Martey Colley",
            "James Dzisi Gazde"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12829",
        "abstract": "This study proposes a novel approach for dynamic load balancing in Software-Defined Networks (SDNs) using a Transformer-based Deep Q-Network (DQN). Traditional load balancing mechanisms, such as Round Robin (RR) and Weighted Round Robin (WRR), are static and often struggle to adapt to fluctuating traffic conditions, leading to inefficiencies in network performance. In contrast, SDNs offer centralized control and flexibility, providing an ideal platform for implementing machine learning-driven optimization strategies. The core of this research combines a Temporal Fusion Transformer (TFT) for accurate traffic prediction with a DQN model to perform real-time dynamic load balancing. The TFT model predicts future traffic loads, which the DQN uses as input, allowing it to make intelligent routing decisions that optimize throughput, minimize latency, and reduce packet loss. The proposed model was tested against RR and WRR in simulated environments with varying data rates, and the results demonstrate significant improvements in network performance. For the 500MB data rate, the DQN model achieved an average throughput of 0.275 compared to 0.202 and 0.205 for RR and WRR, respectively. Additionally, the DQN recorded lower average latency and packet loss. In the 1000MB simulation, the DQN model outperformed the traditional methods in throughput, latency, and packet loss, reinforcing its effectiveness in managing network loads dynamically. This research presents an important step towards enhancing network performance through the integration of machine learning models within SDNs, potentially paving the way for more adaptive, intelligent network management systems.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "50",
        "title": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back Home",
        "author": [
            "Viktor Moskvoretskii",
            "Maria Lysyuk",
            "Mikhail Salnikov",
            "Nikolay Ivanov",
            "Sergey Pletenev",
            "Daria Galimzianova",
            "Nikita Krayko",
            "Vasily Konovalov",
            "Irina Nikishina",
            "Alexander Panchenko"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12835",
        "abstract": "Retrieval Augmented Generation (RAG) improves correctness of Question Answering (QA) and addresses hallucinations in Large Language Models (LLMs), yet greatly increase computational costs. Besides, RAG is not always needed as may introduce irrelevant information. Recent adaptive retrieval methods integrate LLMs' intrinsic knowledge with external information appealing to LLM self-knowledge, but they often neglect efficiency evaluations and comparisons with uncertainty estimation techniques. We bridge this gap by conducting a comprehensive analysis of 35 adaptive retrieval methods, including 8 recent approaches and 27 uncertainty estimation techniques, across 6 datasets using 10 metrics for QA performance, self-knowledge, and efficiency. Our findings show that uncertainty estimation techniques often outperform complex pipelines in terms of efficiency and self-knowledge, while maintaining comparable QA performance.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "51",
        "title": "ACEBench: Who Wins the Match Point in Tool Learning?",
        "author": [
            "Chen Chen",
            "Xinlong Hao",
            "Weiwen Liu",
            "Xu Huang",
            "Xingshan Zeng",
            "Shuai Yu",
            "Dexun Li",
            "Shuai Wang",
            "Weinan Gan",
            "Yuefeng Huang",
            "Xinzhi Wang",
            "Defu Lian",
            "Baoqun Yin",
            "Yasheng Wang",
            "Wu Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12851",
        "abstract": "Large language models (LLMs) have demonstrated significant potential in decision-making and reasoning, especially when combined with various tools to effectively solve complex problems. However, existing evaluation systems for assessing LLM function calling capabilities have several limitations: (1) limited evaluation scenarios, lacking assessments in real multi-turn dialogue contexts; (2) narrow evaluation dimensions, lacking detailed assessments for fine-grained function calls; (3) relying on LLMs or real API executions for result evaluation, which introduces significant overhead. To address these issues, we propose a comprehensive evaluation system named ACEBench. This system is meticulously designed to encompass a wide spectrum of function calling scenarios. Moreover, it categorizes these scenarios into three primary types according to the evaluation methodology: Normal, Special, and Agent. Normal evaluates function calls in basic scenarios; Special evaluates function calls in scenarios with vague or incomplete instructions; Agent introduces multi-agent interactions to simulate function calling evaluation in real-world multi-turn interactions. We conducted extensive experiments on ACEBench, analyzing various LLMs in-depth and performing a more granular analysis of error causes across different data types.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "52",
        "title": "CrossDiff: Diffusion Probabilistic Model With Cross-conditional Encoder-Decoder for Crack Segmentation",
        "author": [
            "Xianglong Shi",
            "Yunhan Jiang",
            "Xiaoheng Jiang",
            "Mingling Xu",
            "Yang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12860",
        "abstract": "Crack Segmentation in industrial concrete surfaces is a challenging task because cracks usually exhibit intricate morphology with slender appearances. Traditional segmentation methods often struggle to accurately locate such cracks, leading to inefficiencies in maintenance and repair processes. In this paper, we propose a novel diffusion-based model with a cross-conditional encoder-decoder, named CrossDiff, which is the first to introduce the diffusion probabilistic model for the crack segmentation task. Specifically, CrossDiff integrates a cross-encoder and a cross-decoder into the diffusion model to constitute a cross-shaped diffusion model structure. The cross-encoder enhances the ability to retain crack details and the cross-decoder helps extract the semantic features of cracks. As a result, CrossDiff can better handle slender cracks. Extensive experiments were conducted on five challenging crack datasets including CFD, CrackTree200, DeepCrack, GAPs384, and Rissbilder. The results demonstrate that the proposed CrossDiff model achieves impressive performance, outperforming other state-of-the-art methods by 8.0% in terms of both Dice score and IoU. The code will be open-source soon.",
        "tags": [
            "Diffusion",
            "Segmentation"
        ]
    },
    {
        "id": "53",
        "title": "WisdomBot: Tuning Large Language Models with Artificial Intelligence Knowledge",
        "author": [
            "Jingyuan Chen",
            "Tao Wu",
            "Wei Ji",
            "Fei Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12877",
        "abstract": "Large language models (LLMs) have emerged as powerful tools in natural language processing (NLP), showing a promising future of artificial generated intelligence (AGI). Despite their notable performance in the general domain, LLMs have remained suboptimal in the field of education, owing to the unique challenges presented by this domain, such as the need for more specialized knowledge, the requirement for personalized learning experiences, and the necessity for concise explanations of complex concepts. To address these issues, this paper presents a novel LLM for education named WisdomBot, which combines the power of LLMs with educational theories, enabling their seamless integration into educational contexts. To be specific, we harness self-instructed knowledge concepts and instructions under the guidance of Bloom's Taxonomy as training data. To further enhance the accuracy and professionalism of model's response on factual questions, we introduce two key enhancements during inference, i.e., local knowledge base retrieval augmentation and search engine retrieval augmentation during inference. We substantiate the effectiveness of our approach by applying it to several Chinese LLMs, thereby showcasing that the fine-tuned models can generate more reliable and professional responses.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "54",
        "title": "Generative AI Misuse Potential in Cyber Security Education: A Case Study of a UK Degree Program",
        "author": [
            "Carlton Shepherd"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12883",
        "abstract": "Recent advances in generative artificial intelligence (AI), such as ChatGPT, Google Gemini, and other large language models (LLMs), pose significant challenges to upholding academic integrity in higher education. This paper investigates the susceptibility of a Master's-level cyber security degree program at a UK Russell Group university, accredited by a leading national body, to LLM misuse. Through the application and extension of a quantitative assessment framework, we identify a high exposure to misuse, particularly in independent project- and report-based assessments. Contributing factors, including block teaching and a predominantly international cohort, are highlighted as potential amplifiers of these vulnerabilities. To address these challenges, we discuss the adoption of LLM-resistant assessments, detection tools, and the importance of fostering an ethical learning environment. These approaches aim to uphold academic standards while preparing students for the complexities of real-world cyber security.",
        "tags": [
            "ChatGPT",
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "55",
        "title": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback",
        "author": [
            "Yafu Li",
            "Xuyang Hu",
            "Xiaoye Qu",
            "Linjie Li",
            "Yu Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12895",
        "abstract": "Large language models (LLMs) demonstrate impressive performance but lack the flexibility to adapt to human preferences quickly without retraining. In this work, we introduce Test-time Preference Optimization (TPO), a framework that aligns LLM outputs with human preferences during inference, removing the need to update model parameters. Rather than relying on purely numerical rewards, TPO translates reward signals into textual critiques and uses them as textual rewards to iteratively refine its response. Evaluations on benchmarks covering instruction following, preference alignment, safety, and mathematics reveal that TPO progressively improves alignment with human preferences. Notably, after only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can surpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO scales efficiently with both the search width and depth during inference. Through case studies, we illustrate how TPO exploits the innate capacity of LLM to interpret and act upon reward signals. Our findings establish TPO as a practical, lightweight alternative for test-time preference optimization, achieving alignment on the fly. Our code is publicly available at https://github.com/yafuly/TPO.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "56",
        "title": "DocTTT: Test-Time Training for Handwritten Document Recognition Using Meta-Auxiliary Learning",
        "author": [
            "Wenhao Gu",
            "Li Gu",
            "Ziqiang Wang",
            "Ching Yee Suen",
            "Yang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12898",
        "abstract": "Despite recent significant advancements in Handwritten Document Recognition (HDR), the efficient and accurate recognition of text against complex backgrounds, diverse handwriting styles, and varying document layouts remains a practical challenge. Moreover, this issue is seldom addressed in academic research, particularly in scenarios with minimal annotated data available. In this paper, we introduce the DocTTT framework to address these challenges. The key innovation of our approach is that it uses test-time training to adapt the model to each specific input during testing. We propose a novel Meta-Auxiliary learning approach that combines Meta-learning and self-supervised Masked Autoencoder~(MAE). During testing, we adapt the visual representation parameters using a self-supervised MAE loss. During training, we learn the model parameters using a meta-learning framework, so that the model parameters are learned to adapt to a new input effectively. Experimental results show that our proposed method significantly outperforms existing state-of-the-art approaches on benchmark datasets.",
        "tags": [
            "Test-Time Training"
        ]
    },
    {
        "id": "57",
        "title": "Unified CNNs and transformers underlying learning mechanism reveals multi-head attention modus vivendi",
        "author": [
            "Ella Koresh",
            "Ronit D. Gross",
            "Yuval Meir",
            "Yarden Tzach",
            "Tal Halevi",
            "Ido Kanter"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12900",
        "abstract": "Convolutional neural networks (CNNs) evaluate short-range correlations in input images which progress along the layers, whereas vision transformer (ViT) architectures evaluate long-range correlations, using repeated transformer encoders composed of fully connected layers. Both are designed to solve complex classification tasks but from different perspectives. This study demonstrates that CNNs and ViT architectures stem from a unified underlying learning mechanism, which quantitatively measures the single-nodal performance (SNP) of each node in feedforward (FF) and multi-head attention (MHA) subblocks. Each node identifies small clusters of possible output labels, with additional noise represented as labels outside these clusters. These features are progressively sharpened along the transformer encoders, enhancing the signal-to-noise ratio. This unified underlying learning mechanism leads to two main findings. First, it enables an efficient applied nodal diagonal connection (ANDC) pruning technique without affecting the accuracy. Second, based on the SNP, spontaneous symmetry breaking occurs among the MHA heads, such that each head focuses its attention on a subset of labels through cooperation among its SNPs. Consequently, each head becomes an expert in recognizing its designated labels, representing a quantitative MHA modus vivendi mechanism. These results are based on a compact convolutional transformer architecture trained on the CIFAR-100 and Flowers-102 datasets and call for their extension to other architectures and applications, such as natural language processing.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "58",
        "title": "Architectural Fusion Through Contextual Partitioning in Large Language Models: A Novel Approach to Parameterized Knowledge Integration",
        "author": [
            "Offa Kingsleigh",
            "Alfred Abercrombie",
            "David Woolstencroft",
            "Beorhtric Meadowcroft",
            "Marcus Irvin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12901",
        "abstract": "Contextual Partitioning introduces an innovative approach to enhancing the architectural design of large-scale computational models through the dynamic segmentation of parameters into context-aware regions. This methodology emphasizes the importance of task-specific specialization, achieved through adaptive parameter allocation mechanisms that align with the linguistic features of input data. Experimental evaluations demonstrated substantial improvements in accuracy, perplexity, and contextual coherence across a variety of linguistic tasks, highlighting the adaptability and scalability of the proposed framework. By reducing redundancy and enhancing computational efficiency, Contextual Partitioning not only streamlines model operations but also expands the scope of applications for advanced language processing systems. The approach operates autonomously, requiring no external fine-tuning, thereby addressing a significant limitation in conventional parameter optimization techniques. Empirical results demonstrate the effectiveness of gradient-driven segmentation, enabling models to dynamically recalibrate and specialize in response to task-specific demands. Furthermore, resource utilization metrics reveal notable reductions in memory usage and training times, confirming the efficiency of the approach. Observations from qualitative analyses illustrate improved contextual coherence and logical flow in generated outputs, reinforcing the practical value of this technique. The findings collectively demonstrate the potential for Contextual Partitioning to redefine the scalability and adaptability of computational language architectures in diverse and complex domains.",
        "tags": [
            "Large Language Models",
            "Segmentation"
        ]
    },
    {
        "id": "59",
        "title": "A Functional Software Reference Architecture for LLM-Integrated Systems",
        "author": [
            "Alessio Bucaioni",
            "Martin Weyssow",
            "Junda He",
            "Yunbo Lyu",
            "David Lo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12904",
        "abstract": "The integration of large language models into software systems is transforming capabilities such as natural language understanding, decision-making, and autonomous task execution. However, the absence of a commonly accepted software reference architecture hinders systematic reasoning about their design and quality attributes. This gap makes it challenging to address critical concerns like privacy, security, modularity, and interoperability, which are increasingly important as these systems grow in complexity and societal impact. In this paper, we describe our \\textit{emerging} results for a preliminary functional reference architecture as a conceptual framework to address these challenges and guide the design, evaluation, and evolution of large language model-integrated systems. We identify key architectural concerns for these systems, informed by current research and practice. We then evaluate how the architecture addresses these concerns and validate its applicability using three open-source large language model-integrated systems in computer vision, text processing, and coding.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "60",
        "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces",
        "author": [
            "Zhenran Xu",
            "Longyue Wang",
            "Jifang Wang",
            "Zhouyi Li",
            "Senbao Shi",
            "Xue Yang",
            "Yiyu Wang",
            "Baotian Hu",
            "Jun Yu",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12909",
        "abstract": "Virtual film production requires intricate decision-making processes, including scriptwriting, virtual cinematography, and precise actor positioning and actions. Motivated by recent advances in automated decision-making with language agent-based societies, this paper introduces FilmAgent, a novel LLM-based multi-agent collaborative framework for end-to-end film automation in our constructed 3D virtual spaces. FilmAgent simulates various crew roles, including directors, screenwriters, actors, and cinematographers, and covers key stages of a film production workflow: (1) idea development transforms brainstormed ideas into structured story outlines; (2) scriptwriting elaborates on dialogue and character actions for each scene; (3) cinematography determines the camera setups for each shot. A team of agents collaborates through iterative feedback and revisions, thereby verifying intermediate scripts and reducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key aspects. Human evaluation shows that FilmAgent outperforms all baselines across all aspects and scores 3.98 out of 5 on average, showing the feasibility of multi-agent collaboration in filmmaking. Further analysis reveals that FilmAgent, despite using the less advanced GPT-4o model, surpasses the single-agent o1, showing the advantage of a well-coordinated multi-agent system. Lastly, we discuss the complementary strengths and weaknesses of OpenAI's text-to-video model Sora and our FilmAgent in filmmaking.",
        "tags": [
            "3D",
            "GPT",
            "Sora",
            "Text-to-Video"
        ]
    },
    {
        "id": "61",
        "title": "PreciseCam: Precise Camera Control for Text-to-Image Generation",
        "author": [
            "Edurne Bernal-Berdun",
            "Ana Serrano",
            "Belen Masia",
            "Matheus Gadelha",
            "Yannick Hold-Geoffroy",
            "Xin Sun",
            "Diego Gutierrez"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12910",
        "abstract": "Images as an artistic medium often rely on specific camera angles and lens distortions to convey ideas or emotions; however, such precise control is missing in current text-to-image models. We propose an efficient and general solution that allows precise control over the camera when generating both photographic and artistic images. Unlike prior methods that rely on predefined shots, we rely solely on four simple extrinsic and intrinsic camera parameters, removing the need for pre-existing geometry, reference 3D objects, and multi-view data. We also present a novel dataset with more than 57,000 images, along with their text prompts and ground-truth camera parameters. Our evaluation shows precise camera control in text-to-image generation, surpassing traditional prompt engineering approaches. Our data, model, and code are publicly available at https://graphics.unizar.es/projects/PreciseCam2024.",
        "tags": [
            "3D",
            "Text-to-Image"
        ]
    },
    {
        "id": "62",
        "title": "Correctness Assessment of Code Generated by Large Language Models Using Internal Representations",
        "author": [
            "Tuan-Dung Bui",
            "Thanh Trong Vu",
            "Thu-Trang Nguyen",
            "Son Nguyen",
            "Hieu Dinh Vo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12934",
        "abstract": "Ensuring the correctness of code generated by Large Language Models (LLMs) presents a significant challenge in AI-driven software development. Existing approaches predominantly rely on black-box (closed-box) approaches that evaluate correctness post-generation, failing to utilize the rich insights embedded in the LLMs' internal states during code generation. In this paper, we introduce OPENIA, a novel white-box (open-box) framework that leverages these internal representations to assess the correctness of LLM-generated code. OPENIA systematically analyzes the intermediate states of representative open-source LLMs specialized for code, including DeepSeek-Coder, CodeLlama, and MagicCoder, across diverse code generation benchmarks. Our empirical analysis reveals that these internal representations encode latent information, which strongly correlates with the correctness of the generated code. Building on these insights, OPENIA uses a white-box/open-box approach to make informed predictions about code correctness, offering significant advantages in adaptability and robustness over traditional classification-based methods and zero-shot approaches. Experimental results demonstrate that OPENIA consistently outperforms baseline models, achieving higher accuracy, precision, recall, and F1-Scores with up to a 2X improvement in standalone code generation and a 46% enhancement in repository-specific scenarios. By unlocking the potential of in-process signals, OPENIA paves the way for more proactive and efficient quality assurance mechanisms in LLM-assisted code generation.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "63",
        "title": "3D Object Manipulation in a Single Image using Generative Models",
        "author": [
            "Ruisi Zhao",
            "Zechuan Zhang",
            "Zongxin Yang",
            "Yi Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12935",
        "abstract": "Object manipulation in images aims to not only edit the object's presentation but also gift objects with motion. Previous methods encountered challenges in concurrently handling static editing and dynamic generation, while also struggling to achieve fidelity in object appearance and scene lighting. In this work, we introduce \\textbf{OMG3D}, a novel framework that integrates the precise geometric control with the generative power of diffusion models, thus achieving significant enhancements in visual performance. Our framework first converts 2D objects into 3D, enabling user-directed modifications and lifelike motions at the geometric level. To address texture realism, we propose CustomRefiner, a texture refinement module that pre-train a customized diffusion model, aligning the details and style of coarse renderings of 3D rough model with the original image, further refine the texture. Additionally, we introduce IllumiCombiner, a lighting processing module that estimates and corrects background lighting to match human visual perception, resulting in more realistic shadow effects. Extensive experiments demonstrate the outstanding visual performance of our approach in both static and dynamic scenarios. Remarkably, all these steps can be done using one NVIDIA 3090. Project page is at https://whalesong-zrs.github.io/OMG3D-projectpage/",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "64",
        "title": "Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained Scheduling",
        "author": [
            "Zhuoran Li",
            "Ruishuo Chen",
            "Hai Zhong",
            "Longbo Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12942",
        "abstract": "Effective multi-user delay-constrained scheduling is crucial in various real-world applications, such as instant messaging, live streaming, and data center management. In these scenarios, schedulers must make real-time decisions to satisfy both delay and resource constraints without prior knowledge of system dynamics, which are often time-varying and challenging to estimate. Current learning-based methods typically require interactions with actual systems during the training stage, which can be difficult or impractical, as it is capable of significantly degrading system performance and incurring substantial service costs. To address these challenges, we propose a novel offline reinforcement learning-based algorithm, named \\underline{S}cheduling By \\underline{O}ffline Learning with \\underline{C}ritic Guidance and \\underline{D}iffusion Generation (SOCD), to learn efficient scheduling policies purely from pre-collected \\emph{offline data}. SOCD innovatively employs a diffusion-based policy network, complemented by a sampling-free critic network for policy guidance. By integrating the Lagrangian multiplier optimization into the offline reinforcement learning, SOCD effectively trains high-quality constraint-aware policies exclusively from available datasets, eliminating the need for online interactions with the system. Experimental results demonstrate that SOCD is resilient to various system dynamics, including partially observable and large-scale environments, and delivers superior performance compared to existing methods.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "65",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "author": [
            "DeepSeek-AI",
            "Daya Guo",
            "Dejian Yang",
            "Haowei Zhang",
            "Junxiao Song",
            "Ruoyu Zhang",
            "Runxin Xu",
            "Qihao Zhu",
            "Shirong Ma",
            "Peiyi Wang",
            "Xiao Bi",
            "Xiaokang Zhang",
            "Xingkai Yu",
            "Yu Wu",
            "Z.F. Wu",
            "Zhibin Gou",
            "Zhihong Shao",
            "Zhuoshu Li",
            "Ziyi Gao",
            "Aixin Liu",
            "Bing Xue",
            "Bingxuan Wang",
            "Bochao Wu",
            "Bei Feng",
            "Chengda Lu",
            "Chenggang Zhao",
            "Chengqi Deng",
            "Chenyu Zhang",
            "Chong Ruan",
            "Damai Dai",
            "Deli Chen",
            "Dongjie Ji",
            "Erhang Li",
            "Fangyun Lin",
            "Fucong Dai",
            "Fuli Luo",
            "Guangbo Hao",
            "Guanting Chen",
            "Guowei Li",
            "H. Zhang",
            "Han Bao",
            "Hanwei Xu",
            "Haocheng Wang",
            "Honghui Ding",
            "Huajian Xin",
            "Huazuo Gao",
            "Hui Qu",
            "Hui Li",
            "Jianzhong Guo",
            "Jiashi Li",
            "Jiawei Wang",
            "Jingchang Chen",
            "Jingyang Yuan",
            "Junjie Qiu",
            "Junlong Li",
            "J.L. Cai",
            "Jiaqi Ni",
            "Jian Liang",
            "Jin Chen",
            "Kai Dong",
            "Kai Hu",
            "Kaige Gao",
            "Kang Guan",
            "Kexin Huang",
            "Kuai Yu",
            "Lean Wang",
            "Lecong Zhang",
            "Liang Zhao",
            "Litong Wang",
            "Liyue Zhang",
            "Lei Xu",
            "Leyi Xia",
            "Mingchuan Zhang",
            "Minghua Zhang",
            "Minghui Tang",
            "Meng Li",
            "Miaojun Wang",
            "Mingming Li",
            "Ning Tian",
            "Panpan Huang",
            "Peng Zhang",
            "Qiancheng Wang",
            "Qinyu Chen",
            "Qiushi Du",
            "Ruiqi Ge",
            "Ruisong Zhang",
            "Ruizhe Pan",
            "Runji Wang",
            "R.J. Chen",
            "R.L. Jin",
            "Ruyi Chen",
            "Shanghao Lu",
            "Shangyan Zhou",
            "Shanhuang Chen",
            "Shengfeng Ye",
            "Shiyu Wang",
            "Shuiping Yu",
            "Shunfeng Zhou",
            "Shuting Pan",
            "S.S. Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12948",
        "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
        "tags": [
            "LLMs",
            "LLaMA",
            "RL"
        ]
    },
    {
        "id": "66",
        "title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models",
        "author": [
            "Pengxiang Zhao",
            "Xiaoming Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12956",
        "abstract": "Large Language Models (LLMs) face significant deployment challenges due to their substantial resource requirements. While low-bit quantized weights can reduce memory usage and improve inference efficiency, current hardware lacks native support for mixed-precision General Matrix Multiplication (mpGEMM), resulting in inefficient dequantization-based implementations. Moreover, uniform quantization methods often fail to capture weight distributions adequately, leading to performance degradation. We propose GANQ (GPU-Adaptive Non-Uniform Quantization), a layer-wise post-training non-uniform quantization framework optimized for hardware-efficient lookup table-based mpGEMM. GANQ achieves superior quantization performance by utilizing a training-free, GPU-adaptive optimization algorithm to efficiently reduce layer-wise quantization errors. Extensive experiments demonstrate GANQ's ability to reduce the perplexity gap from the FP16 baseline compared to state-of-the-art methods for both 3-bit and 4-bit quantization. Furthermore, when deployed on a single NVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup over the baseline, advancing memory and inference efficiency in LLM deployment.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "67",
        "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference",
        "author": [
            "Weizhi Fei",
            "Xueyan Niu",
            "Guoqing Xie",
            "Yingqing Liu",
            "Bo Bai",
            "Wei Han"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12959",
        "abstract": "Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly \"skim through\" input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively reduces the complexity and costs associated with commercial API calls. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "68",
        "title": "It's complicated. The relationship of algorithmic fairness and non-discrimination regulations in the EU AI Act",
        "author": [
            "Kristof Meding"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12962",
        "abstract": "What constitutes a fair decision? This question is not only difficult for humans but becomes more challenging when Artificial Intelligence (AI) models are used. In light of discriminatory algorithmic behaviors, the EU has recently passed the AI Act, which mandates specific rules for AI models, incorporating both traditional legal non-discrimination regulations and machine learning based algorithmic fairness concepts. This paper aims to bridge these two different concepts in the AI Act through: First a high-level introduction of both concepts targeting legal and computer science-oriented scholars, and second an in-depth analysis of the AI Act's relationship between legal non-discrimination regulations and algorithmic fairness. Our analysis reveals three key findings: (1.), most non-discrimination regulations target only high-risk AI systems. (2.), the regulation of high-risk systems encompasses both data input requirements and output monitoring, though these regulations are often inconsistent and raise questions of computational feasibility. (3.) Regulations for General Purpose AI Models, such as Large Language Models that are not simultaneously classified as high-risk systems, currently lack specificity compared to other regulations. Based on these findings, we recommend developing more specific auditing and testing methodologies for AI systems. This paper aims to serve as a foundation for future interdisciplinary collaboration between legal scholars and computer science-oriented machine learning researchers studying discrimination in AI systems.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "69",
        "title": "Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs",
        "author": [
            "Jan Corazza",
            "Ivan Gavran",
            "Gabriela Moreira",
            "Daniel Neider"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12972",
        "abstract": "When blockchain systems are said to be trustless, what this really means is that all the trust is put into software. Thus, there are strong incentives to ensure blockchain software is correct -- vulnerabilities here cost millions and break businesses. One of the most powerful ways of establishing software correctness is by using formal methods. Approaches based on formal methods, however, induce a significant overhead in terms of time and expertise required to successfully employ them. Our work addresses this critical disadvantage by automating the creation of a formal model -- a mathematical abstraction of the software system -- which is often a core task when employing formal methods. We perform model synthesis in three phases: we first transpile the code into model stubs; then we \"fill in the blanks\" using a large language model (LLM); finally, we iteratively repair the generated model, on both syntactical and semantical level. In this way, we significantly reduce the amount of time necessary to create formal models and increase accessibility of valuable software verification methods that rely on them. The practical context of our work was reducing the time-to-value of using formal models for correctness audits of smart contracts.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "70",
        "title": "OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for Small-Large Language Models",
        "author": [
            "Chongren Sun",
            "Yuran Li",
            "Di Wu",
            "Benoit Boulet"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12975",
        "abstract": "Large Language Models (LLMs) are highly capable but require significant computational resources for both training and inference. Within the LLM family, smaller models (those with fewer than 10 billion parameters) also perform well across various tasks. However, these smaller models share similar limitations to their larger counterparts, including the tendency to hallucinate. Despite the existence of many benchmarks to evaluate hallucination in LLMs, few have specifically focused on small LLMs (SLLMs). Additionally, SLLMs show widely varying performance across different benchmarks. In this paper, we introduce OnionEval, a multi-layer structured framework with a specific metric called the context-influence score (CI), designed to effectively assess the fact-conflicting hallucination tendencies of small LLMs across different contextual levels. Our experimental results reveal a key feature of SLLMs: they excel in factual analysis but face challenges with context reasoning. Further investigation shows that a simple Chain-of-Thought strategy can significantly reduce these limitations, improving the practical usefulness of SLLMs in real-world applications.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "71",
        "title": "LiT: Delving into a Simplified Linear Diffusion Transformer for Image Generation",
        "author": [
            "Jiahao Wang",
            "Ning Kang",
            "Lewei Yao",
            "Mengzhao Chen",
            "Chengyue Wu",
            "Songyang Zhang",
            "Shuchen Xue",
            "Yong Liu",
            "Taiqiang Wu",
            "Xihui Liu",
            "Kaipeng Zhang",
            "Shifeng Zhang",
            "Wenqi Shao",
            "Zhenguo Li",
            "Ping Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12976",
        "abstract": "In commonly used sub-quadratic complexity modules, linear attention benefits from simplicity and high parallelism, making it promising for image synthesis tasks. However, the architectural design and learning strategy for linear attention remain underexplored in this field. In this paper, we offer a suite of ready-to-use solutions for efficient linear diffusion Transformers. Our core contributions include: (1) Simplified Linear Attention using few heads, observing the free-lunch effect of performance without latency increase. (2) Weight inheritance from a fully pre-trained diffusion Transformer: initializing linear Transformer using pre-trained diffusion Transformer and loading all parameters except for those related to linear attention. (3) Hybrid knowledge distillation objective: using a pre-trained diffusion Transformer to help the training of the student linear Transformer, supervising not only the predicted noise but also the variance of the reverse diffusion process. These guidelines lead to our proposed Linear Diffusion Transformer (LiT), an efficient text-to-image Transformer that can be deployed offline on a laptop. Experiments show that in class-conditional 256*256 and 512*512 ImageNet benchmark LiT achieves highly competitive FID while reducing training steps by 80% and 77% compared to DiT. LiT also rivals methods based on Mamba or Gated Linear Attention. Besides, for text-to-image generation, LiT allows for the rapid synthesis of up to 1K resolution photorealistic images. Project page: https://techmonsterwang.github.io/LiT/.",
        "tags": [
            "DiT",
            "Diffusion",
            "Diffusion Transformer",
            "Mamba",
            "Text-to-Image",
            "Transformer"
        ]
    },
    {
        "id": "72",
        "title": "Implicit Causality-biases in humans and LLMs as a tool for benchmarking LLM discourse capabilities",
        "author": [
            "Florian Kankowski",
            "Torgrim Solstad",
            "Sina Zarriess",
            "Oliver Bott"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12980",
        "abstract": "In this paper, we compare data generated with mono- and multilingual LLMs spanning a range of model sizes with data provided by human participants in an experimental setting investigating well-established discourse biases. Beyond the comparison as such, we aim to develop a benchmark to assess the capabilities of LLMs with discourse biases as a robust proxy for more general discourse understanding capabilities. More specifically, we investigated Implicit Causality verbs, for which psycholinguistic research has found participants to display biases with regard to three phenomena:\\ the establishment of (i) coreference relations (Experiment 1), (ii) coherence relations (Experiment 2), and (iii) the use of particular referring expressions (Experiments 3 and 4). With regard to coreference biases we found only the largest monolingual LLM (German Bloom 6.4B) to display more human-like biases. For coherence relation, no LLM displayed the explanation bias usually found for humans. For referring expressions, all LLMs displayed a preference for referring to subject arguments with simpler forms than to objects. However, no bias effect on referring expression was found, as opposed to recent studies investigating human biases.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "73",
        "title": "Ehrenfeucht-Haussler Rank and Chain of Thought",
        "author": [
            "Pablo BarcelÃ³",
            "Alexander Kozachinskiy",
            "Tomasz Steifer"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12997",
        "abstract": "The notion of rank of a Boolean function has been a cornerstone in the theory of PAC learning, enabling quasipolynomial-time learning algorithms for polynomial-size decision trees. We present a novel characterization of rank, grounded in the well-known Transformer architecture. We show that the rank of a function $f$ corresponds to the minimum number of Chain of Thought (CoT) steps required by a single-layer transformer decoder with hard attention to compute $f$. Based on this characterization we establish tight bounds on the number of CoT steps required for specific problems, showing that $\\ell$-fold function composition necessitates exactly $\\ell$ CoT steps. Furthermore, we analyze the problem of identifying the position of the $k$-th occurrence of 1 in a Boolean sequence, proving that it requires $k$ CoT steps.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "74",
        "title": "Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament",
        "author": [
            "Yantao Liu",
            "Zijun Yao",
            "Rui Min",
            "Yixin Cao",
            "Lei Hou",
            "Juanzi Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13007",
        "abstract": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large Language Models (LLMs), relies on reward models to select the best candidate solution from multiple generations. However, traditional reward models often assign arbitrary and inconsistent scores, limiting their effectiveness. To address this, we propose a Pairwise Reward Model (Pairwise RM) combined with a knockout tournament for BoN sampling. Instead of assigning absolute scores, given one math problem, Pairwise RM evaluates two candidate solutions' correctness simultaneously. This approach eliminates the need for arbitrary scoring and enables cross-validation of solutions through parallel comparison. In the knockout tournament, Pairwise RM conducts pairwise comparisons between candidate solutions and eliminates the incorrect ones iteratively. We construct \\ourdataset, a large-scale dataset of 443K pairwise comparisons derived from NumiaMath and annotated using \\texttt{gemini-1.5-flash}, and train the Pairwise RM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate significant improvements over traditional discriminative reward models. And a 40\\% to 60\\% relative improvement is achieved on the top 50\\% challenging problems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "75",
        "title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking",
        "author": [
            "Sebastian Farquhar",
            "Vikrant Varma",
            "David Lindner",
            "David Elson",
            "Caleb Biddulph",
            "Ian Goodfellow",
            "Rohin Shah"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13011",
        "abstract": "Future advanced AI systems may learn sophisticated strategies through reinforcement learning (RL) that humans cannot understand well enough to safely evaluate. We propose a training method which avoids agents learning undesired multi-step plans that receive high reward (multi-step \"reward hacks\") even if humans are not able to detect that the behaviour is undesired. The method, Myopic Optimization with Non-myopic Approval (MONA), works by combining short-sighted optimization with far-sighted reward. We demonstrate that MONA can prevent multi-step reward hacking that ordinary RL causes, even without being able to detect the reward hacking and without any extra information that ordinary RL does not get access to. We study MONA empirically in three settings which model different misalignment failure modes including 2-step environments with LLMs representing delegated oversight and encoded reasoning and longer-horizon gridworld environments representing sensor tampering.",
        "tags": [
            "LLMs",
            "RL"
        ]
    },
    {
        "id": "76",
        "title": "Does Table Source Matter? Benchmarking and Improving Multimodal Scientific Table Understanding and Reasoning",
        "author": [
            "Bohao Yang",
            "Yingji Zhang",
            "Dong Liu",
            "AndrÃ© Freitas",
            "Chenghua Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13042",
        "abstract": "Recent large language models (LLMs) have advanced table understanding capabilities but rely on converting tables into text sequences. While multimodal large language models (MLLMs) enable direct visual processing, they face limitations in handling scientific tables due to fixed input image resolutions and insufficient numerical reasoning capabilities. We present a comprehensive framework for multimodal scientific table understanding and reasoning with dynamic input image resolutions. Our framework consists of three key components: (1) MMSci-Pre, a domain-specific table structure learning dataset of 52K scientific table structure recognition samples, (2) MMSci-Ins, an instruction tuning dataset with 12K samples across three table-based tasks, and (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically designed to evaluate numerical reasoning capabilities. Extensive experiments demonstrate that our domain-specific approach with 52K scientific table images achieves superior performance compared to 150K general-domain tables, highlighting the importance of data quality over quantity. Our proposed table-based MLLMs with dynamic input resolutions show significant improvements in both general table understanding and numerical reasoning capabilities, with strong generalisation to held-out datasets. Our code and data are publicly available at https://github.com/Bernard-Yang/MMSci_Table.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "77",
        "title": "Sketch and Patch: Efficient 3D Gaussian Representation for Man-Made Scenes",
        "author": [
            "Yuang Shi",
            "Simone Gasparini",
            "GÃ©raldine Morin",
            "Chenggang Yang",
            "Wei Tsang Ooi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13045",
        "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising representation for photorealistic rendering of 3D scenes. However, its high storage requirements pose significant challenges for practical applications. We observe that Gaussians exhibit distinct roles and characteristics that are analogous to traditional artistic techniques -- Like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features like edges and contours; While other Gaussians represent broader, smoother regions, that are analogous to broader brush strokes that add volume and depth to a painting. Based on this observation, we propose a novel hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which define scene boundaries, and (ii) Patch Gaussians, which cover smooth regions. Sketch Gaussians are efficiently encoded using parametric models, leveraging their geometric coherence, while Patch Gaussians undergo optimized pruning, retraining, and vector quantization to maintain volumetric consistency and storage efficiency. Our comprehensive evaluation across diverse indoor and outdoor scenes demonstrates that this structure-aware approach achieves up to 32.62% improvement in PSNR, 19.12% in SSIM, and 45.41% in LPIPS at equivalent model sizes, and correspondingly, for an indoor scene, our model maintains the visual quality with 2.3% of the original model size.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Vector Quantization"
        ]
    },
    {
        "id": "78",
        "title": "Beyond the Lungs: Extending the Field of View in Chest CT with Latent Diffusion Models",
        "author": [
            "Lianrui Zuo",
            "Kaiwen Xu",
            "Dingjie Su",
            "Xin Yu",
            "Aravind R. Krishnan",
            "Yihao Liu",
            "Shunxing Bao",
            "Thomas Li",
            "Kim L. Sandler",
            "Fabien Maldonado",
            "Bennett A. Landman"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13068",
        "abstract": "The interconnection between the human lungs and other organs, such as the liver and kidneys, is crucial for understanding the underlying risks and effects of lung diseases and improving patient care. However, most research chest CT imaging is focused solely on the lungs due to considerations of cost and radiation dose. This restricted field of view (FOV) in the acquired images poses challenges to comprehensive analysis and hinders the ability to gain insights into the impact of lung diseases on other organs. To address this, we propose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novel approach to capture the inter-organ relationships from CT images and extend the FOV of chest CT images. Our approach first trains a variational autoencoder (VAE) to encode 2D axial CT slices individually, then stacks the latent representations of the VAE to form a 3D context for training a latent diffusion model. Once trained, our approach extends the FOV of CT images in the z-direction by generating new axial slices in a zero-shot manner. We evaluated our approach on the National Lung Screening Trial (NLST) dataset, and results suggest that it effectively extends the FOV to include the liver and kidneys, which are not completely covered in the original NLST data acquisition. Quantitative results on a held-out whole-body dataset demonstrate that the generated slices exhibit high fidelity with acquired data, achieving an SSIM of 0.81.",
        "tags": [
            "3D",
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": "79",
        "title": "Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment",
        "author": [
            "Melissa Kazemi Rad",
            "Huy Nghiem",
            "Andy Luo",
            "Sahil Wadhwa",
            "Mohammad Sorower",
            "Stephen Rawls"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13080",
        "abstract": "Large Language Models (LLMs) have demonstrated powerful capabilities that render them valuable in different applications, including conversational AI products. It is paramount to ensure the security and reliability of these products by mitigating their vulnerabilities towards malicious user interactions, which can lead to the exposure of great risks and reputational repercussions. In this work, we present a comprehensive study on the efficacy of fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs that serve as input moderation guardrails. We systematically explore various tuning methods by leveraging a small set of training data to adapt these models as proxy defense mechanisms to detect malicious inputs and provide a reasoning for their verdicts, thereby preventing the exploitation of conversational agents. We rigorously evaluate the efficacy and robustness of different tuning strategies to generalize across diverse adversarial and malicious query types. Our experimental results outline the potential of alignment processes tailored to a varied range of harmful input queries, even with constrained data resources. These techniques significantly enhance the safety of conversational AI systems and provide a feasible framework for deploying more secure and trustworthy AI-driven interactions.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "80",
        "title": "Orchid: Image Latent Diffusion for Joint Appearance and Geometry Generation",
        "author": [
            "Akshay Krishnan",
            "Xinchen Yan",
            "Vincent Casser",
            "Abhijit Kundu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13087",
        "abstract": "Diffusion models are state-of-the-art for image generation. Trained on large datasets, they capture expressive image priors that have been used for tasks like inpainting, depth, and (surface) normal prediction. However, these models are typically trained for one specific task, e.g., a separate model for each of color, depth, and normal prediction. Such models do not leverage the intrinsic correlation between appearance and geometry, often leading to inconsistent predictions.\nIn this paper, we propose using a novel image diffusion prior that jointly encodes appearance and geometry. We introduce a diffusion model Orchid, comprising a Variational Autoencoder (VAE) to encode color, depth, and surface normals to a latent space, and a Latent Diffusion Model (LDM) for generating these joint latents. Orchid directly generates photo-realistic color images, relative depth, and surface normals from user-provided text, and can be used to create image-aligned partial 3D scenes seamlessly. It can also perform image-conditioned tasks like joint monocular depth and normal prediction and is competitive in accuracy to state-of-the-art methods designed for those tasks alone. Lastly, our model learns a joint prior that can be used zero-shot as a regularizer for many inverse problems that entangle appearance and geometry. For example, we demonstrate its effectiveness in color-depth-normal inpainting, showcasing its applicability to problems in 3D generation from sparse views.",
        "tags": [
            "3D",
            "Diffusion",
            "Inpainting",
            "VAE"
        ]
    },
    {
        "id": "81",
        "title": "Robust Representation Consistency Model via Contrastive Denoising",
        "author": [
            "Jiachen Lei",
            "Julius Berner",
            "Jiongxiao Wang",
            "Zhongzhu Chen",
            "Zhongjia Ba",
            "Kui Ren",
            "Jun Zhu",
            "Anima Anandkumar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13094",
        "abstract": "Robustness is essential for deep neural networks, especially in security-sensitive applications. To this end, randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations. Recently, diffusion models have been successfully employed for randomized smoothing to purify noise-perturbed samples before making predictions with a standard classifier. While these methods excel at small perturbation radii, they struggle with larger perturbations and incur a significant computational overhead during inference compared to classical methods. To address this, we reformulate the generative modeling task along the diffusion trajectories in pixel space as a discriminative task in the latent space. Specifically, we use instance discrimination to achieve consistent representations along the trajectories by aligning temporally adjacent points. After fine-tuning based on the learned representations, our model enables implicit denoising-then-classification via a single prediction, substantially reducing inference costs. We conduct extensive experiments on various datasets and achieve state-of-the-art performance with minimal computation budget during inference. For example, our method outperforms the certified accuracy of diffusion-based methods on ImageNet across all perturbation radii by 5.3% on average, with up to 11.6% at larger radii, while reducing inference costs by 85$\\times$ on average. Codes are available at: https://github.com/jiachenlei/rRCM.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "82",
        "title": "Accelerate High-Quality Diffusion Models with Inner Loop Feedback",
        "author": [
            "Matthew Gwilliam",
            "Han Cai",
            "Di Wu",
            "Abhinav Shrivastava",
            "Zhiyu Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13107",
        "abstract": "We propose Inner Loop Feedback (ILF), a novel approach to accelerate diffusion models' inference. ILF trains a lightweight module to predict future features in the denoising process by leveraging the outputs from a chosen diffusion backbone block at a given time step. This approach exploits two key intuitions; (1) the outputs of a given block at adjacent time steps are similar, and (2) performing partial computations for a step imposes a lower burden on the model than skipping the step entirely. Our method is highly flexible, since we find that the feedback module itself can simply be a block from the diffusion backbone, with all settings copied. Its influence on the diffusion forward can be tempered with a learnable scaling factor from zero initialization. We train this module using distillation losses; however, unlike some prior work where a full diffusion backbone serves as the student, our model freezes the backbone, training only the feedback module. While many efforts to optimize diffusion models focus on achieving acceptable image quality in extremely few steps (1-4 steps), our emphasis is on matching best case results (typically achieved in 20 steps) while significantly reducing runtime. ILF achieves this balance effectively, demonstrating strong performance for both class-to-image generation with diffusion transformer (DiT) and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The quality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP Image Quality Assessment, ImageReward, and qualitative comparisons.",
        "tags": [
            "CLIP",
            "DiT",
            "Diffusion",
            "Diffusion Transformer",
            "Text-to-Image",
            "Transformer"
        ]
    },
    {
        "id": "83",
        "title": "Ensemble score filter with image inpainting for data assimilation in tracking surface quasi-geostrophic dynamics with partial observations",
        "author": [
            "Siming Liang",
            "Hoang Tran",
            "Feng Bao",
            "Hristo G. Chipilski",
            "Peter Jan van Leeuwen",
            "Guannan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12419",
        "abstract": "Data assimilation plays a pivotal role in understanding and predicting turbulent systems within geoscience and weather forecasting, where data assimilation is used to address three fundamental challenges, i.e., high-dimensionality, nonlinearity, and partial observations. Recent advances in machine learning (ML)-based data assimilation methods have demonstrated encouraging results. In this work, we develop an ensemble score filter (EnSF) that integrates image inpainting to solve the data assimilation problems with partial observations. The EnSF method exploits an exclusively designed training-free diffusion models to solve high-dimensional nonlinear data assimilation problems. Its performance has been successfully demonstrated in the context of having full observations, i.e., all the state variables are directly or indirectly observed. However, because the EnSF does not use a covariance matrix to capture the dependence between the observed and unobserved state variables, it is nontrivial to extend the original EnSF method to the partial observation scenario. In this work, we incorporate various image inpainting techniques into the EnSF to predict the unobserved states during data assimilation. At each filtering step, we first use the diffusion model to estimate the observed states by integrating the likelihood information into the score function. Then, we use image inpainting methods to predict the unobserved state variables. We demonstrate the performance of the EnSF with inpainting by tracking the Surface Quasi-Geostrophic (SQG) model dynamics under a variety of scenarios. The successful proof of concept paves the way to more in-depth investigations on exploiting modern image inpainting techniques to advance data assimilation methodology for practical geoscience and weather forecasting problems.",
        "tags": [
            "Diffusion",
            "Inpainting"
        ]
    },
    {
        "id": "84",
        "title": "A Domain Adaptation Framework for Speech Recognition Systems with Only Synthetic data",
        "author": [
            "Minh Tran",
            "Yutong Pang",
            "Debjyoti Paul",
            "Laxmi Pandey",
            "Kevin Jiang",
            "Jinxi Guo",
            "Ke Li",
            "Shun Zhang",
            "Xuedong Zhang",
            "Xin Lei"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12501",
        "abstract": "We introduce DAS (Domain Adaptation with Synthetic data), a novel domain adaptation framework for pre-trained ASR model, designed to efficiently adapt to various language-defined domains without requiring any real data. In particular, DAS first prompts large language models (LLMs) to generate domain-specific texts before converting these texts to speech via text-to-speech technology. The synthetic data is used to fine-tune Whisper with Low-Rank Adapters (LoRAs) for targeted domains such as music, weather, and sports. We introduce a novel one-pass decoding strategy that merges predictions from multiple LoRA adapters efficiently during the auto-regressive text generation process. Experimental results show significant improvements, reducing the Word Error Rate (WER) by 10% to 17% across all target domains compared to the original model, with minimal performance regression in out-of-domain settings (e.g., -1% on Librispeech test sets). We also demonstrate that DAS operates efficiently during inference, introducing an additional 9% increase in Real Time Factor (RTF) compared to the original model when inferring with three LoRA adapters.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "85",
        "title": "Image Motion Blur Removal in the Temporal Dimension with Video Diffusion Models",
        "author": [
            "Wang Pang",
            "Zhihao Zhan",
            "Xiang Zhu",
            "Yechao Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12604",
        "abstract": "Most motion deblurring algorithms rely on spatial-domain convolution models, which struggle with the complex, non-linear blur arising from camera shake and object motion. In contrast, we propose a novel single-image deblurring approach that treats motion blur as a temporal averaging phenomenon. Our core innovation lies in leveraging a pre-trained video diffusion transformer model to capture diverse motion dynamics within a latent space. It sidesteps explicit kernel estimation and effectively accommodates diverse motion patterns. We implement the algorithm within a diffusion-based inverse problem framework. Empirical results on synthetic and real-world datasets demonstrate that our method outperforms existing techniques in deblurring complex motion blur scenarios. This work paves the way for utilizing powerful video diffusion models to address single-image deblurring challenges.",
        "tags": [
            "Deblurring",
            "Diffusion",
            "Diffusion Transformer",
            "Transformer"
        ]
    },
    {
        "id": "86",
        "title": "Sequential Change Point Detection via Denoising Score Matching",
        "author": [
            "Wenbin Zhou",
            "Liyan Xie",
            "Zhigang Peng",
            "Shixiang Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12667",
        "abstract": "Sequential change-point detection plays a critical role in numerous real-world applications, where timely identification of distributional shifts can greatly mitigate adverse outcomes. Classical methods commonly rely on parametric density assumptions of pre- and post-change distributions, limiting their effectiveness for high-dimensional, complex data streams. This paper proposes a score-based CUSUM change-point detection, in which the score functions of the data distribution are estimated by injecting noise and applying denoising score matching. We consider both offline and online versions of score estimation. Through theoretical analysis, we demonstrate that denoising score matching can enhance detection power by effectively controlling the injected noise scale. Finally, we validate the practical efficacy of our method through numerical experiments on two synthetic datasets and a real-world earthquake precursor detection task, demonstrating its effectiveness in challenging scenarios.",
        "tags": [
            "Detection",
            "Score Matching"
        ]
    },
    {
        "id": "87",
        "title": "EmoFormer: A Text-Independent Speech Emotion Recognition using a Hybrid Transformer-CNN model",
        "author": [
            "Rashedul Hasan",
            "Meher Nigar",
            "Nursadul Mamun",
            "Sayan Paul"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12682",
        "abstract": "Speech Emotion Recognition is a crucial area of research in human-computer interaction. While significant work has been done in this field, many state-of-the-art networks struggle to accurately recognize emotions in speech when the data is both speech and speaker-independent. To address this limitation, this study proposes, EmoFormer, a hybrid model combining CNNs (CNNs) with Transformer encoders to capture emotion patterns in speech data for such independent datasets. The EmoFormer network was trained and tested using the Expressive Anechoic Recordings of Speech (EARS) dataset, recently released by META. We experimented with two feature extraction techniques: MFCCs and x-vectors. The model was evaluated on different emotion sets comprising 5, 7, 10, and 23 distinct categories. The results demonstrate that the model achieved its best performance with five emotions, attaining an accuracy of 90%, a precision of 0.92, a recall, and an F1-score of 0.91. However, performance decreased as the number of emotions increased, with an accuracy of 83% for seven emotions compared to 70% for the baseline network. This study highlights the effectiveness of combining CNNs and Transformer-based architectures for emotion recognition from speech, particularly when using MFCC features.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "88",
        "title": "FDG-Diff: Frequency-Domain-Guided Diffusion Framework for Compressed Hazy Image Restoration",
        "author": [
            "Ruicheng Zhang",
            "Kanghui Tian",
            "Zeyu Zhang",
            "Qixiang Liu",
            "Zhi Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12832",
        "abstract": "In this study, we reveal that the interaction between haze degradation and JPEG compression introduces complex joint loss effects, which significantly complicate image restoration. Existing dehazing models often neglect compression effects, which limits their effectiveness in practical applications. To address these challenges, we introduce three key contributions. First, we design FDG-Diff, a novel frequency-domain-guided dehazing framework that improves JPEG image restoration by leveraging frequency-domain information. Second, we introduce the High-Frequency Compensation Module (HFCM), which enhances spatial-domain detail restoration by incorporating frequency-domain augmentation techniques into a diffusion-based restoration framework. Lastly, the introduction of the Degradation-Aware Denoising Timestep Predictor (DADTP) module further enhances restoration quality by enabling adaptive region-specific restoration, effectively addressing regional degradation inconsistencies in compressed hazy images. Experimental results across multiple compressed dehazing datasets demonstrate that our method consistently outperforms the latest state-of-the-art approaches. Code be available at https://github.com/SYSUzrc/FDG-Diff.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "89",
        "title": "Low-dimensional adaptation of diffusion models: Convergence in total variation",
        "author": [
            "Jiadong Liang",
            "Zhihan Huang",
            "Yuxin Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.12982",
        "abstract": "This paper investigates how diffusion generative models leverage (unknown) low-dimensional structure to accelerate sampling. Focusing on two mainstream samplers -- the denoising diffusion implicit model (DDIM) and the denoising diffusion probabilistic model (DDPM) -- and assuming accurate score estimates, we prove that their iteration complexities are no greater than the order of $k/\\varepsilon$ (up to some log factor), where $\\varepsilon$ is the precision in total variation distance and $k$ is some intrinsic dimension of the target distribution. Our results are applicable to a broad family of target distributions without requiring smoothness or log-concavity assumptions. Further, we develop a lower bound that suggests the (near) necessity of the coefficients introduced by Ho et al.(2020) and Song et al.(2020) in facilitating low-dimensional adaptation. Our findings provide the first rigorous evidence for the adaptivity of the DDIM-type samplers to unknown low-dimensional structure, and improve over the state-of-the-art DDPM theory regarding total variation convergence.",
        "tags": [
            "DDIM",
            "DDPM",
            "Diffusion"
        ]
    },
    {
        "id": "90",
        "title": "Why disentanglement-based speaker anonymization systems fail at preserving emotions?",
        "author": [
            "Ãnal Ege Gaznepoglu",
            "Nils Peters"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13000",
        "abstract": "Disentanglement-based speaker anonymization involves decomposing speech into a semantically meaningful representation, altering the speaker embedding, and resynthesizing a waveform using a neural vocoder. State-of-the-art systems of this kind are known to remove emotion information. Possible reasons include mode collapse in GAN-based vocoders, unintended modeling and modification of emotions through speaker embeddings, or excessive sanitization of the intermediate representation. In this paper, we conduct a comprehensive evaluation of a state-of-the-art speaker anonymization system to understand the underlying causes. We conclude that the main reason is the lack of emotion-related information in the intermediate representation. The speaker embeddings also have a high impact, if they are learned in a generative context. The vocoder's out-of-distribution performance has a smaller impact. Additionally, we discovered that synthesis artifacts increase spectral kurtosis, biasing emotion recognition evaluation towards classifying utterances as angry. Therefore, we conclude that reporting unweighted average recall alone for emotion recognition performance is suboptimal.",
        "tags": [
            "GAN"
        ]
    }
]