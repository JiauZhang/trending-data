[
    {
        "id": "1",
        "title": "MERaLiON-TextLLM: Cross-Lingual Understanding of Large Language Models in Chinese, Indonesian, Malay, and Singlish",
        "author": [
            "Xin Huang",
            "Tarun Kumar Vangani",
            "Minh Duc Pham",
            "Xunlong Zou",
            "Bin Wang",
            "Zhengyuan Liu",
            "Ai Ti Aw"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08335",
        "abstract": "Multilingual large language models (MLLMs) have shown impressive capabilities across a variety of languages. However, efficacy can differ greatly between different language families, especially for those with limited linguistic resources. This report presents MERaLiON-TextLLM, a series of open-source language models specifically tailored to improve understanding and generation in Chinese, Indonesian, Malay, and Singlish. The initial released model is built on Llama-3-8B-Base and refined through a meticulously crafted process of continued pre-training and weight merging. Our approach achieves performance improvements across benchmarks in these languages, exceeding the capabilities of the official Llama-3 models. We provide the model checkpoints as a resource to support further research and development in cross-lingual language understanding.",
        "tags": [
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "2",
        "title": "Dynaseal: A Backend-Controlled LLM API Key Distribution Scheme with Constrained Invocation Parameters",
        "author": [
            "Jiahao Zhao",
            "Jiayi Nan",
            "Lai Wei",
            "Yichen Yang",
            "Fan Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08336",
        "abstract": "Due to the exceptional performance of Large Language Models (LLMs) in diverse downstream tasks,there has been an exponential growth in edge-device requests to cloud-based http://models.However, the current authentication mechanism using static Bearer Tokens in request headersfails to provide the flexibility and backend control required for edge-device http://deployments.To address these limitations, we propose Dynaseal,a novel methodology that enables fine-grained backend constraints on model invocations.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "3",
        "title": "SCOT: Self-Supervised Contrastive Pretraining For Zero-Shot Compositional Retrieval",
        "author": [
            "Bhavin Jawade",
            "Joao V. B. Soares",
            "Kapil Thadani",
            "Deen Dayal Mohan",
            "Amir Erfan Eshratifar",
            "Benjamin Culpepper",
            "Paloma de Juan",
            "Srirangaraj Setlur",
            "Venu Govindaraju"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08347",
        "abstract": "Compositional image retrieval (CIR) is a multimodal learning task where a model combines a query image with a user-provided text modification to retrieve a target image. CIR finds applications in a variety of domains including product retrieval (e-commerce) and web search. Existing methods primarily focus on fully-supervised learning, wherein models are trained on datasets of labeled triplets such as FashionIQ and CIRR. This poses two significant challenges: (i) curating such triplet datasets is labor intensive; and (ii) models lack generalization to unseen objects and domains. In this work, we propose SCOT (Self-supervised COmpositional Training), a novel zero-shot compositional pretraining strategy that combines existing large image-text pair datasets with the generative capabilities of large language models to contrastively train an embedding composition network. Specifically, we show that the text embedding from a large-scale contrastively-pretrained vision-language model can be utilized as proxy target supervision during compositional pretraining, replacing the target image embedding. In zero-shot settings, this strategy surpasses SOTA zero-shot compositional retrieval methods as well as many fully-supervised methods on standard benchmarks such as FashionIQ and CIRR.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "4",
        "title": "Towards Best Practices for Open Datasets for LLM Training",
        "author": [
            "Stefan Baack",
            "Stella Biderman",
            "Kasia Odrozek",
            "Aviya Skowron",
            "Ayah Bdeir",
            "Jillian Bommarito",
            "Jennifer Ding",
            "Maximilian Gahntz",
            "Paul Keller",
            "Pierre-Carl Langlais",
            "Greg Lindahl",
            "Sebastian Majstorovic",
            "Nik Marda",
            "Guilherme Penedo",
            "Maarten Van Segbroeck",
            "Jennifer Wang",
            "Leandro von Werra",
            "Mitchell Baker",
            "Julie BeliÃ£o",
            "Kasia Chmielinski",
            "Marzieh Fadaee",
            "Lisa Gutermuth",
            "Hynek KydlÃ­Äek",
            "Greg Leppert",
            "EM Lewis-Jong",
            "Solana Larsen",
            "Shayne Longpre",
            "Angela Oduor Lungati",
            "Cullen Miller",
            "Victor Miller",
            "Max Ryabinin",
            "Kathleen Siminyu",
            "Andrew Strait",
            "Mark Surman",
            "Anna TumadÃ³ttir",
            "Maurice Weber",
            "Rebecca Weiss",
            "Lee White",
            "Thomas Wolf"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08365",
        "abstract": "Many AI companies are training their large language models (LLMs) on data without the permission of the copyright owners. The permissibility of doing so varies by jurisdiction: in countries like the EU and Japan, this is allowed under certain restrictions, while in the United States, the legal landscape is more ambiguous. Regardless of the legal status, concerns from creative producers have led to several high-profile copyright lawsuits, and the threat of litigation is commonly cited as a reason for the recent trend towards minimizing the information shared about training datasets by both corporate and public interest actors. This trend in limiting data information causes harm by hindering transparency, accountability, and innovation in the broader ecosystem by denying researchers, auditors, and impacted individuals access to the information needed to understand AI models.\nWhile this could be mitigated by training language models on open access and public domain data, at the time of writing, there are no such models (trained at a meaningful scale) due to the substantial technical and sociological challenges in assembling the necessary corpus. These challenges include incomplete and unreliable metadata, the cost and complexity of digitizing physical records, and the diverse set of legal and technical skills required to ensure relevance and responsibility in a quickly changing landscape. Building towards a future where AI systems can be trained on openly licensed data that is responsibly curated and governed requires collaboration across legal, technical, and policy domains, along with investments in metadata standards, digitization, and fostering a culture of openness.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "5",
        "title": "3D Gaussian Splatting with Normal Information for Mesh Extraction and Improved Rendering",
        "author": [
            "Meenakshi Krishnan",
            "Liam Fowl",
            "Ramani Duraiswami"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08370",
        "abstract": "Differentiable 3D Gaussian splatting has emerged as an efficient and flexible rendering technique for representing complex scenes from a collection of 2D views and enabling high-quality real-time novel-view synthesis. However, its reliance on photometric losses can lead to imprecisely reconstructed geometry and extracted meshes, especially in regions with high curvature or fine detail. We propose a novel regularization method using the gradients of a signed distance function estimated from the Gaussians, to improve the quality of rendering while also extracting a surface mesh. The regularizing normal supervision facilitates better rendering and mesh reconstruction, which is crucial for downstream applications in video generation, animation, AR-VR and gaming. We demonstrate the effectiveness of our approach on datasets such as Mip-NeRF360, Tanks and Temples, and Deep-Blending. Our method scores higher on photorealism metrics compared to other mesh extracting rendering methods without compromising mesh quality.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Video Generation"
        ]
    },
    {
        "id": "6",
        "title": "OptiChat: Bridging Optimization Models and Practitioners with Large Language Models",
        "author": [
            "Hao Chen",
            "Gonzalo Esteban Constante-Flores",
            "Krishna Sri Ipsit Mantri",
            "Sai Madhukiran Kompalli",
            "Akshdeep Singh Ahluwalia",
            "Can Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08406",
        "abstract": "Optimization models have been applied to solve a wide variety of decision-making problems. These models are usually developed by optimization experts but are used by practitioners without optimization expertise in various application domains. As a result, practitioners often struggle to interact with and draw useful conclusions from optimization models independently. To fill this gap, we introduce OptiChat, a natural language dialogue system designed to help practitioners interpret model formulation, diagnose infeasibility, analyze sensitivity, retrieve information, evaluate modifications, and provide counterfactual explanations. By augmenting large language models (LLMs) with functional calls and code generation tailored for optimization models, we enable seamless interaction and minimize the risk of hallucinations in OptiChat. We develop a new dataset to evaluate OptiChat's performance in explaining optimization models. Experiments demonstrate that OptiChat effectively bridges the gap between optimization models and practitioners, delivering autonomous, accurate, and instant responses.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "7",
        "title": "Ensemble of Large Language Models for Curated Labeling and Rating of Free-text Data",
        "author": [
            "Jiaxing Qiu",
            "Dongliang Guo",
            "Papini Natalie",
            "Peace Noelle",
            "Levinson Cheri",
            "Teague R. Henry"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08413",
        "abstract": "Free-text responses are commonly collected in psychological studies, providing rich qualitative insights that quantitative measures may not capture. Labeling curated topics of research interest in free-text data by multiple trained human coders is typically labor-intensive and time-consuming. Though large language models (LLMs) excel in language processing, LLM-assisted labeling techniques relying on closed-source LLMs cannot be directly applied to free-text data, without explicit consent for external use.\nIn this study, we propose a framework of assembling locally-deployable LLMs to enhance the labeling of predetermined topics in free-text data under privacy constraints. Analogous to annotation by multiple human raters, this framework leverages the heterogeneity of diverse open-source LLMs. The ensemble approach seeks a balance between the agreement and disagreement across LLMs, guided by a relevancy scoring methodology that utilizes embedding distances between topic descriptions and LLMs' reasoning. We evaluated the ensemble approach using both publicly accessible Reddit data from eating disorder related forums, and free-text responses from eating disorder patients, both complemented by human annotations.\nWe found that: (1) there is heterogeneity in the performance of labeling among same-sized LLMs, with some showing low sensitivity but high precision, while others exhibit high sensitivity but low precision. (2) Compared to individual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal precision-sensitivity trade-off in predicting human annotations. (3) The relevancy scores across LLMs showed greater agreement than dichotomous labels, indicating that the relevancy scoring method effectively mitigates the heterogeneity in LLMs' labeling.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "8",
        "title": "Is Stochastic Gradient Descent Effective? A PDE Perspective on Machine Learning processes",
        "author": [
            "Davide Barbieri",
            "Matteo Bonforte",
            "Peio Ibarrondo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08425",
        "abstract": "In this paper we analyze the behaviour of the stochastic gradient descent (SGD), a widely used method in supervised learning for optimizing neural network weights via a minimization of non-convex loss functions. Since the pioneering work of E, Li and Tai (2017), the underlying structure of such processes can be understood via parabolic PDEs of Fokker-Planck type, which are at the core of our analysis. Even if Fokker-Planck equations have a long history and a extensive literature, almost nothing is known when the potential is non-convex or when the diffusion matrix is degenerate, and this is the main difficulty that we face in our analysis.\nWe identify two different regimes: in the initial phase of SGD, the loss function drives the weights to concentrate around the nearest local minimum. We refer to this phase as the drift regime and we provide quantitative estimates on this concentration phenomenon. Next, we introduce the diffusion regime, where stochastic fluctuations help the learning process to escape suboptimal local minima. We analyze the Mean Exit Time (MET) and prove upper and lower bounds of the MET. Finally, we address the asymptotic convergence of SGD, for a non-convex cost function and a degenerate diffusion matrix, that do not allow to use the standard approaches, and require new techniques. For this purpose, we exploit two different methods: duality and entropy methods.\nWe provide new results about the dynamics and effectiveness of SGD, offering a deep connection between stochastic optimization and PDE theory, and some answers and insights to basic questions in the Machine Learning processes: How long does SGD take to escape from a bad minimum? Do neural network parameters converge using SGD? How do parameters evolve in the first stage of training with SGD?",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "9",
        "title": "Religious Bias Landscape in Language and Text-to-Image Models: Analysis, Detection, and Debiasing Strategies",
        "author": [
            "Ajwad Abrar",
            "Nafisa Tabassum Oeshy",
            "Mohsinul Kabir",
            "Sophia Ananiadou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08441",
        "abstract": "Note: This paper includes examples of potentially offensive content related to religious bias, presented solely for academic purposes. The widespread adoption of language models highlights the need for critical examinations of their inherent biases, particularly concerning religion. This study systematically investigates religious bias in both language models and text-to-image generation models, analyzing both open-source and closed-source systems. We construct approximately 400 unique, naturally occurring prompts to probe language models for religious bias across diverse tasks, including mask filling, prompt completion, and image generation. Our experiments reveal concerning instances of underlying stereotypes and biases associated disproportionately with certain religions. Additionally, we explore cross-domain biases, examining how religious bias intersects with demographic factors such as gender, age, and nationality. This study further evaluates the effectiveness of targeted debiasing techniques by employing corrective prompts designed to mitigate the identified biases. Our findings demonstrate that language models continue to exhibit significant biases in both text and image generation tasks, emphasizing the urgent need to develop fairer language models to achieve global acceptability.",
        "tags": [
            "Detection",
            "Text-to-Image"
        ]
    },
    {
        "id": "10",
        "title": "Instruction-Guided Fusion of Multi-Layer Visual Features in Large Vision-Language Models",
        "author": [
            "Xu Li",
            "Yi Zheng",
            "Haotian Chen",
            "Xiaolei Chen",
            "Yuxuan Liang",
            "Chenghang Lai"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08443",
        "abstract": "Large Vision-Language Models (LVLMs) have achieved significant success in multimodal tasks by combining pre-trained vision encoders and large language models. However, current LVLMs mainly rely on features from the final layers of the vision encoder, neglecting complementary information in shallower layers. While recent methods have explored multi-layer features, they are often task-agnostic. We investigate the contributions of visual features from different encoder layers across 18 benchmarks and 6 task categories. Our results show that multi-layer features provide complementary strengths with varying task dependencies, and uniform fusion performs suboptimally. Based on these findings, we propose an instruction-guided vision aggregator that dynamically integrates multi-layer features based on textual instructions, without increasing the number of visual tokens. Extensive evaluations show superior performance, and analysis reveals the dominance of mid-to-high-level features in semantic tasks and the critical role of low-level features in fine-grained perception. This work provides valuable insights into the adaptive use of hierarchical visual features in LVLMs, advancing more flexible multimodal systems.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "11",
        "title": "Poseidon: A ViT-based Architecture for Multi-Frame Pose Estimation with Adaptive Frame Weighting and Multi-Scale Feature Fusion",
        "author": [
            "Cesare Davide Pace",
            "Alessandro Marco De Nunzio",
            "Claudio De Stefano",
            "Francesco Fontanella",
            "Mario Molinara"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08446",
        "abstract": "Human pose estimation, a vital task in computer vision, involves detecting and localising human joints in images and videos. While single-frame pose estimation has seen significant progress, it often fails to capture the temporal dynamics for understanding complex, continuous movements. We propose Poseidon, a novel multi-frame pose estimation architecture that extends the ViTPose model by integrating temporal information for enhanced accuracy and robustness to address these limitations. Poseidon introduces key innovations: (1) an Adaptive Frame Weighting (AFW) mechanism that dynamically prioritises frames based on their relevance, ensuring that the model focuses on the most informative data; (2) a Multi-Scale Feature Fusion (MSFF) module that aggregates features from different backbone layers to capture both fine-grained details and high-level semantics; and (3) a Cross-Attention module for effective information exchange between central and contextual frames, enhancing the model's temporal coherence. The proposed architecture improves performance in complex video scenarios and offers scalability and computational efficiency suitable for real-world applications. Our approach achieves state-of-the-art performance on the PoseTrack21 and PoseTrack18 datasets, achieving mAP scores of 88.3 and 87.8, respectively, outperforming existing methods.",
        "tags": [
            "Pose Estimation",
            "ViT"
        ]
    },
    {
        "id": "12",
        "title": "Active Sampling for Node Attribute Completion on Graphs",
        "author": [
            "Benyuan Liu",
            "Xu Chen",
            "Yanfeng Wang",
            "Ya Zhang",
            "Zhi Cao",
            "Ivor Tsang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08450",
        "abstract": "Node attribute, a type of crucial information for graph analysis, may be partially or completely missing for certain nodes in real world applications. Restoring the missing attributes is expected to benefit downstream graph learning. Few attempts have been made on node attribute completion, but a novel framework called Structure-attribute Transformer (SAT) was recently proposed by using a decoupled scheme to leverage structures and attributes. SAT ignores the differences in contributing to the learning schedule and finding a practical way to model the different importance of nodes with observed attributes is challenging. This paper proposes a novel AcTive Sampling algorithm (ATS) to restore missing node attributes. The representativeness and uncertainty of each node's information are first measured based on graph structure, representation similarity and learning bias. To select nodes as train samples in the next optimization step, a weighting scheme controlled by Beta distribution is then introduced to linearly combine the two properties. Extensive experiments on four public benchmark datasets and two downstream tasks have shown the superiority of ATS in node attribute completion.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "13",
        "title": "Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models",
        "author": [
            "Weichen Fan",
            "Chenyang Si",
            "Junhao Song",
            "Zhenyu Yang",
            "Yinan He",
            "Long Zhuo",
            "Ziqi Huang",
            "Ziyue Dong",
            "Jingwen He",
            "Dongwei Pan",
            "Yi Wang",
            "Yuming Jiang",
            "Yaohui Wang",
            "Peng Gao",
            "Xinyuan Chen",
            "Hengjie Li",
            "Dahua Lin",
            "Yu Qiao",
            "Ziwei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08453",
        "abstract": "We present Vchitect-2.0, a parallel transformer architecture designed to scale up video diffusion models for large-scale text-to-video generation. The overall Vchitect-2.0 system has several key designs. (1) By introducing a novel Multimodal Diffusion Block, our approach achieves consistent alignment between text descriptions and generated video frames, while maintaining temporal coherence across sequences. (2) To overcome memory and computational bottlenecks, we propose a Memory-efficient Training framework that incorporates hybrid parallelism and other memory reduction techniques, enabling efficient training of long video sequences on distributed systems. (3) Additionally, our enhanced data processing pipeline ensures the creation of Vchitect T2V DataVerse, a high-quality million-scale training dataset through rigorous annotation and aesthetic evaluation. Extensive benchmarking demonstrates that Vchitect-2.0 outperforms existing methods in video quality, training efficiency, and scalability, serving as a suitable base for high-fidelity video generation.",
        "tags": [
            "Diffusion",
            "Text-to-Video",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "14",
        "title": "Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack",
        "author": [
            "Sagiv Antebi",
            "Edan Habler",
            "Asaf Shabtai",
            "Yuval Elovici"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08454",
        "abstract": "Large language models (LLMs) have become essential digital task assistance tools. Their training relies heavily on the collection of vast amounts of data, which may include copyright-protected or sensitive information. Recent studies on the detection of pretraining data in LLMs have primarily focused on sentence-level or paragraph-level membership inference attacks (MIAs), usually involving probability analysis of the target model prediction tokens. However, the proposed methods often demonstrate poor performance, specifically in terms of accuracy, failing to account for the semantic importance of textual content and word significance. To address these shortcomings, we propose Tag&Tab, a novel approach for detecting data that has been used as part of the LLM pretraining. Our method leverages advanced natural language processing (NLP) techniques to tag keywords in the input text - a process we term Tagging. Then, the LLM is used to obtain the probabilities of these keywords and calculate their average log-likelihood to determine input text membership, a process we refer to as Tabbing. Our experiments on three benchmark datasets (BookMIA, MIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate an average increase in the AUC scores ranging from 4.1% to 12.1% over state-of-the-art methods. Tag&Tab not only sets a new standard for data leakage detection in LLMs, but its outstanding performance is a testament to the importance of words in MIAs on LLMs.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "15",
        "title": "Large Language Models For Text Classification: Case Study And Comprehensive Review",
        "author": [
            "Arina Kostina",
            "Marios D. Dikaiakos",
            "Dimosthenis Stefanidis",
            "George Pallis"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08457",
        "abstract": "Unlocking the potential of Large Language Models (LLMs) in data classification represents a promising frontier in natural language processing. In this work, we evaluate the performance of different LLMs in comparison with state-of-the-art deep-learning and machine-learning models, in two different classification scenarios: i) the classification of employees' working locations based on job reviews posted online (multiclass classification), and 2) the classification of news articles as fake or not (binary classification). Our analysis encompasses a diverse range of language models differentiating in size, quantization, and architecture. We explore the impact of alternative prompting techniques and evaluate the models based on the weighted F1-score. Also, we examine the trade-off between performance (F1-score) and time (inference response time) for each language model to provide a more nuanced understanding of each model's practical applicability. Our work reveals significant variations in model responses based on the prompting strategies. We find that LLMs, particularly Llama3 and GPT-4, can outperform traditional methods in complex classification tasks, such as multiclass classification, though at the cost of longer inference times. In contrast, simpler ML models offer better performance-to-time trade-offs in simpler binary classification tasks.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "16",
        "title": "Towards Zero-Shot & Explainable Video Description by Reasoning over Graphs of Events in Space and Time",
        "author": [
            "Mihai Masala",
            "Marius Leordeanu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08460",
        "abstract": "In the current era of Machine Learning, Transformers have become the de facto approach across a variety of domains, such as computer vision and natural language processing. Transformer-based solutions are the backbone of current state-of-the-art methods for language generation, image and video classification, segmentation, action and object recognition, among many others. Interestingly enough, while these state-of-the-art methods produce impressive results in their respective domains, the problem of understanding the relationship between vision and language is still beyond our reach. In this work, we propose a common ground between vision and language based on events in space and time in an explainable and programmatic way, to connect learning-based vision and language state of the art models and provide a solution to the long standing problem of describing videos in natural language. We validate that our algorithmic approach is able to generate coherent, rich and relevant textual descriptions on videos collected from a variety of datasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern LLM-as-a-Jury approach.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "17",
        "title": "Time series forecasting for multidimensional telemetry data using GAN and BiLSTM in a Digital Twin",
        "author": [
            "Joao Carmo de Almeida Neto",
            "Claudio Miceli de Farias",
            "Leandro Santiago de Araujo",
            "Leopoldo Andre Dutra Lusquino Filho"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08464",
        "abstract": "The research related to digital twins has been increasing in recent years. Besides the mirroring of the physical word into the digital, there is the need of providing services related to the data collected and transferred to the virtual world. One of these services is the forecasting of physical part future behavior, that could lead to applications, like preventing harmful events or designing improvements to get better performance. One strategy used to predict any system operation it is the use of time series models like ARIMA or LSTM, and improvements were implemented using these algorithms. Recently, deep learning techniques based on generative models such as Generative Adversarial Networks (GANs) have been proposed to create time series and the use of LSTM has gained more relevance in time series forecasting, but both have limitations that restrict the forecasting results. Another issue found in the literature is the challenge of handling multivariate environments/applications in time series generation. Therefore, new methods need to be studied in order to fill these gaps and, consequently, provide better resources for creating useful digital twins. In this proposal, it is going to be studied the integration of a BiLSTM layer with a time series obtained by GAN in order to improve the forecasting of all the features provided by the dataset in terms of accuracy and, consequently, improving behaviour prediction.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "18",
        "title": "Quantifying the Importance of Data Alignment in Downstream Model Performance",
        "author": [
            "Krrish Chawla",
            "Aryan Sahai",
            "Mario DePavia",
            "Sudharsan Sundar",
            "Brando Miranda"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08496",
        "abstract": "Contrary to the conventional emphasis on dataset size, we explore the role of data alignment -- an often overlooked aspect of data quality -- in training capable Large Language Models (LLMs). To do so, we use the Task2Vec-based alignment coefficient, a quantitative measure of the similarity between two datasets, to quantify the impact of alignment between training data and evaluation data on downstream performance. In particular, we conduct controlled \\textit{interventional} experiments for two settings: 1. the impact of increased alignment coefficients between various pre-training (pt) against evaluation datasets, and 2. the impact of increased alignment coefficients between domain specific fine-tuning (ft) against domain specific evaluation. The domain specific task we explore is Autoformalization -- the machine translation task between natural language and code for formal verification. In both settings, we find a strong, predictable negative correlation between the alignment coefficient of a model's training and evaluation data and the model's loss/perplexity on the respective downstream task. These findings suggest a re-evaluation of LLM training approaches, demonstrating the relevance of data alignment compared to data quantity, especially in specialized downstream tasks such as Autoformalization.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "19",
        "title": "Heterogeneous Update Processes Shape Information Cascades in Social Networks",
        "author": [
            "FlÃ¡vio L. Pinheiro",
            "VÃ­tor V. Vasconcelos"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08498",
        "abstract": "A common assumption in the literature on information diffusion is that populations are homogeneous regarding individuals' information acquisition and propagation process: Individuals update their informed and actively communicating state either through imitation (simple contagion) or peer influence (complex contagion). Here, we study the impact of the mixing and placement of individuals with different update processes on how information cascades in social networks. We consider Simple Spreaders, which take information from a random neighbor and communicate it, and Threshold-based Spreaders, which require a threshold number of active neighbors to change their state to active communication. Even though, in a population made exclusively of Simple Spreaders, information reaches all elements of any (connected) network, we show that, when Simple and Threshold-based Spreaders coexist and occupy random positions in a social network, the number of Simple Spreaders systematically amplifies the cascades only in degree heterogeneous networks (exponential and scale-free). In random and modular structures, this cascading effect originated by Simple Spreaders only exists above a critical mass of these individuals. In contrast, when Threshold-based Spreaders are assorted preferentially in the nodes with a higher degree, the cascading effect of Simple Spreaders vanishes, and the spread of information is drastically impaired. Overall, the study highlights the significance of the strategic placement of different roles in networked structures, with Simple Spreaders driving widespread cascades in heterogeneous networks and Threshold-based Spreaders playing a critical regulatory role in information spread with a tunable effect based on the threshold value.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "20",
        "title": "Scalable Bayesian Physics-Informed Kolmogorov-Arnold Networks",
        "author": [
            "Zhiwei Gao",
            "George Em Karniadakis"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08501",
        "abstract": "Uncertainty quantification (UQ) plays a pivotal role in scientific machine learning, especially when surrogate models are used to approximate complex systems. Although multilayer perceptions (MLPs) are commonly employed as surrogates, they often suffer from overfitting due to their large number of parameters. Kolmogorov-Arnold networks (KANs) offer an alternative solution with fewer parameters. However, gradient-based inference methods, such as Hamiltonian Monte Carlo (HMC), may result in computational inefficiency when applied to KANs, especially for large-scale datasets, due to the high cost of http://back-propagation.To address these challenges, we propose a novel approach, combining the dropout Tikhonov ensemble Kalman inversion (DTEKI) with Chebyshev KANs. This gradient-free method effectively mitigates overfitting and enhances numerical stability. Additionally, we incorporate the active subspace method to reduce the parameter-space dimensionality, allowing us to improve the accuracy of predictions and obtain more reliable uncertainty http://estimates.Extensive experiments demonstrate the efficacy of our approach in various test cases, including scenarios with large datasets and high noise levels. Our results show that the new method achieves comparable or better accuracy, much higher efficiency as well as stability compared to HMC, in addition to scalability. Moreover, by leveraging the low-dimensional parameter subspace, our method preserves prediction accuracy while substantially reducing further the computational cost.",
        "tags": [
            "Kolmogorov-Arnold Networks"
        ]
    },
    {
        "id": "21",
        "title": "SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and Unstructured Parameter Prioritization",
        "author": [
            "Waqwoya Abebe",
            "Sadegh Jafari",
            "Sixing Yu",
            "Akash Dutta",
            "Jan Strube",
            "Nathan R. Tallent",
            "Luanzheng Guo",
            "Pablo Munoz",
            "Ali Jannesari"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08504",
        "abstract": "Neural Architecture Search (NAS) is a powerful approach of automating the design of efficient neural architectures. In contrast to traditional NAS methods, recently proposed one-shot NAS methods prove to be more efficient in performing NAS. One-shot NAS works by generating a singular weight-sharing supernetwork that acts as a search space (container) of subnetworks. Despite its achievements, designing the one-shot search space remains a major challenge. In this work we propose a search space design strategy for Vision Transformer (ViT)-based architectures. In particular, we convert the Segment Anything Model (SAM) into a weight-sharing supernetwork called SuperSAM. Our approach involves automating the search space design via layer-wise structured pruning and parameter prioritization. While the structured pruning applies probabilistic removal of certain transformer layers, parameter prioritization performs weight reordering and slicing of MLP-blocks in the remaining layers. We train supernetworks on several datasets using the sandwich rule. For deployment, we enhance subnetwork discovery by utilizing a program autotuner to identify efficient subnetworks within the search space. The resulting subnetworks are 30-70% smaller in size compared to the original pre-trained SAM ViT-B, yet outperform the pretrained model. Our work introduces a new and effective method for ViT NAS search-space design.",
        "tags": [
            "SAM",
            "Segment Anything",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "22",
        "title": "Yuan: Yielding Unblemished Aesthetics Through A Unified Network for Visual Imperfections Removal in Generated Images",
        "author": [
            "Zhenyu Yu",
            "Chee Seng Chan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08505",
        "abstract": "Generative AI presents transformative potential across various domains, from creative arts to scientific visualization. However, the utility of AI-generated imagery is often compromised by visual flaws, including anatomical inaccuracies, improper object placements, and misplaced textual elements. These imperfections pose significant challenges for practical applications. To overcome these limitations, we introduce \\textit{Yuan}, a novel framework that autonomously corrects visual imperfections in text-to-image synthesis. \\textit{Yuan} uniquely conditions on both the textual prompt and the segmented image, generating precise masks that identify areas in need of refinement without requiring manual intervention -- a common constraint in previous methodologies. Following the automated masking process, an advanced inpainting module seamlessly integrates contextually coherent content into the identified regions, preserving the integrity and fidelity of the original image and associated text prompts. Through extensive experimentation on publicly available datasets such as ImageNet100 and Stanford Dogs, along with a custom-generated dataset, \\textit{Yuan} demonstrated superior performance in eliminating visual imperfections. Our approach consistently achieved higher scores in quantitative metrics, including NIQE, BRISQUE, and PI, alongside favorable qualitative evaluations. These results underscore \\textit{Yuan}'s potential to significantly enhance the quality and applicability of AI-generated images across diverse fields.",
        "tags": [
            "Inpainting",
            "Text-to-Image"
        ]
    },
    {
        "id": "23",
        "title": "Multimodal Fake News Video Explanation Generation",
        "author": [
            "Lizhi Chen",
            "Zhong Qian",
            "Peifeng Li",
            "Qiaoming Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08514",
        "abstract": "Multi-modal explanation involves the assessment of the veracity of a variety of different content, and relies on multiple information modalities to comprehensively consider the relevance and consistency between modalities. Most existing fake news video detection methods focus on improving accuracy while ignoring the importance of providing explanations. In this paper, we propose a novel problem - Fake News Video Explanation (FNVE) - Given a multimodal news containing both video and caption text, we aim to generate natural language explanations to reveal the truth of predictions. To this end, we develop FakeNVE, a new dataset of explanations for truthfully multimodal posts, where each explanation is a natural language (English) sentence describing the attribution of a news thread. We benchmark FakeNVE by using a multimodal transformer-based architecture. Subsequently, a BART-based autoregressive decoder is used as the generator. Empirical results show compelling results for various baselines (applicable to FNVE) across multiple evaluation metrics. We also perform human evaluation on explanation generation, achieving high scores for both adequacy and fluency.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "24",
        "title": "Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for Document-level Machine Translation",
        "author": [
            "Jiaxin Guo",
            "Yuanchang Luo",
            "Daimeng Wei",
            "Ling Zhang",
            "Zongyao Li",
            "Hengchao Shang",
            "Zhiqiang Rao",
            "Shaojun Li",
            "Jinlong Yang",
            "Zhanglin Wu",
            "Hao Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08523",
        "abstract": "The field of artificial intelligence has witnessed significant advancements in natural language processing, largely attributed to the capabilities of Large Language Models (LLMs). These models form the backbone of Agents designed to address long-context dependencies, particularly in Document-level Machine Translation (DocMT). DocMT presents unique challenges, with quality, consistency, and fluency being the key metrics for evaluation. Existing approaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise fluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an incremental sentence-level forced decoding strategy \\textbf{to ensure every sentence is translated while enhancing the fluency of adjacent sentences.} Our Agent leverages a Doc-Guided Memory, focusing solely on the summary and its translation, which we find to be an efficient approach to maintaining consistency. Through extensive testing across multiple languages and domains, we demonstrate that Sent2Sent++ outperforms other methods in terms of quality, consistency, and fluency. The results indicate that, our approach has achieved significant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and document-level perplexity (d-ppl). The contributions of this paper include a detailed analysis of current DocMT research, the introduction of the Sent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of its effectiveness across languages and domains.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "25",
        "title": "Knowledge prompt chaining for semantic modeling",
        "author": [
            "Ning Pei Ding",
            "Jingge Du",
            "Zaiwen Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08540",
        "abstract": "The task of building semantics for structured data such as CSV, JSON, and XML files is highly relevant in the knowledge representation field. Even though we have a vast of structured data on the internet, mapping them to domain ontologies to build semantics for them is still very challenging as it requires the construction model to understand and learn graph-structured knowledge. Otherwise, the task will require human beings' effort and cost. In this paper, we proposed a novel automatic semantic modeling framework: Knowledge Prompt Chaining. It can serialize the graph-structured knowledge and inject it into the LLMs properly in a Prompt Chaining architecture. Through this knowledge injection and prompting chaining, the model in our framework can learn the structure information and latent space of the graph and generate the semantic labels and semantic graphs following the chains' insturction naturally. Based on experimental results, our method achieves better performance than existing leading techniques, despite using reduced structured input data.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "26",
        "title": "Comprehensive Subjective and Objective Evaluation Method for Text-generated Video",
        "author": [
            "Zelu Qi",
            "Ping Shi",
            "Shuqi Wang",
            "Zhaoyang Zhang",
            "Zefeng Ying",
            "Da Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08545",
        "abstract": "Recent text-to-video (T2V) technology advancements, as demonstrated by models such as Gen3, Pika, and Sora, have significantly broadened its applicability and popularity. This progress has created a growing demand for accurate quality assessment metrics to evaluate the perceptual quality of text-generated videos and optimize video generation models. However, assessing the quality of text-generated videos remains challenging due to the presence of highly complex distortions, such as unnatural actions and phenomena that defy human cognition. To address these challenges, we constructed a large-scale benchmark dataset for \\textbf{T}ext-generated \\textbf{V}ideo \\textbf{eval}uation, \\textbf{T2VEval-Bench}, comprising 148 textual words and 1,783 videos generated by 12 models. During the subjective evaluation, we collected five key scores: overall impression, video quality, aesthetic quality, realness, and text-video consistency. For objective evaluation, we developed the \\textbf{T2VEval} model, which assesses videos across three branches: quality, authenticity, and consistency. Using an attention-based fusion module, T2VEval effectively integrates features from each branch and predicts scores with the aid of a large oracle model. Additionally, we implemented a progressive training strategy, enabling each branch to learn targeted knowledge while maintaining synergy with the others. Experimental results demonstrate that T2VEval achieves state-of-the-art performance across multiple metrics. The dataset and code will be open-sourced upon completion of the follow-up work.",
        "tags": [
            "Sora",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "27",
        "title": "The Devil is in Temporal Token: High Quality Video Reasoning Segmentation",
        "author": [
            "Sitong Gong",
            "Yunzhi Zhuge",
            "Lu Zhang",
            "Zongxin Yang",
            "Pingping Zhang",
            "Huchuan Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08549",
        "abstract": "Existing methods for Video Reasoning Segmentation rely heavily on a single special token to represent the object in the keyframe or the entire video, inadequately capturing spatial complexity and inter-frame motion. To overcome these challenges, we propose VRS-HQ, an end-to-end video reasoning segmentation approach that leverages Multimodal Large Language Models (MLLMs) to inject rich spatiotemporal features into hierarchical http://tokens.Our key innovations include a Temporal Dynamic Aggregation (TDA) and a Token-driven Keyframe Selection (TKS). Specifically, we design frame-level <SEG> and temporal-level <TAK> tokens that utilize MLLM's autoregressive learning to effectively capture both local and global information. Subsequently, we apply a similarity-based weighted fusion and frame selection strategy, then utilize SAM2 to perform keyframe segmentation and propagation. To enhance keyframe localization accuracy, the TKS filters keyframes based on SAM2's occlusion scores during inference. VRS-HQ achieves state-of-the-art performance on ReVOS, surpassing VISA by 5.9%/12.5%/9.1% in J&F scores across the three subsets. These results highlight the strong temporal reasoning and segmentation capabilities of our method. Code and model weights will be released at VRS-HQ.",
        "tags": [
            "Large Language Models",
            "Segmentation"
        ]
    },
    {
        "id": "28",
        "title": "DynamicFace: High-Quality and Consistent Video Face Swapping using Composable 3D Facial Priors",
        "author": [
            "Runqi Wang",
            "Sijie Xu",
            "Tianyao He",
            "Yang Chen",
            "Wei Zhu",
            "Dejia Song",
            "Nemo Chen",
            "Xu Tang",
            "Yao Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08553",
        "abstract": "Face swapping transfers the identity of a source face to a target face while retaining the attributes like expression, pose, hair, and background of the target face. Advanced face swapping methods have achieved attractive results. However, these methods often inadvertently transfer identity information from the target face, compromising expression-related details and accurate identity. We propose a novel method DynamicFace that leverages the power of diffusion model and plug-and-play temporal layers for video face swapping. First, we introduce four fine-grained face conditions using 3D facial priors. All conditions are designed to be disentangled from each other for precise and unique control. Then, we adopt Face Former and ReferenceNet for high-level and detailed identity injection. Through experiments on the FF++ dataset, we demonstrate that our method achieves state-of-the-art results in face swapping, showcasing superior image quality, identity preservation, and expression accuracy. Besides, our method could be easily transferred to video domain with temporal attention layer. Our code and results will be available on the project page: https://dynamic-face.github.io/",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "29",
        "title": "LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation",
        "author": [
            "Yiran Tao",
            "Jehan Yang",
            "Dan Ding",
            "Zackory Erickson"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08558",
        "abstract": "Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF controllers like joysticks often requires frequent switching between control modes, where each mode maps controller movements to specific robot actions. Manually performing this frequent switching can make teleoperation cumbersome and inefficient. On the other hand, existing automatic mode-switching solutions, such as heuristic-based or learning-based methods, are often task-specific and lack generalizability. In this paper, we introduce LLM-Driven Automatic Mode Switching (LAMS), a novel approach that leverages Large Language Models (LLMs) to automatically switch control modes based on task context. Unlike existing methods, LAMS requires no prior task demonstrations and incrementally improves by integrating user-generated mode-switching examples. We validate LAMS through an ablation study and a user study with 10 participants on complex, long-horizon tasks, demonstrating that LAMS effectively reduces manual mode switches, is preferred over alternative methods, and improves performance over time. The project website with supplementary materials is at https://lams-assistance.github.io/.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Robot"
        ]
    },
    {
        "id": "30",
        "title": "Adaptive Sampled Softmax with Inverted Multi-Index: Methods, Theory and Applications",
        "author": [
            "Jin Chen",
            "Jin Zhang",
            "Xu huang",
            "Yi Yang",
            "Defu Lian",
            "Enhong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08563",
        "abstract": "The softmax function is a cornerstone of multi-class classification, integral to a wide range of machine learning applications, from large-scale retrieval and ranking models to advanced large language models. However, its computational cost grows linearly with the number of classes, which becomes prohibitively expensive in scenarios with millions or even billions of classes. The sampled softmax, which relies on self-normalized importance sampling, has emerged as a powerful alternative, significantly reducing computational complexity. Yet, its estimator remains unbiased only when the sampling distribution matches the true softmax distribution. To improve both approximation accuracy and sampling efficiency, we propose the MIDX Sampler, a novel adaptive sampling strategy based on an inverted multi-index approach. Concretely, we decompose the softmax probability into several multinomial probabilities, each associated with a specific set of codewords and the last associated with the residual score of queries, thus reducing time complexity to the number of codewords instead of the number of classes. To further boost efficiency, we replace the query-specific residual probability with a simple uniform distribution, simplifying the computation while retaining high performance. Our method is backed by rigorous theoretical analysis, addressing key concerns such as sampling bias, gradient bias, convergence rates, and generalization error bounds. The results demonstrate that a smaller divergence from the ideal softmax distribution leads to faster convergence and improved generalization. Extensive experiments on large-scale language models, sequential recommenders, and extreme multi-class classification tasks confirm that the MIDX-Sampler delivers superior effectiveness and efficiency compared to existing approaches.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "31",
        "title": "Information Entropy Invariance: Enhancing Length Extrapolation in Attention Mechanisms",
        "author": [
            "Kewei Li",
            "Yanwen Kong",
            "Yiping Xu",
            "Lan Huang",
            "Ruochi Zhang",
            "Fengfeng Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08570",
        "abstract": "Improving the length extrapolation capabilities of Large Language Models (LLMs) remains a critical challenge in natural language processing. Many recent efforts have focused on modifying the scaled dot-product attention mechanism, and often introduce scaled temperatures without rigorous theoretical justification. To fill this gap, we introduce a novel approach based on information entropy invariance. We propose two new scaled temperatures to enhance length extrapolation. First, a training-free method InfoScale is designed for dot-product attention, and preserves focus on original tokens during length extrapolation by ensuring information entropy remains consistent. Second, we theoretically analyze the impact of scaling (CosScale) on cosine attention. Experimental data demonstrates that combining InfoScale and CosScale achieves state-of-the-art performance on the GAU-{\\alpha} model with a context window extended to 64 times the training length, and outperforms seven existing methods. Our analysis reveals that significantly increasing CosScale approximates windowed attention, and highlights the significance of attention score dilution as a key challenge in long-range context handling. The code and data are available at https://github.com/HT-NEKO/InfoScale.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "32",
        "title": "What Limits LLM-based Human Simulation: LLMs or Our Design?",
        "author": [
            "Qian Wang",
            "Jiaying Wu",
            "Zhenheng Tang",
            "Bingqiao Luo",
            "Nuo Chen",
            "Wei Chen",
            "Bingsheng He"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08579",
        "abstract": "We argue that advancing LLM-based human simulation requires addressing both LLM's inherent limitations and simulation framework design challenges. Recent studies have revealed significant gaps between LLM-based human simulations and real-world observations, highlighting these dual challenges. To address these gaps, we present a comprehensive analysis of LLM limitations and our design issues, proposing targeted solutions for both aspects. Furthermore, we explore future directions that address both challenges simultaneously, particularly in data collection, LLM generation, and evaluation. To support further research in this field, we provide a curated collection of LLM-based human simulation resources.\\footnote{https://github.com/Persdre/llm-human-simulation}",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "33",
        "title": "Densely Connected Parameter-Efficient Tuning for Referring Image Segmentation",
        "author": [
            "Jiaqi Huang",
            "Zunnan Xu",
            "Ting Liu",
            "Yong Liu",
            "Haonan Han",
            "Kehong Yuan",
            "Xiu Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08580",
        "abstract": "In the domain of computer vision, Parameter-Efficient Tuning (PET) is increasingly replacing the traditional paradigm of pre-training followed by full fine-tuning. PET is particularly favored for its effectiveness in large foundation models, as it streamlines transfer learning costs and optimizes hardware utilization. However, the current PET methods are mainly designed for single-modal optimization. While some pioneering studies have undertaken preliminary explorations, they still remain at the level of aligned encoders (e.g., CLIP) and lack exploration of misaligned encoders. These methods show sub-optimal performance with misaligned encoders, as they fail to effectively align the multimodal features during fine-tuning. In this paper, we introduce DETRIS, a parameter-efficient tuning framework designed to enhance low-rank visual feature propagation by establishing dense interconnections between each layer and all preceding layers, which enables effective cross-modal feature interaction and adaptation to misaligned encoders. We also suggest using text adapters to improve textual features. Our simple yet efficient approach greatly surpasses state-of-the-art methods with 0.9% to 1.8% backbone parameter updates, evaluated on challenging benchmarks. Our project is available at \\url{https://github.com/jiaqihuang01/DETRIS}.",
        "tags": [
            "CLIP",
            "Segmentation"
        ]
    },
    {
        "id": "34",
        "title": "LoRS: Efficient Low-Rank Adaptation for Sparse Large Language Model",
        "author": [
            "Yuxuan Hu",
            "Jing Zhang",
            "Xiaodong Chen",
            "Zhe Zhao",
            "Cuiping Li",
            "Hong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08582",
        "abstract": "Existing low-rank adaptation (LoRA) methods face challenges on sparse large language models (LLMs) due to the inability to maintain sparsity. Recent works introduced methods that maintain sparsity by augmenting LoRA techniques with additional masking mechanisms. Despite these successes, such approaches suffer from an increased memory and computation overhead, which affects efficiency of LoRA methods. In response to this limitation, we introduce LoRS, an innovative method designed to achieve both memory and computation efficiency when fine-tuning sparse LLMs. To mitigate the substantial memory and computation demands associated with preserving sparsity, our approach incorporates strategies of weight recompute and computational graph rearrangement. In addition, we also improve the effectiveness of LoRS through better adapter initialization. These innovations lead to a notable reduction in memory and computation consumption during the fine-tuning phase, all while achieving performance levels that outperform existing LoRA approaches.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "35",
        "title": "LlamaRestTest: Effective REST API Testing with Small Language Models",
        "author": [
            "Myeongsoo Kim",
            "Saurabh Sinha",
            "Alessandro Orso"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08598",
        "abstract": "Modern web services rely heavily on REST APIs, typically documented using the OpenAPI specification. The widespread adoption of this standard has resulted in the development of many black-box testing tools that generate tests based on these specifications. Recent advancements in Natural Language Processing (NLP), particularly with Large Language Models (LLMs), have enhanced REST API testing by extracting actionable rules and generating input values from the human-readable portions of the specification. However, these advancements overlook the potential of continuously refining the identified rules and test inputs based on server responses. To address this limitation, we present LlamaRestTest, a novel approach that employs two custom LLMs to generate realistic test inputs and uncover parameter dependencies during the testing process by incorporating server responses. These LLMs are created by fine-tuning the Llama3-8b model, using mined datasets of REST API example values and inter-parameter dependencies. We evaluated LlamaRestTest on 12 real-world services (including popular services such as Spotify), comparing it against RESTGPT, a GPT-powered specification-enhancement tool, as well as several state-of-the-art REST API testing tools, including RESTler, MoRest, EvoMaster, and ARAT-RL. Our results show that fine-tuning enables smaller LLMs to outperform larger models in detecting actionable rules and generating inputs for REST API testing. We evaluated configurations from the base Llama3-8B to fine-tuned versions and explored 2-bit, 4-bit, and 8-bit quantization for efficiency. LlamaRestTest surpasses state-of-the-art tools in code coverage and error detection, even with RESTGPT-enhanced specifications, and an ablation study highlights the impact of its novel components.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models",
            "RL"
        ]
    },
    {
        "id": "36",
        "title": "Assessing the Alignment of FOL Closeness Metrics with Human Judgement",
        "author": [
            "Ramya Keerthy Thatikonda",
            "Wray Buntine",
            "Ehsan Shareghi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08613",
        "abstract": "The recent successful paradigm of solving logical reasoning problems with tool-augmented large language models (LLMs) leverages translation of natural language statements into First-Order Logic~(FOL) and external theorem provers. However, the correctness of FOL statements, comprising operators and text predicates, often goes unverified due to the lack of a reliable evaluation metric for comparing generated and ground-truth FOLs. In this paper, we present a comprehensive study of sensitivity of existing metrics and their alignment with human judgement on FOL evaluation. Using ground-truth FOLs, we carefully designed various perturbations on the ground-truth to assess metric sensitivity. We sample FOL translation candidates for natural language statements and measure the ranking alignment between automatic metrics and human annotators. Our empirical findings highlight oversensitivity in the n-gram metric BLEU for text perturbations, the semantic graph metric Smatch++ for structural perturbations, and FOL metric for operator perturbation. We also observe a closer alignment between BertScore and human judgement. Additionally, we show that combining metrics enhances both alignment and sensitivity compared to using individual metrics.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "37",
        "title": "Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in Large Language Models",
        "author": [
            "Aruna Sankaranarayanan",
            "Dylan Hadfield-Menell",
            "Aaron Mueller"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08618",
        "abstract": "All natural languages are structured hierarchically. In humans, this structural restriction is neurologically coded: when two grammars are presented with identical vocabularies, brain areas responsible for language processing are only sensitive to hierarchical grammars. Using large language models (LLMs), we investigate whether such functionally distinct hierarchical processing regions can arise solely from exposure to large-scale language distributions. We generate inputs using English, Italian, Japanese, or nonce words, varying the underlying grammars to conform to either hierarchical or linear/positional rules. Using these grammars, we first observe that language models show distinct behaviors on hierarchical versus linearly structured inputs. Then, we find that the components responsible for processing hierarchical grammars are distinct from those that process linear grammars; we causally verify this in ablation experiments. Finally, we observe that hierarchy-selective components are also active on nonce grammars; this suggests that hierarchy sensitivity is not tied to meaning, nor in-distribution inputs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "38",
        "title": "CT-PatchTST: Channel-Time Patch Time-Series Transformer for Long-Term Renewable Energy Forecasting",
        "author": [
            "Menghao Huo",
            "Kuan Lu",
            "Yuxiao Li",
            "Qiang Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08620",
        "abstract": "Accurately predicting renewable energy output is crucial for the efficient integration of solar and wind power into modern energy systems. This study develops and evaluates an advanced deep learning model, Channel-Time Patch Time-Series Transformer (CT-PatchTST), to forecast the power output of photovoltaic and wind energy systems using annual offshore wind power, onshore wind power, and solar power generation data from Denmark. While the original Patch Time-Series Transformer(PatchTST) model employs a channel-independent (CI) approach, it tends to overlook inter-channel relationships during training, potentially leading to a loss of critical information. To address this limitation and further leverage the benefits of increased data granularity brought by CI, we propose CT-PatchTST. This enhanced model improves the processing of inter-channel information while maintaining the advantages of the channel-independent approach. The predictive performance of CT-PatchTST is rigorously analyzed, demonstrating its ability to provide precise and reliable energy forecasts. This work contributes to improving the predictability of renewable energy systems, supporting their broader adoption and integration into energy grids.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "39",
        "title": "Self-Organizing Edge Computing Distribution Framework for Visual SLAM",
        "author": [
            "Jussi Kalliola",
            "Lauri Suomela",
            "Sergio Moreschini",
            "David HÃ¤stbacka"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08629",
        "abstract": "Localization within a known environment is a crucial capability for mobile robots. Simultaneous Localization and Mapping (SLAM) is a prominent solution to this problem. SLAM is a framework that consists of a diverse set of computational tasks ranging from real-time tracking to computation-intensive map optimization. This combination can present a challenge for resource-limited mobile robots. Previously, edge-assisted SLAM methods have demonstrated promising real-time execution capabilities by offloading heavy computations while performing real-time tracking onboard. However, the common approach of utilizing a client-server architecture for offloading is sensitive to server and network failures. In this article, we propose a novel edge-assisted SLAM framework capable of self-organizing fully distributed SLAM execution across a network of devices or functioning on a single device without connectivity. The architecture consists of three layers and is designed to be device-agnostic, resilient to network failures, and minimally invasive to the core SLAM system. We have implemented and demonstrated the framework for monocular ORB SLAM3 and evaluated it in both fully distributed and standalone SLAM configurations against the ORB SLAM3. The experiment results demonstrate that the proposed design matches the accuracy and resource utilization of the monolithic approach while enabling collaborative execution.",
        "tags": [
            "SLAM"
        ]
    },
    {
        "id": "40",
        "title": "SWSC: Shared Weight for Similar Channel in LLM",
        "author": [
            "Binrui Zeng",
            "Yongtao Tang",
            "Xiaodong Liu",
            "Xiaopeng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08631",
        "abstract": "Large language models (LLMs) have spurred development in multiple industries. However, the growing number of their parameters brings substantial storage and computing burdens, making it essential to explore model compression techniques for parameter reduction and easier deployment. We propose SWSC, an LLM compression method based on the concept of Shared Weight for Similar Channel. It uses the K-Means clustering algorithm to cluster model weights channel-by-channel, generating clusters with highly similar vectors within each. A representative vector from each cluster is selected to approximately replace all vectors in the cluster, significantly reducing the number of model weight parameters. However, approximate restoration will inevitably cause damage to the performance of the model. To tackle this issue, we perform singular value decomposition on the weight error values before and after compression and retain the larger singular values and their corresponding singular vectors to compensate for the accuracy. The experimental results show that our method can effectively ensure the performance of the compressed LLM even under low-precision conditions.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "41",
        "title": "Reassessing the Role of Chain-of-Thought in Sentiment Analysis: Insights and Limitations",
        "author": [
            "Kaiyuan Zheng",
            "Qinghua Zhao",
            "Lei Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08641",
        "abstract": "The relationship between language and thought remains an unresolved philosophical issue. Existing viewpoints can be broadly categorized into two schools: one asserting their independence, and another arguing that language constrains thought. In the context of large language models, this debate raises a crucial question: Does a language model's grasp of semantic meaning depend on thought processes? To explore this issue, we investigate whether reasoning techniques can facilitate semantic understanding. Specifically, we conceptualize thought as reasoning, employ chain-of-thought prompting as a reasoning technique, and examine its impact on sentiment analysis tasks. The experiments show that chain-of-thought has a minimal impact on sentiment analysis tasks. Both the standard and chain-of-thought prompts focus on aspect terms rather than sentiment in the generated content. Furthermore, counterfactual experiments reveal that the model's handling of sentiment tasks primarily depends on information from demonstrations. The experimental results support the first viewpoint.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "42",
        "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
        "author": [
            "Savya Khosla",
            "Kushal Kafle",
            "Simon Jenni",
            "Handong Zhao",
            "John Collomosse",
            "Jing Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08648",
        "abstract": "While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning, respectively). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we introduce MAGNET, an adaptation of decoder-only LLMs that enhances their ability to generate robust representations and infill missing text spans, while preserving their knowledge and text generation capabilities. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging future context, (3) retain the ability for open-ended text generation without exhibiting repetition problem, and (4) preserve the knowledge gained by the LLM during pretraining.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "43",
        "title": "Joint Learning of Depth and Appearance for Portrait Image Animation",
        "author": [
            "Xinya Ji",
            "Gaspard Zoss",
            "Prashanth Chandran",
            "Lingchen Yang",
            "Xun Cao",
            "Barbara Solenthaler",
            "Derek Bradley"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08649",
        "abstract": "2D portrait animation has experienced significant advancements in recent years. Much research has utilized the prior knowledge embedded in large generative diffusion models to enhance high-quality image manipulation. However, most methods only focus on generating RGB images as output, and the co-generation of consistent visual plus 3D output remains largely under-explored. In our work, we propose to jointly learn the visual appearance and depth simultaneously in a diffusion-based portrait image generator. Our method embraces the end-to-end diffusion paradigm and introduces a new architecture suitable for learning this conditional joint distribution, consisting of a reference network and a channel-expanded diffusion backbone. Once trained, our framework can be efficiently adapted to various downstream applications, such as facial depth-to-image and image-to-depth generation, portrait relighting, and audio-driven talking head animation with consistent 3D output.",
        "tags": [
            "3D",
            "Diffusion",
            "Talking Head"
        ]
    },
    {
        "id": "44",
        "title": "StereoGen: High-quality Stereo Image Generation from a Single Image",
        "author": [
            "Xianqi Wang",
            "Hao Yang",
            "Gangwei Xu",
            "Junda Cheng",
            "Min Lin",
            "Yong Deng",
            "Jinliang Zang",
            "Yurui Chen",
            "Xin Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08654",
        "abstract": "State-of-the-art supervised stereo matching methods have achieved amazing results on various benchmarks. However, these data-driven methods suffer from generalization to real-world scenarios due to the lack of real-world annotated data. In this paper, we propose StereoGen, a novel pipeline for high-quality stereo image generation. This pipeline utilizes arbitrary single images as left images and pseudo disparities generated by a monocular depth estimation model to synthesize high-quality corresponding right images. Unlike previous methods that fill the occluded area in warped right images using random backgrounds or using convolutions to take nearby pixels selectively, we fine-tune a diffusion inpainting model to recover the background. Images generated by our model possess better details and undamaged semantic structures. Besides, we propose Training-free Confidence Generation and Adaptive Disparity Selection. The former suppresses the negative effect of harmful pseudo ground truth during stereo training, while the latter helps generate a wider disparity distribution and better synthetic images. Experiments show that models trained under our pipeline achieve state-of-the-art zero-shot generalization results among all published methods. The code will be available upon publication of the paper.",
        "tags": [
            "Depth Estimation",
            "Diffusion",
            "Inpainting"
        ]
    },
    {
        "id": "45",
        "title": "BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with Multi-modality Refinement Module",
        "author": [
            "Dongzhihan Wang",
            "Yang Yang",
            "Liang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08659",
        "abstract": "Visual odometry (VO) plays a crucial role in autonomous driving, robotic navigation, and other related tasks by estimating the position and orientation of a camera based on visual input. Significant progress has been made in data-driven VO methods, particularly those leveraging deep learning techniques to extract image features and estimate camera poses. However, these methods often struggle in low-light conditions because of the reduced visibility of features and the increased difficulty of matching keypoints. To address this limitation, we introduce BrightVO, a novel VO model based on Transformer architecture, which not only performs front-end visual feature extraction, but also incorporates a multi-modality refinement module in the back-end that integrates Inertial Measurement Unit (IMU) data. Using pose graph optimization, this module iteratively refines pose estimates to reduce errors and improve both accuracy and robustness. Furthermore, we create a synthetic low-light dataset, KiC4R, which includes a variety of lighting conditions to facilitate the training and evaluation of VO frameworks in challenging environments. Experimental results demonstrate that BrightVO achieves state-of-the-art performance on both the KiC4R dataset and the KITTI benchmarks. Specifically, it provides an average improvement of 20% in pose estimation accuracy in normal outdoor environments and 259% in low-light conditions, outperforming existing methods. For widespread use and further development, the research work is fully open-source at https://github.com/Anastasiawd/BrightVO.",
        "tags": [
            "Pose Estimation",
            "Transformer"
        ]
    },
    {
        "id": "46",
        "title": "Augmenting Smart Contract Decompiler Output through Fine-grained Dependency Analysis and LLM-facilitated Semantic Recovery",
        "author": [
            "Zeqin Liao",
            "Yuhong Nan",
            "Zixu Gao",
            "Henglong Liang",
            "Sicheng Hao",
            "Peifan Reng",
            "Zibin Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08670",
        "abstract": "Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction. More specifically, \\system{} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39% for method boundaries, 90.39% for variable types, and 80.65% for contract attributes.",
        "tags": [
            "Detection",
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "47",
        "title": "GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused Odometry with Gaussian Mapping",
        "author": [
            "Sheng Hong",
            "Chunran Zheng",
            "Yishu Shen",
            "Changze Li",
            "Fu Zhang",
            "Tong Qin",
            "Shaojie Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08672",
        "abstract": "In recent years, 3D Gaussian splatting (3D-GS) has emerged as a novel scene representation approach. However, existing vision-only 3D-GS methods often rely on hand-crafted heuristics for point-cloud densification and face challenges in handling occlusions and high GPU memory and computation consumption. LiDAR-Inertial-Visual (LIV) sensor configuration has demonstrated superior performance in localization and dense mapping by leveraging complementary sensing characteristics: rich texture information from cameras, precise geometric measurements from LiDAR, and high-frequency motion data from IMU. Inspired by this, we propose a novel real-time Gaussian-based simultaneous localization and mapping (SLAM) system. Our map system comprises a global Gaussian map and a sliding window of Gaussians, along with an IESKF-based odometry. The global Gaussian map consists of hash-indexed voxels organized in a recursive octree, effectively covering sparse spatial volumes while adapting to different levels of detail and scales. The Gaussian map is initialized through multi-sensor fusion and optimized with photometric gradients. Our system incrementally maintains a sliding window of Gaussians, significantly reducing GPU computation and memory consumption by only optimizing the map within the sliding window. Moreover, we implement a tightly coupled multi-sensor fusion odometry with an iterative error state Kalman filter (IESKF), leveraging real-time updating and rendering of the Gaussian map. Our system represents the first real-time Gaussian-based SLAM framework deployable on resource-constrained embedded systems, demonstrated on the NVIDIA Jetson Orin NX platform. The framework achieves real-time performance while maintaining robust multi-sensor fusion capabilities. All implementation algorithms, hardware designs, and CAD models will be publicly available.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "SLAM"
        ]
    },
    {
        "id": "48",
        "title": "FlexiClip: Locality-Preserving Free-Form Character Animation",
        "author": [
            "Anant Khandelwal"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08676",
        "abstract": "Animating clipart images with seamless motion while maintaining visual fidelity and temporal coherence presents significant challenges. Existing methods, such as AniClipart, effectively model spatial deformations but often fail to ensure smooth temporal transitions, resulting in artifacts like abrupt motions and geometric distortions. Similarly, text-to-video (T2V) and image-to-video (I2V) models struggle to handle clipart due to the mismatch in statistical properties between natural video and clipart styles. This paper introduces FlexiClip, a novel approach designed to overcome these limitations by addressing the intertwined challenges of temporal consistency and geometric integrity. FlexiClip extends traditional BÃ©zier curve-based trajectory modeling with key innovations: temporal Jacobians to correct motion dynamics incrementally, continuous-time modeling via probability flow ODEs (pfODEs) to mitigate temporal noise, and a flow matching loss inspired by GFlowNet principles to optimize smooth motion transitions. These enhancements ensure coherent animations across complex scenarios involving rapid movements and non-rigid deformations. Extensive experiments validate the effectiveness of FlexiClip in generating animations that are not only smooth and natural but also structurally consistent across diverse clipart types, including humans and animals. By integrating spatial and temporal modeling with pre-trained video diffusion models, FlexiClip sets a new standard for high-quality clipart animation, offering robust performance across a wide range of visual content. Project Page: https://creative-gen.github.io/flexiclip.github.io/",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "Text-to-Video"
        ]
    },
    {
        "id": "49",
        "title": "Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching",
        "author": [
            "Chuangtao Ma",
            "Sriom Chakrabarti",
            "Arijit Khan",
            "BÃ¡lint MolnÃ¡r"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08686",
        "abstract": "Traditional similarity-based schema matching methods are incapable of resolving semantic ambiguities and conflicts in domain-specific complex mapping scenarios due to missing commonsense and domain-specific knowledge. The hallucination problem of large language models (LLMs) also makes it challenging for LLM-based schema matching to address the above issues. Therefore, we propose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema Matching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces novel vector-based, graph traversal-based, and query-based graph retrievals, as well as a hybrid approach and ranking schemes that identify the most relevant subgraphs from external large knowledge graphs (KGs). We showcase that KG-based retrieval-augmented LLMs are capable of generating more accurate results for complex matching cases without any re-training. Our experimental results show that KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g., Jellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the MIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the pre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and 21.97% in terms of precision and F1 score on the Synthea dataset, respectively. The results also demonstrate that our approach is more efficient in end-to-end schema matching, and scales to retrieve from large KGs. Our case studies on the dataset from the real-world schema matching scenario exhibit that the hallucination problem of LLMs for schema matching is well mitigated by our solution.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "50",
        "title": "Disentangled Interleaving Variational Encoding",
        "author": [
            "Noelle Y. L. Wong",
            "Eng Yeow Cheu",
            "Zhonglin Chiam"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08710",
        "abstract": "Conflicting objectives present a considerable challenge in interleaving multi-task learning, necessitating the need for meticulous design and balance to ensure effective learning of a representative latent data space across all tasks without mutual negative impact. Drawing inspiration from the concept of marginal and conditional probability distributions in probability theory, we design a principled and well-founded approach to disentangle the original input into marginal and conditional probability distributions in the latent space of a variational autoencoder. Our proposed model, Deep Disentangled Interleaving Variational Encoding (DeepDIVE) learns disentangled features from the original input to form clusters in the embedding space and unifies these features via the cross-attention mechanism in the fusion stage. We theoretically prove that combining the objectives for reconstruction and forecasting fully captures the lower bound and mathematically derive a loss function for disentanglement using NaÃ¯ve Bayes. Under the assumption that the prior is a mixture of log-concave distributions, we also establish that the Kullback-Leibler divergence between the prior and the posterior is upper bounded by a function minimized by the minimizer of the cross entropy loss, informing our adoption of radial basis functions (RBF) and cross entropy with interleaving training for DeepDIVE to provide a justified basis for convergence. Experiments on two public datasets show that DeepDIVE disentangles the original input and yields forecast accuracies better than the original VAE and comparable to existing state-of-the-art baselines.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "51",
        "title": "The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of Instruction Tuning and In-Context Learning Capabilities",
        "author": [
            "Irina Bigoulaeva",
            "Harish Tayyar Madabushi",
            "Iryna Gurevych"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08716",
        "abstract": "Large Language Models (LLMs), trained on extensive web-scale corpora, have demonstrated remarkable abilities across diverse tasks, especially as they are scaled up. Nevertheless, even state-of-the-art models struggle in certain cases, sometimes failing at problems solvable by young children, indicating that traditional notions of task complexity are insufficient for explaining LLM capabilities. However, exploring LLM capabilities is complicated by the fact that most widely-used models are also \"instruction-tuned\" to respond appropriately to prompts. With the goal of disentangling the factors influencing LLM performance, we investigate whether instruction-tuned models possess fundamentally different capabilities from base models that are prompted using in-context examples. Through extensive experiments across various model families, scales and task types, which included instruction tuning 90 different LLMs, we demonstrate that the performance of instruction-tuned models is significantly correlated with the in-context performance of their base counterparts. By clarifying what instruction-tuning contributes, we extend prior research into in-context learning, which suggests that base models use priors from pretraining data to solve tasks. Specifically, we extend this understanding to instruction-tuned models, suggesting that their pretraining data similarly sets a limiting boundary on the tasks they can solve, with the added influence of the instruction-tuning dataset.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "52",
        "title": "Transformed Low-rank Adaptation via Tensor Decomposition and Its Applications to Text-to-image Models",
        "author": [
            "Zerui Tao",
            "Yuhta Takida",
            "Naoki Murata",
            "Qibin Zhao",
            "Yuki Mitsufuji"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08727",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) of text-to-image models has become an increasingly popular technique with many applications. Among the various PEFT methods, Low-Rank Adaptation (LoRA) and its variants have gained significant attention due to their effectiveness, enabling users to fine-tune models with limited computational resources. However, the approximation gap between the low-rank assumption and desired fine-tuning weights prevents the simultaneous acquisition of ultra-parameter-efficiency and better performance. To reduce this gap and further improve the power of LoRA, we propose a new PEFT method that combines two classes of adaptations, namely, transform and residual adaptations. In specific, we first apply a full-rank and dense transform to the pre-trained weight. This learnable transform is expected to align the pre-trained weight as closely as possible to the desired weight, thereby reducing the rank of the residual weight. Then, the residual part can be effectively approximated by more compact and parameter-efficient structures, with a smaller approximation error. To achieve ultra-parameter-efficiency in practice, we design highly flexible and effective tensor decompositions for both the transform and residual adaptations. Additionally, popular PEFT methods such as DoRA can be summarized under this transform plus residual adaptation scheme. Experiments are conducted on fine-tuning Stable Diffusion models in subject-driven and controllable generation. The results manifest that our method can achieve better performances and parameter efficiency compared to LoRA and several baselines.",
        "tags": [
            "Diffusion",
            "LoRA",
            "Text-to-Image"
        ]
    },
    {
        "id": "53",
        "title": "Expanding Vietnamese SentiWordNet to Improve Performance of Vietnamese Sentiment Analysis Models",
        "author": [
            "Hong-Viet Tran",
            "Van-Tan Bui",
            "Lam-Quan Tran"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08758",
        "abstract": "Sentiment analysis is one of the most crucial tasks in Natural Language Processing (NLP), involving the training of machine learning models to classify text based on the polarity of opinions. Pre-trained Language Models (PLMs) can be applied to downstream tasks through fine-tuning, eliminating the need to train the model from scratch. Specifically, PLMs have been employed for Sentiment Analysis, a process that involves detecting, analyzing, and extracting the polarity of text sentiments. Numerous models have been proposed to address this task, with pre-trained PhoBERT-V2 models standing out as the state-of-the-art language models for Vietnamese. The PhoBERT-V2 pre-training approach is based on RoBERTa, optimizing the BERT pre-training method for more robust performance. In this paper, we introduce a novel approach that combines PhoBERT-V2 and SentiWordnet for Sentiment Analysis of Vietnamese reviews. Our proposed model utilizes PhoBERT-V2 for Vietnamese, offering a robust optimization for the prominent BERT model in the context of Vietnamese language, and leverages SentiWordNet, a lexical resource explicitly designed to support sentiment classification applications. Experimental results on the VLSP 2016 and AIVIVN 2019 datasets demonstrate that our sentiment analysis system has achieved excellent performance in comparison to other models.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "54",
        "title": "How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in Software Engineering",
        "author": [
            "Christoph Treude",
            "Marco A. Gerosa"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08774",
        "abstract": "Artificial intelligence (AI), including large language models and generative AI, is emerging as a significant force in software development, offering developers powerful tools that span the entire development lifecycle. Although software engineering research has extensively studied AI tools in software development, the specific types of interactions between developers and these AI-powered tools have only recently begun to receive attention. Understanding and improving these interactions has the potential to improve productivity, trust, and efficiency in AI-driven workflows. In this paper, we propose a taxonomy of interaction types between developers and AI tools, identifying eleven distinct interaction types, such as auto-complete code suggestions, command-driven actions, and conversational assistance. Building on this taxonomy, we outline a research agenda focused on optimizing AI interactions, improving developer control, and addressing trust and usability challenges in AI-assisted development. By establishing a structured foundation for studying developer-AI interactions, this paper aims to stimulate research on creating more effective, adaptive AI tools for software development.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "55",
        "title": "Exploring ChatGPT for Face Presentation Attack Detection in Zero and Few-Shot in-Context Learning",
        "author": [
            "Alain Komaty",
            "Hatef Otroshi Shahreza",
            "Anjith George",
            "Sebastien Marcel"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08799",
        "abstract": "This study highlights the potential of ChatGPT (specifically GPT-4o) as a competitive alternative for Face Presentation Attack Detection (PAD), outperforming several PAD models, including commercial solutions, in specific scenarios. Our results show that GPT-4o demonstrates high consistency, particularly in few-shot in-context learning, where its performance improves as more examples are provided (reference data). We also observe that detailed prompts enable the model to provide scores reliably, a behavior not observed with concise prompts. Additionally, explanation-seeking prompts slightly enhance the model's performance by improving its interpretability. Remarkably, the model exhibits emergent reasoning capabilities, correctly predicting the attack type (print or replay) with high accuracy in few-shot scenarios, despite not being explicitly instructed to classify attack types. Despite these strengths, GPT-4o faces challenges in zero-shot tasks, where its performance is limited compared to specialized PAD systems. Experiments were conducted on a subset of the SOTERIA dataset, ensuring compliance with data privacy regulations by using only data from consenting individuals. These findings underscore GPT-4o's promise in PAD applications, laying the groundwork for future research to address broader data privacy concerns and improve cross-dataset generalization. Code available here: https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad",
        "tags": [
            "ChatGPT",
            "Detection",
            "GPT"
        ]
    },
    {
        "id": "56",
        "title": "IDEA: Image Description Enhanced CLIP-Adapter",
        "author": [
            "Zhipeng Ye",
            "Feng Jiang",
            "Qiufeng Wang",
            "Kaizhu Huang",
            "Jiaqi Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08816",
        "abstract": "CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multimodal learning. However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks. This method captures fine-grained features by leveraging both visual features and textual descriptions of images. IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the model's performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named \"IMD-11\". Our code and data are released at https://github.com/FourierAI/IDEA.",
        "tags": [
            "CLIP",
            "LLaMA"
        ]
    },
    {
        "id": "57",
        "title": "MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term Dense Anticipation",
        "author": [
            "Olga Zatsarynna",
            "Emad Bahrami",
            "Yazan Abu Farha",
            "Gianpiero Francesca",
            "Juergen Gall"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08837",
        "abstract": "Our work addresses the problem of stochastic long-term dense anticipation. The goal of this task is to predict actions and their durations several minutes into the future based on provided video observations. Anticipation over extended horizons introduces high uncertainty, as a single observation can lead to multiple plausible future outcomes. To address this uncertainty, stochastic models are designed to predict several potential future action sequences. Recent work has further proposed to incorporate uncertainty modelling for observed frames by simultaneously predicting per-frame past and future actions in a unified manner. While such joint modelling of actions is beneficial, it requires long-range temporal capabilities to connect events across distant past and future time points. However, the previous work struggles to achieve such a long-range understanding due to its limited and/or sparse receptive field. To alleviate this issue, we propose a novel MANTA (MAmba for ANTicipation) network. Our model enables effective long-term temporal modelling even for very long sequences while maintaining linear complexity in sequence length. We demonstrate that our approach achieves state-of-the-art results on three datasets - Breakfast, 50Salads, and Assembly101 - while also significantly improving computational and memory efficiency.",
        "tags": [
            "Diffusion",
            "Mamba"
        ]
    },
    {
        "id": "58",
        "title": "ToMATO: Verbalizing the Mental States of Role-Playing LLMs for Benchmarking Theory of Mind",
        "author": [
            "Kazutoshi Shinoda",
            "Nobukatsu Hojo",
            "Kyosuke Nishida",
            "Saki Mizuno",
            "Keita Suzuki",
            "Ryo Masumura",
            "Hiroaki Sugiyama",
            "Kuniko Saito"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08838",
        "abstract": "Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in three aspects: 1) they assess a limited range of mental states such as beliefs, 2) false beliefs are not comprehensively explored, and 3) the diverse personality traits of characters are overlooked. To address these challenges, we introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over conversations. ToMATO is generated via LLM-LLM conversations featuring information asymmetry. By employing a prompting method that requires role-playing LLMs to verbalize their thoughts before each utterance, we capture both first- and second-order mental states across five categories: belief, intention, desire, emotion, and knowledge. These verbalized thoughts serve as answers to questions designed to assess the mental states of characters within conversations. Furthermore, the information asymmetry introduced by hiding thoughts from others induces the generation of false beliefs about various mental states. Assigning distinct personality traits to LLMs further diversifies both utterances and thoughts. ToMATO consists of 5.4k questions, 753 conversations, and 15 personality trait patterns. Our analysis shows that this dataset construction approach frequently generates false beliefs due to the information asymmetry between role-playing LLMs, and effectively reflects diverse personalities. We evaluate nine LLMs on ToMATO and find that even GPT-4o mini lags behind human performance, especially in understanding false beliefs, and lacks robustness to various personality traits.",
        "tags": [
            "GPT",
            "LLMs"
        ]
    },
    {
        "id": "59",
        "title": "SLC$^2$-SLAM: Semantic-guided Loop Closure with Shared Latent Code for NeRF SLAM",
        "author": [
            "Yuhang Ming",
            "Di Ma",
            "Weichen Dai",
            "Han Yang",
            "Rui Fan",
            "Guofeng Zhang",
            "Wanzeng Kong"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08880",
        "abstract": "Targeting the notorious cumulative drift errors in NeRF SLAM, we propose a Semantic-guided Loop Closure with Shared Latent Code, dubbed SLC$^2$-SLAM. Especially, we argue that latent codes stored in many NeRF SLAM systems are not fully exploited, as they are only used for better reconstruction. In this paper, we propose a simple yet effective way to detect potential loops using the same latent codes as local features. To further improve the loop detection performance, we use the semantic information, which are also decoded from the same latent codes to guide the aggregation of local features. Finally, with the potential loops detected, we close them with a graph optimization followed by bundle adjustment to refine both the estimated poses and the reconstructed scene. To evaluate the performance of our SLC$^2$-SLAM, we conduct extensive experiments on Replica and ScanNet datasets. Our proposed semantic-guided loop closure significantly outperforms the pre-trained NetVLAD and ORB combined with Bag-of-Words, which are used in all the other NeRF SLAM with loop closure. As a result, our SLC$^2$-SLAM also demonstrated better tracking and reconstruction performance, especially in larger scenes with more loops, like ScanNet.",
        "tags": [
            "Detection",
            "NeRF",
            "SLAM"
        ]
    },
    {
        "id": "60",
        "title": "Enhanced Multi-Scale Cross-Attention for Person Image Generation",
        "author": [
            "Hao Tang",
            "Ling Shao",
            "Nicu Sebe",
            "Luc Van Gool"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08900",
        "abstract": "In this paper, we propose a novel cross-attention-based generative adversarial network (GAN) for the challenging person image generation task. Cross-attention is a novel and intuitive multi-modal fusion method in which an attention/correlation matrix is calculated between two feature maps of different modalities. Specifically, we propose the novel XingGAN (or CrossingGAN), which consists of two generation branches that capture the person's appearance and shape, respectively. Moreover, we propose two novel cross-attention blocks to effectively transfer and update the person's shape and appearance embeddings for mutual improvement. This has not been considered by any other existing GAN-based image generation work. To further learn the long-range correlations between different person poses at different scales and sub-regions, we propose two novel multi-scale cross-attention blocks. To tackle the issue of independent correlation computations within the cross-attention mechanism leading to noisy and ambiguous attention weights, which hinder performance improvements, we propose a module called enhanced attention (EA). Lastly, we introduce a novel densely connected co-attention module to fuse appearance and shape features at different stages effectively. Extensive experiments on two public datasets demonstrate that the proposed method outperforms current GAN-based methods and performs on par with diffusion-based methods. However, our method is significantly faster than diffusion-based methods in both training and inference.",
        "tags": [
            "Diffusion",
            "GAN"
        ]
    },
    {
        "id": "61",
        "title": "GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text Detection Challenge",
        "author": [
            "Liam Dugan",
            "Andrew Zhu",
            "Firoj Alam",
            "Preslav Nakov",
            "Marianna Apidianaki",
            "Chris Callison-Burch"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08913",
        "abstract": "Recently there have been many shared tasks targeting the detection of generated text from Large Language Models (LLMs). However, these shared tasks tend to focus either on cases where text is limited to one particular domain or cases where text can be from many domains, some of which may not be seen during test time. In this shared task, using the newly released RAID benchmark, we aim to answer whether or not models can detect generated text from a large, yet fixed, number of domains and LLMs, all of which are seen during training. Over the course of three months, our task was attempted by 9 teams with 23 detector submissions. We find that multiple participants were able to obtain accuracies of over 99% on machine-generated text from RAID while maintaining a 5% False Positive Rate -- suggesting that detectors are able to robustly detect text from many domains and models simultaneously. We discuss potential interpretations of this result and provide directions for future research.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "62",
        "title": "Disentangling Exploration of Large Language Models by Optimal Exploitation",
        "author": [
            "Tim Grams",
            "Patrick Betz",
            "Christian Bartelt"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08925",
        "abstract": "Exploration is a crucial skill for self-improvement and open-ended problem-solving. However, it remains uncertain whether large language models can effectively explore the state-space. Existing evaluations predominantly focus on the trade-off between exploration and exploitation, often assessed in multi-armed bandit problems. In contrast, this work isolates exploration as the sole objective, tasking the agent with delivering information that enhances future returns. For the evaluation, we propose to decompose missing rewards into exploration and exploitation components by measuring the optimal achievable return for the states already explored. Our experiments with various LLMs reveal that most models struggle to sufficiently explore the state-space and that weak exploration is insufficient. We observe a positive correlation between model size and exploration performance, with larger models demonstrating superior capabilities. Furthermore, we show that our decomposition provides insights into differences in behaviors driven by agent instructions during prompt engineering, offering a valuable tool for refining LLM performance in exploratory tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "63",
        "title": "Physical AI Agents: Integrating Cognitive Intelligence with Real-World Action",
        "author": [
            "Fouad Bousetouane"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08944",
        "abstract": "Vertical AI Agents are revolutionizing industries by delivering domain-specific intelligence and tailored solutions. However, many sectors, such as manufacturing, healthcare, and logistics, demand AI systems capable of extending their intelligence into the physical world, interacting directly with objects, environments, and dynamic conditions. This need has led to the emergence of Physical AI Agents--systems that integrate cognitive reasoning, powered by specialized LLMs, with precise physical actions to perform real-world tasks.\nThis work introduces Physical AI Agents as an evolution of shared principles with Vertical AI Agents, tailored for physical interaction. We propose a modular architecture with three core blocks--perception, cognition, and actuation--offering a scalable framework for diverse industries. Additionally, we present the Physical Retrieval Augmented Generation (Ph-RAG) design pattern, which connects physical intelligence to industry-specific LLMs for real-time decision-making and reporting informed by physical context.\nThrough case studies, we demonstrate how Physical AI Agents and the Ph-RAG framework are transforming industries like autonomous vehicles, warehouse robotics, healthcare, and manufacturing, offering businesses a pathway to integrate embodied AI for operational efficiency and innovation.",
        "tags": [
            "LLMs",
            "RAG",
            "Robotics"
        ]
    },
    {
        "id": "64",
        "title": "Analyzing the Ethical Logic of Six Large Language Models",
        "author": [
            "W. Russell Neuman",
            "Chad Coleman",
            "Manan Shah"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08951",
        "abstract": "This study examines the ethical reasoning of six prominent generative large language models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude 3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these models articulate and apply ethical logic, particularly in response to moral dilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from traditional alignment studies, the study adopts an explainability-transparency framework, prompting models to explain their ethical reasoning. This approach is analyzed through three established ethical typologies: the consequentialist-deontological analytic, Moral Foundations Theory, and the Kohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit largely convergent ethical logic, marked by a rationalist, consequentialist emphasis, with decisions often prioritizing harm minimization and fairness. Despite similarities in pre-training and model architecture, a mixture of nuanced and significant differences in ethical reasoning emerge across models, reflecting variations in fine-tuning and post-training processes. The models consistently display erudition, caution, and self-awareness, presenting ethical reasoning akin to a graduate-level discourse in moral philosophy. In striking uniformity these systems all describe their ethical reasoning as more sophisticated than what is characteristic of typical human moral logic.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "65",
        "title": "Learning to Extract Cross-Domain Aspects and Understanding Sentiments Using Large Language Models",
        "author": [
            "Karukriti Kaushik Ghosh",
            "Chiranjib Sur"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08974",
        "abstract": "Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment analysis that aims to extract and classify sentiments based on specific aspects or features of a product, service, or entity. Unlike traditional sentiment analysis, which assigns a general sentiment score to entire reviews or texts, ABSA focuses on breaking down the text into individual components or aspects (e.g., quality, price, service) and evaluating the sentiment towards each. This allows for a more granular level of understanding of customer opinions, enabling businesses to pinpoint specific areas of strength and improvement. The process involves several key steps, including aspect extraction, sentiment classification, and aspect-level sentiment aggregation for a review paragraph or any other form that the users have provided. ABSA has significant applications in areas such as product reviews, social media monitoring, customer feedback analysis, and market research. By leveraging techniques from natural language processing (NLP) and machine learning, ABSA facilitates the extraction of valuable insights, enabling companies to make data-driven decisions that enhance customer satisfaction and optimize offerings. As ABSA evolves, it holds the potential to greatly improve personalized customer experiences by providing a deeper understanding of sentiment across various product aspects. In this work, we have analyzed the strength of LLMs for a complete cross-domain aspect-based sentiment analysis with the aim of defining the framework for certain products and using it for other similar situations. We argue that it is possible to that at an effectiveness of 92\\% accuracy for the Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "66",
        "title": "Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models",
        "author": [
            "Emma Croxford",
            "Yanjun Gao",
            "Nicholas Pellegrino",
            "Karen K. Wong",
            "Graham Wills",
            "Elliot First",
            "Miranda Schnier",
            "Kyle Burton",
            "Cris G. Ebby",
            "Jillian Gorskic",
            "Matthew Kalscheur",
            "Samy Khalil",
            "Marie Pisani",
            "Tyler Rubeor",
            "Peter Stetson",
            "Frank Liao",
            "Cherodeep Goswami",
            "Brian Patterson",
            "Majid Afshar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08977",
        "abstract": "As Large Language Models (LLMs) are integrated into electronic health record (EHR) workflows, validated instruments are essential to evaluate their performance before implementation. Existing instruments for provider documentation quality are often unsuitable for the complexities of LLM-generated text and lack validation on real-world data. The Provider Documentation Summarization Quality Instrument (PDSQI-9) was developed to evaluate LLM-generated clinical summaries. Multi-document summaries were generated from real-world EHR data across multiple specialties using several LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson correlation for substantive validity, factor analysis and Cronbach's alpha for structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for generalizability, a semi-Delphi process for content validity, and comparisons of high- versus low-quality summaries for discriminant validity. Seven physician raters evaluated 779 summaries and answered 8,329 questions, achieving over 80% power for inter-rater reliability. The PDSQI-9 demonstrated strong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting structural validity and generalizability. Factor analysis identified a 4-factor model explaining 58% of the variance, representing organization, clarity, accuracy, and utility. Substantive validity was supported by correlations between note length and scores for Succinct (rho = -0.200, p = 0.029) and Organized (rho = -0.190, p = 0.037). Discriminant validity distinguished high- from low-quality summaries (p < 0.001). The PDSQI-9 demonstrates robust construct validity, supporting its use in clinical practice to evaluate LLM-generated summaries and facilitate safer integration of LLMs into healthcare workflows.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "67",
        "title": "CityLoc: 6 DoF Localization of Text Descriptions in Large-Scale Scenes with Gaussian Representation",
        "author": [
            "Qi Ma",
            "Runyi Yang",
            "Bin Ren",
            "Ender Konukoglu",
            "Luc Van Gool",
            "Danda Pani Paudel"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08982",
        "abstract": "Localizing text descriptions in large-scale 3D scenes is inherently an ambiguous task. This nonetheless arises while describing general concepts, e.g. all traffic lights in a city.\nTo facilitate reasoning based on such concepts, text localization in the form of distribution is required. In this paper, we generate the distribution of the camera poses conditioned upon the textual description.\nTo facilitate such generation, we propose a diffusion-based architecture that conditionally diffuses the noisy 6DoF camera poses to their plausible locations.\nThe conditional signals are derived from the text descriptions, using the pre-trained text encoders. The connection between text descriptions and pose distribution is established through pretrained Vision-Language-Model, i.e. CLIP. Furthermore, we demonstrate that the candidate poses for the distribution can be further refined by rendering potential poses using 3D Gaussian splatting, guiding incorrectly posed samples towards locations that better align with the textual description, through visual reasoning.\nWe demonstrate the effectiveness of our method by comparing it with both standard retrieval methods and learning-based approaches. Our proposed method consistently outperforms these baselines across all five large-scale datasets. Our source code and dataset will be made publicly available.",
        "tags": [
            "3D",
            "CLIP",
            "Diffusion",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "68",
        "title": "RepVideo: Rethinking Cross-Layer Representation for Video Generation",
        "author": [
            "Chenyang Si",
            "Weichen Fan",
            "Zhengyao Lv",
            "Ziqi Huang",
            "Yu Qiao",
            "Ziwei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08994",
        "abstract": "Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos. However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of representations on the video generation process. In this paper, we initially investigate the characteristics of features in intermediate layers, finding substantial variations in attention maps across different layers. These variations lead to unstable semantic representations and contribute to cumulative differences between features, which ultimately reduce the similarity between adjacent frames and negatively affect temporal coherence. To address this, we propose RepVideo, an enhanced representation framework for text-to-video diffusion models. By accumulating features from neighboring layers to form enriched representations, this approach captures more stable semantic information. These enhanced representations are then used as inputs to the attention mechanism, thereby improving semantic expressiveness while ensuring feature consistency across adjacent frames. Extensive experiments demonstrate that our RepVideo not only significantly enhances the ability to generate accurate spatial appearances, such as capturing complex spatial relationships between multiple objects, but also improves temporal consistency in video generation.",
        "tags": [
            "Diffusion",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "69",
        "title": "Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails",
        "author": [
            "Shaona Ghosh",
            "Prasoon Varshney",
            "Makesh Narsimhan Sreedhar",
            "Aishwarya Padmakumar",
            "Traian Rebedea",
            "Jibin Rajan Varghese",
            "Christopher Parisien"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09004",
        "abstract": "As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications. To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories. This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types. Using a hybrid data generation pipeline that combines human annotations with a multi-LLM \"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a carefully curated collection of 34,248 samples of human-LLM interactions, annotated according to our proposed taxonomy. To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis 2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets. In addition, we introduce a novel training blend that combines safety with topic following http://data.This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference. We plan to open-source Aegis 2.0 data and models to the research community to aid in the safety guardrailing of LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "70",
        "title": "SimGen: A Diffusion-Based Framework for Simultaneous Surgical Image and Segmentation Mask Generation",
        "author": [
            "Aditya Bhat",
            "Rupak Bose",
            "Chinedu Innocent Nwoye",
            "Nicolas Padoy"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09008",
        "abstract": "Acquiring and annotating surgical data is often resource-intensive, ethical constraining, and requiring significant expert involvement. While generative AI models like text-to-image can alleviate data scarcity, incorporating spatial annotations, such as segmentation masks, is crucial for precision-driven surgical applications, simulation, and education. This study introduces both a novel task and method, SimGen, for Simultaneous Image and Mask Generation. SimGen is a diffusion model based on the DDPM framework and Residual U-Net, designed to jointly generate high-fidelity surgical images and their corresponding segmentation masks. The model leverages cross-correlation priors to capture dependencies between continuous image and discrete mask distributions. Additionally, a Canonical Fibonacci Lattice (CFL) is employed to enhance class separability and uniformity in the RGB space of the masks. SimGen delivers high-fidelity images and accurate segmentation masks, outperforming baselines across six public datasets assessed on image and semantic inception distance metrics. Ablation study shows that the CFL improves mask quality and spatial separation. Downstream experiments suggest generated image-mask pairs are usable if regulations limit human data release for research. This work offers a cost-effective solution for generating paired surgical images and complex labels, advancing surgical AI development by reducing the need for expensive manual annotations.",
        "tags": [
            "DDPM",
            "Diffusion",
            "Segmentation",
            "Text-to-Image"
        ]
    },
    {
        "id": "71",
        "title": "Multimodal LLMs Can Reason about Aesthetics in Zero-Shot",
        "author": [
            "Ruixiang Jiang",
            "Changwen Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09012",
        "abstract": "We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method for human preference modeling and perform a systematic correlation analysis between MLLMs' responses and human preference. Our experiments reveal an inherent hallucination issue of MLLMs in art evaluation, associated with response subjectivity. ArtCoT is proposed, demonstrating that art-specific task decomposition and the use of concrete language boost MLLMs' reasoning ability for aesthetics. Our findings offer valuable insights into MLLMs for art and can benefit a wide range of downstream applications, such as style transfer and artistic image generation. Code available at https://github.com/songrise/MLLM4Art.",
        "tags": [
            "LLMs",
            "Style Transfer"
        ]
    },
    {
        "id": "72",
        "title": "How Do Generative Models Draw a Software Engineer? A Case Study on Stable Diffusion Bias",
        "author": [
            "Tosin Fadahunsi",
            "Giordano d'Aloisio",
            "Antinisca Di Marco",
            "Federica Sarro"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09014",
        "abstract": "Generative models are nowadays widely used to generate graphical content used for multiple purposes, e.g. web, art, advertisement. However, it has been shown that the images generated by these models could reinforce societal biases already existing in specific contexts. In this paper, we focus on understanding if this is the case when one generates images related to various software engineering tasks. In fact, the Software Engineering (SE) community is not immune from gender and ethnicity disparities, which could be amplified by the use of these models. Hence, if used without consciousness, artificially generated images could reinforce these biases in the SE domain. Specifically, we perform an extensive empirical evaluation of the gender and ethnicity bias exposed by three versions of the Stable Diffusion (SD) model (a very popular open-source text-to-image model) - SD 2, SD XL, and SD 3 - towards SE tasks. We obtain 6,720 images by feeding each model with two sets of prompts describing different software-related tasks: one set includes the Software Engineer keyword, and one set does not include any specification of the person performing the task. Next, we evaluate the gender and ethnicity disparities in the generated images. Results show how all models are significantly biased towards male figures when representing software engineers. On the contrary, while SD 2 and SD XL are strongly biased towards White figures, SD 3 is slightly more biased towards Asian figures. Nevertheless, all models significantly under-represent Black and Arab figures, regardless of the prompt style used. The results of our analysis highlight severe concerns about adopting those models to generate content for SE tasks and open the field for future research on bias mitigation in this context.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "73",
        "title": "Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion",
        "author": [
            "Jingyuan Chen",
            "Fuchen Long",
            "Jie An",
            "Zhaofan Qiu",
            "Ting Yao",
            "Jiebo Luo",
            "Tao Mei"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09019",
        "abstract": "The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at the queue's head while Gaussian noise is enqueued at the tail. However, FIFO-Diffusion often struggles to keep long-range temporal consistency in the generated videos due to the lack of correspondence modeling across frames. In this paper, we propose Ouroboros-Diffusion, a novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length. Specifically, we introduce a new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames. To enhance subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence. Furthermore, we introduce self-recurrent guidance. This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction. Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency.",
        "tags": [
            "Diffusion",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "74",
        "title": "Operator Learning for Reconstructing Flow Fields from Sparse Measurements: an Energy Transformer Approach",
        "author": [
            "Qian Zhang",
            "Dmitry Krotov",
            "George Em Karniadakis"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08339",
        "abstract": "Machine learning methods have shown great success in various scientific areas, including fluid mechanics. However, reconstruction problems, where full velocity fields must be recovered from partial observations, remain challenging. In this paper, we propose a novel operator learning framework for solving reconstruction problems by using the Energy Transformer (ET), an architecture inspired by associative memory models. We formulate reconstruction as a mapping from incomplete observed data to full reconstructed fields. The method is validated on three fluid mechanics examples using diverse types of data: (1) unsteady 2D vortex street in flow past a cylinder using simulation data; (2) high-speed under-expanded impinging supersonic jets impingement using Schlieren imaging; and (3) 3D turbulent jet flow using particle tracking. The results demonstrate the ability of ET to accurately reconstruct complex flow fields from highly incomplete data (90\\% missing), even for noisy experimental measurements, with fast training and inference on a single GPU. This work provides a promising new direction for tackling reconstruction problems in fluid mechanics and other areas in mechanics, geophysics, weather prediction, and beyond.",
        "tags": [
            "3D",
            "Transformer"
        ]
    },
    {
        "id": "75",
        "title": "SEAL: Speaker Error Correction using Acoustic-conditioned Large Language Models",
        "author": [
            "Anurag Kumar",
            "Rohit Paturi",
            "Amber Afshan",
            "Sundararajan Srinivasan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08421",
        "abstract": "Speaker Diarization (SD) is a crucial component of modern end-to-end ASR pipelines. Traditional SD systems, which are typically audio-based and operate independently of ASR, often introduce speaker errors, particularly during speaker transitions and overlapping speech. Recently, language models including fine-tuned large language models (LLMs) have shown to be effective as a second-pass speaker error corrector by leveraging lexical context in the transcribed output. In this work, we introduce a novel acoustic conditioning approach to provide more fine-grained information from the acoustic diarizer to the LLM. We also show that a simpler constrained decoding strategy reduces LLM hallucinations, while avoiding complicated post-processing. Our approach significantly reduces the speaker error rates by 24-43% across Fisher, Callhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "76",
        "title": "Analytical and Numerical Study of a Convection-Diffusion-Reaction-Source Problem in Multilayered Materials",
        "author": [
            "Guillermo Federico Umbricht",
            "Domingo Alberto Tarzia",
            "Diana Rubio"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08489",
        "abstract": "In this work, a thermal energy transfer problem in a one-dimensional multilayer body is theoretically analyzed, considering diffusion, advection, internal heat generation or loss linearly dependent on temperature in each layer, as well as heat generation due to external sources. Additionally, the thermal contact resistance at the interfaces between each pair of materials is taken into account. The problem is mathematically modeled, and explicit analytical solutions are derived using Fourier techniques. A convergent finite difference scheme is also formulated to simulate specific cases. The solution is consistent with previous results. A numerical example is provided, demonstrating the coherence between the obtained results and the physical behavior of the problem. This work was recently published for a two-layer body; the generalization to m-layer bodies allows for conclusions that enhance the theoretical understanding of heat transfer in multilayer materials and may contribute to improving the thermal design of multilayer engineering systems.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "77",
        "title": "Boosting Diffusion Guidance via Learning Degradation-Aware Models for Blind Super Resolution",
        "author": [
            "Shao-Hao Lu",
            "Ren Wang",
            "Ching-Chun Huang",
            "Wei-Chen Chiu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08819",
        "abstract": "Recently, diffusion-based blind super-resolution (SR) methods have shown great ability to generate high-resolution images with abundant high-frequency detail, but the detail is often achieved at the expense of fidelity. Meanwhile, another line of research focusing on rectifying the reverse process of diffusion models (i.e., diffusion guidance), has demonstrated the power to generate high-fidelity results for non-blind SR. However, these methods rely on known degradation kernels, making them difficult to apply to blind SR. To address these issues, we introduce degradation-aware models that can be integrated into the diffusion guidance framework, eliminating the need to know degradation kernels. Additionally, we propose two novel techniques input perturbation and guidance scalar to further improve our performance. Extensive experimental results show that our proposed method has superior performance over state-of-the-art methods on blind SR benchmarks",
        "tags": [
            "Diffusion",
            "Super Resolution"
        ]
    },
    {
        "id": "78",
        "title": "CrystalGRW: Generative Modeling of Crystal Structures with Targeted Properties via Geodesic Random Walks",
        "author": [
            "Krit Tangsongcharoen",
            "Teerachote Pakornchote",
            "Chayanon Atthapak",
            "Natthaphon Choomphon-anomakhun",
            "Annop Ektarawong",
            "BjÃ¶rn Alling",
            "Christopher Sutton",
            "Thiti Bovornratanaraks",
            "Thiparat Chotibut"
        ],
        "pdf": "https://arxiv.org/pdf/2501.08998",
        "abstract": "Determining whether a candidate crystalline material is thermodynamically stable depends on identifying its true ground-state structure, a central challenge in computational materials science. We introduce CrystalGRW, a diffusion-based generative model on Riemannian manifolds that proposes novel crystal configurations and can predict stable phases validated by density functional theory. The crystal properties, such as fractional coordinates, atomic types, and lattice matrices, are represented on suitable Riemannian manifolds, ensuring that new predictions generated through the diffusion process preserve the periodicity of crystal structures. We incorporate an equivariant graph neural network to also account for rotational and translational symmetries during the generation process. CrystalGRW demonstrates the ability to generate realistic crystal structures that are close to their ground states with accuracy comparable to existing models, while also enabling conditional control, such as specifying a desired crystallographic point group. These features help accelerate materials discovery and inverse design by offering stable, symmetry-consistent crystal candidates for experimental validation.",
        "tags": [
            "Diffusion"
        ]
    }
]