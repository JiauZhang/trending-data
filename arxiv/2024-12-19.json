[
    {
        "id": "1",
        "title": "Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval",
        "author": [
            "Quang Hoang Trung",
            "Nguyen Van Hoang Phuc",
            "Le Trung Hoang",
            "Quang Huu Hieu",
            "Vo Nguyen Le Duy"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13205",
        "abstract": "Text Retrieval (TR) involves finding and retrieving text-based content relevant to a user's query from a large repository, with applications in real-world scenarios such as legal document retrieval. While most existing studies focus on English, limited work addresses Japanese contexts. In this paper, we introduce a new dataset specifically designed for Japanese legal contexts and propose a novel two-phase pipeline tailored to this domain.\nIn the first phase, the model learns a broad understanding of global contexts, enhancing its generalization and adaptability to diverse queries. In the second phase, the model is fine-tuned to address complex queries specific to legal scenarios. Extensive experiments are conducted to demonstrate the superior performance of our method, which outperforms existing baselines.\nFurthermore, our pipeline proves effective in English contexts, surpassing comparable baselines on the MS MARCO dataset. We have made our code publicly available on GitHub, and the model checkpoints are accessible via HuggingFace.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "2",
        "title": "Content-aware Balanced Spectrum Encoding in Masked Modeling for Time Series Classification",
        "author": [
            "Yudong Han",
            "Haocong Wang",
            "Yupeng Hu",
            "Yongshun Gong",
            "Xuemeng Song",
            "Weili Guan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13232",
        "abstract": "Due to the superior ability of global dependency, transformer and its variants have become the primary choice in Masked Time-series Modeling (MTM) towards time-series classification task. In this paper, we experimentally analyze that existing transformer-based MTM methods encounter with two under-explored issues when dealing with time series data: (1) they encode features by performing long-dependency ensemble averaging, which easily results in rank collapse and feature homogenization as the layer goes deeper; (2) they exhibit distinct priorities in fitting different frequency components contained in the time-series, inevitably leading to spectrum energy imbalance of encoded feature. To tackle these issues, we propose an auxiliary content-aware balanced decoder (CBD) to optimize the encoding quality in the spectrum space within masked modeling scheme. Specifically, the CBD iterates on a series of fundamental blocks, and thanks to two tailored units, each block could progressively refine the masked representation via adjusting the interaction pattern based on local content variations of time-series and learning to recalibrate the energy distribution across different frequency components. Moreover, a dual-constraint loss is devised to enhance the mutual optimization of vanilla decoder and our CBD. Extensive experimental results on ten time-series classification datasets show that our method nearly surpasses a bunch of baselines. Meanwhile, a series of explanatory results are showcased to sufficiently demystify the behaviors of our method.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "3",
        "title": "Creating an LLM-based AI-agent: A high-level methodology towards enhancing LLMs with APIs",
        "author": [
            "Ioannis Tzachristas"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13233",
        "abstract": "Large Language Models (LLMs) have revolutionized various aspects of engineering and science. Their utility is often bottlenecked by the lack of interaction with the external digital environment. To overcome this limitation and achieve integration of LLMs and Artificial Intelligence (AI) into real-world applications, customized AI agents are being constructed. Based on the technological trends and techniques, we extract a high-level approach for constructing these AI agents, focusing on their underlying architecture. This thesis serves as a comprehensive guide that elucidates a multi-faceted approach for empowering LLMs with the capability to leverage Application Programming Interfaces (APIs). We present a 7-step methodology that begins with the selection of suitable LLMs and the task decomposition that is necessary for complex problem-solving. This methodology includes techniques for generating training data for API interactions and heuristics for selecting the appropriate API among a plethora of options. These steps eventually lead to the generation of API calls that are both syntactically and semantically aligned with the LLM's understanding of a given task. Moreover, we review existing frameworks and tools that facilitate these processes and highlight the gaps in current attempts. In this direction, we propose an on-device architecture that aims to exploit the functionality of carry-on devices by using small models from the Hugging Face community. We examine the effectiveness of these approaches on real-world applications of various domains, including the generation of a piano sheet. Through an extensive analysis of the literature and available technologies, this thesis aims to set a compass for researchers and practitioners to harness the full potential of LLMs augmented with external tool capabilities, thus paving the way for more autonomous, robust, and context-aware AI agents.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "4",
        "title": "SafeDrive: Knowledge- and Data-Driven Risk-Sensitive Decision-Making for Autonomous Vehicles with Large Language Models",
        "author": [
            "Zhiyuan Zhou",
            "Heye Huang",
            "Boqi Li",
            "Shiyue Zhao",
            "Yao Mu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13238",
        "abstract": "Recent advancements in autonomous vehicles (AVs) use Large Language Models (LLMs) to perform well in normal driving scenarios. However, ensuring safety in dynamic, high-risk environments and managing safety-critical long-tail events remain significant challenges. To address these issues, we propose SafeDrive, a knowledge- and data-driven risk-sensitive decision-making framework to enhance AV safety and adaptability. The proposed framework introduces a modular system comprising: (1) a Risk Module for quantifying multi-factor coupled risks involving driver, vehicle, and road interactions; (2) a Memory Module for storing and retrieving typical scenarios to improve adaptability; (3) a LLM-powered Reasoning Module for context-aware safety decision-making; and (4) a Reflection Module for refining decisions through iterative learning. By integrating knowledge-driven insights with adaptive learning mechanisms, the framework ensures robust decision-making under uncertain conditions. Extensive evaluations on real-world traffic datasets, including highways (HighD), intersections (InD), and roundabouts (RounD), validate the framework's ability to enhance decision-making safety (achieving a 100% safety rate), replicate human-like driving behaviors (with decision alignment exceeding 85%), and adapt effectively to unpredictable scenarios. SafeDrive establishes a novel paradigm for integrating knowledge- and data-driven methods, highlighting significant potential to improve safety and adaptability of autonomous driving in high-risk traffic scenarios.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "5",
        "title": "JudgeBlender: Ensembling Judgments for Automatic Relevance Assessment",
        "author": [
            "Hossein A. Rahmani",
            "Emine Yilmaz",
            "Nick Craswell",
            "Bhaskar Mitra"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13268",
        "abstract": "The effective training and evaluation of retrieval systems require a substantial amount of relevance judgments, which are traditionally collected from human assessors -- a process that is both costly and time-consuming. Large Language Models (LLMs) have shown promise in generating relevance labels for search tasks, offering a potential alternative to manual assessments. Current approaches often rely on a single LLM, such as GPT-4, which, despite being effective, are expensive and prone to intra-model biases that can favour systems leveraging similar models. In this work, we introduce JudgeBlender, a framework that employs smaller, open-source models to provide relevance judgments by combining evaluations across multiple LLMs (LLMBlender) or multiple prompts (PromptBlender). By leveraging the LLMJudge benchmark [18], we compare JudgeBlender with state-of-the-art methods and the top performers in the LLMJudge challenge. Our results show that JudgeBlender achieves competitive performance, demonstrating that very large models are often unnecessary for reliable relevance assessments.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "6",
        "title": "CompactFlowNet: Efficient Real-time Optical Flow Estimation on Mobile Devices",
        "author": [
            "Andrei Znobishchev",
            "Valerii Filev",
            "Oleg Kudashev",
            "Nikita Orlov",
            "Humphrey Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13273",
        "abstract": "We present CompactFlowNet, the first real-time mobile neural network for optical flow prediction, which involves determining the displacement of each pixel in an initial frame relative to the corresponding pixel in a subsequent frame. Optical flow serves as a fundamental building block for various video-related tasks, such as video restoration, motion estimation, video stabilization, object tracking, action recognition, and video generation. While current state-of-the-art methods prioritize accuracy, they often overlook constraints regarding speed and memory usage. Existing light models typically focus on reducing size but still exhibit high latency, compromise significantly on quality, or are optimized for high-performance GPUs, resulting in sub-optimal performance on mobile devices. This study aims to develop a mobile-optimized optical flow model by proposing a novel mobile device-compatible architecture, as well as enhancements to the training pipeline, which optimize the model for reduced weight, low memory utilization, and increased speed while maintaining minimal error. Our approach demonstrates superior or comparable performance to the state-of-the-art lightweight models on the challenging KITTI and Sintel benchmarks. Furthermore, it attains a significantly accelerated inference speed, thereby yielding real-time operational efficiency on the iPhone 8, while surpassing real-time performance levels on more advanced mobile devices.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "7",
        "title": "Enhancing Persona Classification in Dialogue Systems: A Graph Neural Network Approach",
        "author": [
            "Konstantin Zaitsev"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13283",
        "abstract": "In recent years, Large Language Models (LLMs) gain considerable attention for their potential to enhance personalized experiences in virtual assistants and chatbots. A key area of interest is the integration of personas into LLMs to improve dialogue naturalness and user engagement. This study addresses the challenge of persona classification, a crucial component in dialogue understanding, by proposing a framework that combines text embeddings with Graph Neural Networks (GNNs) for effective persona classification. Given the absence of dedicated persona classification datasets, we create a manually annotated dataset to facilitate model training and evaluation. Our method involves extracting semantic features from persona statements using text embeddings and constructing a graph where nodes represent personas and edges capture their similarities. The GNN component uses this graph structure to propagate relevant information, thereby improving classification performance. Experimental results show that our approach, in particular the integration of GNNs, significantly improves classification performance, especially with limited data. Our contributions include the development of a persona classification framework and the creation of a dataset.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "8",
        "title": "Posterior Mean Matching: Generative Modeling through Online Bayesian Inference",
        "author": [
            "Sebastian Salazar",
            "Michal Kucer",
            "Yixin Wang",
            "Emily Casleton",
            "David Blei"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13286",
        "abstract": "This paper introduces posterior mean matching (PMM), a new method for generative modeling that is grounded in Bayesian inference. PMM uses conjugate pairs of distributions to model complex data of various modalities like images and text, offering a flexible alternative to existing methods like diffusion models. PMM models iteratively refine noisy approximations of the target distribution using updates from online Bayesian inference. PMM is flexible because its mechanics are based on general Bayesian models. We demonstrate this flexibility by developing specialized examples: a generative PMM model of real-valued data using the Normal-Normal model, a generative PMM model of count data using a Gamma-Poisson model, and a generative PMM model of discrete data using a Dirichlet-Categorical model. For the Normal-Normal PMM model, we establish a direct connection to diffusion models by showing that its continuous-time formulation converges to a stochastic differential equation (SDE). Additionally, for the Gamma-Poisson PMM, we derive a novel SDE driven by a Cox process, which is a significant departure from traditional Brownian motion-based generative models. PMMs achieve performance that is competitive with generative models for language modeling and image generation.",
        "tags": [
            "Diffusion",
            "SDE"
        ]
    },
    {
        "id": "9",
        "title": "Hint Marginalization for Improved Reasoning in Large Language Models",
        "author": [
            "Soumyasundar Pal",
            "Didier Chételat",
            "Yingxue Zhang",
            "Mark Coates"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13292",
        "abstract": "Large Language Models (LLMs) have exhibited an impressive capability to perform reasoning tasks, especially if they are encouraged to generate a sequence of intermediate steps. Reasoning performance can be improved by suitably combining multiple LLM responses, generated either in parallel in a single query, or via sequential interactions with LLMs throughout the reasoning process. Existing strategies for combination, such as self-consistency and progressive-hint-prompting, make inefficient usage of the LLM responses. We present Hint Marginalization, a novel and principled algorithmic framework to enhance the reasoning capabilities of LLMs. Our approach can be viewed as an iterative sampling strategy for forming a Monte Carlo approximation of an underlying distribution of answers, with the goal of identifying the mode the most likely answer. Empirical evaluation on several benchmark datasets for arithmetic reasoning demonstrates the superiority of the proposed approach.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "10",
        "title": "FastVLM: Efficient Vision Encoding for Vision Language Models",
        "author": [
            "Pavan Kumar Anasosalu Vasu",
            "Fartash Faghri",
            "Chun-Liang Li",
            "Cem Koc",
            "Nate True",
            "Albert Antony",
            "Gokul Santhanam",
            "James Gabriel",
            "Peter Grasch",
            "Oncel Tuzel",
            "Hadi Pouransari"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13303",
        "abstract": "Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2$\\times$ improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152$\\times$1152), FastVLM achieves comparable performance on key benchmarks like SeedBench and MMMU, using the same 0.5B LLM, but with 85$\\times$ faster TTFT and a vision encoder that is 3.4$\\times$ smaller.",
        "tags": [
            "LLaVA"
        ]
    },
    {
        "id": "11",
        "title": "Expansion Span: Combining Fading Memory and Retrieval in Hybrid State Space Models",
        "author": [
            "Elvis Nunez",
            "Luca Zancato",
            "Benjamin Bowman",
            "Aditya Golatkar",
            "Wei Xia",
            "Stefano Soatto"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13328",
        "abstract": "The \"state\" of State Space Models (SSMs) represents their memory, which fades exponentially over an unbounded span. By contrast, Attention-based models have \"eidetic\" (i.e., verbatim, or photographic) memory over a finite span (context size). Hybrid architectures combine State Space layers with Attention, but still cannot recall the distant past and can access only the most recent tokens eidetically. Unlike current methods of combining SSM and Attention layers, we allow the state to be allocated based on relevancy rather than recency. In this way, for every new set of query tokens, our models can \"eidetically\" access tokens from beyond the Attention span of current Hybrid SSMs without requiring extra hardware resources. We describe a method to expand the memory span of the hybrid state by \"reserving\" a fraction of the Attention context for tokens retrieved from arbitrarily distant in the past, thus expanding the eidetic memory span of the overall state. We call this reserved fraction of tokens the \"expansion span,\" and the mechanism to retrieve and aggregate it \"Span-Expanded Attention\" (SE-Attn). To adapt Hybrid models to using SE-Attn, we propose a novel fine-tuning method that extends LoRA to Hybrid models (HyLoRA) and allows efficient adaptation on long spans of tokens. We show that SE-Attn enables us to efficiently adapt pre-trained Hybrid models on sequences of tokens up to 8 times longer than the ones used for pre-training. We show that HyLoRA with SE-Attn is cheaper and more performant than alternatives like LongLoRA when applied to Hybrid models on natural language benchmarks with long-range dependencies, such as PG-19, RULER, and other common natural language downstream tasks.",
        "tags": [
            "LoRA",
            "SSMs",
            "State Space Models"
        ]
    },
    {
        "id": "12",
        "title": "Beyond Accuracy: On the Effects of Fine-tuning Towards Vision-Language Model's Prediction Rationality",
        "author": [
            "Qitong Wang",
            "Tang Li",
            "Kien X. Nguyen",
            "Xi Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13333",
        "abstract": "Vision-Language Models (VLMs), such as CLIP, have already seen widespread applications. Researchers actively engage in further fine-tuning VLMs in safety-critical domains. In these domains, prediction rationality is crucial: the prediction should be correct and based on valid evidence. Yet, for VLMs, the impact of fine-tuning on prediction rationality is seldomly investigated. To study this problem, we proposed two new metrics called Prediction Trustworthiness and Inference Reliability. We conducted extensive experiments on various settings and observed some interesting phenomena. On the one hand, we found that the well-adopted fine-tuning methods led to more correct predictions based on invalid evidence. This potentially undermines the trustworthiness of correct predictions from fine-tuned VLMs. On the other hand, having identified valid evidence of target objects, fine-tuned VLMs were more likely to make correct predictions. Moreover, the findings are also consistent under distributional shifts and across various experimental settings. We hope our research offer fresh insights to VLM fine-tuning.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "13",
        "title": "Experience of Training a 1.7B-Parameter LLaMa Model From Scratch",
        "author": [
            "Miles Q. Li",
            "Benjamin C. M. Fung",
            "Shih-Chia Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13335",
        "abstract": "Pretraining large language models is a complex endeavor influenced by multiple factors, including model architecture, data quality, training continuity, and hardware constraints. In this paper, we share insights gained from the experience of training DMaS-LLaMa-Lite, a fully open source, 1.7-billion-parameter, LLaMa-based model, on approximately 20 billion tokens of carefully curated data. We chronicle the full training trajectory, documenting how evolving validation loss levels and downstream benchmarks reflect transitions from incoherent text to fluent, contextually grounded output. Beyond standard quantitative metrics, we highlight practical considerations such as the importance of restoring optimizer states when resuming from checkpoints, and the impact of hardware changes on training stability and throughput. While qualitative evaluation provides an intuitive understanding of model improvements, our analysis extends to various performance benchmarks, demonstrating how high-quality data and thoughtful scaling enable competitive results with significantly fewer training tokens. By detailing these experiences and offering training logs, checkpoints, and sample outputs, we aim to guide future researchers and practitioners in refining their pretraining strategies. The training script is available on Github at https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code. The model checkpoints are available on Huggingface at https://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb.",
        "tags": [
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "14",
        "title": "Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs",
        "author": [
            "Aldo Pareja",
            "Nikhil Shivakumar Nayak",
            "Hao Wang",
            "Krishnateja Killamsetty",
            "Shivchander Sudalairaj",
            "Wenlong Zhao",
            "Seungwook Han",
            "Abhishek Bhandwaldar",
            "Guangxuan Xu",
            "Kai Xu",
            "Ligong Han",
            "Luke Inglis",
            "Akash Srivastava"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13337",
        "abstract": "The rise of large language models (LLMs) has created a significant disparity: industrial research labs with their computational resources, expert teams, and advanced infrastructures, can effectively fine-tune LLMs, while individual developers and small organizations face barriers due to limited resources. In this paper, we aim to bridge this gap by presenting a comprehensive study on supervised fine-tuning of LLMs using instruction-tuning datasets spanning diverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B parameters) for their cost-efficiency and accessibility. We explore various training configurations and strategies across four open-source pre-trained models. We provide detailed documentation of these configurations, revealing findings that challenge several common training practices, including hyperparameter recommendations from TULU and phased training recommended by Orca. Key insights from our work include: (i) larger batch sizes paired with lower learning rates lead to improved model performance on benchmarks such as MMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics, such as lower gradient norms and higher loss values, are strong indicators of better final model performance, enabling early termination of sub-optimal runs and significant computational savings; (iii) through a thorough exploration of hyperparameters like warmup steps and learning rate schedules, we provide guidance for practitioners and find that certain simplifications do not compromise performance; and (iv) we observed no significant difference in performance between phased and stacked training strategies, but stacked training is simpler and more sample efficient. With these findings holding robustly across datasets and models, we hope this study serves as a guide for practitioners fine-tuning small LLMs and promotes a more inclusive environment for LLM research.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "15",
        "title": "Concept-ROT: Poisoning Concepts in Large Language Models with Model Editing",
        "author": [
            "Keltin Grimes",
            "Marco Christiani",
            "David Shriver",
            "Marissa Connor"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13341",
        "abstract": "Model editing methods modify specific behaviors of Large Language Models by altering a small, targeted set of network weights and require very little data and compute. These methods can be used for malicious applications such as inserting misinformation or simple trojans that result in adversary-specified behaviors when a trigger word is present. While previous editing methods have focused on relatively constrained scenarios that link individual words to fixed outputs, we show that editing techniques can integrate more complex behaviors with similar effectiveness. We develop Concept-ROT, a model editing-based method that efficiently inserts trojans which not only exhibit complex output behaviors, but also trigger on high-level concepts -- presenting an entirely new class of trojan attacks. Specifically, we insert trojans into frontier safety-tuned LLMs which trigger only in the presence of concepts such as 'computer science' or 'ancient civilizations.' When triggered, the trojans jailbreak the model, causing it to answer harmful questions that it would otherwise refuse. Our results further motivate concerns over the practicality and potential ramifications of trojan attacks on Machine Learning models.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "16",
        "title": "Extending LLMs to New Languages: A Case Study of Llama and Persian Adaptation",
        "author": [
            "Samin Mahdizadeh Sani",
            "Pouya Sadeghi",
            "Thuy-Trang Vu",
            "Yadollah Yaghoobzadeh",
            "Gholamreza Haffari"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13375",
        "abstract": "Large language models (LLMs) have made great progress in classification and text generation tasks. However, they are mainly trained on English data and often struggle with low-resource languages. In this study, we explore adding a new language, i.e., Persian, to Llama (a model with a limited understanding of Persian) using parameter-efficient fine-tuning. We employ a multi-stage approach involving pretraining on monolingual Persian data, aligning representations through bilingual pretraining and instruction datasets, and instruction-tuning with task-specific datasets. We evaluate the model's performance at each stage on generation and classification tasks. Our findings suggest that incorporating the Persian language, through bilingual data alignment, can enhance classification accuracy for Persian tasks, with no adverse impact and sometimes even improvements on English tasks. Additionally, the results highlight the model's initial strength as a critical factor when working with limited training data, with cross-lingual alignment offering minimal benefits for the low-resource language. Knowledge transfer from English to Persian has a marginal effect, primarily benefiting simple classification tasks.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "17",
        "title": "DateLogicQA: Benchmarking Temporal Biases in Large Language Models",
        "author": [
            "Gagan Bhatia",
            "MingZe Tang",
            "Cristina Mahanta",
            "Madiha Kazi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13377",
        "abstract": "This paper introduces DateLogicQA, a benchmark with 190 questions covering diverse date formats, temporal contexts, and reasoning types. We propose the Semantic Integrity Metric to assess tokenization quality and analyse two biases: Representation-Level Bias, affecting embeddings, and Logical-Level Bias, influencing reasoning outputs. Our findings provide a comprehensive evaluation of LLMs' capabilities and limitations in temporal reasoning, highlighting key challenges in handling temporal data accurately. The GitHub repository for our work is available at https://github.com/gagan3012/EAIS-Temporal-Bias",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "18",
        "title": "An Automated Explainable Educational Assessment System Built on LLMs",
        "author": [
            "Jiazheng Li",
            "Artem Bobrov",
            "David West",
            "Cesare Aloisi",
            "Yulan He"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13381",
        "abstract": "In this demo, we present AERA Chat, an automated and explainable educational assessment system designed for interactive and visual evaluations of student responses. This system leverages large language models (LLMs) to generate automated marking and rationale explanations, addressing the challenge of limited explainability in automated educational assessment and the high costs associated with annotation. Our system allows users to input questions and student answers, providing educators and researchers with insights into assessment accuracy and the quality of LLM-assessed rationales. Additionally, it offers advanced visualization and robust evaluation tools, enhancing the usability for educational assessment and facilitating efficient rationale verification. Our demo video can be found at https://youtu.be/qUSjz-sxlBc.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "19",
        "title": "An Exploratory Study of ML Sketches and Visual Code Assistants",
        "author": [
            "Luís F. Gomes",
            "Vincent J. Hellendoorn",
            "Jonathan Aldrich",
            "Rui Abreu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13386",
        "abstract": "This paper explores the integration of Visual Code Assistants in Integrated Development Environments (IDEs). In Software Engineering, whiteboard sketching is often the initial step before coding, serving as a crucial collaboration tool for developers. Previous studies have investigated patterns in SE sketches and how they are used in practice, yet methods for directly using these sketches for code generation remain limited. The emergence of visually-equipped large language models presents an opportunity to bridge this gap, which is the focus of our research. In this paper, we built a first prototype of a Visual Code Assistant to get user feedback regarding in-IDE sketch-to-code tools. We conduct an experiment with 19 data scientists, most of whom regularly sketch as part of their job. We investigate developers' mental models by analyzing patterns commonly observed in their sketches when developing an ML workflow. Analysis indicates that diagrams were the preferred organizational component (52.6%), often accompanied by lists (42.1%) and numbered points (36.8%). Our tool converts their sketches into a Python notebook by querying an LLM. We use an LLM-as-judge setup to score the quality of the generated code, finding that even brief sketching can effectively generate useful code outlines. We also find a positive correlation between sketch time and the quality of the generated code. We conclude the study by conducting extensive interviews to assess the tool's usefulness, explore potential use cases, and understand developers' needs. As noted by participants, promising applications for these assistants include education, prototyping, and collaborative settings. Our findings signal promise for the next generation of Code Assistants to integrate visual information, both to improve code generation and to better leverage developers' existing sketching practices.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "20",
        "title": "Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion",
        "author": [
            "Massimiliano Viola",
            "Kevin Qu",
            "Nando Metzger",
            "Bingxin Ke",
            "Alexander Becker",
            "Konrad Schindler",
            "Anton Obukhov"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13389",
        "abstract": "Depth completion upgrades sparse depth measurements into dense depth maps guided by a conventional image. Existing methods for this highly ill-posed task operate in tightly constrained settings and tend to struggle when applied to images outside the training domain or when the available depth measurements are sparse, irregularly distributed, or of varying density. Inspired by recent advances in monocular depth estimation, we reframe depth completion as an image-conditional depth map generation guided by sparse measurements. Our method, Marigold-DC, builds on a pretrained latent diffusion model for monocular depth estimation and injects the depth observations as test-time guidance via an optimization scheme that runs in tandem with the iterative inference of denoising diffusion. The method exhibits excellent zero-shot generalization across a diverse range of environments and handles even extremely sparse guidance effectively. Our results suggest that contemporary monocular depth priors greatly robustify depth completion: it may be better to view the task as recovering dense depth from (dense) image pixels, guided by sparse depth; rather than as inpainting (sparse) depth, guided by an image. Project website: https://MarigoldDepthCompletion.github.io/",
        "tags": [
            "Depth Estimation",
            "Diffusion"
        ]
    },
    {
        "id": "21",
        "title": "MMHMR: Generative Masked Modeling for Hand Mesh Recovery",
        "author": [
            "Muhammad Usama Saleem",
            "Ekkasit Pinyoanuntapong",
            "Mayur Jagdishbhai Patel",
            "Hongfei Xue",
            "Ahmed Helmy",
            "Srijan Das",
            "Pu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13393",
        "abstract": "Reconstructing a 3D hand mesh from a single RGB image is challenging due to complex articulations, self-occlusions, and depth ambiguities. Traditional discriminative methods, which learn a deterministic mapping from a 2D image to a single 3D mesh, often struggle with the inherent ambiguities in 2D-to-3D mapping. To address this challenge, we propose MMHMR, a novel generative masked model for hand mesh recovery that synthesizes plausible 3D hand meshes by learning and sampling from the probabilistic distribution of the ambiguous 2D-to-3D mapping process. MMHMR consists of two key components: (1) a VQ-MANO, which encodes 3D hand articulations as discrete pose tokens in a latent space, and (2) a Context-Guided Masked Transformer that randomly masks out pose tokens and learns their joint distribution, conditioned on corrupted token sequences, image context, and 2D pose cues. This learned distribution facilitates confidence-guided sampling during inference, producing mesh reconstructions with low uncertainty and high precision. Extensive evaluations on benchmark and real-world datasets demonstrate that MMHMR achieves state-of-the-art accuracy, robustness, and realism in 3D hand mesh reconstruction. Project website: https://m-usamasaleem.github.io/publication/MMHMR/mmhmr.html",
        "tags": [
            "3D",
            "Transformer"
        ]
    },
    {
        "id": "22",
        "title": "Zero-Shot Low Light Image Enhancement with Diffusion Prior",
        "author": [
            "Joshua Cho",
            "Sara Aghajanzadeh",
            "Zhen Zhu",
            "D. A. Forsyth"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13401",
        "abstract": "Balancing aesthetic quality with fidelity when enhancing images from challenging, degraded sources is a core objective in computational photography. In this paper, we address low light image enhancement (LLIE), a task in which dark images often contain limited visible information. Diffusion models, known for their powerful image enhancement capacities, are a natural choice for this problem. However, their deep generative priors can also lead to hallucinations, introducing non-existent elements or substantially altering the visual semantics of the original scene. In this work, we introduce a novel zero-shot method for controlling and refining the generative behavior of diffusion models for dark-to-light image conversion tasks. Our method demonstrates superior performance over existing state-of-the-art methods in the task of low-light image enhancement, as evidenced by both quantitative metrics and qualitative analysis.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "23",
        "title": "The Role of Task Complexity in Reducing AI Plagiarism: A Study of Generative AI Tools",
        "author": [
            "Sacip Toker",
            "Mahir Akgun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13412",
        "abstract": "This study investigates whether assessments fostering higher-order thinking skills can reduce plagiarism involving generative AI tools. Participants completed three tasks of varying complexity in four groups: control, e-textbook, Google, and ChatGPT. Findings show that AI plagiarism decreases as task complexity increases, with higher-order tasks resulting in lower similarity scores and AI plagiarism percentages. The study also highlights the distinction between similarity scores and AI plagiarism, recommending both for effective plagiarism detection. Results suggest that assessments promoting higher-order thinking are a viable strategy for minimizing AI-driven plagiarism.",
        "tags": [
            "ChatGPT",
            "Detection"
        ]
    },
    {
        "id": "24",
        "title": "Exploring Transformer-Augmented LSTM for Temporal and Spatial Feature Learning in Trajectory Prediction",
        "author": [
            "Chandra Raskoti",
            "Weizi Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13419",
        "abstract": "Accurate vehicle trajectory prediction is crucial for ensuring safe and efficient autonomous driving. This work explores the integration of Transformer based model with Long Short-Term Memory (LSTM) based technique to enhance spatial and temporal feature learning in vehicle trajectory prediction. Here, a hybrid model that combines LSTMs for temporal encoding with a Transformer encoder for capturing complex interactions between vehicles is proposed. Spatial trajectory features of the neighboring vehicles are processed and goes through a masked scatter mechanism in a grid based environment, which is then combined with temporal trajectory of the vehicles. This combined trajectory data are learned by sequential LSTM encoding and Transformer based attention layers. The proposed model is benchmarked against predecessor LSTM based methods, including STA-LSTM, SA-LSTM, CS-LSTM, and NaiveLSTM. Our results, while not outperforming it's predecessor, demonstrate the potential of integrating Transformers with LSTM based technique to build interpretable trajectory prediction model. Future work will explore alternative architectures using Transformer applications to further enhance performance. This study provides a promising direction for improving trajectory prediction models by leveraging transformer based architectures, paving the way for more robust and interpretable vehicle trajectory prediction system.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "25",
        "title": "Detecting Machine-Generated Music with Explainability -- A Challenge and Early Benchmarks",
        "author": [
            "Yupei Li",
            "Qiyang Sun",
            "Hanqian Li",
            "Lucia Specia",
            "Björn W. Schuller"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13421",
        "abstract": "Machine-generated music (MGM) has become a groundbreaking innovation with wide-ranging applications, such as music therapy, personalised editing, and creative inspiration within the music industry. However, the unregulated proliferation of MGM presents considerable challenges to the entertainment, education, and arts sectors by potentially undermining the value of high-quality human compositions. Consequently, MGM detection (MGMD) is crucial for preserving the integrity of these fields. Despite its significance, MGMD domain lacks comprehensive benchmark results necessary to drive meaningful progress. To address this gap, we conduct experiments on existing large-scale datasets using a range of foundational models for audio processing, establishing benchmark results tailored to the MGMD task. Our selection includes traditional machine learning models, deep neural networks, Transformer-based architectures, and State Space Models (SSM). Recognising the inherently multimodal nature of music, which integrates both melody and lyrics, we also explore fundamental multimodal models in our experiments. Beyond providing basic binary classification outcomes, we delve deeper into model behaviour using multiple explainable Aritificial Intelligence (XAI) tools, offering insights into their decision-making processes. Our analysis reveals that ResNet18 performs the best according to in-domain and out-of-domain tests. By providing a comprehensive comparison of benchmark results and their interpretability, we propose several directions to inspire future research to develop more robust and effective detection methods for MGM.",
        "tags": [
            "Detection",
            "State Space Models",
            "Transformer"
        ]
    },
    {
        "id": "26",
        "title": "Generating Diverse Hypotheses for Inductive Reasoning",
        "author": [
            "Kang-il Lee",
            "Hyukhun Koh",
            "Dongryeol Lee",
            "Seunghyun Yoon",
            "Minsung Kim",
            "Kyomin Jung"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13422",
        "abstract": "Inductive reasoning - the process of inferring general rules from a small number of observations - is a fundamental aspect of human intelligence. Recent works suggest that large language models (LLMs) can engage in inductive reasoning by sampling multiple hypotheses about the rules and selecting the one that best explains the observations. However, due to the IID sampling, semantically redundant hypotheses are frequently generated, leading to significant wastage of compute. In this paper, we 1) demonstrate that increasing the temperature to enhance the diversity is limited due to text degeneration issue, and 2) propose a novel method to improve the diversity while maintaining text quality. We first analyze the effect of increasing the temperature parameter, which is regarded as the LLM's diversity control, on IID hypotheses. Our analysis shows that as temperature rises, diversity and accuracy of hypotheses increase up to a certain point, but this trend saturates due to text degeneration. To generate hypotheses that are more semantically diverse and of higher quality, we propose a novel approach inspired by human inductive reasoning, which we call Mixture of Concepts (MoC). When applied to several inductive reasoning benchmarks, MoC demonstrated significant performance improvements compared to standard IID sampling and other approaches.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "27",
        "title": "Safeguarding System Prompts for LLMs",
        "author": [
            "Zhifeng Jiang",
            "Zhihua Jin",
            "Guoliang He"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13426",
        "abstract": "Large language models (LLMs) are increasingly utilized in applications where system prompts, which guide model outputs, play a crucial role. These prompts often contain business logic and sensitive information, making their protection essential. However, adversarial and even regular user queries can exploit LLM vulnerabilities to expose these hidden prompts. To address this issue, we present PromptKeeper, a novel defense mechanism for system prompt privacy. By reliably detecting worst-case leakage and regenerating outputs without the system prompt when necessary, PromptKeeper ensures robust protection against prompt extraction attacks via either adversarial or regular queries, while preserving conversational capability and runtime efficiency during benign user interactions.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "28",
        "title": "Lightweight Safety Classification Using Pruned Language Models",
        "author": [
            "Mason Sawtell",
            "Tula Masterman",
            "Sandi Besen",
            "Jim Brown"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13435",
        "abstract": "In this paper, we introduce a novel technique for content safety and prompt injection classification for Large Language Models. Our technique, Layer Enhanced Classification (LEC), trains a Penalized Logistic Regression (PLR) classifier on the hidden state of an LLM's optimal intermediate transformer layer. By combining the computational efficiency of a streamlined PLR classifier with the sophisticated language understanding of an LLM, our approach delivers superior performance surpassing GPT-4o and special-purpose models fine-tuned for each task. We find that small general-purpose models (Qwen 2.5 sizes 0.5B, 1.5B, and 3B) and other transformer-based architectures like DeBERTa v3 are robust feature extractors allowing simple classifiers to be effectively trained on fewer than 100 high-quality examples. Importantly, the intermediate transformer layers of these models typically outperform the final layer across both classification tasks. Our results indicate that a single general-purpose LLM can be used to classify content safety, detect prompt injections, and simultaneously generate output tokens. Alternatively, these relatively small LLMs can be pruned to the optimal intermediate layer and used exclusively as robust feature extractors. Since our results are consistent on different transformer architectures, we infer that robust feature extraction is an inherent capability of most, if not all, LLMs.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "29",
        "title": "DarkIR: Robust Low-Light Image Restoration",
        "author": [
            "Daniel Feijoo",
            "Juan C. Benito",
            "Alvaro Garcia",
            "Marcos V. Conde"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13443",
        "abstract": "Photography during night or in dark conditions typically suffers from noise, low light and blurring issues due to the dim environment and the common use of long exposure. Although Deblurring and Low-light Image Enhancement (LLIE) are related under these conditions, most approaches in image restoration solve these tasks separately. In this paper, we present an efficient and robust neural network for multi-task low-light image restoration. Instead of following the current tendency of Transformer-based models, we propose new attention mechanisms to enhance the receptive field of efficient CNNs. Our method reduces the computational costs in terms of parameters and MAC operations compared to previous methods. Our model, DarkIR, achieves new state-of-the-art results on the popular LOLBlur, LOLv2 and Real-LOLBlur datasets, being able to generalize on real-world night and dark images. Code and models at https://github.com/cidautai/DarkIR",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "30",
        "title": "Pre-training a Density-Aware Pose Transformer for Robust LiDAR-based 3D Human Pose Estimation",
        "author": [
            "Xiaoqi An",
            "Lin Zhao",
            "Chen Gong",
            "Jun Li",
            "Jian Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13454",
        "abstract": "With the rapid development of autonomous driving, LiDAR-based 3D Human Pose Estimation (3D HPE) is becoming a research focus. However, due to the noise and sparsity of LiDAR-captured point clouds, robust human pose estimation remains challenging. Most of the existing methods use temporal information, multi-modal fusion, or SMPL optimization to correct biased results. In this work, we try to obtain sufficient information for 3D HPE only by modeling the intrinsic properties of low-quality point clouds. Hence, a simple yet powerful method is proposed, which provides insights both on modeling and augmentation of point clouds. Specifically, we first propose a concise and effective density-aware pose transformer (DAPT) to get stable keypoint representations. By using a set of joint anchors and a carefully designed exchange module, valid information is extracted from point clouds with different densities. Then 1D heatmaps are utilized to represent the precise locations of the keypoints. Secondly, a comprehensive LiDAR human synthesis and augmentation method is proposed to pre-train the model, enabling it to acquire a better human body prior. We increase the diversity of point clouds by randomly sampling human positions and orientations and by simulating occlusions through the addition of laser-level masks. Extensive experiments have been conducted on multiple datasets, including IMU-annotated LidarHuman26M, SLOPER4D, and manually annotated Waymo Open Dataset v2.0 (Waymo), HumanM3. Our method demonstrates SOTA performance in all scenarios. In particular, compared with LPFormer on Waymo, we reduce the average MPJPE by $10.0mm$. Compared with PRN on SLOPER4D, we notably reduce the average MPJPE by $20.7mm$.",
        "tags": [
            "3D",
            "Pose Estimation",
            "Transformer"
        ]
    },
    {
        "id": "31",
        "title": "SAVGBench: Benchmarking Spatially Aligned Audio-Video Generation",
        "author": [
            "Kazuki Shimada",
            "Christian Simon",
            "Takashi Shibuya",
            "Shusuke Takahashi",
            "Yuki Mitsufuji"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13462",
        "abstract": "This work addresses the lack of multimodal generative models capable of producing high-quality videos with spatially aligned audio. While recent advancements in generative models have been successful in video generation, they often overlook the spatial alignment between audio and visuals, which is essential for immersive experiences. To tackle this problem, we establish a new research direction in benchmarking Spatially Aligned Audio-Video Generation (SAVG). We propose three key components for the benchmark: dataset, baseline, and metrics. We introduce a spatially aligned audio-visual dataset, derived from an audio-visual dataset consisting of multichannel audio, video, and spatiotemporal annotations of sound events. We propose a baseline audio-visual diffusion model focused on stereo audio-visual joint learning to accommodate spatial sound. Finally, we present metrics to evaluate video and spatial audio quality, including a new spatial audio-visual alignment metric. Our experimental result demonstrates that gaps exist between the baseline model and ground truth in terms of video and audio quality, and spatial alignment between both modalities.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "32",
        "title": "Transducer Tuning: Efficient Model Adaptation for Software Tasks Using Code Property Graphs",
        "author": [
            "Imam Nur Bani Yusuf",
            "Lingxiao Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13467",
        "abstract": "Large language models have demonstrated promising performance across various software engineering tasks. While fine-tuning is a common practice to adapt these models for downstream tasks, it becomes challenging in resource-constrained environments due to increased memory requirements from growing trainable parameters in increasingly large language models. We introduce \\approach, a technique to adapt large models for downstream code tasks using Code Property Graphs (CPGs). Our approach introduces a modular component called \\transducer that enriches code embeddings with structural and dependency information from CPGs. The Transducer comprises two key components: Graph Vectorization Engine (GVE) and Attention-Based Fusion Layer (ABFL). GVE extracts CPGs from input source code and transforms them into graph feature vectors. ABFL then fuses those graphs feature vectors with initial code embeddings from a large language model. By optimizing these transducers for different downstream tasks, our approach enhances the models without the need to fine-tune them for specific tasks. We have evaluated \\approach on three downstream tasks: code summarization, assert generation, and code translation. Our results demonstrate competitive performance compared to full parameter fine-tuning while reducing up to 99\\% trainable parameters to save memory. \\approach also remains competitive against other fine-tuning approaches (e.g., LoRA, Prompt-Tuning, Prefix-Tuning) while using only 1.5\\%-80\\% of their trainable parameters. Our findings show that integrating structural and dependency information through Transducer Tuning enables more efficient model adaptation, making it easier for users to adapt large models in resource-constrained settings.",
        "tags": [
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "33",
        "title": "Gradual Vigilance and Interval Communication: Enhancing Value Alignment in Multi-Agent Debates",
        "author": [
            "Rui Zou",
            "Mengqi Wei",
            "Jintian Feng",
            "Qian Wan",
            "Jianwen Sun",
            "Sannyuya Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13471",
        "abstract": "In recent years, large language models have shown exceptional performance in fulfilling diverse human needs. However, their training data can introduce harmful content, underscoring the necessity for robust value alignment. Mainstream methods, which depend on feedback learning and supervised training, are resource-intensive and may constrain the full potential of the models. Multi-Agent Debate (MAD) offers a more efficient and innovative solution by enabling the generation of reliable answers through agent interactions. To apply MAD to value alignment, we examine the relationship between the helpfulness and harmlessness of debate outcomes and individual responses, and propose a MAD based framework Gradual Vigilance and Interval Communication (GVIC). GVIC allows agents to assess risks with varying levels of vigilance and to exchange diverse information through interval communication. We theoretically prove that GVIC optimizes debate efficiency while reducing communication overhead. Experimental results demonstrate that GVIC consistently outperforms baseline methods across various tasks and datasets, particularly excelling in harmfulness mitigation and fraud prevention. Additionally, GVIC exhibits strong adaptability across different base model sizes, including both unaligned and aligned models, and across various task types.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "34",
        "title": "A Statistical and Multi-Perspective Revisiting of the Membership Inference Attack in Large Language Models",
        "author": [
            "Bowen Chen",
            "Namgi Han",
            "Yusuke Miyao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13475",
        "abstract": "The lack of data transparency in Large Language Models (LLMs) has highlighted the importance of Membership Inference Attack (MIA), which differentiates trained (member) and untrained (non-member) data. Though it shows success in previous studies, recent research reported a near-random performance in different settings, highlighting a significant performance inconsistency. We assume that a single setting doesn't represent the distribution of the vast corpora, causing members and non-members with different distributions to be sampled and causing inconsistency. In this study, instead of a single setting, we statistically revisit MIA methods from various settings with thousands of experiments for each MIA method, along with study in text feature, embedding, threshold decision, and decoding dynamics of members and non-members. We found that (1) MIA performance improves with model size and varies with domains, while most methods do not statistically outperform baselines, (2) Though MIA performance is generally low, a notable amount of differentiable member and non-member outliers exists and vary across MIA methods, (3) Deciding a threshold to separate members and non-members is an overlooked challenge, (4) Text dissimilarity and long text benefit MIA performance, (5) Differentiable or not is reflected in the LLM embedding, (6) Member and non-members show different decoding dynamics.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "35",
        "title": "Real-time One-Step Diffusion-based Expressive Portrait Videos Generation",
        "author": [
            "Hanzhong Guo",
            "Hongwei Yi",
            "Daquan Zhou",
            "Alexander William Bergman",
            "Michael Lingelbach",
            "Yizhou Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13479",
        "abstract": "Latent diffusion models have made great strides in generating expressive portrait videos with accurate lip-sync and natural motion from a single reference image and audio input. However, these models are far from real-time, often requiring many sampling steps that take minutes to generate even one second of video-significantly limiting practical use. We introduce OSA-LCM (One-Step Avatar Latent Consistency Model), paving the way for real-time diffusion-based avatars. Our method achieves comparable video quality to existing methods but requires only one sampling step, making it more than 10x faster. To accomplish this, we propose a novel avatar discriminator design that guides lip-audio consistency and motion expressiveness to enhance video quality in limited sampling steps. Additionally, we employ a second-stage training architecture using an editing fine-tuned method (EFT), transforming video generation into an editing task during training to effectively address the temporal gap challenge in single-step generation. Experiments demonstrate that OSA-LCM outperforms existing open-source portrait video generation models while operating more efficiently with a single sampling step.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "36",
        "title": "T$^3$-S2S: Training-free Triplet Tuning for Sketch to Scene Generation",
        "author": [
            "Zhenhong Sun",
            "Yifu Wang",
            "Yonhon Ng",
            "Yunfei Duan",
            "Daoyi Dong",
            "Hongdong Li",
            "Pan Ji"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13486",
        "abstract": "Scene generation is crucial to many computer graphics applications. Recent advances in generative AI have streamlined sketch-to-image workflows, easing the workload for artists and designers in creating scene concept art. However, these methods often struggle for complex scenes with multiple detailed objects, sometimes missing small or uncommon instances. In this paper, we propose a Training-free Triplet Tuning for Sketch-to-Scene (T3-S2S) generation after reviewing the entire cross-attention mechanism. This scheme revitalizes the existing ControlNet model, enabling effective handling of multi-instance generations, involving prompt balance, characteristics prominence, and dense tuning. Specifically, this approach enhances keyword representation via the prompt balance module, reducing the risk of missing critical instances. It also includes a characteristics prominence module that highlights TopK indices in each channel, ensuring essential features are better represented based on token sketches. Additionally, it employs dense tuning to refine contour details in the attention map, compensating for instance-related regions. Experiments validate that our triplet tuning approach substantially improves the performance of existing sketch-to-image models. It consistently generates detailed, multi-instance 2D images, closely adhering to the input prompts and enhancing visual quality in complex multi-instance scenes. Code is available at https://github.com/chaos-sun/t3s2s.git.",
        "tags": [
            "ControlNet"
        ]
    },
    {
        "id": "37",
        "title": "Evaluating the Effect of Pretesting with Conversational AI on Retention of Needed Information",
        "author": [
            "Mahir Akgun",
            "Sacip Toker"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13487",
        "abstract": "This study explores the role of pretesting when integrated with conversational AI tools, specifically ChatGPT, in enhancing learning outcomes. Drawing on existing research, which demonstrates the benefits of pretesting in memory activation and retention, this experiment extends these insights into the context of digital learning environments. A randomized true experimental study was utilized. Participants were divided into two groups: one engaged in pretesting before using ChatGPT for a problem-solving task involving chi-square analysis, while the control group accessed ChatGPT immediately. The results indicate that the pretest group significantly outperformed the no-pretest group in a subsequent test, which suggests that pretesting enhances the retention of complex material. This study contributes to the field by demonstrating that pretesting can augment the learning process in technology-assisted environments by preparing the memory and promoting active engagement with the material. The findings also suggest that learning strategies like pretesting retain their relevance in the context of rapidly evolving AI technologies. Further research and practical implications are presented.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "38",
        "title": "Refining Salience-Aware Sparse Fine-Tuning Strategies for Language Models",
        "author": [
            "Xinxin Liu",
            "Aaron Thomas",
            "Cheng Zhang",
            "Jianyi Cheng",
            "Yiren Zhao",
            "Xitong Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13488",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank adaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT (SPEFT), which introduces trainable sparse adaptations to the weight matrices in the model, offering greater flexibility in selecting fine-tuned parameters compared to low-rank methods. We conduct the first systematic evaluation of salience metrics for SPEFT, inspired by zero-cost NAS proxies, and identify simple gradient-based metrics is reliable, and results are on par with the best alternatives, offering both computational efficiency and robust performance. Additionally, we compare static and dynamic masking strategies, finding that static masking, which predetermines non-zero entries before training, delivers efficiency without sacrificing performance, while dynamic masking offers no substantial benefits. Across NLP tasks, a simple gradient-based, static SPEFT consistently outperforms other fine-tuning methods for LLMs, providing a simple yet effective baseline for SPEFT. Our work challenges the notion that complexity is necessary for effective PEFT. Our work is open source and available to the community at [https://github.com/0-ml/speft].",
        "tags": [
            "LLMs",
            "LoRA"
        ]
    },
    {
        "id": "39",
        "title": "Efficient Language-instructed Skill Acquisition via Reward-Policy Co-Evolution",
        "author": [
            "Changxin Huang",
            "Yanbin Chang",
            "Junfan Lin",
            "Junyang Liang",
            "Runhao Zeng",
            "Jianqiang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13492",
        "abstract": "The ability to autonomously explore and resolve tasks with minimal human guidance is crucial for the self-development of embodied intelligence. Although reinforcement learning methods can largely ease human effort, it's challenging to design reward functions for real-world tasks, especially for high-dimensional robotic control, due to complex relationships among joints and tasks. Recent advancements large language models (LLMs) enable automatic reward function design. However, approaches evaluate reward functions by re-training policies from scratch placing an undue burden on the reward function, expecting it to be effective throughout the whole policy improvement process. We argue for a more practical strategy in robotic autonomy, focusing on refining existing policies with policy-dependent reward functions rather than a universal one. To this end, we propose a novel reward-policy co-evolution framework where the reward function and the learned policy benefit from each other's progressive on-the-fly improvements, resulting in more efficient and higher-performing skill acquisition. Specifically, the reward evolution process translates the robot's previous best reward function, descriptions of tasks and environment into text inputs. These inputs are used to query LLMs to generate a dynamic amount of reward function candidates, ensuring continuous improvement at each round of evolution. For policy evolution, our method generates new policy populations by hybridizing historically optimal and random policies. Through an improved Bayesian optimization, our approach efficiently and robustly identifies the most capable and plastic reward-policy combination, which then proceeds to the next round of co-evolution. Despite using less data, our approach demonstrates an average normalized improvement of 95.3% across various high-dimensional robotic skill learning tasks.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Robot"
        ]
    },
    {
        "id": "40",
        "title": "VaeDiff-DocRE: End-to-end Data Augmentation Framework for Document-level Relation Extraction",
        "author": [
            "Khai Phan Tran",
            "Wen Hua",
            "Xue Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13503",
        "abstract": "Document-level Relation Extraction (DocRE) aims to identify relationships between entity pairs within a document. However, most existing methods assume a uniform label distribution, resulting in suboptimal performance on real-world, imbalanced datasets. To tackle this challenge, we propose a novel data augmentation approach using generative models to enhance data from the embedding space. Our method leverages the Variational Autoencoder (VAE) architecture to capture all relation-wise distributions formed by entity pair representations and augment data for underrepresented relations. To better capture the multi-label nature of DocRE, we parameterize the VAE's latent space with a Diffusion Model. Additionally, we introduce a hierarchical training framework to integrate the proposed VAE-based augmentation module into DocRE systems. Experiments on two benchmark datasets demonstrate that our method outperforms state-of-the-art models, effectively addressing the long-tail distribution problem in DocRE.",
        "tags": [
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": "41",
        "title": "Urban Air Temperature Prediction using Conditional Diffusion Models",
        "author": [
            "Siyang Dai",
            "Jun Liu",
            "Ngai-Man Cheung"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13504",
        "abstract": "Urbanization as a global trend has led to many environmental challenges, including the urban heat island (UHI) effect. The increase in temperature has a significant impact on the well-being of urban residents. Air temperature ($T_a$) at 2m above the surface is a key indicator of the UHI effect. How land use land cover (LULC) affects $T_a$ is a critical research question which requires high-resolution (HR) $T_a$ data at neighborhood scale. However, weather stations providing $T_a$ measurements are sparsely distributed e.g. more than 10km apart; and numerical models are impractically slow and computationally expensive. In this work, we propose a novel method to predict HR $T_a$ at 100m ground separation distance (gsd) using land surface temperature (LST) and other LULC related features which can be easily obtained from satellite imagery. Our method leverages diffusion models for the first time to generate accurate and visually realistic HR $T_a$ maps, which outperforms prior methods. We pave the way for meteorological research using computer vision techniques by providing a dataset of an extended spatial and temporal coverage, and a high spatial resolution as a benchmark for future research. Furthermore, we show that our model can be applied to urban planning by simulating the impact of different urban designs on $T_a$.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "42",
        "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data Presentation",
        "author": [
            "Yunqi Guo",
            "Kaiyuan Hou",
            "Heming Fu",
            "Hongkai Chen",
            "Zhenyu Yan",
            "Guoliang Xing",
            "Xiaofan Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13509",
        "abstract": "Understanding sensor data can be challenging for non-experts because of the complexity and unique semantic meanings of sensor modalities. This calls for intuitive and effective methods to present sensor information. However, creating intuitive sensor data visualizations presents three key challenges: the variability of sensor readings, gaps in domain comprehension, and the dynamic nature of sensor data. To address these issues, we develop Vivar, a novel AR system that integrates multi-modal sensor data and presents 3D volumetric content for visualization. In particular, we introduce a cross-modal embedding approach that maps sensor data into a pre-trained visual embedding space through barycentric interpolation. This allows for accurate and continuous integration of multi-modal sensor information. Vivar also incorporates sensor-aware AR scene generation using foundation models and 3D Gaussian Splatting (3DGS) without requiring domain expertise. In addition, Vivar leverages latent reuse and caching strategies to accelerate 2D and AR content generation. Our extensive experiments demonstrate that our system achieves 11$\\times$ latency reduction without compromising quality. A user study involving over 485 participants, including domain experts, demonstrates Vivar's effectiveness in accuracy, consistency, and real-world applicability, paving the way for more intuitive sensor data visualization.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "43",
        "title": "ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning",
        "author": [
            "Yi Huang",
            "Fangyin Cheng",
            "Fan Zhou",
            "Jiahui Li",
            "Jian Gong",
            "Hongjun Yang",
            "Zhidong Fan",
            "Caigao Jiang",
            "Siqiao Xue",
            "Faqiang Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13520",
        "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in data analytics when integrated with Multi-Agent Systems (MAS). However, these systems often struggle with complex tasks that involve diverse functional requirements and intricate data processing challenges, necessitating customized solutions that lack broad applicability. Furthermore, current MAS fail to emulate essential human-like traits such as self-planning, self-monitoring, and collaborative work in dynamic environments, leading to inefficiencies and resource wastage. To address these limitations, we propose ROMAS, a novel Role-Based M ulti-A gent System designed to adapt to various scenarios while enabling low code development and one-click deployment. ROMAS has been effectively deployed in DB-GPT [Xue et al., 2023a, 2024b], a well-known project utilizing LLM-powered database analytics, showcasing its practical utility in real-world scenarios. By integrating role-based collaborative mechanisms for self-monitoring and self-planning, and leveraging existing MAS capabilities to enhance database interactions, ROMAS offers a more effective and versatile solution. Experimental evaluations of ROMAS demonstrate its superiority across multiple scenarios, highlighting its potential to advance the field of multi-agent data analytics.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "44",
        "title": "Hybrid Data-Free Knowledge Distillation",
        "author": [
            "Jialiang Tang",
            "Shuo Chen",
            "Chen Gong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13525",
        "abstract": "Data-free knowledge distillation aims to learn a compact student network from a pre-trained large teacher network without using the original training data of the teacher network. Existing collection-based and generation-based methods train student networks by collecting massive real examples and generating synthetic examples, respectively. However, they inevitably become weak in practical scenarios due to the difficulties in gathering or emulating sufficient real-world data. To solve this problem, we propose a novel method called \\textbf{H}ybr\\textbf{i}d \\textbf{D}ata-\\textbf{F}ree \\textbf{D}istillation (HiDFD), which leverages only a small amount of collected data as well as generates sufficient examples for training student networks. Our HiDFD comprises two primary modules, \\textit{i.e.}, the teacher-guided generation and student distillation. The teacher-guided generation module guides a Generative Adversarial Network (GAN) by the teacher network to produce high-quality synthetic examples from very few real-world collected examples. Specifically, we design a feature integration mechanism to prevent the GAN from overfitting and facilitate the reliable representation learning from the teacher network. Meanwhile, we drive a category frequency smoothing technique via the teacher network to balance the generative training of each category. In the student distillation module, we explore a data inflation strategy to properly utilize a blend of real and synthetic data to train the student network via a classifier-sharing-based feature alignment technique. Intensive experiments across multiple benchmarks demonstrate that our HiDFD can achieve state-of-the-art performance using 120 times less collected data than existing methods. Code is available at https://github.com/tangjialiang97/HiDFD.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "45",
        "title": "Information-Theoretic Generative Clustering of Documents",
        "author": [
            "Xin Du",
            "Kumiko Tanaka-Ishii"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13534",
        "abstract": "We present {\\em generative clustering} (GC) for clustering a set of documents, $\\mathrm{X}$, by using texts $\\mathrm{Y}$ generated by large language models (LLMs) instead of by clustering the original documents $\\mathrm{X}$. Because LLMs provide probability distributions, the similarity between two documents can be rigorously defined in an information-theoretic manner by the KL divergence. We also propose a natural, novel clustering algorithm by using importance sampling. We show that GC achieves the state-of-the-art performance, outperforming any previous clustering method often by a large margin. Furthermore, we show an application to generative document retrieval in which documents are indexed via hierarchical clustering and our method improves the retrieval accuracy.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "46",
        "title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained with Simple Rules",
        "author": [
            "Kejie Chen",
            "Lin Wang",
            "Qinghai Zhang",
            "Renjun Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13536",
        "abstract": "Recent studies have highlighted the limitations of large language models in mathematical reasoning, particularly their inability to capture the underlying logic. Inspired by meta-learning, we propose that models should acquire not only task-specific knowledge but also transferable problem-solving skills. We introduce MetaRuleGPT, a novel Transformer-based architecture that performs precise numerical calculations and complex logical operations by learning and combining different rules. In contrast with traditional training sets, which are heavily composed of massive raw instance data, MetaRuleGPT is pre-trained on much less abstract datasets containing basic, compound, and iterative rules for mathematical reasoning. Extensive experimental results demonstrate MetaRuleGPT can mimic human's rule-following capabilities, break down complexity, and iteratively derive accurate results for complex mathematical problems. These findings prove the potential of rule learning to enhance the numerical reasoning abilities of language models.",
        "tags": [
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "47",
        "title": "Query-centric Audio-Visual Cognition Network for Moment Retrieval, Segmentation and Step-Captioning",
        "author": [
            "Yunbin Tu",
            "Liang Li",
            "Li Su",
            "Qingming Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13543",
        "abstract": "Video has emerged as a favored multimedia format on the internet. To better gain video contents, a new topic HIREST is presented, including video retrieval, moment retrieval, moment segmentation, and step-captioning. The pioneering work chooses the pre-trained CLIP-based model for video retrieval, and leverages it as a feature extractor for other three challenging tasks solved in a multi-task learning paradigm. Nevertheless, this work struggles to learn the comprehensive cognition of user-preferred content, due to disregarding the hierarchies and association relations across modalities. In this paper, guided by the shallow-to-deep principle, we propose a query-centric audio-visual cognition (QUAG) network to construct a reliable multi-modal representation for moment retrieval, segmentation and step-captioning. Specifically, we first design the modality-synergistic perception to obtain rich audio-visual content, by modeling global contrastive alignment and local fine-grained interaction between visual and audio modalities. Then, we devise the query-centric cognition that uses the deep-level query to perform the temporal-channel filtration on the shallow-level audio-visual representation. This can cognize user-preferred content and thus attain a query-centric audio-visual representation for three tasks. Extensive experiments show QUAG achieves the SOTA results on HIREST. Further, we test QUAG on the query-based video summarization task and verify its good generalization.",
        "tags": [
            "CLIP",
            "Segmentation"
        ]
    },
    {
        "id": "48",
        "title": "Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields",
        "author": [
            "Tao Lu",
            "Ankit Dhiman",
            "R Srinath",
            "Emre Arslan",
            "Angela Xing",
            "Yuanbo Xiangli",
            "R Venkatesh Babu",
            "Srinath Sridhar"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13547",
        "abstract": "Novel-view synthesis is an important problem in computer vision with applications in 3D reconstruction, mixed reality, and robotics. Recent methods like 3D Gaussian Splatting (3DGS) have become the preferred method for this task, providing high-quality novel views in real time. However, the training time of a 3DGS model is slow, often taking 30 minutes for a scene with 200 views. In contrast, our goal is to reduce the optimization time by training for fewer steps while maintaining high rendering quality. Specifically, we combine the guidance from both the position error and the appearance error to achieve a more effective densification. To balance the rate between adding new Gaussians and fitting old Gaussians, we develop a convergence-aware budget control mechanism. Moreover, to make the densification process more reliable, we selectively add new Gaussians from mostly visited regions. With these designs, we reduce the Gaussian optimization steps to one-third of the previous approach while achieving a comparable or even better novel view rendering quality. To further facilitate the rapid fitting of 4K resolution images, we introduce a dilation-based rendering technique. Our method, Turbo-GS, speeds up optimization for typical scenes and scales well to high-resolution (4K) scenarios on standard datasets. Through extensive experiments, we show that our method is significantly faster in optimization than other methods while retaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Robotics"
        ]
    },
    {
        "id": "49",
        "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
        "author": [
            "Xuhan Zuo",
            "Minghao Wang",
            "Tianqing Zhu",
            "Shui Yu",
            "Wanlei Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13551",
        "abstract": "Large language models (LLMs) have transformed the way computers understand and process human language, but using them effectively across different organizations remains still difficult. When organizations work together to improve LLMs, they face several main challenges. First, organizations hesitate to share their valuable data with others. Second, competition between organizations creates trust problems during collaboration. Third, new privacy laws require organizations to be able to delete specific data when requested, which is especially difficult when multiple organizations are learning from shared data. Traditional federated learning approaches do not address these interconnected challenges, particularly in scenarios where participants cannot fully trust each other or the central aggregator. To overcome these limitations, we propose a hybrid blockchain-based federated learning framework that uniquely combines public and private blockchain architectures with multi-agent reinforcement learning. Our framework enables transparent sharing of model update through the public blockchain while protecting sensitive computations in private chains. Each organization operates as an intelligent agent, using Q-learning to optimize its participation strategy and resource allocation, thus aligning individual incentives with collective goals. Notably, we introduce an efficient unlearning mechanism based on Low-Rank Adaptation (LoRA) that enables selective removal of specific data contributions without compromising the model's overall performance. Through extensive experimentation on real-world datasets, we demonstrate that our framework effectively balances privacy protection, trust establishment, and regulatory compliance while maintaining high model performance.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "50",
        "title": "Combining Aggregated Attention and Transformer Architecture for Accurate and Efficient Performance of Spiking Neural Networks",
        "author": [
            "Hangming Zhang",
            "Alexander Sboev",
            "Roman Rybka",
            "Qiang Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13553",
        "abstract": "Spiking Neural Networks have attracted significant attention in recent years due to their distinctive low-power characteristics. Meanwhile, Transformer models, known for their powerful self-attention mechanisms and parallel processing capabilities, have demonstrated exceptional performance across various domains, including natural language processing and computer vision. Despite the significant advantages of both SNNs and Transformers, directly combining the low-power benefits of SNNs with the high performance of Transformers remains challenging. Specifically, while the sparse computing mode of SNNs contributes to reduced energy consumption, traditional attention mechanisms depend on dense matrix computations and complex softmax operations. This reliance poses significant challenges for effective execution in low-power scenarios. Given the tremendous success of Transformers in deep learning, it is a necessary step to explore the integration of SNNs and Transformers to harness the strengths of both. In this paper, we propose a novel model architecture, Spike Aggregation Transformer (SAFormer), that integrates the low-power characteristics of SNNs with the high-performance advantages of Transformer models. The core contribution of SAFormer lies in the design of the Spike Aggregated Self-Attention (SASA) mechanism, which significantly simplifies the computation process by calculating attention weights using only the spike matrices query and key, thereby effectively reducing energy consumption. Additionally, we introduce a Depthwise Convolution Module (DWC) to enhance the feature extraction capabilities, further improving overall accuracy. We evaluated and demonstrated that SAFormer outperforms state-of-the-art SNNs in both accuracy and energy consumption, highlighting its significant advantages in low-power and high-performance computing.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "51",
        "title": "Seeking Consistent Flat Minima for Better Domain Generalization via Refining Loss Landscapes",
        "author": [
            "Aodi Li",
            "Liansheng Zhuang",
            "Xiao Long",
            "Minghong Yao",
            "Shafei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13573",
        "abstract": "Domain generalization aims to learn a model from multiple training domains and generalize it to unseen test domains. Recent theory has shown that seeking the deep models, whose parameters lie in the flat minima of the loss landscape, can significantly reduce the out-of-domain generalization error. However, existing methods often neglect the consistency of loss landscapes in different domains, resulting in models that are not simultaneously in the optimal flat minima in all domains, which limits their generalization ability. To address this issue, this paper proposes an iterative Self-Feedback Training (SFT) framework to seek consistent flat minima that are shared across different domains by progressively refining loss landscapes during training. It alternatively generates a feedback signal by measuring the inconsistency of loss landscapes in different domains and refines these loss landscapes for greater consistency using this feedback signal. Benefiting from the consistency of the flat minima within these refined loss landscapes, our SFT helps achieve better out-of-domain generalization. Extensive experiments on DomainBed demonstrate superior performances of SFT when compared to state-of-the-art sharpness-aware methods and other prevalent DG baselines. On average across five DG benchmarks, SFT surpasses the sharpness-aware minimization by 2.6% with ResNet-50 and 1.5% with ViT-B/16, respectively. The code will be available soon.",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "52",
        "title": "Generating Long-form Story Using Dynamic Hierarchical Outlining with Memory-Enhancement",
        "author": [
            "Qianyue Wang",
            "Jinwu Hu",
            "Zhengping Li",
            "Yufeng Wang",
            "daiyuan li",
            "Yu Hu",
            "Mingkui Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13575",
        "abstract": "Long-form story generation task aims to produce coherent and sufficiently lengthy text, essential for applications such as novel writingand interactive storytelling. However, existing methods, including LLMs, rely on rigid outlines or lack macro-level planning, making it difficult to achieve both contextual consistency and coherent plot development in long-form story generation. To address this issues, we propose Dynamic Hierarchical Outlining with Memory-Enhancement long-form story generation method, named DOME, to generate the long-form story with coherent content and plot. Specifically, the Dynamic Hierarchical Outline(DHO) mechanism incorporates the novel writing theory into outline planning and fuses the plan and writing stages together, improving the coherence of the plot by ensuring the plot completeness and adapting to the uncertainty during story generation. A Memory-Enhancement Module (MEM) based on temporal knowledge graphs is introduced to store and access the generated content, reducing contextual conflicts and improving story coherence. Finally, we propose a Temporal Conflict Analyzer leveraging temporal knowledge graphs to automatically evaluate the contextual consistency of long-form story. Experiments demonstrate that DOME significantly improves the fluency, coherence, and overall quality of generated long stories compared to state-of-the-art methods.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "53",
        "title": "Socio-Culturally Aware Evaluation Framework for LLM-Based Content Moderation",
        "author": [
            "Shanu Kumar",
            "Gauri Kholkar",
            "Saish Mendke",
            "Anubhav Sadana",
            "Parag Agrawal",
            "Sandipan Dandapat"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13578",
        "abstract": "With the growth of social media and large language models, content moderation has become crucial. Many existing datasets lack adequate representation of different groups, resulting in unreliable assessments. To tackle this, we propose a socio-culturally aware evaluation framework for LLM-driven content moderation and introduce a scalable method for creating diverse datasets using persona-based generation. Our analysis reveals that these datasets provide broader perspectives and pose greater challenges for LLMs than diversity-focused generation methods without personas. This challenge is especially pronounced in smaller LLMs, emphasizing the difficulties they encounter in moderating such diverse content.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "54",
        "title": "EvoWiki: Evaluating LLMs on Evolving Knowledge",
        "author": [
            "Wei Tang",
            "Yixin Cao",
            "Yang Deng",
            "Jiahao Ying",
            "Bo Wang",
            "Yizhe Yang",
            "Yuyue Zhao",
            "Qi Zhang",
            "Xuanjing Huang",
            "Yugang Jiang",
            "Yong Liao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13582",
        "abstract": "Knowledge utilization is a critical aspect of LLMs, and understanding how they adapt to evolving knowledge is essential for their effective deployment. However, existing benchmarks are predominantly static, failing to capture the evolving nature of LLMs and knowledge, leading to inaccuracies and vulnerabilities such as contamination. In this paper, we introduce EvoWiki, an evolving dataset designed to reflect knowledge evolution by categorizing information into stable, evolved, and uncharted states. EvoWiki is fully auto-updatable, enabling precise evaluation of continuously changing knowledge and newly released LLMs. Through experiments with Retrieval-Augmented Generation (RAG) and Contunual Learning (CL), we evaluate how effectively LLMs adapt to evolving knowledge. Our results indicate that current models often struggle with evolved knowledge, frequently providing outdated or incorrect responses. Moreover, the dataset highlights a synergistic effect between RAG and CL, demonstrating their potential to better adapt to evolving knowledge. EvoWiki provides a robust benchmark for advancing future research on the knowledge evolution capabilities of large language models.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "55",
        "title": "SemiDFL: A Semi-Supervised Paradigm for Decentralized Federated Learning",
        "author": [
            "Xinyang Liu",
            "Pengchao Han",
            "Xuan Li",
            "Bo Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13589",
        "abstract": "Decentralized federated learning (DFL) realizes cooperative model training among connected clients without relying on a central server, thereby mitigating communication bottlenecks and eliminating the single-point failure issue present in centralized federated learning (CFL). Most existing work on DFL focuses on supervised learning, assuming each client possesses sufficient labeled data for local training. However, in real-world applications, much of the data is unlabeled. We address this by considering a challenging yet practical semisupervised learning (SSL) scenario in DFL, where clients may have varying data sources: some with few labeled samples, some with purely unlabeled data, and others with both. In this work, we propose SemiDFL, the first semi-supervised DFL method that enhances DFL performance in SSL scenarios by establishing a consensus in both data and model spaces. Specifically, we utilize neighborhood information to improve the quality of pseudo-labeling, which is crucial for effectively leveraging unlabeled data. We then design a consensusbased diffusion model to generate synthesized data, which is used in combination with pseudo-labeled data to create mixed datasets. Additionally, we develop an adaptive aggregation method that leverages the model accuracy of synthesized data to further enhance SemiDFL performance. Through extensive experimentation, we demonstrate the remarkable performance superiority of the proposed DFL-Semi method over existing CFL and DFL schemes in both IID and non-IID SSL scenarios.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "56",
        "title": "Beyond Outcomes: Transparent Assessment of LLM Reasoning in Games",
        "author": [
            "Wenye Lin",
            "Jonathan Roberts",
            "Yunhan Yang",
            "Samuel Albanie",
            "Zongqing Lu",
            "Kai Han"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13602",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in real-world applications that demand complex reasoning. To track progress, robust benchmarks are required to evaluate their capabilities beyond superficial pattern recognition. However, current LLM reasoning benchmarks often face challenges such as insufficient interpretability, performance saturation or data contamination. To address these challenges, we introduce GAMEBoT, a gaming arena designed for rigorous and transparent assessment of LLM reasoning capabilities. GAMEBoT decomposes complex reasoning in games into predefined modular subproblems. This decomposition allows us to design a suite of Chain-of-Thought (CoT) prompts that leverage domain knowledge to guide LLMs in addressing these subproblems before action selection. Furthermore, we develop a suite of rule-based algorithms to generate ground truth for these subproblems, enabling rigorous validation of the LLMs' intermediate reasoning steps. This approach facilitates evaluation of both the quality of final actions and the accuracy of the underlying reasoning process. GAMEBoT also naturally alleviates the risk of data contamination through dynamic games and head-to-head LLM competitions. We benchmark 17 prominent LLMs across eight games, encompassing various strategic abilities and game characteristics. Our results suggest that GAMEBoT presents a significant challenge, even when LLMs are provided with detailed CoT prompts. Project page: \\url{https://visual-ai.github.io/gamebot}",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "57",
        "title": "Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production",
        "author": [
            "Shengeng Tang",
            "Jiayi He",
            "Dan Guo",
            "Yanyan Wei",
            "Feng Li",
            "Richang Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13609",
        "abstract": "Sign Language Production (SLP) aims to generate semantically consistent sign videos from textual statements, where the conversion from textual glosses to sign poses (G2P) is a crucial step. Existing G2P methods typically treat sign poses as discrete three-dimensional coordinates and directly fit them, which overlooks the relative positional relationships among joints. To this end, we provide a new perspective, constraining joint associations and gesture details by modeling the limb bones to improve the accuracy and naturalness of the generated poses. In this work, we propose a pioneering iconicity disentangled diffusion framework, termed Sign-IDD, specifically designed for SLP. Sign-IDD incorporates a novel Iconicity Disentanglement (ID) module to bridge the gap between relative positions among joints. The ID module disentangles the conventional 3D joint representation into a 4D bone representation, comprising the 3D spatial direction vector and 1D spatial distance vector between adjacent joints. Additionally, an Attribute Controllable Diffusion (ACD) module is introduced to further constrain joint associations, in which the attribute separation layer aims to separate the bone direction and length attributes, and the attribute control layer is designed to guide the pose generation by leveraging the above attributes. The ACD module utilizes the gloss embeddings as semantic conditions and finally generates sign poses from noise embeddings. Extensive experiments on PHOENIX14T and USTC-CSL datasets validate the effectiveness of our method. The code is available at: https://github.com/NaVi-start/Sign-IDD.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "58",
        "title": "Robust Tracking via Mamba-based Context-aware Token Learning",
        "author": [
            "Jinxia Xie",
            "Bineng Zhong",
            "Qihua Liang",
            "Ning Li",
            "Zhiyi Mo",
            "Shuxiang Song"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13611",
        "abstract": "How to make a good trade-off between performance and computational cost is crucial for a tracker. However, current famous methods typically focus on complicated and time-consuming learning that combining temporal and appearance information by input more and more images (or features). Consequently, these methods not only increase the model's computational source and learning burden but also introduce much useless and potentially interfering information. To alleviate the above issues, we propose a simple yet robust tracker that separates temporal information learning from appearance modeling and extracts temporal relations from a set of representative tokens rather than several images (or features). Specifically, we introduce one track token for each frame to collect the target's appearance information in the backbone. Then, we design a mamba-based Temporal Module for track tokens to be aware of context by interacting with other track tokens within a sliding window. This module consists of a mamba layer with autoregressive characteristic and a cross-attention layer with strong global perception ability, ensuring sufficient interaction for track tokens to perceive the appearance changes and movement trends of the target. Finally, track tokens serve as a guidance to adjust the appearance feature for the final prediction in the head. Experiments show our method is effective and achieves competitive performance on multiple benchmarks at a real-time speed. Code and trained models will be available at https://github.com/GXNU-ZhongLab/TemTrack.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "59",
        "title": "Are LLMs Good Literature Review Writers? Evaluating the Literature Review Writing Ability of Large Language Models",
        "author": [
            "Xuemei Tang",
            "Xufeng Duan",
            "Zhenguang G. Cai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13612",
        "abstract": "The literature review is a crucial form of academic writing that involves complex processes of literature collection, organization, and summarization. The emergence of large language models (LLMs) has introduced promising tools to automate these processes. However, their actual capabilities in writing comprehensive literature reviews remain underexplored, such as whether they can generate accurate and reliable references. To address this gap, we propose a framework to assess the literature review writing ability of LLMs automatically. We evaluate the performance of LLMs across three tasks: generating references, writing abstracts, and writing literature reviews. We employ external tools for a multidimensional evaluation, which includes assessing hallucination rates in references, semantic coverage, and factual consistency with human-written context. By analyzing the experimental results, we find that, despite advancements, even the most sophisticated models still cannot avoid generating hallucinated references. Additionally, different models exhibit varying performance in literature review writing across different disciplines.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "60",
        "title": "MambaLCT: Boosting Tracking via Long-term Context State Space Model",
        "author": [
            "Xiaohai Li",
            "Bineng Zhong",
            "Qihua Liang",
            "Guorong Li",
            "Zhiyi Mo",
            "Shuxiang Song"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13615",
        "abstract": "Effectively constructing context information with long-term dependencies from video sequences is crucial for object tracking. However, the context length constructed by existing work is limited, only considering object information from adjacent frames or video clips, leading to insufficient utilization of contextual information. To address this issue, we propose MambaLCT, which constructs and utilizes target variation cues from the first frame to the current frame for robust tracking. First, a novel unidirectional Context Mamba module is designed to scan frame features along the temporal dimension, gathering target change cues throughout the entire sequence. Specifically, target-related information in frame features is compressed into a hidden state space through selective scanning mechanism. The target information across the entire video is continuously aggregated into target variation cues. Next, we inject the target change cues into the attention mechanism, providing temporal information for modeling the relationship between the template and search frames. The advantage of MambaLCT is its ability to continuously extend the length of the context, capturing complete target change cues, which enhances the stability and robustness of the tracker. Extensive experiments show that long-term context information enhances the model's ability to perceive targets in complex scenarios. MambaLCT achieves new SOTA performance on six benchmarks while maintaining real-time running speeds.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "61",
        "title": "LIFT: Improving Long Context Understanding Through Long Input Fine-Tuning",
        "author": [
            "Yansheng Mao",
            "Jiaqi Li",
            "Fanxu Meng",
            "Jing Xiong",
            "Zilong Zheng",
            "Muhan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13626",
        "abstract": "Long context understanding remains challenging for large language models due to their limited context windows. This paper introduces Long Input Fine-Tuning (LIFT) for long context modeling, a novel framework that enhances LLM performance on long-context tasks by adapting model parameters to the context at test time. LIFT enables efficient processing of lengthy inputs without the computational burden of offline long-context adaptation, and can improve the long-context capabilities of arbitrary short-context models. The framework is further enhanced by integrating in-context learning and pre-LIFT supervised fine-tuning. The combination of in-context learning and LIFT enables short-context models like Llama 3 to handle arbitrarily long contexts and consistently improves their performance on popular long-context benchmarks like LooGLE and LongBench. We also provide a comprehensive analysis of the strengths and limitations of LIFT on long context understanding, offering valuable directions for future research.",
        "tags": [
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "62",
        "title": "TAUDiff: Improving statistical downscaling for extreme weather events using generative diffusion models",
        "author": [
            "Rahul Sundar",
            "Nishant Parashar",
            "Antoine Blanchard",
            "Boyko Dodov"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13627",
        "abstract": "Deterministic regression-based downscaling models for climate variables often suffer from spectral bias, which can be mitigated by generative models like diffusion models. To enable efficient and reliable simulation of extreme weather events, it is crucial to achieve rapid turnaround, dynamical consistency, and accurate spatio-temporal spectral recovery. We propose an efficient correction diffusion model, TAUDiff, that combines a deterministic spatio-temporal model for mean field downscaling with a smaller generative diffusion model for recovering the fine-scale stochastic features. We demonstrate the efficacy of this approach on downscaling atmospheric wind velocity fields obtained from coarse GCM simulations. Our approach can not only ensure quicker simulation of extreme events but also reduce overall carbon footprint due to low inference times.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "63",
        "title": "Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model",
        "author": [
            "Xiu Yuan",
            "Tongzhou Mu",
            "Stone Tao",
            "Yunhao Fang",
            "Mengke Zhang",
            "Hao Su"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13630",
        "abstract": "Recent advancements in robot learning have used imitation learning with large models and extensive demonstrations to develop effective policies. However, these models are often limited by the quantity, quality, and diversity of demonstrations. This paper explores improving offline-trained imitation learning models through online interactions with the environment. We introduce Policy Decorator, which uses a model-agnostic residual policy to refine large imitation learning models during online interactions. By implementing controlled exploration strategies, Policy Decorator enables stable, sample-efficient online learning. Our evaluation spans eight tasks across two benchmarks-ManiSkill and Adroit-and involves two state-of-the-art imitation learning models (Behavior Transformer and Diffusion Policy). The results show Policy Decorator effectively improves the offline-trained policies and preserves the smooth motion of imitation learning models, avoiding the erratic behaviors of pure RL policies. See our project page (https://policydecorator.github.io) for videos.",
        "tags": [
            "Diffusion",
            "RL",
            "Robot",
            "Transformer"
        ]
    },
    {
        "id": "64",
        "title": "Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning",
        "author": [
            "Eitan Wagner",
            "Nitay Alon",
            "Joseph M. Barnby",
            "Omri Abend"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13631",
        "abstract": "Theory of Mind (ToM) capabilities in LLMs have recently become a central object of investigation. Cognitive science distinguishes between two steps required for ToM tasks: 1) determine whether to invoke ToM, which includes the appropriate Depth of Mentalizing (DoM), or level of recursion required to complete a task; and 2) applying the correct inference given the DoM. In this position paper, we first identify several lines of work in different communities in AI, including LLM benchmarking, ToM add-ons, ToM probing, and formal models for ToM. We argue that recent work in AI tends to focus exclusively on the second step which are typically framed as static logic problems. We conclude with suggestions for improved evaluation of ToM capabilities inspired by dynamic environments used in cognitive tasks.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "65",
        "title": "Self-control: A Better Conditional Mechanism for Masked Autoregressive Model",
        "author": [
            "Qiaoying Qu",
            "Shiyu Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13635",
        "abstract": "Autoregressive conditional image generation algorithms are capable of generating photorealistic images that are consistent with given textual or image conditions, and have great potential for a wide range of applications. Nevertheless, the majority of popular autoregressive image generation methods rely heavily on vector quantization, and the inherent discrete characteristic of codebook presents a considerable challenge to achieving high-quality image generation. To address this limitation, this paper introduces a novel conditional introduction network for continuous masked autoregressive models. The proposed self-control network serves to mitigate the negative impact of vector quantization on the quality of the generated images, while simultaneously enhancing the conditional control during the generation process. In particular, the self-control network is constructed upon a continuous mask autoregressive generative model, which incorporates multimodal conditional information, including text and images, into a unified autoregressive sequence in a serial manner. Through a self-attention mechanism, the network is capable of generating images that are controllable based on specific conditions. The self-control network discards the conventional cross-attention-based conditional fusion mechanism and effectively unifies the conditional and generative information within the same space, thereby facilitating more seamless learning and fusion of multimodal features.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "66",
        "title": "4D Radar-Inertial Odometry based on Gaussian Modeling and Multi-Hypothesis Scan Matching",
        "author": [
            "Fernando Amodeo",
            "Luis Merino",
            "Fernando Caballero"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13639",
        "abstract": "4D millimeter-wave (mmWave) radars are sensors that provide robustness against adverse weather conditions (rain, snow, fog, etc.), and as such they are increasingly being used for odometry and SLAM applications. However, the noisy and sparse nature of the returned scan data proves to be a challenging obstacle for existing point cloud matching based solutions, especially those originally intended for more accurate sensors such as LiDAR. Inspired by visual odometry research around 3D Gaussian Splatting, in this paper we propose using freely positioned 3D Gaussians to create a summarized representation of a radar point cloud tolerant to sensor noise, and subsequently leverage its inherent probability distribution function for registration (similar to NDT). Moreover, we propose simultaneously optimizing multiple scan matching hypotheses in order to further increase the robustness of the system against local optima of the function. Finally, we fuse our Gaussian modeling and scan matching algorithms into an EKF radar-inertial odometry system designed after current best practices. Experiments show that our Gaussian-based odometry is able to outperform current baselines on a well-known 4D radar dataset used for evaluation.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "SLAM"
        ]
    },
    {
        "id": "67",
        "title": "On the Role of Model Prior in Real-World Inductive Reasoning",
        "author": [
            "Zhuo Liu",
            "Ding Yu",
            "Hangfeng He"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13645",
        "abstract": "Large Language Models (LLMs) show impressive inductive reasoning capabilities, enabling them to generate hypotheses that could generalize effectively to new instances when guided by in-context demonstrations. However, in real-world applications, LLMs' hypothesis generation is not solely determined by these demonstrations but is significantly shaped by task-specific model priors. Despite their critical influence, the distinct contributions of model priors versus demonstrations to hypothesis generation have been underexplored. This study bridges this gap by systematically evaluating three inductive reasoning strategies across five real-world tasks with three LLMs. Our empirical findings reveal that, hypothesis generation is primarily driven by the model's inherent priors; removing demonstrations results in minimal loss of hypothesis quality and downstream usage. Further analysis shows the result is consistent across various label formats with different label configurations, and prior is hard to override, even under flipped labeling. These insights advance our understanding of the dynamics of hypothesis generation in LLMs and highlight the potential for better utilizing model priors in real-world inductive reasoning tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "68",
        "title": "G-VEval: A Versatile Metric for Evaluating Image and Video Captions Using GPT-4o",
        "author": [
            "Tony Cheng Tong",
            "Sirui He",
            "Zhiwen Shao",
            "Dit-Yan Yeung"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13647",
        "abstract": "Evaluation metric of visual captioning is important yet not thoroughly explored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss semantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are limited in zero-shot scenarios. Advanced Language Model-based metrics also struggle with aligning to nuanced human preferences. To address these issues, we introduce G-VEval, a novel metric inspired by G-Eval and powered by the new GPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and supports three modes: reference-free, reference-only, and combined, accommodating both video and image inputs. We also propose MSVD-Eval, a new dataset for video captioning evaluation, to establish a more transparent and consistent framework for both human experts and evaluation metrics. It is designed to address the lack of clear criteria in existing datasets by introducing distinct dimensions of Accuracy, Completeness, Conciseness, and Relevance (ACCR). Extensive results show that G-VEval outperforms existing methods in correlation with human annotations, as measured by Kendall tau-b and Kendall tau-c. This provides a flexible solution for diverse captioning tasks and suggests a straightforward yet effective approach for large language models to understand video content, paving the way for advancements in automated captioning. Codes are available at https://github.com/ztangaj/gveval",
        "tags": [
            "CLIP",
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "69",
        "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
        "author": [
            "Jialong Wu",
            "Zhenglin Wang",
            "Linhai Zhang",
            "Yilong Lai",
            "Yulan He",
            "Deyu Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13649",
        "abstract": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context generation. Despite the numerous efforts in this area, the optimization for the decoding phase is generally ignored. However, we believe such optimization is crucial, especially for long-output generation tasks based on the following two observations: (i) Excessive compression during the prefill phase, which requires specific full context impairs the comprehension of the reasoning task; (ii) Deviation of heavy hitters occurs in the reasoning tasks with long outputs. Therefore, SCOPE, a simple yet efficient framework that separately performs KV cache optimization during the prefill and decoding phases, is introduced. Specifically, the KV cache during the prefill phase is preserved to maintain the essential information, while a novel strategy based on sliding is proposed to select essential heavy hitters for the decoding phase. Memory usage and memory transfer are further optimized using adaptive and discontinuous strategies. Extensive experiments on LongGenBench show the effectiveness and generalization of SCOPE and its compatibility as a plug-in to other prefill-only KV compression methods.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "70",
        "title": "RelationField: Relate Anything in Radiance Fields",
        "author": [
            "Sebastian Koch",
            "Johanna Wald",
            "Mirco Colosi",
            "Narunas Vaskevicius",
            "Pedro Hermosilla",
            "Federico Tombari",
            "Timo Ropinski"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13652",
        "abstract": "Neural radiance fields are an emerging 3D scene representation and recently even been extended to learn features for scene understanding by distilling open-vocabulary features from vision-language models. However, current method primarily focus on object-centric representations, supporting object segmentation or detection, while understanding semantic relationships between objects remains largely unexplored. To address this gap, we propose RelationField, the first method to extract inter-object relationships directly from neural radiance fields. RelationField represents relationships between objects as pairs of rays within a neural radiance field, effectively extending its formulation to include implicit relationship queries. To teach RelationField complex, open-vocabulary relationships, relationship knowledge is distilled from multi-modal LLMs. To evaluate RelationField, we solve open-vocabulary 3D scene graph generation tasks and relationship-guided instance segmentation, achieving state-of-the-art performance in both tasks. See the project website at https://relationfield.github.io.",
        "tags": [
            "3D",
            "Detection",
            "LLMs",
            "Segmentation"
        ]
    },
    {
        "id": "71",
        "title": "GAGS: Granularity-Aware Feature Distillation for Language Gaussian Splatting",
        "author": [
            "Yuning Peng",
            "Haiping Wang",
            "Yuan Liu",
            "Chenglu Wen",
            "Zhen Dong",
            "Bisheng Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13654",
        "abstract": "3D open-vocabulary scene understanding, which accurately perceives complex semantic properties of objects in space, has gained significant attention in recent years. In this paper, we propose GAGS, a framework that distills 2D CLIP features into 3D Gaussian splatting, enabling open-vocabulary queries for renderings on arbitrary viewpoints. The main challenge of distilling 2D features for 3D fields lies in the multiview inconsistency of extracted 2D features, which provides unstable supervision for the 3D feature field. GAGS addresses this challenge with two novel strategies. First, GAGS associates the prompt point density of SAM with the camera distances, which significantly improves the multiview consistency of segmentation results. Second, GAGS further decodes a granularity factor to guide the distillation process and this granularity factor can be learned in a unsupervised manner to only select the multiview consistent 2D features in the distillation process. Experimental results on two datasets demonstrate significant performance and stability improvements of GAGS in visual grounding and semantic segmentation, with an inference speed 2$\\times$ faster than baseline methods. The code and additional results are available at https://pz0826.github.io/GAGS-Webpage/ .",
        "tags": [
            "3D",
            "CLIP",
            "Gaussian Splatting",
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "72",
        "title": "GLCF: A Global-Local Multimodal Coherence Analysis Framework for Talking Face Generation Detection",
        "author": [
            "Xiaocan Chen",
            "Qilin Yin",
            "Jiarui Liu",
            "Wei Lu",
            "Xiangyang Luo",
            "Jiantao Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13656",
        "abstract": "Talking face generation (TFG) allows for producing lifelike talking videos of any character using only facial images and accompanying text. Abuse of this technology could pose significant risks to society, creating the urgent need for research into corresponding detection methods. However, research in this field has been hindered by the lack of public datasets. In this paper, we construct the first large-scale multi-scenario talking face dataset (MSTF), which contains 22 audio and video forgery techniques, filling the gap of datasets in this field. The dataset covers 11 generation scenarios and more than 20 semantic scenarios, closer to the practical application scenario of TFG. Besides, we also propose a TFG detection framework, which leverages the analysis of both global and local coherence in the multimodal content of TFG videos. Therefore, a region-focused smoothness detection module (RSFDM) and a discrepancy capture-time frame aggregation module (DCTAM) are introduced to evaluate the global temporal coherence of TFG videos, aggregating multi-grained spatial information. Additionally, a visual-audio fusion module (V-AFM) is designed to evaluate audiovisual coherence within a localized temporal perspective. Comprehensive experiments demonstrate the reasonableness and challenges of our datasets, while also indicating the superiority of our proposed method compared to the state-of-the-art deepfake detection approaches.",
        "tags": [
            "Detection",
            "Talking Face"
        ]
    },
    {
        "id": "73",
        "title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",
        "author": [
            "Benjamin Warner",
            "Antoine Chaffin",
            "Benjamin Clavié",
            "Orion Weller",
            "Oskar Hallström",
            "Said Taghadouini",
            "Alexis Gallagher",
            "Raja Biswas",
            "Faisal Ladhak",
            "Tom Aarsen",
            "Nathan Cooper",
            "Griffin Adams",
            "Jeremy Howard",
            "Iacopo Poli"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13663",
        "abstract": "Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.",
        "tags": [
            "BERT",
            "Transformer"
        ]
    },
    {
        "id": "74",
        "title": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation",
        "author": [
            "Aneta Zugecova",
            "Dominik Macko",
            "Ivan Srba",
            "Robert Moro",
            "Jakub Kopal",
            "Katarina Marcincinova",
            "Matus Mesarcik"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13666",
        "abstract": "The capabilities of recent large language models (LLMs) to generate high-quality content indistinguishable by humans from human-written texts rises many concerns regarding their misuse. Previous research has shown that LLMs can be effectively misused for generating disinformation news articles following predefined narratives. Their capabilities to generate personalized (in various aspects) content have also been evaluated and mostly found usable. However, a combination of personalization and disinformation abilities of LLMs has not been comprehensively studied yet. Such a dangerous combination should trigger integrated safety filters of the LLMs, if there are some. This study fills this gap by evaluation of vulnerabilities of recent open and closed LLMs, and their willingness to generate personalized disinformation news articles in English. We further explore whether the LLMs can reliably meta-evaluate the personalization quality and whether the personalization affects the generated-texts detectability. Our results demonstrate the need for stronger safety-filters and disclaimers, as those are not properly functioning in most of the evaluated LLMs. Additionally, our study revealed that the personalization actually reduces the safety-filter activations; thus effectively functioning as a jailbreak. Such behavior must be urgently addressed by LLM developers and service providers.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "75",
        "title": "Exploring Multi-Modal Integration with Tool-Augmented LLM Agents for Precise Causal Discovery",
        "author": [
            "ChengAo Shen",
            "Zhengzhang Chen",
            "Dongsheng Luo",
            "Dongkuan Xu",
            "Haifeng Chen",
            "Jingchao Ni"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13667",
        "abstract": "Causal inference is an imperative foundation for decision-making across domains, such as smart health, AI for drug discovery and AIOps. Traditional statistical causal discovery methods, while well-established, predominantly rely on observational data and often overlook the semantic cues inherent in cause-and-effect relationships. The advent of Large Language Models (LLMs) has ushered in an affordable way of leveraging the semantic cues for knowledge-driven causal discovery, but the development of LLMs for causal discovery lags behind other areas, particularly in the exploration of multi-modality data. To bridge the gap, we introduce MATMCD, a multi-agent system powered by tool-augmented LLMs. MATMCD has two key agents: a Data Augmentation agent that retrieves and processes modality-augmented data, and a Causal Constraint agent that integrates multi-modal data for knowledge-driven inference. Delicate design of the inner-workings ensures successful cooperation of the agents. Our empirical study across seven datasets suggests the significant potential of multi-modality enhanced causal discovery.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "76",
        "title": "AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge",
        "author": [
            "Xiaobao Wu",
            "Liangming Pan",
            "Yuxi Xie",
            "Ruiwen Zhou",
            "Shuai Zhao",
            "Yubo Ma",
            "Mingzhe Du",
            "Rui Mao",
            "Anh Tuan Luu",
            "William Yang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13670",
        "abstract": "Data contamination hinders fair LLM evaluation by introducing test data into newer models' training sets. Existing studies solve this challenge by updating benchmarks with newly collected data. However, they fail to guarantee contamination-free evaluation as the newly collected data may contain pre-existing knowledge, and their benchmark updates rely on intensive human labor. To address these issues, we in this paper propose AntiLeak-Bench, an automated anti-leakage benchmarking framework. Instead of simply using newly collected data, we construct samples with explicitly new knowledge absent from LLMs' training sets, which thus ensures strictly contamination-free evaluation. We further design a fully automated workflow to build and update our benchmark without human labor. This significantly reduces the cost of benchmark maintenance to accommodate emerging LLMs. Through extensive experiments, we highlight that data contamination likely exists before LLMs' cutoff time and demonstrate AntiLeak-Bench effectively overcomes this challenge.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "77",
        "title": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese Travel Planning",
        "author": [
            "Jie-Jing Shao",
            "Xiao-Wen Yang",
            "Bo-Wen Zhang",
            "Baizhi Chen",
            "Wen-Da Wei",
            "Lan-Zhe Guo",
            "Yu-feng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13682",
        "abstract": "Recent advances in LLMs, particularly in language reasoning and tool integration, have rapidly sparked the real-world development of Language Agents. Among these, travel planning represents a prominent domain, combining academic challenges with practical value due to its complexity and market demand. However, existing benchmarks fail to reflect the diverse, real-world requirements crucial for deployment. To address this gap, we introduce ChinaTravel, a benchmark specifically designed for authentic Chinese travel planning scenarios. We collect the travel requirements from questionnaires and propose a compositionally generalizable domain-specific language that enables a scalable evaluation process, covering feasibility, constraint satisfaction, and preference comparison. Empirical studies reveal the potential of neuro-symbolic agents in travel planning, achieving a constraint satisfaction rate of 27.9%, significantly surpassing purely neural models at 2.6%. Moreover, we identify key challenges in real-world travel planning deployments, including open language reasoning and unseen concept composition. These findings highlight the significance of ChinaTravel as a pivotal milestone for advancing language agents in complex, real-world planning scenarios.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "78",
        "title": "Towards Efficient and Explainable Hate Speech Detection via Model Distillation",
        "author": [
            "Paloma Piot",
            "Javier Parapar"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13698",
        "abstract": "Automatic detection of hate and abusive language is essential to combat its online spread. Moreover, recognising and explaining hate speech serves to educate people about its negative effects. However, most current detection models operate as black boxes, lacking interpretability and explainability. In this context, Large Language Models (LLMs) have proven effective for hate speech detection and to promote interpretability. Nevertheless, they are computationally costly to run. In this work, we propose distilling big language models by using Chain-of-Thought to extract explanations that support the hate speech classification task. Having small language models for these tasks will contribute to their use in operational settings. In this paper, we demonstrate that distilled models deliver explanations of the same quality as larger models while surpassing them in classification performance. This dual capability, classifying and explaining, advances hate speech detection making it more affordable, understandable and actionable.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "79",
        "title": "Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models",
        "author": [
            "Kunat Pipatanakul",
            "Potsawee Manakul",
            "Natapong Nitarach",
            "Warit Sirichotedumrong",
            "Surapon Nonesung",
            "Teetouch Jaknamon",
            "Parinthapat Pengpun",
            "Pittawat Taveekitworachai",
            "Adisai Na-Thalang",
            "Sittipong Sripaisarnmongkol",
            "Krisanapong Jirayoot",
            "Kasima Tharnpipitchai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13702",
        "abstract": "This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ various post-training techniques to enhance Thai language performance while preserving the base models' original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs simultaneously.",
        "tags": [
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "80",
        "title": "JoVALE: Detecting Human Actions in Video Using Audiovisual and Language Contexts",
        "author": [
            "Taein Son",
            "Soo Won Seo",
            "Jisong Kim",
            "Seok Hwan Lee",
            "Jun Won Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13708",
        "abstract": "Video Action Detection (VAD) involves localizing and categorizing action instances in videos. Videos inherently contain various information sources, including audio, visual cues, and surrounding scene contexts. Effectively leveraging this multi-modal information for VAD is challenging, as the model must accurately focus on action-relevant cues. In this study, we introduce a novel multi-modal VAD architecture called the Joint Actor-centric Visual, Audio, Language Encoder (JoVALE). JoVALE is the first VAD method to integrate audio and visual features with scene descriptive context derived from large image captioning models. The core principle of JoVALE is the actor-centric aggregation of audio, visual, and scene descriptive contexts, where action-related cues from each modality are identified and adaptively combined. We propose a specialized module called the Actor-centric Multi-modal Fusion Network, designed to capture the joint interactions among actors and multi-modal contexts through Transformer architecture. Our evaluation conducted on three popular VAD benchmarks, AVA, UCF101-24, and JHMDB51-21, demonstrates that incorporating multi-modal information leads to significant performance gains. JoVALE achieves state-of-the-art performances. The code will be available at \\texttt{https://github.com/taeiin/AAAI2025-JoVALE}.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "81",
        "title": "TH\\\"OR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared Industrial Spaces",
        "author": [
            "Tiago Rodrigues de Almeida",
            "Tim Schreiter",
            "Andrey Rudenko",
            "Luigi Palmieiri",
            "Johannes A. Stork",
            "Achim J. Lilienthal"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13729",
        "abstract": "Accurate human activity and trajectory prediction are crucial for ensuring safe and reliable human-robot interactions in dynamic environments, such as industrial settings, with mobile robots. Datasets with fine-grained action labels for moving people in industrial environments with mobile robots are scarce, as most existing datasets focus on social navigation in public spaces. This paper introduces the THÖR-MAGNI Act dataset, a substantial extension of the THÖR-MAGNI dataset, which captures participant movements alongside robots in diverse semantic and spatial contexts. THÖR-MAGNI Act provides 8.3 hours of manually labeled participant actions derived from egocentric videos recorded via eye-tracking glasses. These actions, aligned with the provided THÖR-MAGNI motion cues, follow a long-tailed distribution with diversified acceleration, velocity, and navigation distance profiles. We demonstrate the utility of THÖR-MAGNI Act for two tasks: action-conditioned trajectory prediction and joint action and trajectory prediction. We propose two efficient transformer-based models that outperform the baselines to address these tasks. These results underscore the potential of THÖR-MAGNI Act to develop predictive models for enhanced human-robot interaction in complex environments.",
        "tags": [
            "Robot",
            "Transformer"
        ]
    },
    {
        "id": "82",
        "title": "Text2Relight: Creative Portrait Relighting with Text Guidance",
        "author": [
            "Junuk Cha",
            "Mengwei Ren",
            "Krishna Kumar Singh",
            "He Zhang",
            "Yannick Hold-Geoffroy",
            "Seunghyun Yoon",
            "HyunJoon Jung",
            "Jae Shin Yoon",
            "Seungryul Baek"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13734",
        "abstract": "We present a lighting-aware image editing pipeline that, given a portrait image and a text prompt, performs single image relighting. Our model modifies the lighting and color of both the foreground and background to align with the provided text description. The unbounded nature in creativeness of a text allows us to describe the lighting of a scene with any sensory features including temperature, emotion, smell, time, and so on. However, the modeling of such mapping between the unbounded text and lighting is extremely challenging due to the lack of dataset where there exists no scalable data that provides large pairs of text and relighting, and therefore, current text-driven image editing models does not generalize to lighting-specific use cases. We overcome this problem by introducing a novel data synthesis pipeline: First, diverse and creative text prompts that describe the scenes with various lighting are automatically generated under a crafted hierarchy using a large language model (*e.g.,* ChatGPT). A text-guided image generation model creates a lighting image that best matches the text. As a condition of the lighting images, we perform image-based relighting for both foreground and background using a single portrait image or a set of OLAT (One-Light-at-A-Time) images captured from lightstage system. Particularly for the background relighting, we represent the lighting image as a set of point lights and transfer them to other background images. A generative diffusion model learns the synthesized large-scale data with auxiliary task augmentation (*e.g.,* portrait delighting and light positioning) to correlate the latent text and lighting distribution for text-guided portrait relighting.",
        "tags": [
            "ChatGPT",
            "Diffusion",
            "Image Editing"
        ]
    },
    {
        "id": "83",
        "title": "Immersive Human-in-the-Loop Control: Real-Time 3D Surface Meshing and Physics Simulation",
        "author": [
            "Sait Akturk",
            "Justin Valentine",
            "Junaid Ahmad",
            "Martin Jagersand"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13752",
        "abstract": "This paper introduces the TactiMesh Teleoperator Interface (TTI), a novel predictive visual and haptic system designed explicitly for human-in-the-loop robot control using a head-mounted display (HMD). By employing simultaneous localization and mapping (SLAM)in tandem with a space carving method (CARV), TTI creates a real time 3D surface mesh of remote environments from an RGB camera mounted on a Barrett WAM arm. The generated mesh is integrated into a physics simulator, featuring a digital twin of the WAM robot arm to create a virtual environment. In this virtual environment, TTI provides haptic feedback directly in response to the operator's movements, eliminating the problem with delayed response from the haptic follower robot. Furthermore, texturing the 3D mesh with keyframes from SLAM allows the operator to control the viewpoint of their Head Mounted Display (HMD) independently of the arm-mounted robot camera, giving a better visual immersion and improving manipulation speed. Incorporating predictive visual and haptic feedback significantly improves teleoperation in applications such as search and rescue, inspection, and remote maintenance.",
        "tags": [
            "3D",
            "Robot",
            "SLAM"
        ]
    },
    {
        "id": "84",
        "title": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for E-Learning Platforms",
        "author": [
            "Ali Hamdi",
            "Ahmed Abdelmoneim Mazrou",
            "Mohamed Shaltout"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13765",
        "abstract": "Current methods for analyzing student engagement in e-learning platforms, including automated systems, often struggle with challenges such as handling fuzzy sentiment in text comments and relying on limited metadata. Traditional approaches, such as surveys and questionnaires, also face issues like small sample sizes and scalability. In this paper, we introduce LLM-SEM (Language Model-Based Student Engagement Metric), a novel approach that leverages video metadata and sentiment analysis of student comments to measure engagement. By utilizing recent Large Language Models (LLMs), we generate high-quality sentiment predictions to mitigate text fuzziness and normalize key features such as views and likes. Our holistic method combines comprehensive metadata with sentiment polarity scores to gauge engagement at both the course and lesson levels. Extensive experiments were conducted to evaluate various LLM models, demonstrating the effectiveness of LLM-SEM in providing a scalable and accurate measure of student engagement. We fine-tuned LLMs, including AraBERT, TXLM-RoBERTa, LLama 3B and Gemma 9B from Ollama, using human-annotated sentiment datasets to enhance prediction accuracy.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "85",
        "title": "Designing an LLM-Based Copilot for Manufacturing Equipment Selection",
        "author": [
            "Jonas Werheid",
            "Oleksandr Melnychuk",
            "Hans Zhou",
            "Meike Huber",
            "Christoph Rippe",
            "Dominik Joosten",
            "Zozan Keskin",
            "Max Wittstamm",
            "Sathya Subramani",
            "Benny Drescher",
            "Amon Göppert",
            "Anas Abdelrazeq",
            "Robert H. Schmitt"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13774",
        "abstract": "Effective decision-making in automation equipment selection is critical for reducing ramp-up time and maintaining production quality, especially in the face of increasing product variation and market demands. However, limited expertise and resource constraints often result in inefficiencies during the ramp-up phase when new products are integrated into production lines. Existing methods often lack structured and tailored solutions to support automation engineers in reducing ramp-up time, leading to compromises in quality. This research investigates whether large-language models (LLMs), combined with Retrieval-Augmented Generation (RAG), can assist in streamlining equipment selection in ramp-up planning. We propose a factual-driven copilot integrating LLMs with structured and semi-structured knowledge retrieval for three component types (robots, feeders and vision systems), providing a guided and traceable state-machine process for decision-making in automation equipment selection. The system was demonstrated to an industrial partner, who tested it on three internal use-cases. Their feedback affirmed its capability to provide logical and actionable recommendations for automation equipment. More specifically, among 22 equipment prompts analyzed, 19 involved selecting the correct equipment while considering most requirements, and in 6 cases, all requirements were fully met.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "86",
        "title": "Meta-Reflection: A Feedback-Free Reflection Learning Framework",
        "author": [
            "Yaoke Wang",
            "Yun Zhu",
            "Xintong Bao",
            "Wenqiao Zhang",
            "Suyang Dai",
            "Kehan Chen",
            "Wenqiang Li",
            "Gang Huang",
            "Siliang Tang",
            "Yueting Zhuang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13781",
        "abstract": "Despite the remarkable capabilities of large language models (LLMs) in natural language understanding and reasoning, they often display undesirable behaviors, such as generating hallucinations and unfaithful reasoning. A prevalent strategy to mitigate these issues is the use of reflection, which refines responses through an iterative process. However, while promising, reflection heavily relies on high-quality external feedback and requires iterative multi-agent inference processes, thus hindering its practical application. In this paper, we propose Meta-Reflection, a novel feedback-free reflection mechanism that necessitates only a single inference pass without external feedback. Motivated by the human ability to remember and retrieve reflections from past experiences when encountering similar problems, Meta-Reflection integrates reflective insights into a codebook, allowing the historical insights to be stored, retrieved, and used to guide LLMs in problem-solving. To thoroughly investigate and evaluate the practicality of Meta-Reflection in real-world scenarios, we introduce an industrial e-commerce benchmark named E-commerce Customer Intent Detection (ECID). Extensive experiments conducted on both public datasets and the ECID benchmark highlight the effectiveness and efficiency of our proposed approach.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "87",
        "title": "Knowledge Editing with Dynamic Knowledge Graphs for Multi-hop Question Answering",
        "author": [
            "Yifan Lu",
            "Yigeng Zhou",
            "Jing Li",
            "Yequan Wang",
            "Xuebo Liu",
            "Daojing He",
            "Fangming Liu",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13782",
        "abstract": "Multi-hop question answering (MHQA) poses a significant challenge for large language models (LLMs) due to the extensive knowledge demands involved. Knowledge editing, which aims to precisely modify the LLMs to incorporate specific knowledge without negatively impacting other unrelated knowledge, offers a potential solution for addressing MHQA challenges with LLMs. However, current solutions struggle to effectively resolve issues of knowledge conflicts. Most parameter-preserving editing methods are hindered by inaccurate retrieval and overlook secondary editing issues, which can introduce noise into the reasoning process of LLMs. In this paper, we introduce KEDKG, a novel knowledge editing method that leverages a dynamic knowledge graph for MHQA, designed to ensure the reliability of answers. KEDKG involves two primary steps: dynamic knowledge graph construction and knowledge graph augmented generation. Initially, KEDKG autonomously constructs a dynamic knowledge graph to store revised information while resolving potential knowledge conflicts. Subsequently, it employs a fine-grained retrieval strategy coupled with an entity and relation detector to enhance the accuracy of graph retrieval for LLM generation. Experimental results on benchmarks show that KEDKG surpasses previous state-of-the-art models, delivering more accurate and reliable answers in environments with dynamic information.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "88",
        "title": "Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics Problems with Large Language Models",
        "author": [
            "Xinyu Pang",
            "Ruixin Hong",
            "Zhanke Zhou",
            "Fangrui Lv",
            "Xinwei Yang",
            "Zhilong Liang",
            "Bo Han",
            "Changshui Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13791",
        "abstract": "Physics problems constitute a significant aspect of reasoning, necessitating complicated reasoning ability and abundant physics knowledge. However, existing large language models (LLMs) frequently fail due to a lack of knowledge or incorrect knowledge application. To mitigate these issues, we propose Physics Reasoner, a knowledge-augmented framework to solve physics problems with LLMs. Specifically, the proposed framework constructs a comprehensive formula set to provide explicit physics knowledge and utilizes checklists containing detailed instructions to guide effective knowledge application. Namely, given a physics problem, Physics Reasoner solves it through three stages: problem analysis, formula retrieval, and guided reasoning. During the process, checklists are employed to enhance LLMs' self-improvement in the analysis and reasoning stages. Empirically, Physics Reasoner mitigates the issues of insufficient knowledge and incorrect application, achieving state-of-the-art performance on SciBench with an average accuracy improvement of 5.8%.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "89",
        "title": "MATCHED: Multimodal Authorship-Attribution To Combat Human Trafficking in Escort-Advertisement Data",
        "author": [
            "Vageesh Saxena",
            "Benjamin Bashpole",
            "Gijs Van Dijck",
            "Gerasimos Spanakis"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13794",
        "abstract": "Human trafficking (HT) remains a critical issue, with traffickers increasingly leveraging online escort advertisements (ads) to advertise victims anonymously. Existing detection methods, including Authorship Attribution (AA), often center on text-based analyses and neglect the multimodal nature of online escort ads, which typically pair text with images. To address this gap, we introduce MATCHED, a multimodal dataset of 27,619 unique text descriptions and 55,115 unique images collected from the Backpage escort platform across seven U.S. cities in four geographical regions. Our study extensively benchmarks text-only, vision-only, and multimodal baselines for vendor identification and verification tasks, employing multitask (joint) training objectives that achieve superior classification and retrieval performance on in-distribution and out-of-distribution (OOD) datasets. Integrating multimodal features further enhances this performance, capturing complementary patterns across text and images. While text remains the dominant modality, visual data adds stylistic cues that enrich model performance. Moreover, text-image alignment strategies like CLIP and BLIP2 struggle due to low semantic overlap and vague connections between the modalities of escort ads, with end-to-end multimodal training proving more robust. Our findings emphasize the potential of multimodal AA (MAA) to combat HT, providing LEAs with robust tools to link ads and disrupt trafficking networks.",
        "tags": [
            "CLIP",
            "Detection"
        ]
    },
    {
        "id": "90",
        "title": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN",
        "author": [
            "Pengxiang Li",
            "Lu Yin",
            "Shiwei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13795",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success, yet recent findings reveal that their deeper layers often contribute minimally and can be pruned without affecting overall performance. While some view this as an opportunity for model compression, we identify it as a training shortfall rooted in the widespread use of Pre-Layer Normalization (Pre-LN). We demonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads to diminished gradient norms in its deeper layers, reducing their effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger gradient norms in deeper layers but suffers from vanishing gradients in earlier layers. To address this, we introduce Mix-LN, a novel normalization technique that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring more uniform gradients across layers. This allows all parts of the network--both shallow and deep layers--to contribute effectively to training. Extensive experiments with various model sizes from 70M to 7B demonstrate that Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more balanced, healthier gradient norms throughout the network, and enhancing the overall quality of LLM pre-training. Furthermore, we demonstrate that models pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN during supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), highlighting the critical importance of high-quality deep layers. By effectively addressing the inefficiencies of deep layers in current LLMs, Mix-LN unlocks their potential, enhancing model capacity without increasing model size. Our code is available at https://github.com/pixeli99/MixLN.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "91",
        "title": "A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on Method-Level Code Smell Detection",
        "author": [
            "Beiqi Zhang",
            "Peng Liang",
            "Xin Zhou",
            "Xiyu Zhou",
            "David Lo",
            "Qiong Feng",
            "Zengyang Li",
            "Lin Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13801",
        "abstract": "Code smells are suboptimal coding practices that negatively impact the quality of software systems. Existing detection methods, relying on heuristics or Machine Learning (ML) and Deep Learning (DL) techniques, often face limitations such as unsatisfactory performance. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a resource-efficient approach for adapting LLMs to specific tasks, but their effectiveness for method-level code smell detection remains underexplored. In this regard, this study evaluates state-of-the-art PEFT methods on both small and large Language Models (LMs) for detecting two types of method-level code smells: Complex Conditional and Complex Method. Using high-quality datasets sourced from GitHub, we fine-tuned four small LMs and six LLMs with PEFT techniques, including prompt tuning, prefix tuning, LoRA, and (IA)3. Results show that PEFT methods achieve comparable or better performance than full fine-tuning while consuming less GPU memory. Notably, LLMs did not outperform small LMs, suggesting smaller models' suitability for this task. Additionally, increasing training dataset size significantly boosted performance, while increasing trainable parameters did not. Our findings highlight PEFT methods as effective and scalable solutions, outperforming existing heuristic-based and DL-based detectors.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "92",
        "title": "Extreme Multi-label Completion for Semantic Document Labelling with Taxonomy-Aware Parallel Learning",
        "author": [
            "Julien Audiffren",
            "Christophe Broillet",
            "Ljiljana Dolamic",
            "Philippe Cudré-Mauroux"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13809",
        "abstract": "In Extreme Multi Label Completion (XMLCo), the objective is to predict the missing labels of a collection of documents. Together with XML Classification, XMLCo is arguably one of the most challenging document classification tasks, as the very high number of labels (at least ten of thousands) is generally very large compared to the number of available labelled documents in the training dataset. Such a task is often accompanied by a taxonomy that encodes the labels organic relationships, and many methods have been proposed to leverage this hierarchy to improve the results of XMLCo algorithms. In this paper, we propose a new approach to this problem, TAMLEC (Taxonomy-Aware Multi-task Learning for Extreme multi-label Completion). TAMLEC divides the problem into several Taxonomy-Aware Tasks, i.e. subsets of labels adapted to the hierarchical paths of the taxonomy, and trains on these tasks using a dynamic Parallel Feature sharing approach, where some parts of the model are shared between tasks while others are task-specific. Then, at inference time, TAMLEC uses the labels available in a document to infer the appropriate tasks and to predict missing labels. To achieve this result, TAMLEC uses a modified transformer architecture that predicts ordered sequences of labels on a Weak-Semilattice structure that is naturally induced by the tasks. This approach yields multiple advantages. First, our experiments on real-world datasets show that TAMLEC outperforms state-of-the-art methods for various XMLCo problems. Second, TAMLEC is by construction particularly suited for few-shots XML tasks, where new tasks or labels are introduced with only few examples, and extensive evaluations highlight its strong performance compared to existing methods.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "93",
        "title": "Object Style Diffusion for Generalized Object Detection in Urban Scene",
        "author": [
            "Hao Li",
            "Xiangyuan Yang",
            "Mengzhu Wang",
            "Long Lan",
            "Ke Liang",
            "Xinwang Liu",
            "Kenli Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13815",
        "abstract": "Object detection is a critical task in computer vision, with applications in various domains such as autonomous driving and urban scene monitoring. However, deep learning-based approaches often demand large volumes of annotated data, which are costly and difficult to acquire, particularly in complex and unpredictable real-world environments. This dependency significantly hampers the generalization capability of existing object detection techniques. To address this issue, we introduce a novel single-domain object detection generalization method, named GoDiff, which leverages a pre-trained model to enhance generalization in unseen domains. Central to our approach is the Pseudo Target Data Generation (PTDG) module, which employs a latent diffusion model to generate pseudo-target domain data that preserves source domain characteristics while introducing stylistic variations. By integrating this pseudo data with source domain data, we diversify the training dataset. Furthermore, we introduce a cross-style instance normalization technique to blend style features from different domains generated by the PTDG module, thereby increasing the detector's robustness. Experimental results demonstrate that our method not only enhances the generalization ability of existing detectors but also functions as a plug-and-play enhancement for other single-domain generalization methods, achieving state-of-the-art performance in autonomous driving scenarios.",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "94",
        "title": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection",
        "author": [
            "Le Yang",
            "Ziwei Zheng",
            "Boxu Chen",
            "Zhengyu Zhao",
            "Chenhao Lin",
            "Chao Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13817",
        "abstract": "Recent studies have shown that large vision-language models (LVLMs) often suffer from the issue of object hallucinations (OH). To mitigate this issue, we introduce an efficient method that edits the model weights based on an unsafe subspace, which we call HalluSpace in this paper. With truthful and hallucinated text prompts accompanying the visual content as inputs, the HalluSpace can be identified by extracting the hallucinated embedding features and removing the truthful representations in LVLMs. By orthogonalizing the model weights, input features will be projected into the Null space of the HalluSpace to reduce OH, based on which we name our method Nullu. We reveal that HalluSpaces generally contain statistical bias and unimodal priors of the large language models (LLMs) applied to build LVLMs, which have been shown as essential causes of OH in previous studies. Therefore, null space projection suppresses the LLMs' priors to filter out the hallucinated features, resulting in contextually accurate outputs. Experiments show that our method can effectively mitigate OH across different LVLM families without extra inference costs and also show strong performance in general LVLM benchmarks. Code is released at \\url{https://github.com/Ziwei-Zheng/Nullu}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "95",
        "title": "Prompt Categories Cluster for Weakly Supervised Semantic Segmentation",
        "author": [
            "Wangyu Wu",
            "Xianglin Qiu",
            "Siqi Song",
            "Xiaowei Huang",
            "Fei Ma",
            "Jimin Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13823",
        "abstract": "Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level labels, has garnered significant attention due to its cost-effectiveness. The previous methods mainly strengthen the inter-class differences to avoid class semantic ambiguity which may lead to erroneous activation. However, they overlook the positive function of some shared information between similar classes. Categories within the same cluster share some similar features. Allowing the model to recognize these features can further relieve the semantic ambiguity between these classes. To effectively identify and utilize this shared information, in this paper, we introduce a novel WSSS framework called Prompt Categories Clustering (PCC). Specifically, we explore the ability of Large Language Models (LLMs) to derive category clusters through prompts. These clusters effectively represent the intrinsic relationships between categories. By integrating this relational information into the training network, our model is able to better learn the hidden connections between categories. Experimental results demonstrate the effectiveness of our approach, showing its ability to enhance performance on the PASCAL VOC 2012 dataset and surpass existing state-of-the-art methods in WSSS.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Segmentation"
        ]
    },
    {
        "id": "96",
        "title": "Maybe you are looking for CroQS: Cross-modal Query Suggestion for Text-to-Image Retrieval",
        "author": [
            "Giacomo Pacini",
            "Fabio Carrara",
            "Nicola Messina",
            "Nicola Tonellotto",
            "Giuseppe Amato",
            "Fabrizio Falchi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13834",
        "abstract": "Query suggestion, a technique widely adopted in information retrieval, enhances system interactivity and the browsing experience of document collections. In cross-modal retrieval, many works have focused on retrieving relevant items from natural language queries, while few have explored query suggestion solutions. In this work, we address query suggestion in cross-modal retrieval, introducing a novel task that focuses on suggesting minimal textual modifications needed to explore visually consistent subsets of the collection, following the premise of ''Maybe you are looking for''. To facilitate the evaluation and development of methods, we present a tailored benchmark named CroQS. This dataset comprises initial queries, grouped result sets, and human-defined suggested queries for each group. We establish dedicated metrics to rigorously evaluate the performance of various methods on this task, measuring representativeness, cluster specificity, and similarity of the suggested queries to the original ones. Baseline methods from related fields, such as image captioning and content summarization, are adapted for this task to provide reference performance scores. Although relatively far from human performance, our experiments reveal that both LLM-based and captioning-based methods achieve competitive results on CroQS, improving the recall on cluster specificity by more than 115% and representativeness mAP by more than 52% with respect to the initial query. The dataset, the implementation of the baseline methods and the notebooks containing our experiments are available here: https://paciosoft.com/CroQS-benchmark/",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "97",
        "title": "RACQUET: Unveiling the Dangers of Overlooked Referential Ambiguity in Visual LLMs",
        "author": [
            "Alberto Testoni",
            "Barbara Plank",
            "Raquel Fernández"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13835",
        "abstract": "Ambiguity resolution is key to effective communication. While humans effortlessly address ambiguity through conversational grounding strategies, the extent to which current language models can emulate these strategies remains unclear. In this work, we examine referential ambiguity in image-based question answering by introducing RACQUET, a carefully curated dataset targeting distinct aspects of ambiguity. Through a series of evaluations, we reveal significant limitations and problems of overconfidence of state-of-the-art large multimodal language models in addressing ambiguity in their responses. The overconfidence issue becomes particularly relevant for RACQUET-BIAS, a subset designed to analyze a critical yet underexplored problem: failing to address ambiguity leads to stereotypical, socially biased responses. Our results underscore the urgency of equipping models with robust strategies to deal with uncertainty without resorting to undesirable stereotypes.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "98",
        "title": "Coupled Eikonal problems to model cardiac reentries in Purkinje network and myocardium",
        "author": [
            "Samuele Brunati",
            "Michele Bucelli",
            "Roberto Piersanti",
            "Luca Dede'",
            "Christian Vergara"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13837",
        "abstract": "We propose a novel partitioned scheme based on Eikonal equations to model the coupled propagation of the electrical signal in the His-Purkinje system and in the myocardium for cardiac electrophysiology. This scheme allows, for the first time in Eikonal-based modeling, to capture all possible signal reentries between the Purkinje network and the cardiac muscle that may occur under pathological conditions. As part of the proposed scheme, we introduce a new pseudo-time method for the Eikonal-diffusion problem in the myocardium, to correctly enforce electrical stimuli coming from the Purkinje network. We test our approach by performing numerical simulations of cardiac electrophysiology in a real biventricular geometry, under both pathological and therapeutic conditions, to demonstrate its flexibility, robustness, and accuracy.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "99",
        "title": "IDEQ: an improved diffusion model for the TSP",
        "author": [
            "Mickael Basson",
            "Philippe Preux"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13858",
        "abstract": "We investigate diffusion models to solve the Traveling Salesman Problem. Building on the recent DIFUSCO and T2TCO approaches, we propose IDEQ. IDEQ improves the quality of the solutions by leveraging the constrained structure of the state space of the TSP. Another key component of IDEQ consists in replacing the last stages of DIFUSCO curriculum learning by considering a uniform distribution over the Hamiltonian tours whose orbits by the 2-opt operator converge to the optimal solution as the training objective. Our experiments show that IDEQ improves the state of the art for such neural network based techniques on synthetic instances. More importantly, our experiments show that IDEQ performs very well on the instances of the TSPlib, a reference benchmark in the TSP community: it closely matches the performance of the best heuristics, LKH3, being even able to obtain better solutions than LKH3 on 2 instances of the TSPlib defined on 1577 and 3795 cities. IDEQ obtains 0.3% optimality gap on TSP instances made of 500 cities, and 0.5% on TSP instances with 1000 cities. This sets a new SOTA for neural based methods solving the TSP. Moreover, IDEQ exhibits a lower variance and better scales-up with the number of cities with regards to DIFUSCO and T2TCO.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "100",
        "title": "Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image Classification Using Large Language Models",
        "author": [
            "Anna Scius-Bertrand",
            "Michael Jungo",
            "Lars Vögtlin",
            "Jean-Marc Spat",
            "Andreas Fischer"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13859",
        "abstract": "Classifying scanned documents is a challenging problem that involves image, layout, and text analysis for document understanding. Nevertheless, for certain benchmark datasets, notably RVL-CDIP, the state of the art is closing in to near-perfect performance when considering hundreds of thousands of training samples. With the advent of large language models (LLMs), which are excellent few-shot learners, the question arises to what extent the document classification problem can be addressed with only a few training samples, or even none at all. In this paper, we investigate this question in the context of zero-shot prompting and few-shot model fine-tuning, with the aim of reducing the need for human-annotated training samples as much as possible.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "101",
        "title": "Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation on Nepali",
        "author": [
            "Sharad Duwal",
            "Suraj Prasai",
            "Suresh Manandhar"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13860",
        "abstract": "Continual learning has emerged as an important research direction due to the infeasibility of retraining large language models (LLMs) from scratch in the event of new data availability. Of great interest is the domain-adaptive pre-training (DAPT) paradigm, which focuses on continually training a pre-trained language model to adapt it to a domain it was not originally trained on. In this work, we evaluate the feasibility of DAPT in a low-resource setting, namely the Nepali language. We use synthetic data to continue training Llama 3 8B to adapt it to the Nepali language in a 4-bit QLoRA setting. We evaluate the adapted model on its performance, forgetting, and knowledge acquisition. We compare the base model and the final model on their Nepali generation abilities, their performance on popular benchmarks, and run case-studies to probe their linguistic knowledge in Nepali. We see some unsurprising forgetting in the final model, but also surprisingly find that increasing the number of shots during evaluation yields better percent increases in the final model (as high as 19.29% increase) compared to the base model (4.98%), suggesting latent retention. We also explore layer-head self-attention heatmaps to establish dependency resolution abilities of the final model in Nepali.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "102",
        "title": "LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via Hierarchical Window Transformer",
        "author": [
            "Yipeng Zhang",
            "Yifan Liu",
            "Zonghao Guo",
            "Yidan Zhang",
            "Xuesong Yang",
            "Chi Chen",
            "Jun Song",
            "Bo Zheng",
            "Yuan Yao",
            "Zhiyuan Liu",
            "Tat-Seng Chua",
            "Maosong Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13871",
        "abstract": "In multimodal large language models (MLLMs), vision transformers (ViTs) are widely employed for visual encoding. However, their performance in solving universal MLLM tasks is not satisfactory. We attribute it to a lack of information from diverse visual levels, impeding alignment with the various semantic granularity required for language generation. To address this issue, we present LLaVA-UHD v2, an advanced MLLM centered around a Hierarchical window transformer that enables capturing diverse visual granularity by constructing and integrating a high-resolution feature pyramid. As a vision-language projector, Hiwin transformer comprises two primary modules: (i) an inverse feature pyramid, constructed by a ViT-derived feature up-sampling process utilizing high-frequency details from an image pyramid, and (ii) hierarchical window attention, focusing on a set of key sampling features within cross-scale windows to condense multi-level feature maps. Extensive experiments demonstrate that LLaVA-UHD v2 achieves superior performance over existing MLLMs on popular benchmarks. Notably, our design brings an average boost of 3.7% across 14 benchmarks compared with the baseline method, 9.3% on DocVQA for instance. We make all the data, model checkpoint, and code publicly available to facilitate future research.",
        "tags": [
            "LLaVA",
            "Large Language Models",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "103",
        "title": "Crabs: Consuming Resrouce via Auto-generation for LLM-DoS Attack under Black-box Settings",
        "author": [
            "Yuanhe Zhang",
            "Zhenhong Zhou",
            "Wei Zhang",
            "Xinyue Wang",
            "Xiaojun Jia",
            "Yang Liu",
            "Sen Su"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13879",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks. LLMs continue to be vulnerable to external threats, particularly Denial-of-Service (DoS) attacks. Specifically, LLM-DoS attacks aim to exhaust computational resources and block services. However, prior works tend to focus on performing white-box attacks, overlooking black-box settings. In this work, we propose an automated algorithm designed for black-box LLMs, called Auto-Generation for LLM-DoS Attack (AutoDoS). AutoDoS introduces DoS Attack Tree and optimizes the prompt node coverage to enhance effectiveness under black-box conditions. Our method can bypass existing defense with enhanced stealthiness via semantic improvement of prompt nodes. Furthermore, we reveal that implanting Length Trojan in Basic DoS Prompt aids in achieving higher attack efficacy. Experimental results show that AutoDoS amplifies service response latency by over 250 $\\times \\uparrow$, leading to severe resource consumption in terms of GPU utilization and memory usage. Our code is available at \\url{https://github.com/shuita2333/AutoDoS}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "104",
        "title": "Energy-Efficient SLAM via Joint Design of Sensing, Communication, and Exploration Speed",
        "author": [
            "Zidong Han",
            "Ruibo Jin",
            "Xiaoyang Li",
            "Bingpeng Zhou",
            "Qinyu Zhang",
            "Yi Gong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13912",
        "abstract": "To support future spatial machine intelligence applications, lifelong simultaneous localization and mapping (SLAM) has drawn significant attentions. SLAM is usually realized based on various types of mobile robots performing simultaneous and continuous sensing and communication. This paper focuses on analyzing the energy efficiency of robot operation for lifelong SLAM by jointly considering sensing, communication and mechanical factors. The system model is built based on a robot equipped with a 2D light detection and ranging (LiDAR) and an odometry. The cloud point raw data as well as the odometry data are wirelessly transmitted to data center where real-time map reconstruction is realized based on an unsupervised deep learning based method. The sensing duration, transmit power, transmit duration and exploration speed are jointly optimized to minimize the energy consumption. Simulations and experiments demonstrate the performance of our proposed method.",
        "tags": [
            "Detection",
            "Robot",
            "SLAM"
        ]
    },
    {
        "id": "105",
        "title": "Pipeline Analysis for Developing Instruct LLMs in Low-Resource Languages: A Case Study on Basque",
        "author": [
            "Ander Corral",
            "Ixak Sarasua",
            "Xabier Saralegi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13922",
        "abstract": "Large language models (LLMs) are typically optimized for resource-rich languages like English, exacerbating the gap between high-resource and underrepresented languages. This work presents a detailed analysis of strategies for developing a model capable of following instructions in a low-resource language, specifically Basque, by focusing on three key stages: pre-training, instruction tuning, and alignment with human preferences. Our findings demonstrate that continual pre-training with a high-quality Basque corpus of around 600 million words improves natural language understanding (NLU) of the foundational model by over 12 points. Moreover, instruction tuning and human preference alignment using automatically translated datasets proved highly effective, resulting in a 24-point improvement in instruction-following performance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct, establish a new state-of-the-art for Basque in the sub-10B parameter category.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "106",
        "title": "CoRa: A Collision-Resistant LoRa Symbol Detector of Low Complexity",
        "author": [
            "José Álamos",
            "Thomas C. Schmidt",
            "Matthias Wählisch"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13930",
        "abstract": "Long range communication with LoRa has become popular as it avoids the complexity of multi-hop communication at low cost and low energy consumption. LoRa is openly accessible, but its packets are particularly vulnerable to collisions due to long time on air in a shared band. This degrades com- munication performance. Existing techniques for demodulating LoRa symbols under collisions face challenges such as high computational complexity, reliance on accurate symbol boundary information, or error-prone peak detection methods. In this paper, we introduce CoRa , a symbol detector for demodulating LoRa symbols under severe collisions. CoRa employs a Bayesian classifier to accurately identify the true symbol amidst inter- ference from other LoRa transmissions, leveraging empirically derived features from raw symbol data. Evaluations using real- world and simulated packet traces demonstrate that CoRa clearly outperforms the related state-of-the-art, i.e., up to 29% better decoding performance than TnB and 178% better than CIC. Compared to the LoRa baseline demodulator, CoRa magnifies the packet reception rate by up to 11.53x. CoRa offers a significant reduction in computational complexity compared to existing solutions by only adding a constant overhead to the baseline demodulator, while also eliminating the need for peak detection and accurately identifying colliding frames.",
        "tags": [
            "Detection",
            "LoRA"
        ]
    },
    {
        "id": "107",
        "title": "Spatio-Temporal Forecasting of PM2.5 via Spatial-Diffusion guided Encoder-Decoder Architecture",
        "author": [
            "Malay Pandey",
            "Vaishali Jain",
            "Nimit Godhani",
            "Sachchida Nand Tripathi",
            "Piyush Rai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13935",
        "abstract": "In many problem settings that require spatio-temporal forecasting, the values in the time-series not only exhibit spatio-temporal correlations but are also influenced by spatial diffusion across locations. One such example is forecasting the concentration of fine particulate matter (PM2.5) in the atmosphere which is influenced by many complex factors, the most important ones being diffusion due to meteorological factors as well as transport across vast distances over a period of time. We present a novel Spatio-Temporal Graph Neural Network architecture, that specifically captures these dependencies to forecast the PM2.5 concentration. Our model is based on an encoder-decoder architecture where the encoder and decoder parts leverage gated recurrent units (GRU) augmented with a graph neural network (TransformerConv) to account for spatial diffusion. Our model can also be seen as a generalization of various existing models for time-series or spatio-temporal forecasting. We demonstrate the model's effectiveness on two real-world PM2.5 datasets: (1) data collected by us using a recently deployed network of low-cost PM$_{2.5}$ sensors from 511 locations spanning the entirety of the Indian state of Bihar over a period of one year, and (2) another publicly available dataset that covers severely polluted regions from China for a period of 4 years. Our experimental results show our model's impressive ability to account for both spatial as well as temporal dependencies precisely.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "108",
        "title": "A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies for Human Explanations to Collect Label Distributions on NLI",
        "author": [
            "Beiduo Chen",
            "Siyao Peng",
            "Anna Korhonen",
            "Barbara Plank"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13942",
        "abstract": "Disagreement in human labeling is ubiquitous, and can be captured in human judgment distributions (HJDs). Recent research has shown that explanations provide valuable information for understanding human label variation (HLV) and large language models (LLMs) can approximate HJD from a few human-provided label-explanation pairs. However, collecting explanations for every label is still time-consuming. This paper examines whether LLMs can be used to replace humans in generating explanations for approximating HJD. Specifically, we use LLMs as annotators to generate model explanations for a few given human labels. We test ways to obtain and combine these label-explanations with the goal to approximate human judgment distribution. We further compare the resulting human with model-generated explanations, and test automatic and human explanation selection. Our experiments show that LLM explanations are promising for NLI: to estimate HJD, generated explanations yield comparable results to human's when provided with human labels. Importantly, our results generalize from datasets with human explanations to i) datasets where they are not available and ii) challenging out-of-distribution test sets.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "109",
        "title": "Real Classification by Description: Extending CLIP's Limits of Part Attributes Recognition",
        "author": [
            "Ethan Baron",
            "Idan Tankel",
            "Peter Tu",
            "Guy Ben-Yosef"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13947",
        "abstract": "In this study, we define and tackle zero shot \"real\" classification by description, a novel task that evaluates the ability of Vision-Language Models (VLMs) like CLIP to classify objects based solely on descriptive attributes, excluding object class names. This approach highlights the current limitations of VLMs in understanding intricate object descriptions, pushing these models beyond mere object recognition. To facilitate this exploration, we introduce a new challenge and release description data for six popular fine-grained benchmarks, which omit object names to encourage genuine zero-shot learning within the research community. Additionally, we propose a method to enhance CLIP's attribute detection capabilities through targeted training using ImageNet21k's diverse object categories, paired with rich attribute descriptions generated by large language models. Furthermore, we introduce a modified CLIP architecture that leverages multiple resolutions to improve the detection of fine-grained part attributes. Through these efforts, we broaden the understanding of part-attribute recognition in CLIP, improving its performance in fine-grained classification tasks across six popular benchmarks, as well as in the PACO dataset, a widely used benchmark for object-attribute recognition. Code is available at: https://github.com/ethanbar11/grounding_ge_public.",
        "tags": [
            "CLIP",
            "Detection",
            "Large Language Models"
        ]
    },
    {
        "id": "110",
        "title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence",
        "author": [
            "Jinghan He",
            "Kuan Zhu",
            "Haiyun Guo",
            "Junfeng Fang",
            "Zhenglin Hua",
            "Yuheng Jia",
            "Ming Tang",
            "Tat-Seng Chua",
            "Jinqiao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13949",
        "abstract": "Large vision-language models (LVLMs) have made substantial progress in integrating large language models (LLMs) with visual inputs, enabling advanced multimodal reasoning. Despite their success, a persistent challenge is hallucination-where generated text fails to accurately reflect visual content-undermining both accuracy and reliability. Existing methods focus on alignment training or decoding refinements but primarily address symptoms at the generation stage without probing the underlying causes. In this work, we investigate the internal mechanisms driving hallucination in LVLMs, with an emphasis on the multi-head attention module. Specifically, we introduce Vision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of attention head outputs to visual context. Based on this, our findings reveal the presence of vision-aware attention heads that are more attuned to visual information; however, the model's overreliance on its prior language patterns is closely related to hallucinations. Building on these insights, we propose Vision-aware Head Reinforcement (VHR), a training-free approach to mitigate hallucination by enhancing the role of vision-aware attention heads. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art approaches in mitigating hallucinations, while maintaining high efficiency with negligible additional time overhead.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "111",
        "title": "Prompting Strategies for Enabling Large Language Models to Infer Causation from Correlation",
        "author": [
            "Eleni Sgouritsa",
            "Virginia Aglietti",
            "Yee Whye Teh",
            "Arnaud Doucet",
            "Arthur Gretton",
            "Silvia Chiappa"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13952",
        "abstract": "The reasoning abilities of Large Language Models (LLMs) are attracting increasing attention. In this work, we focus on causal reasoning and address the task of establishing causal relationships based on correlation information, a highly challenging problem on which several LLMs have shown poor performance. We introduce a prompting strategy for this problem that breaks the original task into fixed subquestions, with each subquestion corresponding to one step of a formal causal discovery algorithm, the PC algorithm. The proposed prompting strategy, PC-SubQ, guides the LLM to follow these algorithmic steps, by sequentially prompting it with one subquestion at a time, augmenting the next subquestion's prompt with the answer to the previous one(s). We evaluate our approach on an existing causal benchmark, Corr2Cause: our experiments indicate a performance improvement across five LLMs when comparing PC-SubQ to baseline prompting strategies. Results are robust to causal query perturbations, when modifying the variable names or paraphrasing the expressions.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "112",
        "title": "Self-attentive Transformer for Fast and Accurate Postprocessing of Temperature and Wind Speed Forecasts",
        "author": [
            "Aaron Van Poecke",
            "Tobias Sebastian Finn",
            "Ruoke Meng",
            "Joris Van den Bergh",
            "Geert Smet",
            "Jonathan Demaeyer",
            "Piet Termonia",
            "Hossein Tabari",
            "Peter Hellinckx"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13957",
        "abstract": "Current postprocessing techniques often require separate models for each lead time and disregard possible inter-ensemble relationships by either correcting each member separately or by employing distributional approaches. In this work, we tackle these shortcomings with an innovative, fast and accurate Transformer which postprocesses each ensemble member individually while allowing information exchange across variables, spatial dimensions and lead times by means of multi-headed self-attention. Weather foreacasts are postprocessed over 20 lead times simultaneously while including up to twelve meteorological predictors. We use the EUPPBench dataset for training which contains ensemble predictions from the European Center for Medium-range Weather Forecasts' integrated forecasting system alongside corresponding observations. The work presented here is the first to postprocess the ten and one hundred-meter wind speed forecasts within this benchmark dataset, while also correcting the two-meter temperature. Our approach significantly improves the original forecasts, as measured by the CRPS, with 17.5 % for two-meter temperature, nearly 5% for ten-meter wind speed and 5.3 % for one hundred-meter wind speed, outperforming a classical member-by-member approach employed as competitive benchmark. Furthermore, being up to 75 times faster, it fulfills the demand for rapid operational weather forecasts in various downstream applications, including renewable energy forecasting.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "113",
        "title": "Comparative Analysis of Machine Learning-Based Imputation Techniques for Air Quality Datasets with High Missing Data Rates",
        "author": [
            "Sen Yan",
            "David J. O'Connor",
            "Xiaojun Wang",
            "Noel E. O'Connor",
            "Alan. F. Smeaton",
            "Mingming Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13966",
        "abstract": "Urban pollution poses serious health risks, particularly in relation to traffic-related air pollution, which remains a major concern in many cities. Vehicle emissions contribute to respiratory and cardiovascular issues, especially for vulnerable and exposed road users like pedestrians and cyclists. Therefore, accurate air quality monitoring with high spatial resolution is vital for good urban environmental management. This study aims to provide insights for processing spatiotemporal datasets with high missing data rates. In this study, the challenge of high missing data rates is a result of the limited data available and the fine granularity required for precise classification of PM2.5 levels. The data used for analysis and imputation were collected from both mobile sensors and fixed stations by Dynamic Parcel Distribution, the Environmental Protection Agency, and Google in Dublin, Ireland, where the missing data rate was approximately 82.42%, making accurate Particulate Matter 2.5 level predictions particularly difficult. Various imputation and prediction approaches were evaluated and compared, including ensemble methods, deep learning models, and diffusion models. External features such as traffic flow, weather conditions, and data from the nearest stations were incorporated to enhance model performance. The results indicate that diffusion methods with external features achieved the highest F1 score, reaching 0.9486 (Accuracy: 94.26%, Precision: 94.42%, Recall: 94.82%), with ensemble models achieving the highest accuracy of 94.82%, illustrating that good performance can be obtained despite a high missing data rate.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "114",
        "title": "GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians",
        "author": [
            "Xiaobao Wei",
            "Peng Chen",
            "Ming Lu",
            "Hui Chen",
            "Feng Tian"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13983",
        "abstract": "Rendering photorealistic head avatars from arbitrary viewpoints is crucial for various applications like virtual reality. Although previous methods based on Neural Radiance Fields (NeRF) can achieve impressive results, they lack fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have improved rendering quality and real-time performance but still require significant storage overhead. In this paper, we introduce a method called GraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians for the head avatar. Specifically, GraphAvatar trains a geometric GNN and an appearance GNN to generate the attributes of the 3D Gaussians from the tracked mesh. Therefore, our method can store the GNN models instead of the 3D Gaussians, significantly reducing the storage overhead to just 10MB. To reduce the impact of face-tracking errors, we also present a novel graph-guided optimization module to refine face-tracking parameters during training. Finally, we introduce a 3D-aware enhancer for post-processing to enhance the rendering quality. We conduct comprehensive experiments to demonstrate the advantages of GraphAvatar, surpassing existing methods in visual fidelity and storage consumption. The ablation study sheds light on the trade-offs between rendering quality and model size. The code will be released at: https://github.com/ucwxb/GraphAvatar",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "115",
        "title": "RAG for Effective Supply Chain Security Questionnaire Automation",
        "author": [
            "Zaynab Batool Reza",
            "Abdul Rafay Syed",
            "Omer Iqbal",
            "Ethel Mensah",
            "Qian Liu",
            "Maxx Richard Rahman",
            "Wolfgang Maass"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13988",
        "abstract": "In an era where digital security is crucial, efficient processing of security-related inquiries through supply chain security questionnaires is imperative. This paper introduces a novel approach using Natural Language Processing (NLP) and Retrieval-Augmented Generation (RAG) to automate these responses. We developed QuestSecure, a system that interprets diverse document formats and generates precise responses by integrating large language models (LLMs) with an advanced retrieval system. Our experiments show that QuestSecure significantly improves response accuracy and operational efficiency. By employing advanced NLP techniques and tailored retrieval mechanisms, the system consistently produces contextually relevant and semantically rich responses, reducing cognitive load on security teams and minimizing potential errors. This research offers promising avenues for automating complex security management tasks, enhancing organizational security processes.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "116",
        "title": "What makes a good metric? Evaluating automatic metrics for text-to-image consistency",
        "author": [
            "Candace Ross",
            "Melissa Hall",
            "Adriana Romero Soriano",
            "Adina Williams"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13989",
        "abstract": "Language models are increasingly being incorporated as components in larger AI systems for various purposes, from prompt optimization to automatic evaluation. In this work, we analyze the construct validity of four recent, commonly used methods for measuring text-to-image consistency - CLIPScore, TIFA, VPEval, and DSG - which rely on language models and/or VQA models as components. We define construct validity for text-image consistency metrics as a set of desiderata that text-image consistency metrics should have, and find that no tested metric satisfies all of them. We find that metrics lack sufficient sensitivity to language and visual properties. Next, we find that TIFA, VPEval and DSG contribute novel information above and beyond CLIPScore, but also that they correlate highly with each other. We also ablate different aspects of the text-image consistency metrics and find that not all model components are strictly necessary, also a symptom of insufficient sensitivity to visual information. Finally, we show that all three VQA-based metrics likely rely on familiar text shortcuts (such as yes-bias in QA) that call their aptitude as quantitative evaluations of model performance into question.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "117",
        "title": "Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with Neural Processes",
        "author": [
            "Katarzyna Kobalczyk",
            "Claudio Fanconi",
            "Hao Sun",
            "Mihaela van der Schaar"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13998",
        "abstract": "As large language models (LLMs) become increasingly embedded in everyday applications, ensuring their alignment with the diverse preferences of individual users has become a critical challenge. Currently deployed approaches typically assume homogeneous user objectives and rely on single-objective fine-tuning. However, human preferences are inherently heterogeneous, influenced by various unobservable factors, leading to conflicting signals in preference data. Existing solutions addressing this diversity often require costly datasets labelled for specific objectives and involve training multiple reward models or LLM policies, which is computationally expensive and impractical. In this work, we present a novel framework for few-shot steerable alignment, where users' underlying preferences are inferred from a small sample of their choices. To achieve this, we extend the Bradley-Terry-Luce model to handle heterogeneous preferences with unobserved variability factors and propose its practical implementation for reward modelling and LLM fine-tuning. Thanks to our proposed approach of functional parameter-space conditioning, LLMs trained with our framework can be adapted to individual preferences at inference time, generating outputs over a continuum of behavioural modes. We empirically validate the effectiveness of methods, demonstrating their ability to capture and align with diverse human preferences in a data-efficient manner. Our code is made available at: https://github.com/kasia-kobalczyk/few-shot-steerable-alignment.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "118",
        "title": "InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal Large Language Models",
        "author": [
            "Cong Wei",
            "Yujie Zhong",
            "Haoxian Tan",
            "Yingsen Zeng",
            "Yong Liu",
            "Zheng Zhao",
            "Yujiu Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14006",
        "abstract": "Boosted by Multi-modal Large Language Models (MLLMs), text-guided universal segmentation models for the image and video domains have made rapid progress recently. However, these methods are often developed separately for specific domains, overlooking the similarities in task settings and solutions across these two areas. In this paper, we define the union of referring segmentation and reasoning segmentation at both the image and video levels as Instructed Visual Segmentation (IVS). Correspondingly, we propose InstructSeg, an end-to-end segmentation pipeline equipped with MLLMs for IVS. Specifically, we employ an object-aware video perceiver to extract temporal and object information from reference frames, facilitating comprehensive video understanding. Additionally, we introduce vision-guided multi-granularity text fusion to better integrate global and detailed text information with fine-grained visual guidance. By leveraging multi-task and end-to-end training, InstructSeg demonstrates superior performance across diverse image and video segmentation tasks, surpassing both segmentation specialists and MLLM-based methods with a single model. Our code is available at https://github.com/congvvc/InstructSeg.",
        "tags": [
            "Large Language Models",
            "Segmentation"
        ]
    },
    {
        "id": "119",
        "title": "FarExStance: Explainable Stance Detection for Farsi",
        "author": [
            "Majid Zarharan",
            "Maryam Hashemi",
            "Malika Behroozrazegh",
            "Sauleh Eetemadi",
            "Mohammad Taher Pilehvar",
            "Jennifer Foster"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14008",
        "abstract": "We introduce FarExStance, a new dataset for explainable stance detection in Farsi. Each instance in this dataset contains a claim, the stance of an article or social media post towards that claim, and an extractive explanation which provides evidence for the stance label. We compare the performance of a fine-tuned multilingual RoBERTa model to several large language models in zero-shot, few-shot, and parameter-efficient fine-tuned settings on our new dataset. On stance detection, the most accurate models are the fine-tuned RoBERTa model, the LLM Aya-23-8B which has been fine-tuned using parameter-efficient fine-tuning, and few-shot Claude-3.5-Sonnet. Regarding the quality of the explanations, our automatic evaluation metrics indicate that few-shot GPT-4o generates the most coherent explanations, while our human evaluation reveals that the best Overall Explanation Score (OES) belongs to few-shot Claude-3.5-Sonnet. The fine-tuned Aya-32-8B model produced explanations most closely aligned with the reference explanations.",
        "tags": [
            "Detection",
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "120",
        "title": "Towards an optimised evaluation of teachers' discourse: The case of engaging messages",
        "author": [
            "Samuel Falcon",
            "Jaime Leon"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14011",
        "abstract": "Evaluating teachers' skills is crucial for enhancing education quality and student outcomes. Teacher discourse, significantly influencing student performance, is a key component. However, coding this discourse can be laborious. This study addresses this issue by introducing a new methodology for optimising the assessment of teacher discourse. The research consisted of two studies, both within the framework of engaging messages used by secondary education teachers. The first study involved training two large language models on real-world examples from audio-recorded lessons over two academic years to identify and classify the engaging messages from the lessons' transcripts. This resulted in sensitivities of 84.31% and 91.11%, and specificities of 97.69% and 86.36% in identification and classification, respectively. The second study applied these models to transcripts of audio-recorded lessons from a third academic year to examine the frequency and distribution of message types by educational level and moment of the academic year. Results showed teachers predominantly use messages emphasising engagement benefits, linked to improved outcomes, while one-third highlighted non-engagement disadvantages, associated with increased anxiety. The use of engaging messages declined in Grade 12 and towards the academic year's end. These findings suggest potential interventions to optimise engaging message use, enhancing teaching quality and student outcomes.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "121",
        "title": "Discovering maximally consistent distribution of causal tournaments with Large Language Models",
        "author": [
            "Federico Baldo",
            "Simon Ferreira",
            "Charles K. Assaad"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14019",
        "abstract": "Causal discovery is essential for understanding complex systems, yet traditional methods often depend on strong, untestable assumptions, making the process challenging. Large Language Models (LLMs) present a promising alternative for extracting causal insights from text-based metadata, which consolidates domain expertise. However, LLMs are prone to unreliability and hallucinations, necessitating strategies that account for their limitations. One such strategy involves leveraging a consistency measure to evaluate reliability. Additionally, most text metadata does not clearly distinguish direct causal relationships from indirect ones, further complicating the inference of causal graphs. As a result, focusing on causal orderings, rather than causal graphs, emerges as a more practical and robust approach. We propose a novel method to derive a distribution of acyclic tournaments (representing plausible causal orders) that maximizes a consistency score. Our approach begins by computing pairwise consistency scores between variables, yielding a cyclic tournament that aggregates these scores. From this structure, we identify optimal acyclic tournaments compatible with the original tournament, prioritizing those that maximize consistency across all configurations. We tested our method on both classical and well-established bechmarks, as well as real-world datasets from epidemiology and public health. Our results demonstrate the effectiveness of our approach in recovering distributions causal orders with minimal error.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "122",
        "title": "Hansel: Output Length Controlling Framework for Large Language Models",
        "author": [
            "Seoha Song",
            "Junhyun Lee",
            "Hyeonmok Ko"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14033",
        "abstract": "Despite the great success of large language models (LLMs), efficiently controlling the length of the output sequence still remains a challenge. In this paper, we propose Hansel, an efficient framework for length control in LLMs without affecting its generation ability. Hansel utilizes periodically outputted hidden special tokens to keep track of the remaining target length of the output sequence. Together with techniques to avoid abrupt termination of the output, this seemingly simple method proved to be efficient and versatile, while not harming the coherency and fluency of the generated text. The framework can be applied to any pre-trained LLMs during the finetuning stage of the model, regardless of its original positional encoding method. We demonstrate this by finetuning four different LLMs with Hansel and show that the mean absolute error of the output sequence decreases significantly in every model and dataset compared to the prompt-based length control finetuning. Moreover, the framework showed a substantially improved ability to extrapolate to target lengths unseen during finetuning, such as long dialog responses or extremely short summaries. This indicates that the model learns the general means of length control, rather than learning to match output lengths to those seen during training.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "123",
        "title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds",
        "author": [
            "Danila Rukhovich",
            "Elona Dupont",
            "Dimitrios Mallis",
            "Kseniya Cherenkova",
            "Anis Kacem",
            "Djamila Aouada"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14042",
        "abstract": "Computer-Aided Design (CAD) models are typically constructed by sequentially drawing parametric sketches and applying CAD operations to obtain a 3D model. The problem of 3D CAD reverse engineering consists of reconstructing the sketch and CAD operation sequences from 3D representations such as point clouds. In this paper, we address this challenge through novel contributions across three levels: CAD sequence representation, network design, and dataset. In particular, we represent CAD sketch-extrude sequences as Python code. The proposed CAD-Recode translates a point cloud into Python code that, when executed, reconstructs the CAD model. Taking advantage of the exposure of pre-trained Large Language Models (LLMs) to Python code, we leverage a relatively small LLM as a decoder for CAD-Recode and combine it with a lightweight point cloud projector. CAD-Recode is trained solely on a proposed synthetic dataset of one million diverse CAD sequences. CAD-Recode significantly outperforms existing methods across three datasets while requiring fewer input points. Notably, it achieves 10 times lower mean Chamfer distance than state-of-the-art methods on DeepCAD and Fusion360 datasets. Furthermore, we show that our CAD Python code output is interpretable by off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering from point clouds.",
        "tags": [
            "3D",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "124",
        "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation",
        "author": [
            "Vera Neplenbroek",
            "Arianna Bisazza",
            "Raquel Fernández"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14050",
        "abstract": "Recent generative large language models (LLMs) show remarkable performance in non-English languages, but when prompted in those languages they tend to express higher harmful social biases and toxicity levels. Prior work has shown that finetuning on specialized datasets can mitigate this behavior, and doing so in English can transfer to other languages. In this work, we investigate the impact of different finetuning methods on the model's bias and toxicity, but also on its ability to produce fluent and diverse text. Our results show that finetuning on curated non-harmful text is more effective for mitigating bias, and finetuning on direct preference optimization (DPO) datasets is more effective for mitigating toxicity. The mitigation caused by applying these methods in English also transfers to non-English languages. We find evidence that the extent to which transfer takes place can be predicted by the amount of data in a given language present in the model's pretraining data. However, this transfer of bias and toxicity mitigation often comes at the expense of decreased language generation ability in non-English languages, highlighting the importance of developing language-specific bias and toxicity mitigation methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "125",
        "title": "Digestion Algorithm in Hierarchical Symbolic Forests: A Fast Text Normalization Algorithm and Semantic Parsing Framework for Specific Scenarios and Lightweight Deployment",
        "author": [
            "Kevin You"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14054",
        "abstract": "Text Normalization and Semantic Parsing have numerous applications in natural language processing, such as natural language programming, paraphrasing, data augmentation, constructing expert systems, text matching, and more. Despite the prominent achievements of deep learning in Large Language Models (LLMs), the interpretability of neural network architectures is still poor, which affects their credibility and hence limits the deployments of risk-sensitive scenarios. In certain scenario-specific domains with scarce data, rapidly obtaining a large number of supervised learning labels is challenging, and the workload of manually labeling data would be enormous. Catastrophic forgetting in neural networks further leads to low data utilization rates. In situations where swift responses are vital, the density of the model makes local deployment difficult and the response time long, which is not conducive to local applications of these fields. Inspired by the multiplication rule, a principle of combinatorial mathematics, and human thinking patterns, a multilayer framework along with its algorithm, the Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF), is proposed to address these above issues, combining text normalization and semantic parsing workflows. The Chinese Scripting Language \"Fire Bunny Intelligent Development Platform V2.0\" is an important test and application of the technology discussed in this paper. DAHSF can run locally in scenario-specific domains on little datasets, with model size and memory usage optimized by at least two orders of magnitude, thus improving the execution speed, and possessing a promising optimization outlook.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "126",
        "title": "A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future",
        "author": [
            "Shilin Sun",
            "Wenbin An",
            "Feng Tian",
            "Fang Nan",
            "Qidong Liu",
            "Jun Liu",
            "Nazaraf Shah",
            "Ping Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14056",
        "abstract": "Artificial intelligence (AI) has rapidly developed through advancements in computational power and the growth of massive datasets. However, this progress has also heightened challenges in interpreting the \"black-box\" nature of AI models. To address these concerns, eXplainable AI (XAI) has emerged with a focus on transparency and interpretability to enhance human understanding and trust in AI decision-making processes. In the context of multimodal data fusion and complex reasoning scenarios, the proposal of Multimodal eXplainable AI (MXAI) integrates multiple modalities for prediction and explanation tasks. Meanwhile, the advent of Large Language Models (LLMs) has led to remarkable breakthroughs in natural language processing, yet their complexity has further exacerbated the issue of MXAI. To gain key insights into the development of MXAI methods and provide crucial guidance for building more transparent, fair, and trustworthy AI systems, we review the MXAI methods from a historical perspective and categorize them across four eras: traditional machine learning, deep learning, discriminative foundation models, and generative LLMs. We also review evaluation metrics and datasets used in MXAI research, concluding with a discussion of future challenges and directions. A project related to this review has been created at https://github.com/ShilinSun/mxai_review.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "127",
        "title": "Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets",
        "author": [
            "Simon Thorne"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14062",
        "abstract": "Generative AI and Large Language Models (LLMs) hold promise for automating spreadsheet formula creation. However, due to hallucinations, bias and variable user skill, outputs obtained from generative AI cannot be assumed to be accurate or trustworthy. To address these challenges, a trustworthiness framework is proposed based on evaluating the transparency and dependability of the formula. The transparency of the formula is explored through explainability (understanding the formula's reasoning) and visibility (inspecting the underlying algorithms). The dependability of the generated formula is evaluated in terms of reliability (consistency and accuracy) and ethical considerations (bias and fairness). The paper also examines the drivers to these metrics in the form of hallucinations, training data bias and poorly constructed prompts. Finally, examples of mistrust in technology are considered and the consequences explored.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "128",
        "title": "Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification",
        "author": [
            "Kyle Thompson",
            "Nuno Saavedra",
            "Pedro Carrott",
            "Kevin Fisher",
            "Alex Sanchez-Stern",
            "Yuriy Brun",
            "João F. Ferreira",
            "Sorin Lerner",
            "Emily First"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14063",
        "abstract": "Formal verification using proof assistants, such as Coq, enables the creation of high-quality software. However, the verification process requires significant expertise and manual effort to write proofs. Recent work has explored automating proof synthesis using machine learning and large language models (LLMs). This work has shown that identifying relevant premises, such as lemmas and definitions, can aid synthesis. We present Rango, a fully automated proof synthesis tool for Coq that automatically identifies relevant premises and also similar proofs from the current project and uses them during synthesis. Rango uses retrieval augmentation at every step of the proof to automatically determine which proofs and premises to include in the context of its fine-tuned LLM. In this way, Rango adapts to the project and to the evolving state of the proof. We create a new dataset, CoqStoq, of 2,226 open-source Coq projects and 196,929 theorems from GitHub, which includes both training data and a curated evaluation benchmark of well-maintained projects. On this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is 29% more theorems than the prior state-of-the-art tool Tactician. Our evaluation also shows that Rango adding relevant proofs to its context leads to a 47% increase in the number of theorems proven.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "129",
        "title": "Future Research Avenues for Artificial Intelligence in Digital Gaming: An Exploratory Report",
        "author": [
            "Markus Dablander"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14085",
        "abstract": "Video games are a natural and synergistic application domain for artificial intelligence (AI) systems, offering both the potential to enhance player experience and immersion, as well as providing valuable benchmarks and virtual environments to advance AI technologies in general. This report presents a high-level overview of five promising research pathways for applying state-of-the-art AI methods, particularly deep learning, to digital gaming within the context of the current research landscape. The objective of this work is to outline a curated, non-exhaustive list of encouraging research directions at the intersection of AI and video games that may serve to inspire more rigorous and comprehensive research efforts in the future. We discuss (i) investigating large language models as core engines for game agent modelling, (ii) using neural cellular automata for procedural game content generation, (iii) accelerating computationally expensive in-game simulations via deep surrogate modelling, (iv) leveraging self-supervised learning to obtain useful video game state embeddings, and (v) training generative models of interactive worlds using unlabelled video data. We also briefly address current technical challenges associated with the integration of advanced deep learning systems into video game development, and indicate key areas where further progress is likely to be beneficial.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "130",
        "title": "SEKE: Specialised Experts for Keyword Extraction",
        "author": [
            "Matej Martinc",
            "Hanh Thi Hong Tran",
            "Senja Pollak",
            "Boshko Koloski"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14087",
        "abstract": "Keyword extraction involves identifying the most descriptive words in a document, allowing automatic categorisation and summarisation of large quantities of diverse textual data. Relying on the insight that real-world keyword detection often requires handling of diverse content, we propose a novel supervised keyword extraction approach based on the mixture of experts (MoE) technique. MoE uses a learnable routing sub-network to direct information to specialised experts, allowing them to specialize in distinct regions of the input space. SEKE, a mixture of Specialised Experts for supervised Keyword Extraction, uses DeBERTa as the backbone model and builds on the MoE framework, where experts attend to each token, by integrating it with a recurrent neural network (RNN), to allow successful extraction even on smaller corpora, where specialisation is harder due to lack of training data. The MoE framework also provides an insight into inner workings of individual experts, enhancing the explainability of the approach. We benchmark SEKE on multiple English datasets, achieving state-of-the-art performance compared to strong supervised and unsupervised baselines. Our analysis reveals that depending on data size and type, experts specialize in distinct syntactic and semantic components, such as punctuation, stopwords, parts-of-speech, or named entities. Code is available at: https://github.com/matejMartinc/SEKE_keyword_extraction",
        "tags": [
            "Detection",
            "RNN"
        ]
    },
    {
        "id": "131",
        "title": "Alignment faking in large language models",
        "author": [
            "Ryan Greenblatt",
            "Carson Denison",
            "Benjamin Wright",
            "Fabien Roger",
            "Monte MacDiarmid",
            "Sam Marks",
            "Johannes Treutlein",
            "Tim Belonax",
            "Jack Chen",
            "David Duvenaud",
            "Akbir Khan",
            "Julian Michael",
            "Sören Mindermann",
            "Ethan Perez",
            "Linda Petrini",
            "Jonathan Uesato",
            "Jared Kaplan",
            "Buck Shlegeris",
            "Samuel R. Bowman",
            "Evan Hubinger"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14093",
        "abstract": "We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data--and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference--as in this case--or not.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "132",
        "title": "Design choices made by LLM-based test generators prevent them from finding bugs",
        "author": [
            "Noble Saji Mathews",
            "Meiyappan Nagappan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14137",
        "abstract": "There is an increasing amount of research and commercial tools for automated test case generation using Large Language Models (LLMs). This paper critically examines whether recent LLM-based test generation tools, such as Codium CoverAgent and CoverUp, can effectively find bugs or unintentionally validate faulty code. Considering bugs are only exposed by failing test cases, we explore the question: can these tools truly achieve the intended objectives of software testing when their test oracles are designed to pass? Using real human-written buggy code as input, we evaluate these tools, showing how LLM-generated tests can fail to detect bugs and, more alarmingly, how their design can worsen the situation by validating bugs in the generated test suite and rejecting bug-revealing tests. These findings raise important questions about the validity of the design behind LLM-based test generation tools and their impact on software quality and test suite reliability.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "133",
        "title": "GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking",
        "author": [
            "Darshan Deshpande",
            "Selvan Sunitha Ravi",
            "Sky CH-Wang",
            "Bartosz Mielczarek",
            "Anand Kannappan",
            "Rebecca Qian"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14140",
        "abstract": "The LLM-as-judge paradigm is increasingly being adopted for automated evaluation of model outputs. While LLM judges have shown promise on constrained evaluation tasks, closed source LLMs display critical shortcomings when deployed in real world applications due to challenges of fine grained metrics and explainability, while task specific evaluation models lack cross-domain generalization. We introduce GLIDER, a powerful 3B evaluator LLM that can score any text input and associated context on arbitrary user defined criteria. GLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly outperforms prior evaluation models, achieving comparable performance to LLMs 17x its size. GLIDER supports fine-grained scoring, multilingual reasoning, span highlighting and was trained on 685 domains and 183 criteria. Extensive qualitative analysis shows that GLIDER scores are highly correlated with human judgments, with 91.3% human agreement. We have open-sourced GLIDER to facilitate future research.",
        "tags": [
            "GPT",
            "LLMs"
        ]
    },
    {
        "id": "134",
        "title": "LLMs can realize combinatorial creativity: generating creative ideas via LLMs for scientific research",
        "author": [
            "Tianyang Gu",
            "Jingjin Wang",
            "Zhihao Zhang",
            "HaoHong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14141",
        "abstract": "Scientific idea generation has been extensively studied in creativity theory and computational creativity research, providing valuable frameworks for understanding and implementing creative processes. However, recent work using Large Language Models (LLMs) for research idea generation often overlooks these theoretical foundations. We present a framework that explicitly implements combinatorial creativity theory using LLMs, featuring a generalization-level retrieval system for cross-domain knowledge discovery and a structured combinatorial process for idea generation. The retrieval system maps concepts across different abstraction levels to enable meaningful connections between disparate domains, while the combinatorial process systematically analyzes and recombines components to generate novel solutions. Experiments on the OAG-Bench dataset demonstrate our framework's effectiveness, consistently outperforming baseline approaches in generating ideas that align with real research developments (improving similarity scores by 7\\%-10\\% across multiple metrics). Our results provide strong evidence that LLMs can effectively realize combinatorial creativity when guided by appropriate theoretical frameworks, contributing both to practical advancement of AI-assisted research and theoretical understanding of machine creativity.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "135",
        "title": "Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics with Large Language Models",
        "author": [
            "Atin Sakkeer Hussain"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14146",
        "abstract": "This paper presents the Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework designed to augment Large Language Models (LLMs) for solving complex, multi-step data analytics tasks. ARTEMIS-DA integrates three core components: the Planner, which dissects complex user queries into structured, sequential instructions encompassing data preprocessing, transformation, predictive modeling, and visualization; the Coder, which dynamically generates and executes Python code to implement these instructions; and the Grapher, which interprets generated visualizations to derive actionable insights. By orchestrating the collaboration between these components, ARTEMIS-DA effectively manages sophisticated analytical workflows involving advanced reasoning, multi-step transformations, and synthesis across diverse data modalities. The framework achieves state-of-the-art (SOTA) performance on benchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to tackle intricate analytical tasks with precision and adaptability. By combining the reasoning capabilities of LLMs with automated code generation and execution and visual analysis, ARTEMIS-DA offers a robust, scalable solution for multi-step insight synthesis, addressing a wide range of challenges in data analytics.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "136",
        "title": "MCMat: Multiview-Consistent and Physically Accurate PBR Material Generation",
        "author": [
            "Shenhao Zhu",
            "Lingteng Qiu",
            "Xiaodong Gu",
            "Zhengyi Zhao",
            "Chao Xu",
            "Yuxiao He",
            "Zhe Li",
            "Xiaoguang Han",
            "Yao Yao",
            "Xun Cao",
            "Siyu Zhu",
            "Weihao Yuan",
            "Zilong Dong",
            "Hao Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14148",
        "abstract": "Existing 2D methods utilize UNet-based diffusion models to generate multi-view physically-based rendering (PBR) maps but struggle with multi-view inconsistency, while some 3D methods directly generate UV maps, encountering generalization issues due to the limited 3D data. To address these problems, we propose a two-stage approach, including multi-view generation and UV materials refinement. In the generation stage, we adopt a Diffusion Transformer (DiT) model to generate PBR materials, where both the specially designed multi-branch DiT and reference-based DiT blocks adopt a global attention mechanism to promote feature interaction and fusion between different views, thereby improving multi-view consistency. In addition, we adopt a PBR-based diffusion loss to ensure that the generated materials align with realistic physical principles. In the refinement stage, we propose a material-refined DiT that performs inpainting in empty areas and enhances details in UV space. Except for the normal condition, this refinement also takes the material map from the generation stage as an additional condition to reduce the learning difficulty and improve generalization. Extensive experiments show that our method achieves state-of-the-art performance in texturing 3D objects with PBR materials and provides significant advantages for graphics relighting applications. Project Page: https://lingtengqiu.github.io/2024/MCMat/",
        "tags": [
            "3D",
            "DiT",
            "Diffusion",
            "Diffusion Transformer",
            "Transformer"
        ]
    },
    {
        "id": "137",
        "title": "AKiRa: Augmentation Kit on Rays for optical video generation",
        "author": [
            "Xi Wang",
            "Robin Courant",
            "Marc Christie",
            "Vicky Kalogeiton"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14158",
        "abstract": "Recent advances in text-conditioned video diffusion have greatly improved video quality. However, these methods offer limited or sometimes no control to users on camera aspects, including dynamic camera motion, zoom, distorted lens and focus shifts. These motion and optical aspects are crucial for adding controllability and cinematic elements to generation frameworks, ultimately resulting in visual content that draws focus, enhances mood, and guides emotions according to filmmakers' controls. In this paper, we aim to close the gap between controllable video generation and camera optics. To achieve this, we propose AKiRa (Augmentation Kit on Rays), a novel augmentation framework that builds and trains a camera adapter with a complex camera model over an existing video generation backbone. It enables fine-tuned control over camera motion as well as complex optical parameters (focal length, distortion, aperture) to achieve cinematic effects such as zoom, fisheye effect, and bokeh. Extensive experiments demonstrate AKiRa's effectiveness in combining and composing camera optics while outperforming all state-of-the-art methods. This work sets a new landmark in controlled and optically enhanced video generation, paving the way for future optical video generation methods.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "138",
        "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks",
        "author": [
            "Frank F. Xu",
            "Yufan Song",
            "Boxuan Li",
            "Yuxuan Tang",
            "Kritanjali Jain",
            "Mengxue Bao",
            "Zora Z. Wang",
            "Xuhui Zhou",
            "Zhitong Guo",
            "Murong Cao",
            "Mingyang Yang",
            "Hao Yang Lu",
            "Amaad Martin",
            "Zhe Su",
            "Leander Maben",
            "Raj Mehta",
            "Wayne Chi",
            "Lawrence Jang",
            "Yiqing Xie",
            "Shuyan Zhou",
            "Graham Neubig"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14161",
        "abstract": "We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at helping to accelerate or even autonomously perform work-related tasks? The answer to this question has important implications for both industry looking to adopt AI into their workflows, and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper, we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that with the most competitive agent, 24% of the tasks can be completed autonomously. This paints a nuanced picture on task automation with LM agents -- in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "139",
        "title": "MetaMorph: Multimodal Understanding and Generation via Instruction Tuning",
        "author": [
            "Shengbang Tong",
            "David Fan",
            "Jiachen Zhu",
            "Yunyang Xiong",
            "Xinlei Chen",
            "Koustuv Sinha",
            "Michael Rabbat",
            "Yann LeCun",
            "Saining Xie",
            "Zhuang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14164",
        "abstract": "In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a simple and effective extension to visual instruction tuning that enables a pretrained LLM to quickly morph into an unified autoregressive model capable of generating both text and visual tokens. VPiT teaches an LLM to predict discrete text tokens and continuous visual tokens from any input sequence of image and text data curated in an instruction-following format. Our empirical investigation reveals several intriguing properties of VPiT: (1) visual generation ability emerges as a natural byproduct of improved visual understanding, and can be unlocked efficiently with a small amount of generation data; (2) while we find understanding and generation to be mutually beneficial, understanding data contributes to both capabilities more effectively than generation data. Building upon these findings, we train our MetaMorph model and achieve competitive performance on both visual understanding and generation. In visual generation, MetaMorph can leverage the world knowledge and reasoning abilities gained from LLM pretraining, and overcome common failure modes exhibited by other generation models. Our results suggest that LLMs may have strong \"prior\" vision capabilities that can be efficiently adapted to both visual understanding and generation with a relatively simple instruction tuning process.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "140",
        "title": "VideoDPO: Omni-Preference Alignment for Video Diffusion Generation",
        "author": [
            "Runtao Liu",
            "Haoyu Wu",
            "Zheng Ziqiang",
            "Chen Wei",
            "Yingqing He",
            "Renjie Pi",
            "Qifeng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14167",
        "abstract": "Recent progress in generative diffusion models has greatly advanced text-to-video generation. While text-to-video models trained on large-scale, diverse datasets can produce varied outputs, these generations often deviate from user preferences, highlighting the need for preference alignment on pre-trained models. Although Direct Preference Optimization (DPO) has demonstrated significant improvements in language and image generation, we pioneer its adaptation to video diffusion models and propose a VideoDPO pipeline by making several key adjustments. Unlike previous image alignment methods that focus solely on either (i) visual quality or (ii) semantic alignment between text and videos, we comprehensively consider both dimensions and construct a preference score accordingly, which we term the OmniScore. We design a pipeline to automatically collect preference pair data based on the proposed OmniScore and discover that re-weighting these pairs based on the score significantly impacts overall preference alignment. Our experiments demonstrate substantial improvements in both visual quality and semantic alignment, ensuring that no preference aspect is neglected. Code and data will be shared at https://videodpo.github.io/.",
        "tags": [
            "Diffusion",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "141",
        "title": "Autoregressive Video Generation without Vector Quantization",
        "author": [
            "Haoge Deng",
            "Ting Pan",
            "Haiwen Diao",
            "Zhengxiong Luo",
            "Yufeng Cui",
            "Huchuan Lu",
            "Shiguang Shan",
            "Yonggang Qi",
            "Xinlong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14169",
        "abstract": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
        "tags": [
            "Diffusion",
            "GPT",
            "Text-to-Image",
            "Vector Quantization",
            "Video Generation"
        ]
    },
    {
        "id": "142",
        "title": "E-CAR: Efficient Continuous Autoregressive Image Generation via Multistage Modeling",
        "author": [
            "Zhihang Yuan",
            "Yuzhang Shang",
            "Hanling Zhang",
            "Tongcheng Fang",
            "Rui Xie",
            "Bingxin Xu",
            "Yan Yan",
            "Shengen Yan",
            "Guohao Dai",
            "Yu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14170",
        "abstract": "Recent advances in autoregressive (AR) models with continuous tokens for image generation show promising results by eliminating the need for discrete tokenization. However, these models face efficiency challenges due to their sequential token generation nature and reliance on computationally intensive diffusion-based sampling. We present ECAR (Efficient Continuous Auto-Regressive Image Generation via Multistage Modeling), an approach that addresses these limitations through two intertwined innovations: (1) a stage-wise continuous token generation strategy that reduces computational complexity and provides progressively refined token maps as hierarchical conditions, and (2) a multistage flow-based distribution modeling method that transforms only partial-denoised distributions at each stage comparing to complete denoising in normal diffusion models. Holistically, ECAR operates by generating tokens at increasing resolutions while simultaneously denoising the image at each stage. This design not only reduces token-to-image transformation cost by a factor of the stage number but also enables parallel processing at the token level. Our approach not only enhances computational efficiency but also aligns naturally with image generation principles by operating in continuous token space and following a hierarchical generation process from coarse to fine details. Experimental results demonstrate that ECAR achieves comparable image quality to DiT Peebles & Xie [2023] while requiring 10$\\times$ FLOPs reduction and 5$\\times$ speedup to generate a 256$\\times$256 image.",
        "tags": [
            "DiT",
            "Diffusion"
        ]
    },
    {
        "id": "143",
        "title": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces",
        "author": [
            "Jihan Yang",
            "Shusheng Yang",
            "Anjali W. Gupta",
            "Rilyn Han",
            "Li Fei-Fei",
            "Saining Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14171",
        "abstract": "Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also ``think in space'' from videos? We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive - though subhuman - visual-spatial intelligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs' spatial distance ability.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "144",
        "title": "AniDoc: Animation Creation Made Easier",
        "author": [
            "Yihao Meng",
            "Hao Ouyang",
            "Hanlin Wang",
            "Qiuyu Wang",
            "Wen Wang",
            "Ka Leong Cheng",
            "Zhiheng Liu",
            "Yujun Shen",
            "Huamin Qu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14173",
        "abstract": "The production of 2D animation follows an industry-standard workflow, encompassing four essential stages: character design, keyframe animation, in-betweening, and coloring. Our research focuses on reducing the labor costs in the above process by harnessing the potential of increasingly powerful generative AI. Using video diffusion models as the foundation, AniDoc emerges as a video line art colorization tool, which automatically converts sketch sequences into colored animations following the reference character specification. Our model exploits correspondence matching as an explicit guidance, yielding strong robustness to the variations (e.g., posture) between the reference character and each line art frame. In addition, our model could even automate the in-betweening process, such that users can easily create a temporally consistent animation by simply providing a character image as well as the start and end sketches. Our code is available at: https://yihao-meng.github.io/AniDoc_demo.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "145",
        "title": "Model Decides How to Tokenize: Adaptive DNA Sequence Tokenization with MxDNA",
        "author": [
            "Lifeng Qiao",
            "Peng Ye",
            "Yuchen Ren",
            "Weiqiang Bai",
            "Chaoqi Liang",
            "Xinzhu Ma",
            "Nanqing Dong",
            "Wanli Ouyang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13716",
        "abstract": "Foundation models have made significant strides in understanding the genomic language of DNA sequences. However, previous models typically adopt the tokenization methods designed for natural language, which are unsuitable for DNA sequences due to their unique characteristics. In addition, the optimal approach to tokenize DNA remains largely under-explored, and may not be intuitively understood by humans even if discovered. To address these challenges, we introduce MxDNA, a novel framework where the model autonomously learns an effective DNA tokenization strategy through gradient decent. MxDNA employs a sparse Mixture of Convolution Experts coupled with a deformable convolution to model the tokenization process, with the discontinuous, overlapping, and ambiguous nature of meaningful genomic segments explicitly considered. On Nucleotide Transformer Benchmarks and Genomic Benchmarks, MxDNA demonstrates superior performance to existing methods with less pretraining data and time, highlighting its effectiveness. Finally, we show that MxDNA learns unique tokenization strategy distinct to those of previous methods and captures genomic functionalities at a token level during self-supervised pretraining. Our MxDNA aims to provide a new perspective on DNA tokenization, potentially offering broad applications in various domains and yielding profound insights.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "146",
        "title": "SongEditor: Adapting Zero-Shot Song Generation Language Model as a Multi-Task Editor",
        "author": [
            "Chenyu Yang",
            "Shuai Wang",
            "Hangting Chen",
            "Jianwei Yu",
            "Wei Tan",
            "Rongzhi Gu",
            "Yaoxun Xu",
            "Yizhi Zhou",
            "Haina Zhu",
            "Haizhou Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13786",
        "abstract": "The emergence of novel generative modeling paradigms, particularly audio language models, has significantly advanced the field of song generation. Although state-of-the-art models are capable of synthesizing both vocals and accompaniment tracks up to several minutes long concurrently, research about partial adjustments or editing of existing songs is still underexplored, which allows for more flexible and effective production. In this paper, we present SongEditor, the first song editing paradigm that introduces the editing capabilities into language-modeling song generation approaches, facilitating both segment-wise and track-wise modifications. SongEditor offers the flexibility to adjust lyrics, vocals, and accompaniments, as well as synthesizing songs from scratch. The core components of SongEditor include a music tokenizer, an autoregressive language model, and a diffusion generator, enabling generating an entire section, masked lyrics, or even separated vocals and background music. Extensive experiments demonstrate that the proposed SongEditor achieves exceptional performance in end-to-end song editing, as evidenced by both objective and subjective metrics. Audio samples are available in \\url{https://cypress-yang.github.io/SongEditor_demo/}.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "147",
        "title": "Diagnosising Helicobacter pylori using AutoEncoders and Limited Annotations through Anomalous Staining Patterns in IHC Whole Slide Images",
        "author": [
            "Pau Cano",
            "Eva Musulen",
            "Debora Gil"
        ],
        "pdf": "https://arxiv.org/pdf/2412.13857",
        "abstract": "Purpose: This work addresses the detection of Helicobacter pylori (H. pylori) in histological images with immunohistochemical staining. This analysis is a time demanding task, currently done by an expert pathologist that visually inspects the samples. Given the effort required to localise the pathogen in images, a limited number of annotations might be available in an initial setting. Our goal is to design an approach that, using a limited set of annotations, is capable of obtaining results good enough to be used as a support tool. Methods: We propose to use autoencoders to learn the latent patterns of healthy patches and formulate a specific measure of the reconstruction error of the image in HSV space. ROC analysis is used to set the optimal threshold of this measure and the percentage of positive patches in a sample that determines the presence of H. pylori. Results: Our method has been tested on an own database of 245 Whole Slide Images (WSI) having 117 cases without H. pylori and different density of the bacteria in the remaining ones. The database has 1211 annotated patches, with only 163 positive patches. This dataset of positive annotations was used to train a baseline thresholding and an SVM using the features of a pre-trained RedNet18 and ViT models. A 10-fold cross-validation shows that our method has better performance with 91% accuracy, 86% sensitivity, 96% specificity and 0.97 AUC in the diagnosis of H. pylori. Conclusion: Unlike classification approaches, our shallow autoencoder with threshold adaptation for the detection of anomalous staining is able to achieve competitive results with a limited set of annotated data. This initial approach is good enough to be used as a guide for fast annotation of infected patches.",
        "tags": [
            "Detection",
            "ViT"
        ]
    }
]