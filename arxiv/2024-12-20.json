[
    {
        "id": "1",
        "title": "Steering Large Text-to-Image Model for Abstract Art Synthesis: Preference-based Prompt Optimization and Visualization",
        "author": [
            "Aven-Le Zhou",
            "Wei Wu",
            "Yu-Ao Wang",
            "Kang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14174",
        "abstract": "With the advancement of neural generative capabilities, the art community has increasingly embraced GenAI (Generative Artificial Intelligence), particularly large text-to-image models, for producing aesthetically compelling results. However, the process often lacks determinism and requires a tedious trial-and-error process as users often struggle to devise effective prompts to achieve their desired outcomes. This paper introduces a prompting-free generative approach that applies a genetic algorithm and real-time iterative human feedback to optimize prompt generation, enabling the creation of user-preferred abstract art through a customized Artist Model. The proposed two-part approach begins with constructing an Artist Model capable of deterministically generating abstract art in specific styles, e.g., Kandinsky's Bauhaus style. The second phase integrates real-time user feedback to optimize the prompt generation and obtains an Optimized Prompting Model, which adapts to user preferences and generates prompts automatically. When combined with the Artist Model, this approach allows users to create abstract art tailored to their personal preferences and artistic style.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "2",
        "title": "Ontology-Aware RAG for Improved Question-Answering in Cybersecurity Education",
        "author": [
            "Chengshuai Zhao",
            "Garima Agrawal",
            "Tharindu Kumarage",
            "Zhen Tan",
            "Yuli Deng",
            "Ying-Chih Chen",
            "Huan Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14191",
        "abstract": "Integrating AI into education has the potential to transform the teaching of science and technology courses, particularly in the field of cybersecurity. AI-driven question-answering (QA) systems can actively manage uncertainty in cybersecurity problem-solving, offering interactive, inquiry-based learning experiences. Large language models (LLMs) have gained prominence in AI-driven QA systems, offering advanced language understanding and user engagement. However, they face challenges like hallucinations and limited domain-specific knowledge, which reduce their reliability in educational settings. To address these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented generation (RAG) approach for developing a reliable and safe QA system in cybersecurity education. CyberRAG employs a two-step approach: first, it augments the domain-specific knowledge by retrieving validated cybersecurity documents from a knowledge base to enhance the relevance and accuracy of the response. Second, it mitigates hallucinations and misuse by integrating a knowledge graph ontology to validate the final answer. Experiments on publicly available cybersecurity datasets show that CyberRAG delivers accurate, reliable responses aligned with domain knowledge, demonstrating the potential of AI tools to enhance education.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "3",
        "title": "Advancing Vehicle Plate Recognition: Multitasking Visual Language Models with VehiclePaliGemma",
        "author": [
            "Nouar AlDahoul",
            "Myles Joshua Toledo Tan",
            "Raghava Reddy Tera",
            "Hezerul Abdul Karim",
            "Chee How Lim",
            "Manish Kumar Mishra",
            "Yasir Zaki"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14197",
        "abstract": "License plate recognition (LPR) involves automated systems that utilize cameras and computer vision to read vehicle license plates. Such plates collected through LPR can then be compared against databases to identify stolen vehicles, uninsured drivers, crime suspects, and more. The LPR system plays a significant role in saving time for institutions such as the police force. In the past, LPR relied heavily on Optical Character Recognition (OCR), which has been widely explored to recognize characters in images. Usually, collected plate images suffer from various limitations, including noise, blurring, weather conditions, and close characters, making the recognition complex. Existing LPR methods still require significant improvement, especially for distorted images. To fill this gap, we propose utilizing visual language models (VLMs) such as OpenAI GPT4o, Google Gemini 1.5, Google PaliGemma (Pathways Language and Image model + Gemma model), Meta Llama 3.2, Anthropic Claude 3.5 Sonnet, LLaVA, NVIDIA VILA, and moondream2 to recognize such unclear plates with close characters. This paper evaluates the VLM's capability to address the aforementioned problems. Additionally, we introduce ``VehiclePaliGemma'', a fine-tuned Open-sourced PaliGemma VLM designed to recognize plates under challenging conditions. We compared our proposed VehiclePaliGemma with state-of-the-art methods and other VLMs using a dataset of Malaysian license plates collected under complex conditions. The results indicate that VehiclePaliGemma achieved superior performance with an accuracy of 87.6\\%. Moreover, it is able to predict the car's plate at a speed of 7 frames per second using A100-80GB GPU. Finally, we explored the multitasking capability of VehiclePaliGemma model to accurately identify plates containing multiple cars of various models and colors, with plates positioned and oriented in different directions.",
        "tags": [
            "LLaMA",
            "LLaVA"
        ]
    },
    {
        "id": "4",
        "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with Large Language Models",
        "author": [
            "Boris Ruf",
            "Marcin Detyniecki"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14201",
        "abstract": "We propose a simple way to use large language models (LLMs) in education. Specifically, our method aims to improve individual comprehension by adding a novel feature to online videos. We combine the low threshold for interactivity in digital experiences with the benefits of rephrased and elaborated explanations typical of face-to-face interactions, thereby supporting to close knowledge gaps at scale. To demonstrate the technical feasibility of our approach, we conducted a proof-of-concept experiment and implemented a prototype which is available for testing online. Through the use case, we also show how caching can be applied in LLM-powered applications to reduce their carbon footprint.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "5",
        "title": "BlenderLLM: Training Large Language Models for Computer-Aided Design with Self-improvement",
        "author": [
            "Yuhao Du",
            "Shunian Chen",
            "Wenbo Zan",
            "Peizhao Li",
            "Mingxuan Wang",
            "Dingjie Song",
            "Bo Li",
            "Yan Hu",
            "Benyou Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14203",
        "abstract": "The application of Large Language Models (LLMs) in Computer-Aided Design (CAD) remains an underexplored area, despite their remarkable advancements in other domains. In this paper, we present BlenderLLM, a novel framework for training LLMs specifically for CAD tasks leveraging a self-improvement methodology. To support this, we developed a bespoke training dataset, BlendNet, and introduced a comprehensive evaluation suite, CADBench. Our results reveal that existing models demonstrate significant limitations in generating accurate CAD scripts. However, through minimal instruction-based fine-tuning and iterative self-improvement, BlenderLLM significantly surpasses these models in both functionality and accuracy of CAD script generation. This research establishes a strong foundation for the application of LLMs in CAD while demonstrating the transformative potential of self-improving models in advancing CAD automation. We encourage further exploration and adoption of these methodologies to drive innovation in the field. The dataset, model, benchmark, and source code are publicly available at https://github.com/FreedomIntelligence/BlenderLLM",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "6",
        "title": "Mobilizing Waldo: Evaluating Multimodal AI for Public Mobilization",
        "author": [
            "Manuel Cebrian",
            "Petter Holme",
            "Niccolo Pescetelli"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14210",
        "abstract": "Advancements in multimodal Large Language Models (LLMs), such as OpenAI's GPT-4o, offer significant potential for mediating human interactions across various contexts. However, their use in areas such as persuasion, influence, and recruitment raises ethical and security concerns. To evaluate these models ethically in public influence and persuasion scenarios, we developed a prompting strategy using \"Where's Waldo?\" images as proxies for complex, crowded gatherings. This approach provides a controlled, replicable environment to assess the model's ability to process intricate visual information, interpret social dynamics, and propose engagement strategies while avoiding privacy concerns. By positioning Waldo as a hypothetical agent tasked with face-to-face mobilization, we analyzed the model's performance in identifying key individuals and formulating mobilization tactics. Our results show that while the model generates vivid descriptions and creative strategies, it cannot accurately identify individuals or reliably assess social dynamics in these scenarios. Nevertheless, this methodology provides a valuable framework for testing and benchmarking the evolving capabilities of multimodal LLMs in social contexts.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "7",
        "title": "Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution",
        "author": [
            "Ziyi Ni",
            "Yifan Li",
            "Daxiang Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14212",
        "abstract": "The exceptional capabilities of large language models (LLMs) have substantially accelerated the rapid rise and widespread adoption of agents. Recent studies have demonstrated that generating Python code to consolidate LLM-based agents' actions into a unified action space (CodeAct) is a promising approach for developing real-world LLM agents. However, this step-by-step code generation approach often lacks consistency and robustness, leading to instability in agent applications, particularly for complex reasoning and out-of-domain tasks. In this paper, we propose a novel approach called Tree-of-Code (ToC) to tackle the challenges of complex problem planning and execution with an end-to-end mechanism. By integrating key ideas from both Tree-of-Thought and CodeAct, ToC combines their strengths to enhance solution exploration. In our framework, each final code execution result is treated as a node in the decision tree, with a breadth-first search strategy employed to explore potential solutions. The final outcome is determined through a voting mechanism based on the outputs of the nodes.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "8",
        "title": "GraphicsDreamer: Image to 3D Generation with Physical Consistency",
        "author": [
            "Pei Chen",
            "Fudong Wang",
            "Yixuan Tong",
            "Jingdong Chen",
            "Ming Yang",
            "Minghui Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14214",
        "abstract": "Recently, the surge of efficient and automated 3D AI-generated content (AIGC) methods has increasingly illuminated the path of transforming human imagination into complex 3D structures. However, the automated generation of 3D content is still significantly lags in industrial application. This gap exists because 3D modeling demands high-quality assets with sharp geometry, exquisite topology, and physically based rendering (PBR), among other criteria. To narrow the disparity between generated results and artists' expectations, we introduce GraphicsDreamer, a method for creating highly usable 3D meshes from single images. To better capture the geometry and material details, we integrate the PBR lighting equation into our cross-domain diffusion model, concurrently predicting multi-view color, normal, depth images, and PBR materials. In the geometry fusion stage, we continue to enforce the PBR constraints, ensuring that the generated 3D objects possess reliable texture details, supporting realistic relighting. Furthermore, our method incorporates topology optimization and fast UV unwrapping capabilities, allowing the 3D products to be seamlessly imported into graphics engines. Extensive experiments demonstrate that our model can produce high quality 3D assets in a reasonable time cost compared to previous methods.",
        "tags": [
            "3D",
            "Diffusion",
            "Image-to-3D"
        ]
    },
    {
        "id": "9",
        "title": "Distilled Pooling Transformer Encoder for Efficient Realistic Image Dehazing",
        "author": [
            "Le-Anh Tran",
            "Dong-Chul Park"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14220",
        "abstract": "This paper proposes a lightweight neural network designed for realistic image dehazing, utilizing a Distilled Pooling Transformer Encoder, named DPTE-Net. Recently, while vision transformers (ViTs) have achieved great success in various vision tasks, their self-attention (SA) module's complexity scales quadratically with image resolution, hindering their applicability on resource-constrained devices. To overcome this, the proposed DPTE-Net substitutes traditional SA modules with efficient pooling mechanisms, significantly reducing computational demands while preserving ViTs' learning capabilities. To further enhance semantic feature learning, a distillation-based training process is implemented which transfers rich knowledge from a larger teacher network to DPTE-Net. Additionally, DPTE-Net is trained within a generative adversarial network (GAN) framework, leveraging the strong generalization of GAN in image restoration, and employs a transmission-aware loss function to dynamically adapt to varying haze densities. Experimental results on various benchmark datasets have shown that the proposed DPTE-Net can achieve competitive dehazing performance when compared to state-of-the-art methods while maintaining low computational complexity, making it a promising solution for resource-limited applications. The code of this work is available at https://github.com/tranleanh/dpte-net.",
        "tags": [
            "GAN",
            "Transformer"
        ]
    },
    {
        "id": "10",
        "title": "A Survey on Large Language Model-based Agents for Statistics and Data Science",
        "author": [
            "Maojun Sun",
            "Ruijian Han",
            "Binyan Jiang",
            "Houduo Qi",
            "Defeng Sun",
            "Yancheng Yuan",
            "Jian Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14222",
        "abstract": "In recent years, data science agents powered by Large Language Models (LLMs), known as \"data agents,\" have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "11",
        "title": "ViTmiX: Vision Transformer Explainability Augmented by Mixed Visualization Methods",
        "author": [
            "Eduard Hogea",
            "Darian M. Onchis",
            "Ana Coporan",
            "Adina Magda Florea",
            "Codruta Istin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14231",
        "abstract": "Recent advancements in Vision Transformers (ViT) have demonstrated exceptional results in various visual recognition tasks, owing to their ability to capture long-range dependencies in images through self-attention mechanisms. However, the complex nature of ViT models requires robust explainability methods to unveil their decision-making processes. Explainable Artificial Intelligence (XAI) plays a crucial role in improving model transparency and trustworthiness by providing insights into model predictions. Current approaches to ViT explainability, based on visualization techniques such as Layer-wise Relevance Propagation (LRP) and gradient-based methods, have shown promising but sometimes limited results. In this study, we explore a hybrid approach that mixes multiple explainability techniques to overcome these limitations and enhance the interpretability of ViT models. Our experiments reveal that this hybrid approach significantly improves the interpretability of ViT models compared to individual methods. We also introduce modifications to existing techniques, such as using geometric mean for mixing, which demonstrates notable results in object segmentation tasks. To quantify the explainability gain, we introduced a novel post-hoc explainability measure by applying the Pigeonhole principle. These findings underscore the importance of refining and optimizing explainability methods for ViT models, paving the way to reliable XAI-based segmentations.",
        "tags": [
            "Segmentation",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "12",
        "title": "Syzygy: Dual Code-Test C to (safe) Rust Translation using LLMs and Dynamic Analysis",
        "author": [
            "Manish Shetty",
            "Naman Jain",
            "Adwait Godbole",
            "Sanjit A. Seshia",
            "Koushik Sen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14234",
        "abstract": "Despite extensive usage in high-performance, low-level systems programming applications, C is susceptible to vulnerabilities due to manual memory management and unsafe pointer operations. Rust, a modern systems programming language, offers a compelling alternative. Its unique ownership model and type system ensure memory safety without sacrificing performance.\nIn this paper, we present Syzygy, an automated approach to translate C to safe Rust. Our technique uses a synergistic combination of LLM-driven code and test translation guided by dynamic-analysis-generated execution information. This paired translation runs incrementally in a loop over the program in dependency order of the code elements while maintaining per-step correctness. Our approach exposes novel insights on combining the strengths of LLMs and dynamic analysis in the context of scaling and combining code generation with testing. We apply our approach to successfully translate Zopfli, a high-performance compression library with ~3000 lines of code and 98 functions. We validate the translation by testing equivalence with the source C program on a set of inputs. To our knowledge, this is the largest automated and test-validated C to safe Rust code translation achieved so far.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "13",
        "title": "Fake News Detection: Comparative Evaluation of BERT-like Models and Large Language Models with Generative AI-Annotated Data",
        "author": [
            "haina Raza",
            "Drai Paulen-Patterson",
            "Chen Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14276",
        "abstract": "Fake news poses a significant threat to public opinion and social stability in modern society. This study presents a comparative evaluation of BERT-like encoder-only models and autoregressive decoder-only large language models (LLMs) for fake news detection. We introduce a dataset of news articles labeled with GPT-4 assistance (an AI-labeling method) and verified by human experts to ensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned on this dataset. Additionally, we developed an instruction-tuned LLM approach with majority voting during inference for label generation. Our analysis reveals that BERT-like models generally outperform LLMs in classification tasks, while LLMs demonstrate superior robustness against text perturbations. Compared to weak labels (distant supervision) data, the results show that AI labels with human supervision achieve better classification results. This study highlights the effectiveness of combining AI-based annotation with human oversight and demonstrates the performance of different families of machine learning models for fake news detection",
        "tags": [
            "BERT",
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "14",
        "title": "PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation",
        "author": [
            "Liyao Jiang",
            "Negar Hassanpour",
            "Mohammad Salameh",
            "Mohammadreza Samadi",
            "Jiao He",
            "Fengyu Sun",
            "Di Niu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14283",
        "abstract": "Recent research explores the potential of Diffusion Models (DMs) for consistent object editing, which aims to modify object position, size, and composition, etc., while preserving the consistency of objects and background without changing their texture and attributes. Current inference-time methods often rely on DDIM inversion, which inherently compromises efficiency and the achievable consistency of edited images. Recent methods also utilize energy guidance which iteratively updates the predicted noise and can drive the latents away from the original image, resulting in distortions. In this paper, we propose PixelMan, an inversion-free and training-free method for achieving consistent object editing via Pixel Manipulation and generation, where we directly create a duplicate copy of the source object at target location in the pixel space, and introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location, while ensuring image consistency by anchoring the edited image to be generated to the pixel-manipulated image as well as by introducing various consistency-preserving optimization techniques during inference. Experimental evaluations based on benchmark datasets as well as extensive visual comparisons show that in as few as 16 inference steps, PixelMan outperforms a range of state-of-the-art training-based and training-free methods (usually requiring 50 steps) on multiple consistent object editing tasks.",
        "tags": [
            "DDIM",
            "Diffusion"
        ]
    },
    {
        "id": "15",
        "title": "TRecViT: A Recurrent Video Transformer",
        "author": [
            "Viorica Pătrăucean",
            "Xu Owen He",
            "Joseph Heyward",
            "Chuhan Zhang",
            "Mehdi S. M. Sajjadi",
            "George-Cristian Muraru",
            "Artem Zholus",
            "Mahdi Karami",
            "Ross Goroshin",
            "Yutian Chen",
            "Simon Osindero",
            "João Carreira",
            "Razvan Pascanu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14294",
        "abstract": "We propose a novel block for video modelling. It relies on a time-space-channel factorisation with dedicated blocks for each dimension: gated linear recurrent units (LRUs) perform information mixing over time, self-attention layers perform mixing over space, and MLPs over channels. The resulting architecture TRecViT performs well on sparse and dense tasks, trained in supervised or self-supervised regimes. Notably, our model is causal and outperforms or is on par with a pure attention model ViViT-L on large scale video datasets (SSv2, Kinetics400), while having $3\\times$ less parameters, $12\\times$ smaller memory footprint, and $5\\times$ lower FLOPs count. Code and checkpoints will be made available online at https://github.com/google-deepmind/trecvit.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "16",
        "title": "Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation",
        "author": [
            "Benjamin Steenhoek",
            "Michele Tufano",
            "Neel Sundaresan",
            "Alexey Svyatkovskiy"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14308",
        "abstract": "Software testing is a crucial but time-consuming aspect of software development, and recently, Large Language Models (LLMs) have gained popularity for automated test case generation. However, because LLMs are trained on vast amounts of open-source code, they often generate test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose Reinforcement Learning from Static Quality Metrics (RLSQM), wherein we utilize Reinforcement Learning to generate high-quality unit tests based on static analysis-based quality metrics. First, we analyzed LLM-generated tests and show that LLMs frequently do generate undesirable test smells -- up to 37% of the time. Then, we implemented lightweight static analysis-based reward model and trained LLMs using this reward model to optimize for five code quality metrics. Our experimental results demonstrate that the RL-optimized Codex model consistently generated higher-quality test cases than the base LLM, improving quality metrics by up to 23%, and generated nearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all code quality metrics, in spite of training a substantially cheaper Codex model. We provide insights into how reliably utilize RL to improve test generation quality and show that RLSQM is a significant step towards enhancing the overall efficiency and reliability of automated software testing. Our data are available at https://doi.org/10.6084/m9.figshare.25983166.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models",
            "RL"
        ]
    },
    {
        "id": "17",
        "title": "Personalized Generative Low-light Image Denoising and Enhancement",
        "author": [
            "Xijun Wang",
            "Prateek Chennuri",
            "Yu Yuan",
            "Bole Ma",
            "Xingguang Zhang",
            "Stanley Chan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14327",
        "abstract": "While smartphone cameras today can produce astonishingly good photos, their performance in low light is still not completely satisfactory because of the fundamental limits in photon shot noise and sensor read noise. Generative image restoration methods have demonstrated promising results compared to traditional methods, but they suffer from hallucinatory content generation when the signal-to-noise ratio (SNR) is low. Recognizing the availability of personalized photo galleries on users' smartphones, we propose Personalized Generative Denoising (PGD) by building a diffusion model customized for different users. Our core innovation is an identity-consistent physical buffer that extracts the physical attributes of the person from the gallery. This ID-consistent physical buffer provides a strong prior that can be integrated with the diffusion model to restore the degraded images, without the need of fine-tuning. Over a wide range of low-light testing scenarios, we show that PGD achieves superior image denoising and enhancement performance compared to existing diffusion-based denoising approaches.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "18",
        "title": "Semantic Role Labeling of NomBank Partitives",
        "author": [
            "Adam Meyers",
            "Advait Pravin Savant",
            "John E. Ortega"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14328",
        "abstract": "This article is about Semantic Role Labeling for English partitive nouns (5%/REL of the price/ARG1; The price/ARG1 rose 5 percent/REL) in the NomBank annotated corpus. Several systems are described using traditional and transformer-based machine learning, as well as ensembling. Our highest scoring system achieves an F1 of 91.74% using \"gold\" parses from the Penn Treebank and 91.12% when using the Berkeley Neural parser. This research includes both classroom and experimental settings for system development.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "19",
        "title": "Joint Co-Speech Gesture and Expressive Talking Face Generation using Diffusion with Adapters",
        "author": [
            "Steven Hogue",
            "Chenxu Zhang",
            "Yapeng Tian",
            "Xiaohu Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14333",
        "abstract": "Recent advances in co-speech gesture and talking head generation have been impressive, yet most methods focus on only one of the two tasks. Those that attempt to generate both often rely on separate models or network modules, increasing training complexity and ignoring the inherent relationship between face and body movements. To address the challenges, in this paper, we propose a novel model architecture that jointly generates face and body motions within a single network. This approach leverages shared weights between modalities, facilitated by adapters that enable adaptation to a common latent space. Our experiments demonstrate that the proposed framework not only maintains state-of-the-art co-speech gesture and talking head generation performance but also significantly reduces the number of parameters required.",
        "tags": [
            "Diffusion",
            "Talking Face",
            "Talking Head"
        ]
    },
    {
        "id": "20",
        "title": "State Space Models are Strong Text Rerankers",
        "author": [
            "Zhichao Xu",
            "Jinghua Yan",
            "Ashim Gupta",
            "Vivek Srikumar"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14354",
        "abstract": "Transformers dominate NLP and IR; but their inference inefficiencies and challenges in extrapolating to longer contexts have sparked interest in alternative model architectures. Among these, state space models (SSMs) like Mamba offer promising advantages, particularly $O(1)$ time complexity in inference. Despite their potential, SSMs' effectiveness at text reranking -- a task requiring fine-grained query-document interaction and long-context understanding -- remains underexplored.\nThis study benchmarks SSM-based architectures (specifically, Mamba-1 and Mamba-2) against transformer-based models across various scales, architectures, and pre-training objectives, focusing on performance and efficiency in text reranking tasks. We find that (1) Mamba architectures achieve competitive text ranking performance, comparable to transformer-based models of similar size; (2) they are less efficient in training and inference compared to transformers with flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance and efficiency. These results underscore the potential of state space models as a transformer alternative and highlight areas for improvement in future IR applications.",
        "tags": [
            "Flash Attention",
            "Mamba",
            "SSMs",
            "State Space Models",
            "Transformer"
        ]
    },
    {
        "id": "21",
        "title": "ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals",
        "author": [
            "Utkarsh Saxena",
            "Sayeh Sharify",
            "Kaushik Roy",
            "Xin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14363",
        "abstract": "Post-training quantization (PTQ) of large language models (LLMs) holds the promise in reducing the prohibitive computational cost at inference time. Quantization of all weight, activation and key-value (KV) cache tensors to 4-bit without significantly degrading generalizability is challenging, due to the high quantization error caused by extreme outliers in activations. To tackle this problem, we propose ResQ, a PTQ method that pushes further the state-of-the-art. By means of principal component analysis (PCA), it identifies a low-rank subspace (in practice 1/8 of the hidden dimension) in which activation variances are highest, and keep the coefficients within this subspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit. Within each subspace, invariant random rotation is applied to further suppress outliers. We show that this is a provably optimal mixed precision quantization scheme that minimizes error. With the Llama families of models, we demonstrate that ResQ outperforms recent uniform and mixed precision PTQ methods on a variety of benchmarks, achieving up to 33% lower perplexity on Wikitext than the next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code is available at https://github.com/utkarsh-dmx/project-resq.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "22",
        "title": "Surrealistic-like Image Generation with Vision-Language Models",
        "author": [
            "Elif Ayten",
            "Shuai Wang",
            "Hjalmar Snoep"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14366",
        "abstract": "Recent advances in generative AI make it convenient to create different types of content, including text, images, and code. In this paper, we explore the generation of images in the style of paintings in the surrealism movement using vision-language generative models, including DALL-E, Deep Dream Generator, and DreamStudio. Our investigation starts with the generation of images under various image generation settings and different models. The primary objective is to identify the most suitable model and settings for producing such images. Additionally, we aim to understand the impact of using edited base images on the generated resulting images. Through these experiments, we evaluate the performance of selected models and gain valuable insights into their capabilities in generating such images. Our analysis shows that Dall-E 2 performs the best when using the generated prompt by ChatGPT.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "23",
        "title": "Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation",
        "author": [
            "Yuxuan Jiang",
            "Francis Ferraro"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14368",
        "abstract": "Recently, Large Language Models (LLMs) have shown impressive performance in character understanding tasks, such as analyzing the roles, personalities, and relationships of fictional characters. However, the extensive pre-training corpora used by LLMs raise concerns that they may rely on memorizing popular fictional works rather than genuinely understanding and reasoning about them. In this work, we argue that 'gist memory'-capturing essential meaning - should be the primary mechanism for character understanding tasks, as opposed to 'verbatim memory' - exact match of a string. We introduce a simple yet effective method to mitigate mechanized memorization in character understanding evaluations while preserving the essential implicit cues needed for comprehension and reasoning. Our approach reduces memorization-driven performance on popular fictional works from 96% accuracy to 72% and results in up to an 18% drop in accuracy across various character understanding tasks. These findings underscore the issue of data contamination in existing benchmarks, which often measure memorization rather than true character understanding.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "24",
        "title": "ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling",
        "author": [
            "William Han",
            "Chaojing Duan",
            "Michael A. Rosenberg",
            "Emerson Liu",
            "Ding Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14373",
        "abstract": "Large Language Models (LLMs) have shown remarkable adaptability across domains beyond text, specifically electrocardiograms (ECGs). More specifically, there is a growing body of work exploring the task of generating text from a multi-channeled ECG and corresponding textual prompt. Current approaches typically involve pretraining an ECG-specific encoder with a self-supervised learning (SSL) objective and using the features output by the pretrained encoder to finetune a LLM for natural language generation (NLG). However, these methods are limited by 1) inefficiency from two-stage training and 2) interpretability challenges with encoder-generated features. To address these limitations, we introduce ECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for autoregressive language modeling of ECGs. This approach compresses and encodes ECG signals into tokens, enabling end-to-end LLM training by combining ECG and text tokens directly, while being much more interpretable since the ECG tokens can be directly mapped back to the original signal. Using ECG-Byte, we achieve competitive performance in NLG tasks in only half the time and ~48% of the data required by two-stage approaches.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "25",
        "title": "I0T: Embedding Standardization Method Towards Zero Modality Gap",
        "author": [
            "Na Min An",
            "Eunki Kim",
            "James Thorne",
            "Hyunjung Shim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14384",
        "abstract": "Contrastive Language-Image Pretraining (CLIP) enables zero-shot inference in downstream tasks such as image-text retrieval and classification. However, recent works extending CLIP suffer from the issue of modality gap, which arises when the image and text embeddings are projected to disparate manifolds, deviating from the intended objective of image-text contrastive learning. We discover that this phenomenon is linked to the modality-specific characteristic that each image/text encoder independently possesses and propose two methods to address the modality gap: (1) a post-hoc embedding standardization method, $\\text{I0T}_{\\text{post}}$ that reduces the modality gap approximately to zero and (2) a trainable method, $\\text{I0T}_{\\text{async}}$, to alleviate the modality gap problem by adding two normalization layers for each encoder. Our I0T framework can significantly reduce the modality gap while preserving the original embedding representations of trained models with their locked parameters. In practice, $\\text{I0T}_{\\text{post}}$ can serve as an alternative explainable automatic evaluation metric of widely used CLIPScore (CLIP-S).",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "26",
        "title": "LLMSA: A Compositional Neuro-Symbolic Approach to Compilation-free and Customizable Static Analysis",
        "author": [
            "Chengpeng Wang",
            "Yifei Gao",
            "Wuqi Zhang",
            "Xuwei Liu",
            "Qingkai Shi",
            "Xiangyu Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14399",
        "abstract": "Static analysis is essential for program optimization, bug detection, and debugging, but its reliance on compilation and limited customization hampers practical use. Advances in LLMs enable a new paradigm of compilation-free, customizable analysis via prompting. LLMs excel in interpreting program semantics on small code snippets and allow users to define analysis tasks in natural language with few-shot examples. However, misalignment with program semantics can cause hallucinations, especially in sophisticated semantic analysis upon lengthy code snippets.\nWe propose LLMSA, a compositional neuro-symbolic approach for compilation-free, customizable static analysis with reduced hallucinations. Specifically, we propose an analysis policy language to support users decomposing an analysis problem into several sub-problems that target simple syntactic or semantic properties upon smaller code snippets. The problem decomposition enables the LLMs to target more manageable semantic-related sub-problems, while the syntactic ones are resolved by parsing-based analysis without hallucinations. An analysis policy is evaluated with lazy, incremental, and parallel prompting, which mitigates the hallucinations and improves the performance. It is shown that LLMSA achieves comparable and even superior performance to existing techniques in various clients. For instance, it attains 66.27% precision and 78.57% recall in taint vulnerability detection, surpassing an industrial approach in F1 score by 0.20.",
        "tags": [
            "Detection",
            "LLMs"
        ]
    },
    {
        "id": "27",
        "title": "ChainRank-DPO: Chain Rank Direct Preference Optimization for LLM Rankers",
        "author": [
            "Haowei Liu",
            "Xuyang Wu",
            "Guohao Sun",
            "Zhiqiang Tao",
            "Yi Fang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14405",
        "abstract": "Large language models (LLMs) have demonstrated remarkable effectiveness in text reranking through works like RankGPT, leveraging their human-like reasoning about relevance. However, supervised fine-tuning for ranking often diminishes these models' general-purpose capabilities, including the crucial reasoning abilities that make them valuable for ranking. We introduce a novel approach integrating Chain-of-Thought prompting with an SFT-DPO (Supervised Fine-Tuning followed by Direct Preference Optimization) pipeline to preserve these capabilities while improving ranking performance. Our experiments on TREC 2019 and 2020 Deep Learning datasets show that our approach outperforms the state-of-the-art RankZephyr while maintaining strong performance on the Massive Multitask Language Understanding (MMLU) benchmark, demonstrating effective preservation of general-purpose capabilities through thoughtful fine-tuning strategies. Our code and data will be publicly released upon the acceptance of the paper.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "28",
        "title": "DriveGPT: Scaling Autoregressive Behavior Models for Driving",
        "author": [
            "Xin Huang",
            "Eric M. Wolff",
            "Paul Vernaza",
            "Tung Phan-Minh",
            "Hongge Chen",
            "David S. Hayden",
            "Mark Edmonds",
            "Brian Pierce",
            "Xinxin Chen",
            "Pratik Elias Jacob",
            "Xiaobai Chen",
            "Chingiz Tairbekov",
            "Pratik Agarwal",
            "Tianshi Gao",
            "Yuning Chai",
            "Siddhartha Srinivasa"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14415",
        "abstract": "We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms a state-of-the-art baseline and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "29",
        "title": "Cutting Sequence Diffuser: Sim-to-Real Transferable Planning for Object Shaping by Grinding",
        "author": [
            "Takumi Hachimine",
            "Jun Morimoto",
            "Takamitsu Matsubara"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14417",
        "abstract": "Automating object shaping by grinding with a robot is a crucial industrial process that involves removing material with a rotating grinding belt. This process generates removal resistance depending on such process conditions as material type, removal volume, and robot grinding posture, all of which complicate the analytical modeling of shape transitions. Additionally, a data-driven approach based on real-world data is challenging due to high data collection costs and the irreversible nature of the process. This paper proposes a Cutting Sequence Diffuser (CSD) for object shaping by grinding. The CSD, which only requires simple simulation data for model learning, offers an efficient way to plan long-horizon action sequences transferable to the real world. Our method designs a smooth action space with constrained small removal volumes to suppress the complexity of the shape transitions caused by removal resistance, thus reducing the reality gap in simulations. Moreover, by using a diffusion model to generate long-horizon action sequences, our approach reduces the planning time and allows for grinding the target shape while adhering to the constraints of a small removal volume per step. Through evaluations in both simulation and real robot experiments, we confirmed that our CSD was effective for grinding to different materials and various target shapes in a short time.",
        "tags": [
            "Diffusion",
            "Robot"
        ]
    },
    {
        "id": "30",
        "title": "Enhancing Diffusion Models for High-Quality Image Generation",
        "author": [
            "Jaineet Shah",
            "Michael Gromis",
            "Rickston Pinto"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14422",
        "abstract": "This report presents the comprehensive implementation, evaluation, and optimization of Denoising Diffusion Probabilistic Models (DDPMs) and Denoising Diffusion Implicit Models (DDIMs), which are state-of-the-art generative models. During inference, these models take random noise as input and iteratively generate high-quality images as output. The study focuses on enhancing their generative capabilities by incorporating advanced techniques such as Classifier-Free Guidance (CFG), Latent Diffusion Models with Variational Autoencoders (VAE), and alternative noise scheduling strategies. The motivation behind this work is the growing demand for efficient and scalable generative AI models that can produce realistic images across diverse datasets, addressing challenges in applications such as art creation, image synthesis, and data augmentation. Evaluations were conducted on datasets including CIFAR-10 and ImageNet-100, with a focus on improving inference speed, computational efficiency, and image quality metrics like Frechet Inception Distance (FID). Results demonstrate that DDIM + CFG achieves faster inference and superior image quality. Challenges with VAE and noise scheduling are also highlighted, suggesting opportunities for future optimization. This work lays the groundwork for developing scalable, efficient, and high-quality generative AI systems to benefit industries ranging from entertainment to robotics.",
        "tags": [
            "DDIM",
            "Diffusion",
            "Robotics",
            "VAE"
        ]
    },
    {
        "id": "31",
        "title": "All-in-One Tuning and Structural Pruning for Domain-Specific LLMs",
        "author": [
            "Lei Lu",
            "Zhepeng Wang",
            "Ruexue Bao",
            "Mengbing Wang",
            "Fangyi Li",
            "Yawen Wu",
            "Weiwen Jiang",
            "Jie Xu",
            "Yanzhi Wang",
            "Shangqian Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14426",
        "abstract": "Existing pruning techniques for large language models (LLMs) targeting domain-specific applications typically follow a two-stage process: pruning the pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on specific domains. However, the pruning decisions, derived from the pretrained weights, remain unchanged during fine-tuning, even if the weights have been updated. Therefore, such a combination of the pruning decisions and the finetuned weights may be suboptimal, leading to non-negligible performance degradation. To address these limitations, we propose ATP: All-in-One Tuning and Structural Pruning, a unified one-stage structural pruning and fine-tuning approach that dynamically identifies the current optimal substructure throughout the fine-tuning phase via a trainable pruning decision generator. Moreover, given the limited available data for domain-specific applications, Low-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In ATP, we introduce LoRA-aware forward and sparsity regularization to ensure that the substructures corresponding to the learned pruning decisions can be directly removed after the ATP process. ATP outperforms the state-of-the-art two-stage pruning methods on tasks in the legal and healthcare domains. More specifically, ATP recovers up to 88% and 91% performance of the dense model when pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "32",
        "title": "IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features",
        "author": [
            "Anand Kumar",
            "Jiteng Mu",
            "Nuno Vasconcelos"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14432",
        "abstract": "Text-to-image (T2I) models have gained widespread adoption among content creators and the general public. However, this has sparked significant concerns regarding data privacy and copyright infringement among artists. Consequently, there is an increasing demand for T2I models to incorporate mechanisms that prevent the generation of specific artistic styles, thereby safeguarding intellectual property rights. Existing methods for style extraction typically necessitate the collection of custom datasets and the training of specialized models. This, however, is resource-intensive, time-consuming, and often impractical for real-time applications. Moreover, it may not adequately address the dynamic nature of artistic styles and the rapidly evolving landscape of digital art. We present a novel, training-free framework to solve the style attribution problem, using the features produced by a diffusion model alone, without any external modules or retraining. This is denoted as introspective style attribution (IntroStyle) and demonstrates superior performance to state-of-the-art models for style retrieval. We also introduce a synthetic dataset of Style Hacks (SHacks) to isolate artistic style and evaluate fine-grained style attribution performance.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "33",
        "title": "ORBIT: Cost-Effective Dataset Curation for Large Language Model Domain Adaptation with an Astronomy Case Study",
        "author": [
            "Eric Modesitt",
            "Ke Yang",
            "Spencer Hulsey",
            "Chengxiang Zhai",
            "Volodymyr Kindratenko"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14436",
        "abstract": "Recent advances in language modeling demonstrate the need for high-quality domain-specific training data, especially for tasks that require specialized knowledge. General-purpose models, while versatile, often lack the depth needed for expert-level tasks because of limited domain-specific information. Domain adaptation training can enhance these models, but it demands substantial, high-quality data. To address this, we propose ORBIT, a cost-efficient methodology for curating massive, high-quality domain-specific datasets from noisy web sources, tailored for training specialist large language models. Using astronomy as a primary case study, we refined the 1.3T-token FineWeb-Edu dataset into a high-quality, 10B-token subset focused on astronomy. Fine-tuning \\textsc{LLaMA-3-8B} on a 1B-token astronomy subset improved performance on the MMLU astronomy benchmark from 69\\% to 76\\% and achieved top results on AstroBench, an astronomy-specific benchmark. Moreover, our model (Orbit-LLaMA) outperformed \\textsc{LLaMA-3-8B-base}, with GPT-4o evaluations preferring it in 73\\% of cases across 1000 astronomy-specific questions. Additionally, we validated ORBIT's generalizability by applying it to law and medicine, achieving a significant improvement of data quality compared to an unfiltered baseline. We open-source the ORBIT methodology, including the curated datasets, the codebase, and the resulting model at \\href{https://github.com/ModeEric/ORBIT-Llama}{https://github.com/ModeEric/ORBIT-Llama}.",
        "tags": [
            "GPT",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "34",
        "title": "GenHMR: Generative Human Mesh Recovery",
        "author": [
            "Muhammad Usama Saleem",
            "Ekkasit Pinyoanuntapong",
            "Pu Wang",
            "Hongfei Xue",
            "Srijan Das",
            "Chen Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14444",
        "abstract": "Human mesh recovery (HMR) is crucial in many computer vision applications; from health to arts and entertainment. HMR from monocular images has predominantly been addressed by deterministic methods that output a single prediction for a given 2D image. However, HMR from a single image is an ill-posed problem due to depth ambiguity and occlusions. Probabilistic methods have attempted to address this by generating and fusing multiple plausible 3D reconstructions, but their performance has often lagged behind deterministic approaches. In this paper, we introduce GenHMR, a novel generative framework that reformulates monocular HMR as an image-conditioned generative task, explicitly modeling and mitigating uncertainties in the 2D-to-3D mapping process. GenHMR comprises two key components: (1) a pose tokenizer to convert 3D human poses into a sequence of discrete tokens in a latent space, and (2) an image-conditional masked transformer to learn the probabilistic distributions of the pose tokens, conditioned on the input image prompt along with randomly masked token sequence. During inference, the model samples from the learned conditional distribution to iteratively decode high-confidence pose tokens, thereby reducing 3D reconstruction uncertainties. To further refine the reconstruction, a 2D pose-guided refinement technique is proposed to directly fine-tune the decoded pose tokens in the latent space, which forces the projected 3D body mesh to align with the 2D pose clues. Experiments on benchmark datasets demonstrate that GenHMR significantly outperforms state-of-the-art methods. Project website can be found at https://m-usamasaleem.github.io/publication/GenHMR/GenHMR.html",
        "tags": [
            "3D",
            "Transformer"
        ]
    },
    {
        "id": "35",
        "title": "Multimodal Latent Diffusion Model for Complex Sewing Pattern Generation",
        "author": [
            "Shengqi Liu",
            "Yuhao Cheng",
            "Zhuo Chen",
            "Xingyu Ren",
            "Wenhan Zhu",
            "Lincheng Li",
            "Mengxiao Bi",
            "Xiaokang Yang",
            "Yichao Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14453",
        "abstract": "Generating sewing patterns in garment design is receiving increasing attention due to its CG-friendly and flexible-editing nature. Previous sewing pattern generation methods have been able to produce exquisite clothing, but struggle to design complex garments with detailed control. To address these issues, we propose SewingLDM, a multi-modal generative model that generates sewing patterns controlled by text prompts, body shapes, and garment sketches. Initially, we extend the original vector of sewing patterns into a more comprehensive representation to cover more intricate details and then compress them into a compact latent space. To learn the sewing pattern distribution in the latent space, we design a two-step training strategy to inject the multi-modal conditions, \\ie, body shapes, text prompts, and garment sketches, into a diffusion model, ensuring the generated garments are body-suited and detail-controlled. Comprehensive qualitative and quantitative experiments show the effectiveness of our proposed method, significantly surpassing previous approaches in terms of complex garment design and various body adaptability. Our project page: https://shengqiliu1.github.io/SewingLDM.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "36",
        "title": "LEDiff: Latent Exposure Diffusion for HDR Generation",
        "author": [
            "Chao Wang",
            "Zhihao Xia",
            "Thomas Leimkuehler",
            "Karol Myszkowski",
            "Xuaner Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14456",
        "abstract": "While consumer displays increasingly support more than 10 stops of dynamic range, most image assets such as internet photographs and generative AI content remain limited to 8-bit low dynamic range (LDR), constraining their utility across high dynamic range (HDR) applications. Currently, no generative model can produce high-bit, high-dynamic range content in a generalizable way. Existing LDR-to-HDR conversion methods often struggle to produce photorealistic details and physically-plausible dynamic range in the clipped areas. We introduce LEDiff, a method that enables a generative model with HDR content generation through latent space fusion inspired by image-space exposure fusion techniques. It also functions as an LDR-to-HDR converter, expanding the dynamic range of existing low-dynamic range images. Our approach uses a small HDR dataset to enable a pretrained diffusion model to recover detail and dynamic range in clipped highlights and shadows. LEDiff brings HDR capabilities to existing generative models and converts any LDR image to HDR, creating photorealistic HDR outputs for image generation, image-based lighting (HDR environment map generation), and photographic effects such as depth of field simulation, where linear HDR data is essential for realistic quality.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "37",
        "title": "From Human Annotation to LLMs: SILICON Annotation Workflow for Management Research",
        "author": [
            "Xiang Cheng",
            "Raveesh Mayya",
            "João Sedoc"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14461",
        "abstract": "Unstructured text data annotation and analysis are fundamental to management research, often relying on human annotators through crowdsourcing platforms. While Large Language Models (LLMs) promise to provide a cost-effective and efficient alternative to human annotation, there lacks a systematic workflow that evaluate when LLMs are suitable or how to proceed with LLM-based text annotation in a reproducible manner. This paper addresses this methodological gap by introducing the ``SILICON\" (\\textbf{S}ystematic \\textbf{I}nference with \\textbf{L}LMs for \\textbf{I}nformation \\textbf{C}lassificati\\textbf{o}n and \\textbf{N}otation) workflow. The workflow integrates established principles of human annotation with systematic prompt optimization and model selection, addressing challenges such as developing robust annotation guidelines, establishing high-quality human baselines, optimizing prompts, and ensuring reproducibility across LLMs. We validate the SILICON workflow through seven case studies covering common management research tasks, including business proposal evaluation, dialog intent and breakdown analysis, review attribute detection. Our findings highlight the importance of validating annotation guideline agreement, the superiority of expert-developed human baselines over crowdsourced ones, the iterative nature of prompt optimization, and the necessity of testing multiple LLMs. Notably, we propose a regression-based methodology to empirically compare LLM outputs across prompts and models. Our workflow advances management research by establishing reproducible processes for LLM-based annotation that maintain scientific rigor. We provide practical guidance for researchers to effectively navigate the evolving landscape of generative AI tools effectively while maintaining transparency and reproducibility.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "38",
        "title": "Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion",
        "author": [
            "Jixuan He",
            "Wanhua Li",
            "Ye Liu",
            "Junsik Kim",
            "Donglai Wei",
            "Hanspeter Pfister"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14462",
        "abstract": "As a common image editing operation, image composition involves integrating foreground objects into background scenes. In this paper, we expand the application of the concept of Affordance from human-centered image composition tasks to a more general object-scene composition framework, addressing the complex interplay between foreground objects and background scenes. Following the principle of Affordance, we define the affordance-aware object insertion task, which aims to seamlessly insert any object into any scene with various position prompts. To address the limited data issue and incorporate this task, we constructed the SAM-FB dataset, which contains over 3 million examples across more than 3,000 object categories. Furthermore, we propose the Mask-Aware Dual Diffusion (MADD) model, which utilizes a dual-stream architecture to simultaneously denoise the RGB image and the insertion mask. By explicitly modeling the insertion mask in the diffusion process, MADD effectively facilitates the notion of affordance. Extensive experimental results show that our method outperforms the state-of-the-art methods and exhibits strong generalization performance on in-the-wild images. Please refer to our code on https://github.com/KaKituken/affordance-aware-any.",
        "tags": [
            "Diffusion",
            "Image Editing",
            "SAM"
        ]
    },
    {
        "id": "39",
        "title": "LiftRefine: Progressively Refined View Synthesis from 3D Lifting with Volume-Triplane Representations",
        "author": [
            "Tung Do",
            "Thuan Hoang Nguyen",
            "Anh Tuan Tran",
            "Rang Nguyen",
            "Binh-Son Hua"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14464",
        "abstract": "We propose a new view synthesis method via synthesizing a 3D neural field from both single or few-view input images. To address the ill-posed nature of the image-to-3D generation problem, we devise a two-stage method that involves a reconstruction model and a diffusion model for view synthesis. Our reconstruction model first lifts one or more input images to the 3D space from a volume as the coarse-scale 3D representation followed by a tri-plane as the fine-scale 3D representation. To mitigate the ambiguity in occluded regions, our diffusion model then hallucinates missing details in the rendered images from tri-planes. We then introduce a new progressive refinement technique that iteratively applies the reconstruction and diffusion model to gradually synthesize novel views, boosting the overall quality of the 3D representations and their rendering. Empirical evaluation demonstrates the superiority of our method over state-of-the-art methods on the synthetic SRN-Car dataset, the in-the-wild CO3D dataset, and large-scale Objaverse dataset while achieving both sampling efficacy and multi-view consistency.",
        "tags": [
            "3D",
            "Diffusion",
            "Image-to-3D"
        ]
    },
    {
        "id": "40",
        "title": "DiffusionTrend: A Minimalist Approach to Virtual Fashion Try-On",
        "author": [
            "Wengyi Zhan",
            "Mingbao Lin",
            "Shuicheng Yan",
            "Rongrong Ji"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14465",
        "abstract": "We introduce DiffusionTrend for virtual fashion try-on, which forgoes the need for retraining diffusion models. Using advanced diffusion models, DiffusionTrend harnesses latent information rich in prior information to capture the nuances of garment details. Throughout the diffusion denoising process, these details are seamlessly integrated into the model image generation, expertly directed by a precise garment mask crafted by a lightweight and compact CNN. Although our DiffusionTrend model initially demonstrates suboptimal metric performance, our exploratory approach offers some important advantages: (1) It circumvents resource-intensive retraining of diffusion models on large datasets. (2) It eliminates the necessity for various complex and user-unfriendly model inputs. (3) It delivers a visually compelling try-on experience, underscoring the potential of training-free diffusion model. This initial foray into the application of untrained diffusion models in virtual try-on technology potentially paves the way for further exploration and refinement in this industrially and academically valuable field.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "41",
        "title": "Agent-SafetyBench: Evaluating the Safety of LLM Agents",
        "author": [
            "Zhexin Zhang",
            "Shiyao Cui",
            "Yida Lu",
            "Jingzhuo Zhou",
            "Junxiao Yang",
            "Hongning Wang",
            "Minlie Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14470",
        "abstract": "As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through quantitative analysis, we identify critical failure modes and summarize two fundamental safety detects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone is insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. We release Agent-SafetyBench at \\url{https://github.com/thu-coai/Agent-SafetyBench} to facilitate further research and innovation in agent safety evaluation and improvement.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "42",
        "title": "Why We Build Local Large Language Models: An Observational Analysis from 35 Japanese and Multilingual LLMs",
        "author": [
            "Koshiro Saito",
            "Sakae Mizuki",
            "Masanari Ohi",
            "Taishi Nakamura",
            "Taihei Shiotani",
            "Koki Maeda",
            "Youmi Ma",
            "Kakeru Hattori",
            "Kazuki Fujii",
            "Takumi Okamoto",
            "Shigeki Ishida",
            "Hiroya Takamura",
            "Rio Yokota",
            "Naoaki Okazaki"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14471",
        "abstract": "Why do we build local large language models (LLMs)? What should a local LLM learn from the target language? Which abilities can be transferred from other languages? Do language-specific scaling laws exist? To explore these research questions, we evaluated 35 Japanese, English, and multilingual LLMs on 19 evaluation benchmarks for Japanese and English, taking Japanese as a local language. Adopting an observational approach, we analyzed correlations of benchmark scores, and conducted principal component analysis (PCA) on the scores to derive \\textit{ability factors} of local LLMs. We found that training on English text can improve the scores of academic subjects in Japanese (JMMLU). In addition, it is unnecessary to specifically train on Japanese text to enhance abilities for solving Japanese code generation, arithmetic reasoning, commonsense, and reading comprehension tasks. In contrast, training on Japanese text could improve question-answering tasks about Japanese knowledge and English-Japanese translation, which indicates that abilities for solving these two tasks can be regarded as \\textit{Japanese abilities} for LLMs. Furthermore, we confirmed that the Japanese abilities scale with the computational budget for Japanese text.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "43",
        "title": "DirectorLLM for Human-Centric Video Generation",
        "author": [
            "Kunpeng Song",
            "Tingbo Hou",
            "Zecheng He",
            "Haoyu Ma",
            "Jialiang Wang",
            "Animesh Sinha",
            "Sam Tsai",
            "Yaqiao Luo",
            "Xiaoliang Dai",
            "Li Chen",
            "Xide Xia",
            "Peizhao Zhang",
            "Peter Vajda",
            "Ahmed Elgammal",
            "Felix Juefei-Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14484",
        "abstract": "In this paper, we introduce DirectorLLM, a novel video generation model that employs a large language model (LLM) to orchestrate human poses within videos. As foundational text-to-video models rapidly evolve, the demand for high-quality human motion and interaction grows. To address this need and enhance the authenticity of human motions, we extend the LLM from a text generator to a video director and human motion simulator. Utilizing open-source resources from Llama 3, we train the DirectorLLM to generate detailed instructional signals, such as human poses, to guide video generation. This approach offloads the simulation of human motion from the video generator to the LLM, effectively creating informative outlines for human-centric scenes. These signals are used as conditions by the video renderer, facilitating more realistic and prompt-following video generation. As an independent LLM module, it can be applied to different video renderers, including UNet and DiT, with minimal effort. Experiments on automatic evaluation benchmarks and human evaluations show that our model outperforms existing ones in generating videos with higher human motion fidelity, improved prompt faithfulness, and enhanced rendered subject naturalness.",
        "tags": [
            "DiT",
            "LLaMA",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "44",
        "title": "Token Preference Optimization with Self-Calibrated Visual-Anchored Rewards for Hallucination Mitigation",
        "author": [
            "Jihao Gu",
            "Yingyao Wang",
            "Meng Cao",
            "Pi Bu",
            "Jun Song",
            "Yancheng He",
            "Shilong Li",
            "Bo Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14487",
        "abstract": "Direct Preference Optimization (DPO) has been demonstrated to be highly effective in mitigating hallucinations in Large Vision Language Models (LVLMs) by aligning their outputs more closely with human preferences. Despite the recent progress, existing methods suffer from two drawbacks: 1) Lack of scalable token-level rewards; and 2) Neglect of visual-anchored tokens. To this end, we propose a novel Token Preference Optimization model with self-calibrated rewards (dubbed as TPO), which adaptively attends to visual-correlated tokens without fine-grained annotations. Specifically, we introduce a token-level \\emph{visual-anchored} \\emph{reward} as the difference of the logistic distributions of generated tokens conditioned on the raw image and the corrupted one. In addition, to highlight the informative visual-anchored tokens, a visual-aware training objective is proposed to enhance more accurate token-level optimization. Extensive experimental results have manifested the state-of-the-art performance of the proposed TPO. For example, by building on top of LLAVA-1.5-7B, our TPO boosts the performance absolute improvement for hallucination benchmarks.",
        "tags": [
            "LLaVA"
        ]
    },
    {
        "id": "45",
        "title": "Drive-1-to-3: Enriching Diffusion Priors for Novel View Synthesis of Real Vehicles",
        "author": [
            "Chuang Lin",
            "Bingbing Zhuang",
            "Shanlin Sun",
            "Ziyu Jiang",
            "Jianfei Cai",
            "Manmohan Chandraker"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14494",
        "abstract": "The recent advent of large-scale 3D data, e.g. Objaverse, has led to impressive progress in training pose-conditioned diffusion models for novel view synthesis. However, due to the synthetic nature of such 3D data, their performance drops significantly when applied to real-world images. This paper consolidates a set of good practices to finetune large pretrained models for a real-world task -- harvesting vehicle assets for autonomous driving applications. To this end, we delve into the discrepancies between the synthetic data and real driving data, then develop several strategies to account for them properly. Specifically, we start with a virtual camera rotation of real images to ensure geometric alignment with synthetic data and consistency with the pose manifold defined by pretrained models. We also identify important design choices in object-centric data curation to account for varying object distances in real driving scenes -- learn across varying object scales with fixed camera focal length. Further, we perform occlusion-aware training in latent spaces to account for ubiquitous occlusions in real data, and handle large viewpoint changes by leveraging a symmetric prior. Our insights lead to effective finetuning that results in a $68.8\\%$ reduction in FID for novel view synthesis over prior arts.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "46",
        "title": "Content-style disentangled representation for controllable artistic image stylization and generation",
        "author": [
            "Ma Zhuoqi",
            "Zhang Yixuan",
            "You Zejun",
            "Tian Long",
            "Liu Xiyang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14496",
        "abstract": "Controllable artistic image stylization and generation aims to render the content provided by text or image with the learned artistic style, where content and style decoupling is the key to achieve satisfactory results. However, current methods for content and style disentanglement primarily rely on image information for supervision, which leads to two problems: 1) models can only support one modality for style or content input;2) incomplete disentanglement resulting in semantic interference from the reference image. To address the above issues, this paper proposes a content-style representation disentangling method for controllable artistic image stylization and generation. We construct a WikiStyle+ dataset consists of artworks with corresponding textual descriptions for style and content. Based on the multimodal dataset, we propose a disentangled content and style representations guided diffusion model. The disentangled representations are first learned by Q-Formers and then injected into a pre-trained diffusion model using learnable multi-step cross-attention layers for better controllable stylization. This approach allows model to accommodate inputs from different modalities. Experimental results show that our method achieves a thorough disentanglement of content and style in reference images under multimodal supervision, thereby enabling a harmonious integration of content and style in the generated outputs, successfully producing style-consistent and expressive stylized images.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "47",
        "title": "Guided Diffusion Model for Sensor Data Obfuscation",
        "author": [
            "Xin Yang",
            "Omid Ardakanian"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14499",
        "abstract": "Sensor data collected by Internet of Things (IoT) devices carries detailed information about individuals in their vicinity. Sharing this data with a semi-trusted service provider may compromise the individuals' privacy, as sensitive information can be extracted by powerful machine learning models. Data obfuscation empowered by generative models is a promising approach to generate synthetic sensor data such that the useful information contained in the original data is preserved and the sensitive information is obscured. This newly generated data will then be shared with the service provider instead of the original sensor data. In this work, we propose PrivDiffuser, a novel data obfuscation technique based on a denoising diffusion model that attains a superior trade-off between data utility and privacy through effective guidance techniques. Specifically, we extract latent representations that contain information about public and private attributes from sensor data to guide the diffusion model, and impose mutual information-based regularization when learning the latent representations to alleviate the entanglement of public and private attributes, thereby increasing the effectiveness of guidance. Evaluation on three real-world datasets containing different sensing modalities reveals that PrivDiffuser yields a better privacy-utility trade-off than the state-of-the-art obfuscation model, decreasing the utility loss by up to $1.81\\%$ and the privacy loss by up to $3.42\\%$. Moreover, we showed that users with diverse privacy needs can use PrivDiffuser to protect their privacy without having to retrain the model.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "48",
        "title": "Do Large Language Models Defend Inferentialist Semantics?: On the Logical Expressivism and Anti-Representationalism of LLMs",
        "author": [
            "Yuzuki Arai",
            "Sho Tsugawa"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14501",
        "abstract": "The philosophy of language, which has historically been developed through an anthropocentric lens, is now being forced to move towards post-anthropocentrism due to the advent of large language models (LLMs) like ChatGPT (OpenAI), Claude (Anthropic), which are considered to possess linguistic abilities comparable to those of humans. Traditionally, LLMs have been explained through distributional semantics as their foundational semantics. However, recent research is exploring alternative foundational semantics beyond distributional semantics. This paper proposes Robert Brandom's inferentialist semantics as an suitable foundational semantics for LLMs, specifically focusing on the issue of linguistic representationalism within this post-anthropocentric trend. Here, we show that the anti-representationalism and logical expressivism of inferential semantics, as well as quasi-compositionality, are useful in interpreting the characteristics and behaviors of LLMs. Further, we propose a \\emph{consensus theory of truths} for LLMs. This paper argues that the characteristics of LLMs challenge mainstream assumptions in philosophy of language, such as semantic externalism and compositionality. We believe the argument in this paper leads to a re-evaluation of anti\\hyphen{}representationalist views of language, potentially leading to new developments in the philosophy of language.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "49",
        "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
        "author": [
            "Jiayi Wu",
            "Hengyi Cai",
            "Lingyong Yan",
            "Hao Sun",
            "Xiang Li",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Ming Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14510",
        "abstract": "The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "50",
        "title": "Relational Programming with Foundation Models",
        "author": [
            "Ziyang Li",
            "Jiani Huang",
            "Jason Liu",
            "Felix Zhu",
            "Eric Zhao",
            "William Dodds",
            "Neelay Velingker",
            "Rajeev Alur",
            "Mayur Naik"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14515",
        "abstract": "Foundation models have vast potential to enable diverse AI applications. The powerful yet incomplete nature of these models has spurred a wide range of mechanisms to augment them with capabilities such as in-context learning, information retrieval, and code interpreting. We propose Vieira, a declarative framework that unifies these mechanisms in a general solution for programming with foundation models. Vieira follows a probabilistic relational paradigm and treats foundation models as stateless functions with relational inputs and outputs. It supports neuro-symbolic applications by enabling the seamless combination of such models with logic programs, as well as complex, multi-modal applications by streamlining the composition of diverse sub-models. We implement Vieira by extending the Scallop compiler with a foreign interface that supports foundation models as plugins. We implement plugins for 12 foundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9 challenging tasks that span language, vision, and structured and vector databases. Our evaluation shows that programs in Vieira are concise, can incorporate modern foundation models, and have comparable or better accuracy than competitive baselines.",
        "tags": [
            "CLIP",
            "GPT",
            "SAM"
        ]
    },
    {
        "id": "51",
        "title": "Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment",
        "author": [
            "Teng Xiao",
            "Yige Yuan",
            "Huaisheng Zhu",
            "Mingxiao Li",
            "Vasant G Honavar"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14516",
        "abstract": "We study the problem of aligning large language models (LLMs) with human preference data. Contrastive preference optimization has shown promising results in aligning LLMs with available preference data by optimizing the implicit reward associated with the policy. However, the contrastive objective focuses mainly on the relative values of implicit rewards associated with two responses while ignoring their actual values, resulting in suboptimal alignment with human preferences. To address this limitation, we propose calibrated direct preference optimization (Cal-DPO), a simple yet effective algorithm. We show that substantial improvement in alignment with the given preferences can be achieved simply by calibrating the implicit reward to ensure that the learned implicit rewards are comparable in scale to the ground-truth rewards. We demonstrate the theoretical advantages of Cal-DPO over existing approaches. The results of our experiments on a variety of standard benchmarks show that Cal-DPO remarkably improves off-the-shelf methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "52",
        "title": "Efficient Self-Supervised Video Hashing with Selective State Spaces",
        "author": [
            "Jinpeng Wang",
            "Niu Lian",
            "Jun Li",
            "Yuting Wang",
            "Yan Feng",
            "Bin Chen",
            "Yongbing Zhang",
            "Shu-Tao Xia"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14518",
        "abstract": "Self-supervised video hashing (SSVH) is a practical task in video indexing and retrieval. Although Transformers are predominant in SSVH for their impressive temporal modeling capabilities, they often suffer from computational and memory inefficiencies. Drawing inspiration from Mamba, an advanced state-space model, we explore its potential in SSVH to achieve a better balance between efficacy and efficiency. We introduce S5VH, a Mamba-based video hashing model with an improved self-supervised learning paradigm. Specifically, we design bidirectional Mamba layers for both the encoder and decoder, which are effective and efficient in capturing temporal relationships thanks to the data-dependent selective scanning mechanism with linear complexity. In our learning strategy, we transform global semantics in the feature space into semantically consistent and discriminative hash centers, followed by a center alignment loss as a global learning signal. Our self-local-global (SLG) paradigm significantly improves learning efficiency, leading to faster and better convergence. Extensive experiments demonstrate S5VH's improvements over state-of-the-art methods, superior transferability, and scalable advantages in inference efficiency. Code is available at https://github.com/gimpong/AAAI25-S5VH.",
        "tags": [
            "Mamba",
            "Selective State Spaces"
        ]
    },
    {
        "id": "53",
        "title": "Dynamic User Interface Generation for Enhanced Human-Computer Interaction Using Variational Autoencoders",
        "author": [
            "Runsheng Zhang",
            "Shixiao Wang",
            "Tianfang Xie",
            "Shiyu Duan",
            "Mengmeng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14521",
        "abstract": "This study presents a novel approach for intelligent user interaction interface generation and optimization, grounded in the variational autoencoder (VAE) model. With the rapid advancement of intelligent technologies, traditional interface design methods struggle to meet the evolving demands for diversity and personalization, often lacking flexibility in real-time adjustments to enhance the user experience. Human-Computer Interaction (HCI) plays a critical role in addressing these challenges by focusing on creating interfaces that are functional, intuitive, and responsive to user needs. This research leverages the RICO dataset to train the VAE model, enabling the simulation and creation of user interfaces that align with user aesthetics and interaction habits. By integrating real-time user behavior data, the system dynamically refines and optimizes the interface, improving usability and underscoring the importance of HCI in achieving a seamless user experience. Experimental findings indicate that the VAE-based approach significantly enhances the quality and precision of interface generation compared to other methods, including autoencoders (AE), generative adversarial networks (GAN), conditional GANs (cGAN), deep belief networks (DBN), and VAE-GAN. This work contributes valuable insights into HCI, providing robust technical solutions for automated interface generation and enhanced user experience optimization.",
        "tags": [
            "GAN",
            "VAE"
        ]
    },
    {
        "id": "54",
        "title": "CAE-T: A Channelwise AutoEncoder with Transformer for EEG Abnormality Detection",
        "author": [
            "Youshen Zhao",
            "Keiji Iramina"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14522",
        "abstract": "Electroencephalogram (EEG) signals are critical for detecting abnormal brain activity, but their high dimensionality and complexity pose significant challenges for effective analysis. In this paper, we propose CAE-T, a novel framework that combines a channelwise CNN-based autoencoder with a single-head transformer classifier for efficient EEG abnormality detection. The channelwise autoencoder compresses raw EEG signals while preserving channel independence, reducing computational costs and retaining biologically meaningful features. The compressed representations are then fed into the transformer-based classifier, which efficiently models long-term dependencies to distinguish between normal and abnormal signals. Evaluated on the TUH Abnormal EEG Corpus, the proposed model achieves 85.0% accuracy, 76.2% sensitivity, and 91.2% specificity at the per-case level, outperforming baseline models such as EEGNet, Deep4Conv, and FusionCNN. Furthermore, CAE-T requires only 202M FLOPs and 2.9M parameters, making it significantly more efficient than transformer-based alternatives. The framework retains interpretability through its channelwise design, demonstrating great potential for future applications in neuroscience research and clinical practice. The source code is available at https://github.com/YossiZhao/CAE-T.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "55",
        "title": "Knowledge Distillation in RNN-Attention Models for Early Prediction of Student Performance",
        "author": [
            "Sukrit Leelaluk",
            "Cheng Tang",
            "Valdemar Švábenský",
            "Atsushi Shimada"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14526",
        "abstract": "Educational data mining (EDM) is a part of applied computing that focuses on automatically analyzing data from learning contexts. Early prediction for identifying at-risk students is a crucial and widely researched topic in EDM research. It enables instructors to support at-risk students to stay on track, preventing student dropout or failure. Previous studies have predicted students' learning performance to identify at-risk students by using machine learning on data collected from e-learning platforms. However, most studies aimed to identify at-risk students utilizing the entire course data after the course finished. This does not correspond to the real-world scenario that at-risk students may drop out before the course ends. To address this problem, we introduce an RNN-Attention-KD (knowledge distillation) framework to predict at-risk students early throughout a course. It leverages the strengths of Recurrent Neural Networks (RNNs) in handling time-sequence data to predict students' performance at each time step and employs an attention mechanism to focus on relevant time steps for improved predictive accuracy. At the same time, KD is applied to compress the time steps to facilitate early prediction. In an empirical evaluation, RNN-Attention-KD outperforms traditional neural network models in terms of recall and F1-measure. For example, it obtained recall and F1-measure of 0.49 and 0.51 for Weeks 1--3 and 0.51 and 0.61 for Weeks 1--6 across all datasets from four years of a university course. Then, an ablation study investigated the contributions of different knowledge transfer methods (distillation objectives). We found that hint loss from the hidden layer of RNN and context vector loss from the attention module on RNN could enhance the model's prediction performance for identifying at-risk students. These results are relevant for EDM researchers employing deep learning models.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "56",
        "title": "Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models",
        "author": [
            "Xiao Cui",
            "Mo Zhu",
            "Yulei Qin",
            "Liang Xie",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14528",
        "abstract": "Knowledge distillation (KD) has become a prevalent technique for compressing large language models (LLMs). Existing KD methods are constrained by the need for identical tokenizers (i.e., vocabularies) between teacher and student models, limiting their versatility in handling LLMs of different architecture families. In this paper, we introduce the Multi-Level Optimal Transport (MultiLevelOT), a novel approach that advances the optimal transport for universal cross-tokenizer knowledge distillation. Our method aligns the logit distributions of the teacher and the student at both token and sequence levels using diverse cost matrices, eliminating the need for dimensional or token-by-token correspondence. At the token level, MultiLevelOT integrates both global and local information by jointly optimizing all tokens within a sequence to enhance robustness. At the sequence level, we efficiently capture complex distribution structures of logits via the Sinkhorn distance, which approximates the Wasserstein distance for divergence measures. Extensive experiments on tasks such as extractive QA, generative QA, and summarization demonstrate that the MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under various settings. Our approach is robust to different student and teacher models across model families, architectures, and parameter sizes.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "57",
        "title": "Consistent Human Image and Video Generation with Spatially Conditioned Diffusion",
        "author": [
            "Mingdeng Cao",
            "Chong Mou",
            "Ziyang Yuan",
            "Xintao Wang",
            "Zhaoyang Zhang",
            "Ying Shan",
            "Yinqiang Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14531",
        "abstract": "Consistent human-centric image and video synthesis aims to generate images or videos with new poses while preserving appearance consistency with a given reference image, which is crucial for low-cost visual content creation. Recent advances based on diffusion models typically rely on separate networks for reference appearance feature extraction and target visual generation, leading to inconsistent domain gaps between references and targets. In this paper, we frame the task as a spatially-conditioned inpainting problem, where the target image is inpainted to maintain appearance consistency with the reference. This approach enables the reference features to guide the generation of pose-compliant targets within a unified denoising network, thereby mitigating domain gaps. Additionally, to better maintain the reference appearance information, we impose a causal feature interaction framework, in which reference features can only query from themselves, while target features can query appearance information from both the reference and the target. To further enhance computational efficiency and flexibility, in practical implementation, we decompose the spatially-conditioned generation process into two stages: reference appearance extraction and conditioned target generation. Both stages share a single denoising network, with interactions restricted to self-attention layers. This proposed method ensures flexible control over the appearance of generated human images and videos. By fine-tuning existing base diffusion models on human video data, our method demonstrates strong generalization to unseen human identities and poses without requiring additional per-instance fine-tuning. Experimental results validate the effectiveness of our approach, showing competitive performance compared to existing methods for consistent human image and video synthesis.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "58",
        "title": "Downscaling Precipitation with Bias-informed Conditional Diffusion Model",
        "author": [
            "Ran Lyu",
            "Linhan Wang",
            "Yanshen Sun",
            "Hedanqiu Bai",
            "Chang-Tien Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14539",
        "abstract": "Climate change is intensifying rainfall extremes, making high-resolution precipitation projections crucial for society to better prepare for impacts such as flooding. However, current Global Climate Models (GCMs) operate at spatial resolutions too coarse for localized analyses. To address this limitation, deep learning-based statistical downscaling methods offer promising solutions, providing high-resolution precipitation projections with a moderate computational cost. In this work, we introduce a bias-informed conditional diffusion model for statistical downscaling of precipitation. Specifically, our model leverages a conditional diffusion approach to learn distribution priors from large-scale, high-resolution precipitation datasets. The long-tail distribution of precipitation poses a unique challenge for training diffusion models; to address this, we apply gamma correction during preprocessing. Additionally, to correct biases in the downscaled results, we employ a guided-sampling strategy to enhance bias correction. Our experiments demonstrate that the proposed model achieves highly accurate results in an 8 times downscaling setting, outperforming previous deterministic methods. The code and dataset are available at https://github.com/RoseLV/research_super-resolution",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "59",
        "title": "Transformer models are gauge invariant: A mathematical connection between AI and particle physics",
        "author": [
            "Leo van Nierop"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14543",
        "abstract": "In particle physics, the fundamental forces are subject to symmetries called gauge invariance. It is a redundancy in the mathematical description of any physical system. In this article I will demonstrate that the transformer architecture exhibits the same properties, and show that the default representation of transformers has partially, but not fully removed the gauge invariance.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "60",
        "title": "Bright-NeRF:Brightening Neural Radiance Field with Color Restoration from Low-light Raw Images",
        "author": [
            "Min Wang",
            "Xin Huang",
            "Guoqing Zhou",
            "Qifeng Guo",
            "Qing Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14547",
        "abstract": "Neural Radiance Fields (NeRFs) have demonstrated prominent performance in novel view synthesis. However, their input heavily relies on image acquisition under normal light conditions, making it challenging to learn accurate scene representation in low-light environments where images typically exhibit significant noise and severe color distortion. To address these challenges, we propose a novel approach, Bright-NeRF, which learns enhanced and high-quality radiance fields from multi-view low-light raw images in an unsupervised manner. Our method simultaneously achieves color restoration, denoising, and enhanced novel view synthesis. Specifically, we leverage a physically-inspired model of the sensor's response to illumination and introduce a chromatic adaptation loss to constrain the learning of response, enabling consistent color perception of objects regardless of lighting conditions. We further utilize the raw data's properties to expose the scene's intensity automatically. Additionally, we have collected a multi-view low-light raw image dataset to advance research in this field. Experimental results demonstrate that our proposed method significantly outperforms existing 2D and 3D approaches. Our code and dataset will be made publicly available.",
        "tags": [
            "3D",
            "NeRF"
        ]
    },
    {
        "id": "61",
        "title": "The Current Challenges of Software Engineering in the Era of Large Language Models",
        "author": [
            "Cuiyun Gao",
            "Xing Hu",
            "Shan Gao",
            "Xin Xia",
            "Zhi Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14554",
        "abstract": "With the advent of large language models (LLMs) in the artificial intelligence (AI) area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape's challenges and opportunities.\nThe paper aims at revisiting the software development life cycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The paper first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as software engineering and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement & design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "62",
        "title": "CitaLaw: Enhancing LLM with Citations in Legal Domain",
        "author": [
            "Kepu Zhang",
            "Weijie Yu",
            "Sunhao Dai",
            "Jun Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14556",
        "abstract": "In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "63",
        "title": "ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model",
        "author": [
            "Shunlin Lu",
            "Jingbo Wang",
            "Zeyu Lu",
            "Ling-Hao Chen",
            "Wenxun Dai",
            "Junting Dong",
            "Zhiyang Dou",
            "Bo Dai",
            "Ruimao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14559",
        "abstract": "The scaling law has been validated in various domains, such as natural language processing (NLP) and massive computer vision tasks; however, its application to motion generation remains largely unexplored. In this paper, we introduce a scalable motion generation framework that includes the motion tokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through comprehensive experiments, we observe the scaling behavior of this system. For the first time, we confirm the existence of scaling laws within the context of motion generation. Specifically, our results demonstrate that the normalized test loss of our prefix autoregressive models adheres to a logarithmic law in relation to compute budgets. Furthermore, we also confirm the power law between Non-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect to compute budgets respectively. Leveraging the scaling law, we predict the optimal transformer size, vocabulary size, and data requirements for a compute budget of $1e18$. The test loss of the system, when trained with the optimal model size, vocabulary size, and required data, aligns precisely with the predicted test loss, thereby validating the scaling law.",
        "tags": [
            "Transformer",
            "VAE"
        ]
    },
    {
        "id": "64",
        "title": "Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF Separation",
        "author": [
            "Yongsung Kim",
            "Minjun Park",
            "Jooyoung Choi",
            "Sungroh Yoon"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14568",
        "abstract": "Recent learning-based Multi-View Stereo models have demonstrated state-of-the-art performance in sparse-view 3D reconstruction. However, directly applying 3D Gaussian Splatting (3DGS) as a refinement step following these models presents challenges. We hypothesize that the excessive positional degrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting color patterns at the cost of structural fidelity. To address this, we propose reprojection-based DoF separation, a method distinguishing positional DoFs in terms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To independently manage each DoF, we introduce a reprojection process along with tailored constraints for each DoF. Through experiments across various datasets, we confirm that separating the positional DoFs of Gaussians and applying targeted constraints effectively suppresses geometric artifacts, producing reconstruction results that are both visually and geometrically plausible.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "65",
        "title": "Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models",
        "author": [
            "Wenhan Liu",
            "Xinyu Ma",
            "Yutao Zhu",
            "Ziliang Zhao",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Zhicheng Dou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14574",
        "abstract": "Large Language Models (LLMs) have shown exciting performance in listwise passage ranking. Due to the limited input length, existing methods often adopt the sliding window strategy. Such a strategy, though effective, is inefficient as it involves repetitive and serialized processing, which usually re-evaluates relevant passages multiple times. As a result, it incurs redundant API costs, which are proportional to the number of inference tokens. The development of long-context LLMs enables the full ranking of all passages within a single inference, avoiding redundant API costs. In this paper, we conduct a comprehensive study of long-context LLMs for ranking tasks in terms of efficiency and effectiveness. Surprisingly, our experiments reveal that full ranking with long-context LLMs can deliver superior performance in the supervised fine-tuning setting with a huge efficiency improvement. Furthermore, we identify two limitations of fine-tuning the full ranking model based on existing methods: (1) sliding window strategy fails to produce a full ranking list as a training label, and (2) the language modeling loss cannot emphasize top-ranked passage IDs in the label. To alleviate these issues, we propose a new complete listwise label construction approach and a novel importance-aware learning objective for full ranking. Experiments show the superior performance of our method over baselines. Our codes are available at \\url{https://github.com/8421BCD/fullrank}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "66",
        "title": "GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting",
        "author": [
            "Qianpu Sun",
            "Changyong Shu",
            "Sifan Zhou",
            "Zichen Yu",
            "Yan Chen",
            "Dawei Yang",
            "Yuan Chun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14579",
        "abstract": "3D occupancy perception is gaining increasing attention due to its capability to offer detailed and precise environment representations. Previous weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU varying by 5-10 points due to sampling count along camera rays. Recently, real-time Gaussian splatting has gained widespread popularity in 3D reconstruction, and the occupancy prediction task can also be viewed as a reconstruction task. Consequently, we propose GSRender, which naturally employs 3D Gaussian Splatting for occupancy prediction, simplifying the sampling process. In addition, the limitations of 2D supervision result in duplicate predictions along the same camera ray. We implemented the Ray Compensation (RC) module, which mitigates this issue by compensating for features from adjacent frames. Finally, we redesigned the loss to eliminate the impact of dynamic objects from adjacent frames. Extensive experiments demonstrate that our approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while narrowing the gap with 3D supervision methods. Our code will be released soon.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "67",
        "title": "DiffSim: Taming Diffusion Models for Evaluating Visual Similarity",
        "author": [
            "Yiren Song",
            "Xiaokang Liu",
            "Mike Zheng Shou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14580",
        "abstract": "Diffusion models have fundamentally transformed the field of generative models, making the assessment of similarity between customized model outputs and reference inputs critically important. However, traditional perceptual similarity metrics operate primarily at the pixel and patch levels, comparing low-level colors and textures but failing to capture mid-level similarities and differences in image layout, object pose, and semantic content. Contrastive learning-based CLIP and self-supervised learning-based DINO are often used to measure semantic similarity, but they highly compress image features, inadequately assessing appearance details. This paper is the first to discover that pretrained diffusion models can be utilized for measuring visual similarity and introduces the DiffSim method, addressing the limitations of traditional metrics in capturing perceptual consistency in custom generation tasks. By aligning features in the attention layers of the denoising U-Net, DiffSim evaluates both appearance and style similarity, showing superior alignment with human visual preferences. Additionally, we introduce the Sref and IP benchmarks to evaluate visual similarity at the level of style and instance, respectively. Comprehensive evaluations across multiple benchmarks demonstrate that DiffSim achieves state-of-the-art performance, providing a robust tool for measuring visual coherence in generative models.",
        "tags": [
            "CLIP",
            "Diffusion"
        ]
    },
    {
        "id": "68",
        "title": "CORD: Balancing COnsistency and Rank Distillation for Robust Retrieval-Augmented Generation",
        "author": [
            "Youngwon Lee",
            "Seung-won Hwang",
            "Daniel Campos",
            "Filip Graliński",
            "Zhewei Yao",
            "Yuxiong He"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14581",
        "abstract": "With the adoption of retrieval-augmented generation (RAG), large language models (LLMs) are expected to ground their generation to the retrieved contexts. Yet, this is hindered by position bias of LLMs, failing to evenly attend to all contexts. Previous work has addressed this by synthesizing contexts with perturbed positions of gold segment, creating a position-diversified train set. We extend this intuition to propose consistency regularization with augmentation and distillation. First, we augment each training instance with its position perturbation to encourage consistent predictions, regardless of ordering. We also distill behaviors of this pair, although it can be counterproductive in certain RAG scenarios where the given order from the retriever is crucial for generation quality. We thus propose CORD, balancing COnsistency and Rank Distillation. CORD adaptively samples noise-controlled perturbations from an interpolation space, ensuring both consistency and respect for the rank prior. Empirical results show this balance enables CORD to outperform consistently in diverse RAG benchmarks.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "69",
        "title": "Simulation-Free Hierarchical Latent Policy Planning for Proactive Dialogues",
        "author": [
            "Tao He",
            "Lizi Liao",
            "Yixin Cao",
            "Yuanxing Liu",
            "Yiheng Sun",
            "Zerui Chen",
            "Ming Liu",
            "Bing Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14584",
        "abstract": "Recent advancements in proactive dialogues have garnered significant attention, particularly for more complex objectives (e.g. emotion support and persuasion). Unlike traditional task-oriented dialogues, proactive dialogues demand advanced policy planning and adaptability, requiring rich scenarios and comprehensive policy repositories to develop such systems. However, existing approaches tend to rely on Large Language Models (LLMs) for user simulation and online learning, leading to biases that diverge from realistic scenarios and result in suboptimal efficiency. Moreover, these methods depend on manually defined, context-independent, coarse-grained policies, which not only incur high expert costs but also raise concerns regarding their completeness. In our work, we highlight the potential for automatically discovering policies directly from raw, real-world dialogue records. To this end, we introduce a novel dialogue policy planning framework, LDPP. It fully automates the process from mining policies in dialogue records to learning policy planning. Specifically, we employ a variant of the Variational Autoencoder to discover fine-grained policies represented as latent vectors. After automatically annotating the data with these latent policy labels, we propose an Offline Hierarchical Reinforcement Learning (RL) algorithm in the latent space to develop effective policy planning capabilities. Our experiments demonstrate that LDPP outperforms existing methods on two proactive scenarios, even surpassing ChatGPT with only a 1.8-billion-parameter LLM.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models",
            "RL"
        ]
    },
    {
        "id": "70",
        "title": "HiCM$^2$: Hierarchical Compact Memory Modeling for Dense Video Captioning",
        "author": [
            "Minkuk Kim",
            "Hyeon Bae Kim",
            "Jinyoung Moon",
            "Jinwoo Choi",
            "Seong Tae Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14585",
        "abstract": "With the growing demand for solutions to real-world video challenges, interest in dense video captioning (DVC) has been on the rise. DVC involves the automatic captioning and localization of untrimmed videos. Several studies highlight the challenges of DVC and introduce improved methods utilizing prior knowledge, such as pre-training and external memory. In this research, we propose a model that leverages the prior knowledge of human-oriented hierarchical compact memory inspired by human memory hierarchy and cognition. To mimic human-like memory recall, we construct a hierarchical memory and a hierarchical memory reading module. We build an efficient hierarchical compact memory by employing clustering of memory events and summarization using large language models. Comparative experiments demonstrate that this hierarchical memory recall process improves the performance of DVC by achieving state-of-the-art performance on YouCook2 and ViTT datasets.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "71",
        "title": "Spike2Former: Efficient Spiking Transformer for High-performance Image Segmentation",
        "author": [
            "Zhenxin Lei",
            "Man Yao",
            "Jiakui Hu",
            "Xinhao Luo",
            "Yanye Lu",
            "Bo Xu",
            "Guoqi Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14587",
        "abstract": "Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly in image segmentation tasks. The reason is that directly converting neural networks with complex architectural designs for segmentation tasks into spiking versions leads to performance degradation and non-convergence. To address this challenge, we first identify the modules in the architecture design that lead to the severe reduction in spike firing, make targeted improvements, and propose Spike2Former architecture. Second, we propose normalized integer spiking neurons to solve the training stability problem of SNNs with complex architectures. We set a new state-of-the-art for SNNs in various semantic segmentation datasets, with a significant improvement of +12.7% mIoU and 5.0 efficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU and 6.6 efficiency on CityScapes.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "72",
        "title": "Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning",
        "author": [
            "Kepu Zhang",
            "Haoyue Yang",
            "Xu Tang",
            "Weijie Yu",
            "Jun Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14588",
        "abstract": "In legal practice, judges apply the trichotomous dogmatics of criminal law, sequentially assessing the elements of the offense, unlawfulness, and culpability to determine whether an individual's conduct constitutes a crime. Although current legal large language models (LLMs) show promising accuracy in judgment prediction, they lack trichotomous reasoning capabilities due to the absence of an appropriate benchmark dataset, preventing them from predicting innocent outcomes. As a result, every input is automatically assigned a charge, limiting their practical utility in legal contexts. To bridge this gap, we introduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with Innocent Verdicts. Adhering to the trichotomous dogmatics, we extend three widely-used legal datasets through LLM-based augmentation and manual verification. Our experiments with state-of-the-art legal LLMs and novel strategies that integrate trichotomous reasoning into zero-shot prompting and fine-tuning reveal: (1) current legal LLMs have significant room for improvement, with even the best models achieving an F1 score of less than 0.3 on LJPIV; and (2) our strategies notably enhance both in-domain and cross-domain judgment prediction accuracy, especially for cases resulting in an innocent verdict.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "73",
        "title": "MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design",
        "author": [
            "Zhen Zheng",
            "Xiaonan Song",
            "Chuanjie Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14590",
        "abstract": "Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. In this paper, we make a comprehensive analysis of the general quantization principles on their effect to the triangle of accuracy, memory consumption and system efficiency. We propose MixLLM that explores the new optimization space of mixed-precision quantization between output features based on the insight that different output features matter differently in the model. MixLLM identifies the output features with high salience in the global view rather than within each single layer, effectively assigning the larger bit-width to output features that need it most to achieve good accuracy with low memory consumption. We present the sweet spot of quantization configuration of algorithm-system co-design that leads to high accuracy and system efficiency. To address the system challenge, we design the two-step dequantization to make use of the int8 Tensor Core easily and fast data type conversion to reduce dequantization overhead significantly, and present the software pipeline to overlap the memory access, dequantization and the MatMul to the best. Extensive experiments show that with only 10% more bits, the PPL increasement can be reduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on average MMLU-Pro improves by 0.93 over the SOTA of three popular models. In addition to its superior accuracy, MixLLM also achieves state-of-the-art system efficiency.",
        "tags": [
            "LLMs",
            "LLaMA"
        ]
    },
    {
        "id": "74",
        "title": "LDP: Generalizing to Multilingual Visual Information Extraction by Language Decoupled Pretraining",
        "author": [
            "Huawen Shen",
            "Gengluo Li",
            "Jinwen Zhong",
            "Yu Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14596",
        "abstract": "Visual Information Extraction (VIE) plays a crucial role in the comprehension of semi-structured documents, and several pre-trained models have been developed to enhance performance. However, most of these works are monolingual (usually English). Due to the extremely unbalanced quantity and quality of pre-training corpora between English and other languages, few works can extend to non-English scenarios. In this paper, we conduct systematic experiments to show that vision and layout modality hold invariance among images with different languages. If decoupling language bias from document images, a vision-layout-based model can achieve impressive cross-lingual generalization. Accordingly, we present a simple but effective multilingual training paradigm LDP (Language Decoupled Pre-training) for better utilization of monolingual pre-training data. Our proposed model LDM (Language Decoupled Model) is first pre-trained on the language-independent data, where the language knowledge is decoupled by a diffusion model, and then the LDM is fine-tuned on the downstream languages. Extensive experiments show that the LDM outperformed all SOTA multilingual pre-trained models, and also maintains competitiveness on downstream monolingual/English benchmarks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "75",
        "title": "Can We Get Rid of Handcrafted Feature Extractors? SparseViT: Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization Through Spare-Coding Transformer",
        "author": [
            "Lei Su",
            "Xiaochen Ma",
            "Xuekang Zhu",
            "Chaoqun Niu",
            "Zeyu Lei",
            "Ji-Zhe Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14598",
        "abstract": "Non-semantic features or semantic-agnostic features, which are irrelevant to image context but sensitive to image manipulations, are recognized as evidential to Image Manipulation Localization (IML). Since manual labels are impossible, existing works rely on handcrafted methods to extract non-semantic features. Handcrafted non-semantic features jeopardize IML model's generalization ability in unseen or complex scenarios. Therefore, for IML, the elephant in the room is: How to adaptively extract non-semantic features? Non-semantic features are context-irrelevant and manipulation-sensitive. That is, within an image, they are consistent across patches unless manipulation occurs. Then, spare and discrete interactions among image patches are sufficient for extracting non-semantic features. However, image semantics vary drastically on different patches, requiring dense and continuous interactions among image patches for learning semantic representations. Hence, in this paper, we propose a Sparse Vision Transformer (SparseViT), which reformulates the dense, global self-attention in ViT into a sparse, discrete manner. Such sparse self-attention breaks image semantics and forces SparseViT to adaptively extract non-semantic features for images. Besides, compared with existing IML models, the sparse self-attention mechanism largely reduced the model size (max 80% in FLOPs), achieving stunning parameter efficiency and computation reduction. Extensive experiments demonstrate that, without any handcrafted feature extractors, SparseViT is superior in both generalization and efficiency across benchmark datasets.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "76",
        "title": "Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry",
        "author": [
            "Andrea Gurioli",
            "Maurizio Gabbrielli",
            "Stefano Zacchiroli"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14611",
        "abstract": "With the increasing popularity of LLM-based code completers, like GitHub Copilot, the interest in automatically detecting AI-generated code is also increasing-in particular in contexts where the use of LLMs to program is forbidden by policy due to security, intellectual property, or ethical http://concerns.We introduce a novel technique for AI code stylometry, i.e., the ability to distinguish code generated by LLMs from code written by humans, based on a transformer-based encoder classifier. Differently from previous work, our classifier is capable of detecting AI-written code across 10 different programming languages with a single machine learning model, maintaining high average accuracy across all languages (84.1% $\\pm$ 3.8%).Together with the classifier we also release H-AIRosettaMP, a novel open dataset for AI code stylometry tasks, consisting of 121 247 code snippets in 10 popular programming languages, labeled as either human-written or AI-generated. The experimental pipeline (dataset, training code, resulting models) is the first fully reproducible one for the AI code stylometry task. Most notably our experiments rely only on open LLMs, rather than on proprietary/closed ones like ChatGPT.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Transformer"
        ]
    },
    {
        "id": "77",
        "title": "How good is GPT at writing political speeches for the White House?",
        "author": [
            "Jacques Savoy"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14617",
        "abstract": "Using large language models (LLMs), computers are able to generate a written text in response to a us er request. As this pervasive technology can be applied in numerous contexts, this study analyses the written style of one LLM called GPT by comparing its generated speeches with those of the recent US presidents. To achieve this objective, the State of the Union (SOTU) addresses written by Reagan to Biden are contrasted to those produced by both GPT-3.5 and GPT-4.o versions. Compared to US presidents, GPT tends to overuse the lemma \"we\" and produce shorter messages with, on average, longer sentences. Moreover, GPT opts for an optimistic tone, opting more often for political (e.g., president, Congress), symbolic (e.g., freedom), and abstract terms (e.g., freedom). Even when imposing an author's style to GPT, the resulting speech remains distinct from addresses written by the target author. Finally, the two GPT versions present distinct characteristics, but both appear overall dissimilar to true presidential messages.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "78",
        "title": "Learning to Generate Research Idea with Dynamic Control",
        "author": [
            "Ruochen Li",
            "Liqiang Jing",
            "Chi Han",
            "Jiawei Zhou",
            "Xinya Du"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14626",
        "abstract": "The rapid advancements in large language models (LLMs) have demonstrated their potential to accelerate scientific discovery, particularly in automating the process of research ideation. LLM-based systems have shown promise in generating hypotheses and research ideas. However, current approaches predominantly rely on prompting-based pre-trained models, limiting their ability to optimize generated content effectively. Moreover, they also lack the capability to deal with the complex interdependence and inherent restrictions among novelty, feasibility, and effectiveness, which remains challenging due to the inherent trade-offs among these dimensions, such as the innovation-feasibility conflict. To address these limitations, we for the first time propose fine-tuning LLMs to be better idea proposers and introduce a novel framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model learns foundational patterns from pairs of research papers and follow-up ideas. In the RL stage, multi-dimensional reward modeling, guided by fine-grained feedback, evaluates and optimizes the generated ideas across key metrics. Dimensional controllers enable dynamic adjustment of generation, while a sentence-level decoder ensures context-aware emphasis during inference. Our framework provides a balanced approach to research ideation, achieving high-quality outcomes by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RL"
        ]
    },
    {
        "id": "79",
        "title": "Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models",
        "author": [
            "Keith G. Mills",
            "Mohammad Salameh",
            "Ruichen Chen",
            "Negar Hassanpour",
            "Wei Lu",
            "Di Niu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14628",
        "abstract": "Diffusion Models (DM) have democratized AI image generation through an iterative denoising process. Quantization is a major technique to alleviate the inference cost and reduce the size of DM denoiser networks. However, as denoisers evolve from variants of convolutional U-Nets toward newer Transformer architectures, it is of growing importance to understand the quantization sensitivity of different weight layers, operations and architecture types to performance. In this work, we address this challenge with Qua$^2$SeDiMo, a mixed-precision Post-Training Quantization framework that generates explainable insights on the cost-effectiveness of various model weight quantization methods for different denoiser operation types and block structures. We leverage these insights to make high-quality mixed-precision quantization decisions for a myriad of diffusion models ranging from foundational U-Nets to state-of-the-art Transformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit, 3.65-bit and 3.7-bit weight quantization on PixArt-${\\alpha}$, PixArt-${\\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our weight-quantization configurations with 6-bit activation quantization and outperform existing approaches in terms of quantitative metrics and generative image quality.",
        "tags": [
            "DiT",
            "Diffusion",
            "SDXL",
            "Transformer"
        ]
    },
    {
        "id": "80",
        "title": "Unified Image Restoration and Enhancement: Degradation Calibrated Cycle Reconstruction Diffusion Model",
        "author": [
            "Minglong Xue",
            "Jinhong He",
            "Shivakumara Palaiahnakote",
            "Mingliang Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14630",
        "abstract": "Image restoration and enhancement are pivotal for numerous computer vision applications, yet unifying these tasks efficiently remains a significant challenge. Inspired by the iterative refinement capabilities of diffusion models, we propose CycleRDM, a novel framework designed to unify restoration and enhancement tasks while achieving high-quality mapping. Specifically, CycleRDM first learns the mapping relationships among the degraded domain, the rough normal domain, and the normal domain through a two-stage diffusion inference process. Subsequently, we transfer the final calibration process to the wavelet low-frequency domain using discrete wavelet transform, performing fine-grained calibration from a frequency domain perspective by leveraging task-specific frequency spaces. To improve restoration quality, we design a feature gain module for the decomposed wavelet high-frequency domain to eliminate redundant features. Additionally, we employ multimodal textual prompts and Fourier transform to drive stable denoising and reduce randomness during the inference process. After extensive validation, CycleRDM can be effectively generalized to a wide range of image restoration and enhancement tasks while requiring only a small number of training samples to be significantly superior on various benchmarks of reconstruction quality and perceptual quality. The source code will be available at https://github.com/hejh8/CycleRDM.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "81",
        "title": "Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit Post-Training Quantization in Vision Transformers",
        "author": [
            "Rui Ding",
            "Liang Yong",
            "Sihuan Zhao",
            "Jing Nie",
            "Lihui Chen",
            "Haijun Liu",
            "Xichuan Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14633",
        "abstract": "Due to its efficiency, Post-Training Quantization (PTQ) has been widely adopted for compressing Vision Transformers (ViTs). However, when quantized into low-bit representations, there is often a significant performance drop compared to their full-precision counterparts. To address this issue, reconstruction methods have been incorporated into the PTQ framework to improve performance in low-bit quantization settings. Nevertheless, existing related methods predefine the reconstruction granularity and seldom explore the progressive relationships between different reconstruction granularities, which leads to sub-optimal quantization results in ViTs. To this end, in this paper, we propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method for accurate PTQ, which significantly improves the performance of low-bit quantized vision transformers. Specifically, we define multi-head self-attention and multi-layer perceptron modules along with their shortcuts as the finest reconstruction units. After reconstructing these two fine-grained units, we combine them to form coarser blocks and reconstruct them at a coarser granularity level. We iteratively perform this combination and reconstruction process, achieving progressive fine-to-coarse reconstruction. Additionally, we introduce a Progressive Optimization Strategy (POS) for PFCR to alleviate the difficulty of training, thereby further enhancing model performance. Experimental results on the ImageNet dataset demonstrate that our proposed method achieves the best Top-1 accuracy among state-of-the-art methods, particularly attaining 75.61% for 3-bit quantized ViT-B in PTQ. Besides, quantization results on the COCO dataset reveal the effectiveness and generalization of our proposed method on other computer vision tasks like object detection and instance segmentation.",
        "tags": [
            "Detection",
            "Segmentation",
            "ViT"
        ]
    },
    {
        "id": "82",
        "title": "Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention for Fine-Grained Few-Shot Learning",
        "author": [
            "Eric Brouwer",
            "Jan Erik van Woerden",
            "Gertjan Burghouts",
            "Matias Valedenegro-Toro",
            "Marco Zullich"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14640",
        "abstract": "Few-shot, fine-grained classification in computer vision poses significant challenges due to the need to differentiate subtle class distinctions with limited data. This paper presents a novel method that enhances the Contrastive Language-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided by real-time visual inputs. Unlike existing techniques such as Context Optimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by static prompts or visual token reliance, the proposed approach leverages a cross-attention mechanism to dynamically refine text prompts for the image at hand. This enables an image-specific alignment of textual features with image patches extracted from the Vision Transformer, making the model more effective for datasets with high intra-class variance and low inter-class differences. The method is evaluated on several datasets, including CUBirds, Oxford Flowers, and FGVC Aircraft, showing significant performance gains over static prompt tuning approaches. To ensure these performance gains translate into trustworthy predictions, we integrate Monte-Carlo Dropout in our approach to improve the reliability of the model predictions and uncertainty estimates. This integration provides valuable insights into the model's predictive confidence, helping to identify when predictions can be trusted and when additional verification is necessary. This dynamic approach offers a robust solution, advancing the state-of-the-art for few-shot fine-grained classification.",
        "tags": [
            "CLIP",
            "Transformer"
        ]
    },
    {
        "id": "83",
        "title": "RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios",
        "author": [
            "Jie Huang",
            "Ruibing Hou",
            "Jiahe Zhao",
            "Hong Chang",
            "Shiguang Shan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14643",
        "abstract": "Human-centric perceptions play a crucial role in real-world applications. While recent human-centric works have achieved impressive progress, these efforts are often constrained to the visual domain and lack interaction with human instructions, limiting their applicability in broader scenarios such as chatbots and sports analysis. This paper introduces Referring Human Perceptions, where a referring prompt specifies the person of interest in an image. To tackle the new task, we propose RefHCM (Referring Human-Centric Model), a unified framework to integrate a wide range of human-centric referring tasks. Specifically, RefHCM employs sequence mergers to convert raw multimodal data -- including images, text, coordinates, and parsing maps -- into semantic tokens. This standardized representation enables RefHCM to reformulate diverse human-centric referring tasks into a sequence-to-sequence paradigm, solved using a plain encoder-decoder transformer architecture. Benefiting from a unified learning strategy, RefHCM effectively facilitates knowledge transfer across tasks and exhibits unforeseen capabilities in handling complex reasoning. This work represents the first attempt to address referring human perceptions with a general-purpose framework, while simultaneously establishing a corresponding benchmark that sets new standards for the field. Extensive experiments showcase RefHCM's competitive and even superior performance across multiple human-centric referring tasks. The code and data are publicly at https://github.com/JJJYmmm/RefHCM.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "84",
        "title": "Length Controlled Generation for Black-box LLMs",
        "author": [
            "Yuxuan Gu",
            "Wenjie Wang",
            "Xiaocheng Feng",
            "Weihong Zhong",
            "Kun Zhu",
            "Lei Huang",
            "Tat-Seng Chua",
            "Bing Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14656",
        "abstract": "Large language models (LLMs) have demonstrated impressive instruction following capabilities, while still struggling to accurately manage the length of the generated text, which is a fundamental requirement in many real-world applications. Existing length control methods involve fine-tuning the parameters of LLMs, which is inefficient and suboptimal for practical use. In this paper, we propose a novel iterative sampling framework for text length control, integrating the Metropolis-Hastings algorithm with an importance sampling acceleration strategy. This framework efficiently and reliably regulates LLMs to generate length-constrained text without modifying the underlying parameters, thereby preserving the original capabilities of LLMs. Experimental results demonstrate that our framework achieves almost 100\\% success rates of length control on Llama3.1 for tasks such as length-controlled abstractive summarization and length-constrained instruction following, with minimal additional computational overhead. This also highlights the significant potential of our method for precise length control across a broader range of applications, without compromising the versatility of LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "85",
        "title": "Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models",
        "author": [
            "Zijun Chen",
            "Wenbo Hu",
            "Guande He",
            "Zhijie Deng",
            "Zheng Zhang",
            "Richang Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14660",
        "abstract": "Multimodal large language models (MLLMs) combine visual and textual data for tasks such as image captioning and visual question answering. Proper uncertainty calibration is crucial, yet challenging, for reliable use in areas like healthcare and autonomous driving. This paper investigates representative MLLMs, focusing on their calibration across various scenarios, including before and after visual fine-tuning, as well as before and after multimodal training of the base LLMs. We observed miscalibration in their performance, and at the same time, no significant differences in calibration across these scenarios. We also highlight how uncertainty differs between text and images and how their integration affects overall uncertainty. To better understand MLLMs' miscalibration and their ability to self-assess uncertainty, we construct the IDK (I don't know) dataset, which is key to evaluating how they handle unknowns. Our findings reveal that MLLMs tend to give answers rather than admit uncertainty, but this self-assessment improves with proper prompt adjustments. Finally, to calibrate MLLMs and enhance model reliability, we propose techniques such as temperature scaling and iterative prompt optimization. Our results provide insights into improving MLLMs for effective and responsible deployment in multimodal applications. Code and IDK dataset: \\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "86",
        "title": "Analysis and Visualization of Linguistic Structures in Large Language Models: Neural Representations of Verb-Particle Constructions in BERT",
        "author": [
            "Hassane Kissane",
            "Achim Schilling",
            "Patrick Krauss"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14670",
        "abstract": "This study investigates the internal representations of verb-particle combinations within transformer-based large language models (LLMs), specifically examining how these models capture lexical and syntactic nuances at different neural network layers. Employing the BERT architecture, we analyse the representational efficacy of its layers for various verb-particle constructions such as 'agree on', 'come back', and 'give up'. Our methodology includes a detailed dataset preparation from the British National Corpus, followed by extensive model training and output analysis through techniques like multi-dimensional scaling (MDS) and generalized discrimination value (GDV) calculations. Results show that BERT's middle layers most effectively capture syntactic structures, with significant variability in representational accuracy across different verb categories. These findings challenge the conventional uniformity assumed in neural network processing of linguistic elements and suggest a complex interplay between network architecture and linguistic representation. Our research contributes to a better understanding of how deep learning models comprehend and process language, offering insights into the potential and limitations of current neural approaches to linguistic analysis. This study not only advances our knowledge in computational linguistics but also prompts further research into optimizing neural architectures for enhanced linguistic precision.",
        "tags": [
            "BERT",
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "87",
        "title": "LLMs as mediators: Can they diagnose conflicts accurately?",
        "author": [
            "Özgecan Koçak",
            "Phanish Puranam",
            "Afşar Yegin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14675",
        "abstract": "Prior research indicates that to be able to mediate conflict, observers of disagreements between parties must be able to reliably distinguish the sources of their disagreement as stemming from differences in beliefs about what is true (causality) vs. differences in what they value (morality). In this paper, we test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this task and whether one or other type of disagreement proves particularly challenging for LLM's to diagnose. We replicate study 1 in Koçak et al. (2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We find that both LLMs have similar semantic understanding of the distinction between causal and moral codes as humans and can reliably distinguish between them. When asked to diagnose the source of disagreement in a conversation, both LLMs, compared to humans, exhibit a tendency to overestimate the extent of causal disagreement and underestimate the extent of moral disagreement in the moral misalignment condition. This tendency is especially pronounced for GPT 4 when using a proximate scale that relies on concrete language specific to an issue. GPT 3.5 does not perform as well as GPT4 or humans when using either the proximate or the distal scale. The study provides a first test of the potential for using LLMs to mediate conflict by diagnosing the root of disagreements in causal and evaluative codes.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "88",
        "title": "Logic Induced High-Order Reasoning Network for Event-Event Relation Extraction",
        "author": [
            "Peixin Huang",
            "Xiang Zhao",
            "Minghao Hu",
            "Zhen Tan",
            "Weidong Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14688",
        "abstract": "To understand a document with multiple events, event-event relation extraction (ERE) emerges as a crucial task, aiming to discern how natural events temporally or structurally associate with each other. To achieve this goal, our work addresses the problems of temporal event relation extraction (TRE) and subevent relation extraction (SRE). The latest methods for such problems have commonly built document-level event graphs for global reasoning across sentences. However, the edges between events are usually derived from external tools heuristically, which are not always reliable and may introduce noise. Moreover, they are not capable of preserving logical constraints among event relations, e.g., coreference constraint, symmetry constraint and conjunction constraint. These constraints guarantee coherence between different relation types,enabling the generation of a uniffed event evolution graph. In this work, we propose a novel method named LogicERE, which performs high-order event relation reasoning through modeling logic constraints. Speciffcally, different from conventional event graphs, we design a logic constraint induced graph (LCG) without any external tools. LCG involves event nodes where the interactions among them can model the coreference constraint, and event pairs nodes where the interactions among them can retain the symmetry constraint and conjunction constraint. Then we perform high-order reasoning on LCG with relational graph transformer to obtain enhanced event and event pair embeddings. Finally, we further incorporate logic constraint information via a joint logic learning module. Extensive experiments demonstrate the effectiveness of the proposed method with state-of-the-art performance on benchmark datasets.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "89",
        "title": "How to Synthesize Text Data without Model Collapse?",
        "author": [
            "Xuekai Zhu",
            "Daixuan Cheng",
            "Hengli Li",
            "Kaiyan Zhang",
            "Ermo Hua",
            "Xingtai Lv",
            "Ning Ding",
            "Zhouhan Lin",
            "Zilong Zheng",
            "Bowen Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14689",
        "abstract": "Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "90",
        "title": "Event-assisted 12-stop HDR Imaging of Dynamic Scene",
        "author": [
            "Shi Guo",
            "Zixuan Chen",
            "Ziran Zhang",
            "Yutian Chen",
            "Gangwei Xu",
            "Tianfan Xue"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14705",
        "abstract": "High dynamic range (HDR) imaging is a crucial task in computational photography, which captures details across diverse lighting conditions. Traditional HDR fusion methods face limitations in dynamic scenes with extreme exposure differences, as aligning low dynamic range (LDR) frames becomes challenging due to motion and brightness variation. In this work, we propose a novel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera system with an event camera and an RGB camera. The event camera provides temporally dense, high dynamic range signals that improve alignment between LDR frames with large exposure differences, reducing ghosting artifacts caused by motion. Also, a real-world finetuning strategy is proposed to increase the generalization of alignment module on real-world events. Additionally, we introduce a diffusion-based fusion module that incorporates image priors from pre-trained diffusion models to address artifacts in high-contrast regions and minimize errors from the alignment process. To support this work, we developed the ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized event signals, and validated our approach on both simulated and real-world data. Extensive experiments demonstrate that our method achieves state-of-the-art performance, successfully extending HDR imaging to 12 stops in dynamic scenes.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "91",
        "title": "EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space",
        "author": [
            "Jianrong Zhang",
            "Hehe Fan",
            "Yi Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14706",
        "abstract": "Diffusion models, particularly latent diffusion models, have demonstrated remarkable success in text-driven human motion generation. However, it remains challenging for latent diffusion models to effectively compose multiple semantic concepts into a single, coherent motion sequence. To address this issue, we propose EnergyMoGen, which includes two spectrums of Energy-Based Models: (1) We interpret the diffusion model as a latent-aware energy-based model that generates motions by composing a set of diffusion models in latent space; (2) We introduce a semantic-aware energy model based on cross-attention, which enables semantic composition and adaptive gradient descent for text embeddings. To overcome the challenges of semantic inconsistency and motion distortion across these two spectrums, we introduce Synergistic Energy Fusion. This design allows the motion latent diffusion model to synthesize high-quality, complex motions by combining multiple energy terms corresponding to textual descriptions. Experiments show that our approach outperforms existing state-of-the-art models on various motion generation tasks, including text-to-motion generation, compositional motion generation, and multi-concept motion generation. Additionally, we demonstrate that our method can be used to extend motion datasets and improve the text-to-motion task.",
        "tags": [
            "Diffusion",
            "Energy-Based Models"
        ]
    },
    {
        "id": "92",
        "title": "Creation of AI-driven Smart Spaces for Enhanced Indoor Environments -- A Survey",
        "author": [
            "Aygün Varol",
            "Naser Hossein Motlagh",
            "Mirka Leino",
            "Sasu Tarkoma",
            "Johanna Virkki"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14708",
        "abstract": "Smart spaces are ubiquitous computing environments that integrate diverse sensing and communication technologies to enhance space functionality, optimize energy utilization, and improve user comfort and well-being. The integration of emerging AI methodologies into these environments facilitates the formation of AI-driven smart spaces, which further enhance functionalities of the spaces by enabling advanced applications such as personalized comfort settings, interactive living spaces, and automatization of the space systems, all resulting in enhanced indoor experiences of the users. In this paper, we present a systematic survey of existing research on the foundational components of AI-driven smart spaces, including sensor technologies, data communication protocols, sensor network management and maintenance strategies, as well as the data collection, processing and analytics. Given the pivotal role of AI in establishing AI-powered smart spaces, we explore the opportunities and challenges associated with traditional machine learning (ML) approaches, such as deep learning (DL), and emerging methodologies including large language models (LLMs). Finally, we provide key insights necessary for the development of AI-driven smart spaces, propose future research directions, and sheds light on the path forward.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "93",
        "title": "On Verbalized Confidence Scores for LLMs",
        "author": [
            "Daniel Yang",
            "Yao-Hung Hubert Tsai",
            "Makoto Yamada"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14737",
        "abstract": "The rise of large language models (LLMs) and their tight integration into our daily life make it essential to dedicate efforts towards their trustworthiness. Uncertainty quantification for LLMs can establish more human trust into their responses, but also allows LLM agents to make more informed decisions based on each other's uncertainty. To estimate the uncertainty in a response, internal token logits, task-specific proxy models, or sampling of multiple responses are commonly used. This work focuses on asking the LLM itself to verbalize its uncertainty with a confidence score as part of its output tokens, which is a promising way for prompt- and model-agnostic uncertainty quantification with low overhead. Using an extensive benchmark, we assess the reliability of verbalized confidence scores with respect to different datasets, models, and prompt methods. Our results reveal that the reliability of these scores strongly depends on how the model is asked, but also that it is possible to extract well-calibrated confidence scores with certain prompt methods. We argue that verbalized confidence scores can become a simple but effective and versatile uncertainty quantification method in the future. Our code is available at https://github.com/danielyxyang/llm-verbalized-uq .",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "94",
        "title": "On the Use of Deep Learning Models for Semantic Clone Detection",
        "author": [
            "Subroto Nag Pinku",
            "Debajyoti Mondal",
            "Chanchal K. Roy"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14739",
        "abstract": "Detecting and tracking code clones can ease various software development and maintenance tasks when changes in a code fragment should be propagated over all its copies. Several deep learning-based clone detection models have appeared in the literature for detecting syntactic and semantic clones, widely evaluated with the BigCloneBench dataset. However, class imbalance and the small number of semantic clones make BigCloneBench less ideal for interpreting model performance. Researchers also use other datasets such as GoogleCodeJam, OJClone, and SemanticCloneBench to understand model generalizability. To overcome the limitations of existing datasets, the GPT-assisted semantic and cross-language clone dataset GPTCloneBench has been released. However, how these models compare across datasets remains unclear. In this paper, we propose a multi-step evaluation approach for five state-of-the-art clone detection models leveraging existing benchmark datasets, including GPTCloneBench, and using mutation operators to study model ability. Specifically, we examine three highly-performing single-language models (ASTNN, GMN, CodeBERT) on BigCloneBench, SemanticCloneBench, and GPTCloneBench, testing their robustness with mutation operations. Additionally, we compare them against cross-language models (C4, CLCDSA) known for detecting semantic clones. While single-language models show high F1 scores for BigCloneBench, their performance on SemanticCloneBench varies (up to 20%). Interestingly, the cross-language model (C4) shows superior performance (around 7%) on SemanticCloneBench over other models and performs similarly on BigCloneBench and GPTCloneBench. On mutation-based datasets, C4 has more robust performance (less than 1% difference) compared to single-language models, which show high variability.",
        "tags": [
            "Detection",
            "GPT"
        ]
    },
    {
        "id": "95",
        "title": "A Meshfree RBF-FD Constant along Normal Method for Solving PDEs on Surfaces",
        "author": [
            "Víctor Bayona",
            "Argyrios Petras",
            "Cécile Piret",
            "Steven J. Ruuth"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14761",
        "abstract": "This paper introduces a novel meshfree methodology based on Radial Basis Function-Finite Difference (RBF-FD) approximations for the numerical solution of partial differential equations (PDEs) on surfaces of codimension 1 embedded in $\\mathbb{R}^3$. The method is built upon the principles of the closest point method, without the use of a grid or a closest point mapping. We show that the combination of local embedded stencils with these principles can be employed to approximate surface derivatives using polyharmonic spline kernels and polynomials (PHS+Poly) RBF-FD. Specifically, we show that it is enough to consider a constant extension along the normal direction only at a single node to overcome the rank deficiency of the polynomial basis. An extensive parameter analysis is presented to test the dependence of the approach. We demonstrate high-order convergence rates on problems involving surface advection and surface diffusion, and solve Turing pattern formations on surfaces defined either implicitly or by point clouds. Moreover, a simple coupling approach with a particle tracking method demonstrates the potential of the proposed method in solving PDEs on evolving surfaces in the normal direction. Our numerical results confirm the stability, flexibility, and high-order algebraic convergence of the approach.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "96",
        "title": "CodeRepoQA: A Large-scale Benchmark for Software Engineering Question Answering",
        "author": [
            "Ruida Hu",
            "Chao Peng",
            "Jingyi Ren",
            "Bo Jiang",
            "Xiangxin Meng",
            "Qinyun Wu",
            "Pengfei Gao",
            "Xinchen Wang",
            "Cuiyun Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14764",
        "abstract": "In this work, we introduce CodeRepoQA, a large-scale benchmark specifically designed for evaluating repository-level question-answering capabilities in the field of software engineering. CodeRepoQA encompasses five programming languages and covers a wide range of scenarios, enabling comprehensive evaluation of language models. To construct this dataset, we crawl data from 30 well-known repositories in GitHub, the largest platform for hosting and collaborating on code, and carefully filter raw data. In total, CodeRepoQA is a multi-turn question-answering benchmark with 585,687 entries, covering a diverse array of software engineering scenarios, with an average of 6.62 dialogue turns per entry.\nWe evaluate ten popular large language models on our dataset and provide in-depth analysis. We find that LLMs still have limitations in question-answering capabilities in the field of software engineering, and medium-length contexts are more conducive to LLMs' performance. The entire benchmark is publicly available at https://github.com/kinesiatricssxilm14/CodeRepoQA.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "97",
        "title": "FLAMe: Federated Learning with Attention Mechanism using Spatio-Temporal Keypoint Transformers for Pedestrian Fall Detection in Smart Cities",
        "author": [
            "Byeonghun Kim",
            "Byeongjoon Noh"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14768",
        "abstract": "In smart cities, detecting pedestrian falls is a major challenge to ensure the safety and quality of life of citizens. In this study, we propose a novel fall detection system using FLAMe (Federated Learning with Attention Mechanism), a federated learning (FL) based algorithm. FLAMe trains around important keypoint information and only transmits the trained important weights to the server, reducing communication costs and preserving data privacy. Furthermore, the lightweight keypoint transformer model is integrated into the FL framework to effectively learn spatio-temporal features. We validated the experiment using 22,672 video samples from the \"Fall Accident Risk Behavior Video-Sensor Pair data\" dataset from AI-Hub. As a result of the experiment, the FLAMe-based system achieved an accuracy of 94.02% with about 190,000 transmission parameters, maintaining performance similar to that of existing centralized learning while maximizing efficiency by reducing communication costs by about 40% compared to the existing FL algorithm, FedAvg. Therefore, the FLAMe algorithm has demonstrated that it provides robust performance in the distributed environment of smart cities and is a practical and effective solution for public safety.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "98",
        "title": "ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in Palestine",
        "author": [
            "Rabee Qasem",
            "Mohannad Hendi",
            "Banan Tantour"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14771",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in diverse domains, yet their application in the legal sector, particularly in low-resource contexts, remains limited. This study addresses the challenges of adapting LLMs to the Palestinian legal domain, where political instability, fragmented legal frameworks, and limited AI resources hinder effective machine-learning applications. We present a fine-tuned model based on a quantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set derived from Palestinian legal texts. Using smaller-scale models and strategically generated question-answer pairs, we achieve a cost-effective, locally sustainable solution that provides accurate and contextually relevant legal guidance. Our experiments demonstrate promising performance on various query types, ranging from yes/no questions and narrative explanations to complex legal differentiations, while highlighting areas for improvement, such as handling calculation-based inquiries and structured list formatting. This work provides a pathway for the deployment of AI-driven legal assistance tools tailored to the needs of resource-constrained environments.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "99",
        "title": "Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model Fine-tuning",
        "author": [
            "Ziang Ye",
            "Zhenru Zhang",
            "Yang Zhang",
            "Jianxin Ma",
            "Junyang Lin",
            "Fuli Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14780",
        "abstract": "When using agent-task datasets to enhance agent capabilities for Large Language Models (LLMs), current methodologies often treat all tokens within a sample equally. However, we argue that tokens serving different roles - specifically, reasoning tokens versus boilerplate tokens (e.g., those governing output format) - differ significantly in importance and learning complexity, necessitating their disentanglement and distinct treatment. To address this, we propose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token discrimination. SHAD classifies tokens by exploiting predictability differences observed after shuffling input-output combinations across samples: boilerplate tokens, due to their repetitive nature among samples, maintain predictability, whereas reasoning tokens do not. Using SHAD, we propose the Reasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes reasoning tokens during fine-tuning, yielding notable performance gains over common Supervised Fine-Tuning (SFT).",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "100",
        "title": "Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations",
        "author": [
            "Yucheng Hu",
            "Yanjiang Guo",
            "Pengchao Wang",
            "Xiaoyu Chen",
            "Yen-Jen Wang",
            "Jianke Zhang",
            "Koushil Sreenath",
            "Chaochao Lu",
            "Jianyu Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14803",
        "abstract": "Recent advancements in robotics have focused on developing generalist policies capable of performing multiple tasks. Typically, these policies utilize pre-trained vision encoders to capture crucial information from current observations. However, previous vision encoders, which trained on two-image contrastive learning or single-image reconstruction, can not perfectly capture the sequential information essential for embodied tasks. Recently, video diffusion models (VDMs) have demonstrated the capability to accurately predict future image sequences, exhibiting a good understanding of physical dynamics. Motivated by the strong visual prediction capabilities of VDMs, we hypothesize that they inherently possess visual representations that reflect the evolution of the physical world, which we term predictive visual representations. Building on this hypothesis, we propose the Video Prediction Policy (VPP), a generalist robotic policy conditioned on the predictive visual representations from VDMs. To further enhance these representations, we incorporate diverse human or robotic manipulation datasets, employing unified video-generation training objectives. VPP consistently outperforms existing methods across two simulated and two real-world benchmarks. Notably, it achieves a 28.1\\% relative improvement in the Calvin ABC-D benchmark compared to the previous state-of-the-art and delivers a 28.8\\% increase in success rates for complex real-world dexterous manipulation tasks.",
        "tags": [
            "Diffusion",
            "Robot",
            "Robotics",
            "Video Generation"
        ]
    },
    {
        "id": "101",
        "title": "ResoFilter: Rine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis",
        "author": [
            "Zeao Tu",
            "Xiangdi Meng",
            "Yu He",
            "Zihan Yao",
            "Tianyu Qi",
            "Jun Liu",
            "Ming Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14809",
        "abstract": "Large language models (LLMs) have shown remarkable effectiveness across various domains, with data augmentation methods utilizing GPT for synthetic data generation becoming prevalent. However, the quality and utility of augmented data remain questionable, and current methods lack clear metrics for evaluating data characteristics. To address these challenges, we propose ResoFilter, a novel method that integrates models, data, and tasks to refine datasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter features for data selection, offering improved interpretability by representing data characteristics through model weights. Our experiments demonstrate that ResoFilter achieves comparable results to full-scale fine-tuning using only half the data in mathematical tasks and exhibits strong generalization across different models and domains. This method provides valuable insights for constructing synthetic datasets and evaluating high-quality data, offering a promising solution for enhancing data augmentation techniques and improving training dataset quality for LLMs. For reproducibility, we will release our code and data upon acceptance.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "102",
        "title": "MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data",
        "author": [
            "Camillo Maria Caruso",
            "Paolo Soda",
            "Valerio Guarrasi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14810",
        "abstract": "In healthcare, the integration of multimodal data is pivotal for developing comprehensive diagnostic and predictive models. However, managing missing data remains a significant challenge in real-world applications. We introduce MARIA (Multimodal Attention Resilient to Incomplete datA), a novel transformer-based deep learning model designed to address these challenges through an intermediate fusion strategy. Unlike conventional approaches that depend on imputation, MARIA utilizes a masked self-attention mechanism, which processes only the available data without generating synthetic values. This approach enables it to effectively handle incomplete datasets, enhancing robustness and minimizing biases introduced by imputation methods. We evaluated MARIA against 10 state-of-the-art machine learning and deep learning models across 8 diagnostic and prognostic tasks. The results demonstrate that MARIA outperforms existing methods in terms of performance and resilience to varying levels of data incompleteness, underscoring its potential for critical healthcare applications.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "103",
        "title": "Answer Set Networks: Casting Answer Set Programming into Deep Learning",
        "author": [
            "Arseny Skryagin",
            "Daniel Ochs",
            "Phillip Deibert",
            "Simon Kohaut",
            "Devendra Singh Dhami",
            "Kristian Kersting"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14814",
        "abstract": "Although Answer Set Programming (ASP) allows constraining neural-symbolic (NeSy) systems, its employment is hindered by the prohibitive costs of computing stable models and the CPU-bound nature of state-of-the-art solvers. To this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on Graph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep Probabilistic Logic Programming (DPPL). Specifically, we show how to translate ASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded problem by leveraging GPU's batching and parallelization capabilities. Our experimental evaluations demonstrate that ASNs outperform state-of-the-art CPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following two contributions based on the strengths of ASNs. Namely, we are the first to show the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs to guide the training with logic. Further, we show the \"constitutional navigation\" of drones, i.e., encoding public aviation laws in an ASN for routing Unmanned Aerial Vehicles in uncertain environments.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "104",
        "title": "PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR Semantic Segmentation",
        "author": [
            "Shoumeng Qiu",
            "Xinrun Li",
            "XiangYang Xue",
            "Jian Pu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14821",
        "abstract": "Although multiview fusion has demonstrated potential in LiDAR segmentation, its dependence on computationally intensive point-based interactions, arising from the lack of fixed correspondences between views such as range view and Bird's-Eye View (BEV), hinders its practical deployment. This paper challenges the prevailing notion that multiview fusion is essential for achieving high performance. We demonstrate that significant gains can be realized by directly fusing Polar and Cartesian partitioning strategies within the BEV space. Our proposed BEV-only segmentation model leverages the inherent fixed grid correspondences between these partitioning schemes, enabling a fusion process that is orders of magnitude faster (170$\\times$ speedup) than conventional point-based methods. Furthermore, our approach facilitates dense feature fusion, preserving richer contextual information compared to sparse point-based alternatives. To enhance scene understanding while maintaining inference efficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive evaluation on the SemanticKITTI and nuScenes datasets provides compelling evidence that our method outperforms previous multiview fusion approaches in terms of both performance and inference speed, highlighting the potential of BEV-based fusion for LiDAR segmentation. Code is available at \\url{https://github.com/skyshoumeng/PC-BEV.}",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "105",
        "title": "Mention Attention for Pronoun Translation",
        "author": [
            "Gongbo Tang",
            "Christian Hardmeier"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14829",
        "abstract": "Most pronouns are referring expressions, computers need to resolve what do the pronouns refer to, and there are divergences on pronoun usage across languages. Thus, dealing with these divergences and translating pronouns is a challenge in machine translation. Mentions are referring candidates of pronouns and have closer relations with pronouns compared to general tokens. We assume that extracting additional mention features can help pronoun translation. Therefore, we introduce an additional mention attention module in the decoder to pay extra attention to source mentions but not non-mention tokens. Our mention attention module not only extracts features from source mentions, but also considers target-side context which benefits pronoun translation. In addition, we also introduce two mention classifiers to train models to recognize mentions, whose outputs guide the mention attention. We conduct experiments on the WMT17 English-German translation task, and evaluate our models on general translation and pronoun translation, using BLEU, APT, and contrastive evaluation metrics. Our proposed model outperforms the baseline Transformer model in terms of APT and BLEU scores, this confirms our hypothesis that we can improve pronoun translation by paying additional attention to source mentions, and shows that our introduced additional modules do not have negative effect on the general translation quality.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "106",
        "title": "Progressive Multimodal Reasoning via Active Retrieval",
        "author": [
            "Guanting Dong",
            "Chenghao Zhang",
            "Mengjie Deng",
            "Yutao Zhu",
            "Zhicheng Dou",
            "Ji-Rong Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14835",
        "abstract": "Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "107",
        "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs",
        "author": [
            "Xiabin Zhou",
            "Wenbin Wang",
            "Minyan Zeng",
            "Jiaxian Guo",
            "Xuebo Liu",
            "Li Shen",
            "Min Zhang",
            "Liang Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14838",
        "abstract": "Efficient KV cache management in LLMs is crucial for long-context tasks like RAG and summarization. Existing KV cache compression methods enforce a fixed pattern, neglecting task-specific characteristics and reducing the retention of essential information. However, we observe distinct activation patterns across layers in various tasks, highlighting the need for adaptive strategies tailored to each task's unique demands. Based on this insight, we propose DynamicKV, a method that dynamically optimizes token retention by adjusting the number of tokens retained at each layer to adapt to the specific task. DynamicKV establishes global and per-layer maximum KV cache budgets, temporarily retaining the maximum budget for the current layer, and periodically updating the KV cache sizes of all preceding layers during inference. Our method retains only 1.7% of the KV cache size while achieving ~85% of the Full KV cache performance on LongBench. Notably, even under extreme compression (0.9%), DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be released.",
        "tags": [
            "LLMs",
            "RAG"
        ]
    },
    {
        "id": "108",
        "title": "Helping LLMs Improve Code Generation Using Feedback from Testing and Static Analysis",
        "author": [
            "Greta Dolcetti",
            "Vincenzo Arceri",
            "Eleonora Iotti",
            "Sergio Maffeis",
            "Agostino Cortesi",
            "Enea Zaffanella"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14841",
        "abstract": "Large Language Models (LLMs) are one of the most promising developments in the field of artificial intelligence, and the software engineering community has readily noticed their potential role in the software development life-cycle. Developers routinely ask LLMs to generate code snippets, increasing productivity but also potentially introducing ownership, privacy, correctness, and security issues. Previous work highlighted how code generated by mainstream commercial LLMs is often not safe, containing vulnerabilities, bugs, and code smells. In this paper, we present a framework that leverages testing and static analysis to assess the quality, and guide the self-improvement, of code generated by general-purpose, open-source LLMs.\nFirst, we ask LLMs to generate C code to solve a number of programming tasks. Then we employ ground-truth tests to assess the (in)correctness of the generated code, and a static analysis tool to detect potential safety vulnerabilities. Next, we assess the models ability to evaluate the generated code, by asking them to detect errors and vulnerabilities. Finally, we test the models ability to fix the generated code, providing the reports produced during the static analysis and incorrectness evaluation phases as feedback.\nOur results show that models often produce incorrect code, and that the generated code can include safety issues. Moreover, they perform very poorly at detecting either issue. On the positive side, we observe a substantial ability to fix flawed code when provided with information about failed tests or potential vulnerabilities, indicating a promising avenue for improving the safety of LLM-based code generation tools.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "109",
        "title": "Mapping and Influencing the Political Ideology of Large Language Models using Synthetic Personas",
        "author": [
            "Pietro Bernardelle",
            "Leon Fröhling",
            "Stefano Civelli",
            "Riccardo Lunardi",
            "Kevin Roiter",
            "Gianluca Demartini"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14843",
        "abstract": "The analysis of political biases in large language models (LLMs) has primarily examined these systems as single entities with fixed viewpoints. While various methods exist for measuring such biases, the impact of persona-based prompting on LLMs' political orientation remains unexplored. In this work we leverage PersonaHub, a collection of synthetic persona descriptions, to map the political distribution of persona-based prompted LLMs using the Political Compass Test (PCT). We then examine whether these initial compass distributions can be manipulated through explicit ideological prompting towards diametrically opposed political orientations: right-authoritarian and left-libertarian. Our experiments reveal that synthetic personas predominantly cluster in the left-libertarian quadrant, with models demonstrating varying degrees of responsiveness when prompted with explicit ideological descriptors. While all models demonstrate significant shifts towards right-authoritarian positions, they exhibit more limited shifts towards left-libertarian positions, suggesting an asymmetric response to ideological manipulation that may reflect inherent biases in model training.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "110",
        "title": "A Survey of RWKV",
        "author": [
            "Zhiyuan Li",
            "Tingyu Xia",
            "Yi Chang",
            "Yuan Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14847",
        "abstract": "The Receptance Weighted Key Value (RWKV) model offers a novel alternative to the Transformer architecture, merging the benefits of recurrent and attention-based systems. Unlike conventional Transformers, which depend heavily on self-attention, RWKV adeptly captures long-range dependencies with minimal computational demands. By utilizing a recurrent framework, RWKV addresses some computational inefficiencies found in Transformers, particularly in tasks with long sequences. RWKV has recently drawn considerable attention for its robust performance across multiple domains. Despite its growing popularity, no systematic review of the RWKV model exists. This paper seeks to fill this gap as the first comprehensive review of the RWKV architecture, its core principles, and its varied applications, such as natural language generation, natural language understanding, and computer vision. We assess how RWKV compares to traditional Transformer models, highlighting its capability to manage long sequences efficiently and lower computational costs. Furthermore, we explore the challenges RWKV encounters and propose potential directions for future research and advancement. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/RWKV-Survey.",
        "tags": [
            "RWKV",
            "Transformer"
        ]
    },
    {
        "id": "111",
        "title": "DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for Few-Shot Aspect-Based Sentiment Analysis",
        "author": [
            "Hongling Xu",
            "Yice Zhang",
            "Qianlong Wang",
            "Ruifeng Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14849",
        "abstract": "Recently developed large language models (LLMs) have presented promising new avenues to address data scarcity in low-resource scenarios. In few-shot aspect-based sentiment analysis (ABSA), previous efforts have explored data augmentation techniques, which prompt LLMs to generate new samples by modifying existing ones. However, these methods fail to produce adequately diverse data, impairing their effectiveness. Besides, some studies apply in-context learning for ABSA by using specific instructions and a few selected examples as prompts. Though promising, LLMs often yield labels that deviate from task requirements. To overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize data from two complementary perspectives: \\textit{key-point-driven} and \\textit{instance-driven}, which effectively generate diverse and high-quality ABSA samples in low-resource settings. Furthermore, a \\textit{label refinement} module is integrated to improve the synthetic labels. Extensive experiments demonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA solutions and other LLM-oriented data generation methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "112",
        "title": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling",
        "author": [
            "Junyi Li",
            "Hwee Tou Ng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14860",
        "abstract": "Despite their outstanding capabilities, large language models (LLMs) are prone to hallucination and producing factually incorrect information. This challenge has spurred efforts in attributed text generation, which prompts LLMs to generate content with supporting evidence. In this paper, we propose a novel framework, called Think&Cite, and formulate attributed text generation as a multi-step reasoning problem integrated with search. Specifically, we propose Self-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the self-reflection capability of LLMs to reflect on the intermediate states of MCTS for guiding the tree expansion process. To provide reliable and comprehensive feedback, we introduce Progress Reward Models to measure the progress of tree search from the root to the current state from two aspects, i.e., generation and attribution progress. We conduct extensive experiments on three datasets and the results show that our approach significantly outperforms baseline approaches.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "113",
        "title": "Graph-Convolutional Networks: Named Entity Recognition and Large Language Model Embedding in Document Clustering",
        "author": [
            "Imed Keraghel",
            "Mohamed Nadif"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14867",
        "abstract": "Recent advances in machine learning, particularly Large Language Models (LLMs) such as BERT and GPT, provide rich contextual embeddings that improve text representation. However, current document clustering approaches often ignore the deeper relationships between named entities (NEs) and the potential of LLM embeddings. This paper proposes a novel approach that integrates Named Entity Recognition (NER) and LLM embeddings within a graph-based framework for document clustering. The method builds a graph with nodes representing documents and edges weighted by named entity similarity, optimized using a graph-convolutional network (GCN). This ensures a more effective grouping of semantically related documents. Experimental results indicate that our approach outperforms conventional co-occurrence-based methods in clustering, notably for documents rich in named entities.",
        "tags": [
            "BERT",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "114",
        "title": "Multimodal Hypothetical Summary for Retrieval-based Multi-image Question Answering",
        "author": [
            "Peize Li",
            "Qingyi Si",
            "Peng Fu",
            "Zheng Lin",
            "Yan Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14880",
        "abstract": "Retrieval-based multi-image question answering (QA) task involves retrieving multiple question-related images and synthesizing these images to generate an answer. Conventional \"retrieve-then-answer\" pipelines often suffer from cascading errors because the training objective of QA fails to optimize the retrieval stage. To address this issue, we propose a novel method to effectively introduce and reference retrieved information into the QA. Given the image set to be retrieved, we employ a multimodal large language model (visual perspective) and a large language model (textual perspective) to obtain multimodal hypothetical summary in question-form and description-form. By combining visual and textual perspectives, MHyS captures image content more specifically and replaces real images in retrieval, which eliminates the modality gap by transforming into text-to-text retrieval and helps improve retrieval. To more advantageously introduce retrieval with QA, we employ contrastive learning to align queries (questions) with MHyS. Moreover, we propose a coarse-to-fine strategy for calculating both sentence-level and word-level similarity scores, to further enhance retrieval and filter out irrelevant details. Our approach achieves a 3.7% absolute improvement over state-of-the-art methods on RETVQA and a 14.5% improvement over CLIP. Comprehensive experiments and detailed ablation studies demonstrate the superiority of our method.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "115",
        "title": "Diffusion priors for Bayesian 3D reconstruction from incomplete measurements",
        "author": [
            "Julian L. Möbius",
            "Michael Habeck"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14897",
        "abstract": "Many inverse problems are ill-posed and need to be complemented by prior information that restricts the class of admissible models. Bayesian approaches encode this information as prior distributions that impose generic properties on the model such as sparsity, non-negativity or smoothness. However, in case of complex structured models such as images, graphs or three-dimensional (3D) objects,generic prior distributions tend to favor models that differ largely from those observed in the real world. Here we explore the use of diffusion models as priors that are combined with experimental data within a Bayesian framework. We use 3D point clouds to represent 3D objects such as household items or biomolecular complexes formed from proteins and nucleic acids. We train diffusion models that generate coarse-grained 3D structures at a medium resolution and integrate these with incomplete and noisy experimental data. To demonstrate the power of our approach, we focus on the reconstruction of biomolecular assemblies from cryo-electron microscopy (cryo-EM) images, which is an important inverse problem in structural biology. We find that posterior sampling with diffusion model priors allows for 3D reconstruction from very sparse, low-resolution and partial observations.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "116",
        "title": "MagicNaming: Consistent Identity Generation by Finding a \"Name Space\" in T2I Diffusion Models",
        "author": [
            "Jing Zhao",
            "Heliang Zheng",
            "Chaoyue Wang",
            "Long Lan",
            "Wanrong Hunag",
            "Yuhua Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14902",
        "abstract": "Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable of generating famous persons by simply referring to their names. Is it possible to make such models generate generic identities as simple as the famous ones, e.g., just use a name? In this paper, we explore the existence of a \"Name Space\", where any point in the space corresponds to a specific identity. Fortunately, we find some clues in the feature space spanned by text embedding of celebrities' names. Specifically, we first extract the embeddings of celebrities' names in the Laion5B dataset with the text encoder of diffusion models. Such embeddings are used as supervision to learn an encoder that can predict the name (actually an embedding) of a given face image. We experimentally find that such name embeddings work well in promising the generated image with good identity consistency. Note that like the names of celebrities, our predicted name embeddings are disentangled from the semantics of text inputs, making the original generation capability of text-to-image models well-preserved. Moreover, by simply plugging such name embeddings, all variants (e.g., from Civitai) derived from the same base model (i.e., SDXL) readily become identity-aware text-to-image models. Project homepage: \\url{https://magicfusion.github.io/MagicNaming/}.",
        "tags": [
            "Diffusion",
            "SDXL",
            "Text-to-Image"
        ]
    },
    {
        "id": "117",
        "title": "Dehallucinating Parallel Context Extension for Retrieval-Augmented Generation",
        "author": [
            "Zexiong Ma",
            "Shengnan An",
            "Zeqi Lin",
            "Yanzhen Zou",
            "Jian-Guang Lou",
            "Bing Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14905",
        "abstract": "Large language models (LLMs) are susceptible to generating hallucinated information, despite the integration of retrieval-augmented generation (RAG). Parallel context extension (PCE) is a line of research attempting to effectively integrating parallel (unordered) contexts, while it still suffers from hallucinations when adapted to RAG scenarios. In this paper, we propose DePaC (Dehallucinating Parallel Context Extension), which alleviates the hallucination problem with context-aware negative training and information-calibrated aggregation. DePaC is designed to alleviate two types of in-context hallucination: fact fabrication (i.e., LLMs present claims that are not supported by the contexts) and fact omission (i.e., LLMs fail to present claims that can be supported by the contexts). Specifically, (1) for fact fabrication, we apply the context-aware negative training that fine-tunes the LLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to answer when contexts are not related to questions; (2) for fact omission, we propose the information-calibrated aggregation which prioritizes context windows with higher information increment from their contexts. The experimental results on nine RAG tasks demonstrate that DePaC significantly alleviates the two types of hallucination and consistently achieves better performances on these tasks.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "118",
        "title": "RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response",
        "author": [
            "Junyu Luo",
            "Xiao Luo",
            "Kaize Ding",
            "Jingyang Yuan",
            "Zhiping Xiao",
            "Ming Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14922",
        "abstract": "Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (RobustFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate RobustFT's exceptional performance in noisy scenarios.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "119",
        "title": "Automatic Spectral Calibration of Hyperspectral Images:Method, Dataset and Benchmark",
        "author": [
            "Zhuoran Du",
            "Shaodi You",
            "Cheng Cheng",
            "Shikui Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14925",
        "abstract": "Hyperspectral image (HSI) densely samples the world in both the space and frequency domain and therefore is more distinctive than RGB images. Usually, HSI needs to be calibrated to minimize the impact of various illumination conditions. The traditional way to calibrate HSI utilizes a physical reference, which involves manual operations, occlusions, and/or limits camera mobility. These limitations inspire this paper to automatically calibrate HSIs using a learning-based method. Towards this goal, a large-scale HSI calibration dataset is created, which has 765 high-quality HSI pairs covering diversified natural scenes and illuminations. The dataset is further expanded to 7650 pairs by combining with 10 different physically measured illuminations. A spectral illumination transformer (SIT) together with an illumination attention module is proposed. Extensive benchmarks demonstrate the SoTA performance of the proposed SIT. The benchmarks also indicate that low-light conditions are more challenging than normal conditions. The dataset and codes are available online:https://github.com/duranze/Automatic-spectral-calibration-of-HSI",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "120",
        "title": "Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination",
        "author": [
            "Leonardo Barcellona",
            "Andrii Zadaianchuk",
            "Davide Allegro",
            "Samuele Papa",
            "Stefano Ghidoni",
            "Efstratios Gavves"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14957",
        "abstract": "A world model provides an agent with a representation of its environment, enabling it to predict the causal consequences of its actions. Current world models typically cannot directly and explicitly imitate the actual environment in front of a robot, often resulting in unrealistic behaviors and hallucinations that make them unsuitable for real-world applications. In this paper, we introduce a new paradigm for constructing world models that are explicit representations of the real world and its dynamics. By integrating cutting-edge advances in real-time photorealism with Gaussian Splatting and physics simulators, we propose the first compositional manipulation world model, which we call DreMa. DreMa replicates the observed world and its dynamics, allowing it to imagine novel configurations of objects and predict the future consequences of robot actions. We leverage this capability to generate new data for imitation learning by applying equivariant transformations to a small set of demonstrations. Our evaluations across various settings demonstrate significant improvements in both accuracy and robustness by incrementing actions and object distributions, reducing the data needed to learn a policy and improving the generalization of the agents. As a highlight, we show that a real Franka Emika Panda robot, powered by DreMa's imagination, can successfully learn novel physical tasks from just a single example per task variation (one-shot policy learning). Our project page and source code can be found in https://leobarcellona.github.io/DreamToManipulate/",
        "tags": [
            "Gaussian Splatting",
            "Robot"
        ]
    },
    {
        "id": "121",
        "title": "Understanding the Dark Side of LLMs' Intrinsic Self-Correction",
        "author": [
            "Qingjie Zhang",
            "Han Qiu",
            "Di Wang",
            "Haoting Qian",
            "Yiming Li",
            "Tianwei Zhang",
            "Minlie Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14959",
        "abstract": "Intrinsic self-correction was proposed to improve LLMs' responses via feedback prompts solely based on their inherent capability. However, recent works show that LLMs' intrinsic self-correction fails without oracle labels as feedback prompts. In this paper, we aim to interpret LLMs' intrinsic self-correction for different tasks, especially for those failure cases. By including one simple task and three complex tasks with state-of-the-art (SOTA) LLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B, and 3.1-8B), we design three interpretation methods to reveal the dark side of LLMs' intrinsic self-correction. We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers and lead to prompt bias on simple factual questions; (2) introduce human-like cognitive bias on complex tasks. In light of our findings, we also provide two simple yet effective strategies for alleviation: question repeating and supervised fine-tuning with a few samples. We open-source our work at https://x-isc.info/.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "LLaMA"
        ]
    },
    {
        "id": "122",
        "title": "TDCNet: Transparent Objects Depth Completion with CNN-Transformer Dual-Branch Parallel Network",
        "author": [
            "Xianghui Fan",
            "Chao Ye",
            "Anping Deng",
            "Xiaotian Wu",
            "Mengyang Pan",
            "Hang Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14961",
        "abstract": "The sensing and manipulation of transparent objects present a critical challenge in industrial and laboratory robotics. Conventional sensors face challenges in obtaining the full depth of transparent objects due to the refraction and reflection of light on their surfaces and their lack of visible texture. Previous research has attempted to obtain complete depth maps of transparent objects from RGB and damaged depth maps (collected by depth sensor) using deep learning models. However, existing methods fail to fully utilize the original depth map, resulting in limited accuracy for deep completion. To solve this problem, we propose TDCNet, a novel dual-branch CNN-Transformer parallel network for transparent object depth completion. The proposed framework consists of two different branches: one extracts features from partial depth maps, while the other processes RGB-D images. Experimental results demonstrate that our model achieves state-of-the-art performance across multiple public datasets. Our code and the pre-trained model are publicly available at https://github.com/XianghuiFan/TDCNet.",
        "tags": [
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "123",
        "title": "IDOL: Instant Photorealistic 3D Human Creation from a Single Image",
        "author": [
            "Yiyu Zhuang",
            "Jiaxi Lv",
            "Hao Wen",
            "Qing Shuai",
            "Ailing Zeng",
            "Hao Zhu",
            "Shifeng Chen",
            "Yujiu Yang",
            "Xun Cao",
            "Wei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14963",
        "abstract": "Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce a large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K diverse, photorealistic sets of human images. Each set contains 24-view frames in specific human poses, generated using a pose-controllable image-to-multi-view model. Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop a scalable feed-forward transformer model to predict a 3D human Gaussian representation in a uniform space from a given human image. This model is trained to disentangle human pose, body shape, clothing geometry, and texture. The estimated Gaussians can be animated without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the ability to efficiently reconstruct photorealistic humans at 1K resolution from a single input image using a single GPU instantly. Additionally, it seamlessly supports various applications, as well as shape and texture editing tasks.",
        "tags": [
            "3D",
            "Transformer"
        ]
    },
    {
        "id": "124",
        "title": "Knowledge Injection via Prompt Distillation",
        "author": [
            "Kalle Kujanpää",
            "Harri Valpola",
            "Alexander Ilin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14964",
        "abstract": "In many practical applications, large language models (LLMs) need to incorporate new knowledge not present in their pre-training data. The primary methods for this are fine-tuning and retrieval-augmented generation (RAG). Although RAG has emerged as the industry standard for knowledge injection, fine-tuning has not yet achieved comparable success. In this paper, we propose a new fine-tuning technique for learning new knowledge and show that it can reach the performance of RAG. The proposed method is based on the self-distillation approach, which we call prompt distillation. First, we generate question-answer pairs about the new knowledge. Then, we fine-tune a student model on the question-answer pairs to imitate the output distributions of a teacher model, which additionally receives the new knowledge in its prompt. The student model is identical to the teacher, except it is equipped with a LoRA adapter. This training procedure facilitates distilling the new knowledge from the teacher's prompt into the student's weights.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA",
            "RAG"
        ]
    },
    {
        "id": "125",
        "title": "AI and Cultural Context: An Empirical Investigation of Large Language Models' Performance on Chinese Social Work Professional Standards",
        "author": [
            "Zia Qi",
            "Brian E. Perron",
            "Miao Wang",
            "Cao Fang",
            "Sitao Chen",
            "Bryan G. Victor"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14971",
        "abstract": "Objective: This study examines how well leading Chinese and Western large language models understand and apply Chinese social work principles, focusing on their foundational knowledge within a non-Western professional setting. We test whether the cultural context in the developing country influences model reasoning and accuracy.\nMethod: Using a published self-study version of the Chinese National Social Work Examination (160 questions) covering jurisprudence and applied knowledge, we administered three testing conditions to eight cloud-based large language models - four Chinese and four Western. We examined their responses following official guidelines and evaluated their explanations' reasoning quality.\nResults: Seven models exceeded the 60-point passing threshold in both sections. Chinese models performed better in jurisprudence (median = 77.0 vs. 70.3) but slightly lower in applied knowledge (median = 65.5 vs. 67.0). Both groups showed cultural biases, particularly regarding gender equality and family dynamics. Models demonstrated strong professional terminology knowledge but struggled with culturally specific interventions. Valid reasoning in incorrect answers ranged from 16.4% to 45.0%.\nConclusions: While both Chinese and Western models show foundational knowledge of Chinese social work principles, technical language proficiency does not ensure cultural competence. Chinese models demonstrate advantages in regulatory content, yet both Chinese and Western models struggle with culturally nuanced practice scenarios. These findings contribute to informing responsible AI integration into cross-cultural social work practice.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "126",
        "title": "Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small Language Models Write Young Students Texts",
        "author": [
            "Ioana Buhnila",
            "Georgeta Cislaru",
            "Amalia Todirascu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14986",
        "abstract": "Large Language Models (LLMs) have been used to generate texts in response to different writing tasks: reports, essays, story telling. However, language models do not have a meta-representation of the text writing process, nor inherent communication learning needs, comparable to those of young human students. This paper introduces a fine-grained linguistic and textual analysis of multilingual Small Language Models' (SLMs) writing. With our method, Chain-of-MetaWriting, SLMs can imitate some steps of the human writing process, such as planning and evaluation. We mainly focused on short story and essay writing tasks in French for schoolchildren and undergraduate students respectively. Our results show that SLMs encounter difficulties in assisting young students on sensitive topics such as violence in the schoolyard, and they sometimes use words too complex for the target audience. In particular, the output is quite different from the human produced texts in term of text cohesion and coherence regarding temporal connectors, topic progression, reference.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "127",
        "title": "RoboCup@Home 2024 OPL Winner NimbRo: Anthropomorphic Service Robots using Foundation Models for Perception and Planning",
        "author": [
            "Raphael Memmesheimer",
            "Jan Nogga",
            "Bastian Pätzold",
            "Evgenii Kruzhkov",
            "Simon Bultmann",
            "Michael Schreiber",
            "Jonas Bode",
            "Bertan Karacora",
            "Juhui Park",
            "Alena Savinykh",
            "Sven Behnke"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14989",
        "abstract": "We present the approaches and contributions of the winning team NimbRo@Home at the RoboCup@Home 2024 competition in the Open Platform League held in Eindhoven, NL. Further, we describe our hardware setup and give an overview of the results for the task stages and the final demonstration. For this year's competition, we put a special emphasis on open-vocabulary object segmentation and grasping approaches that overcome the labeling overhead of supervised vision approaches, commonly used in RoboCup@Home. We successfully demonstrated that we can segment and grasp non-labeled objects by text descriptions. Further, we extensively employed LLMs for natural language understanding and task planning. Throughout the competition, our approaches showed robustness and generalization capabilities. A video of our performance can be found online.",
        "tags": [
            "LLMs",
            "Segmentation"
        ]
    },
    {
        "id": "128",
        "title": "HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs",
        "author": [
            "Pham Vu Tuan Dat",
            "Long Doan",
            "Huynh Thi Thanh Binh"
        ],
        "pdf": "https://arxiv.org/pdf/2412.14995",
        "abstract": "Automatic Heuristic Design (AHD) is an active research area due to its utility in solving complex search and NP-hard combinatorial optimization problems in the real world. The recent advancements in Large Language Models (LLMs) introduce new possibilities by coupling LLMs with evolutionary computation to automatically generate heuristics, known as LLM-based Evolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtained great performance on various tasks, there is still a gap in understanding the properties of heuristic search spaces and achieving a balance between exploration and exploitation, which is a critical factor in large heuristic search spaces. In this study, we address this gap by proposing two diversity measurement metrics and perform an analysis on previous LLM-EPS approaches, including FunSearch, EoH, and ReEvo. Results on black-box AHD problems reveal that while EoH demonstrates higher diversity than FunSearch and ReEvo, its objective score is unstable. Conversely, ReEvo's reflection mechanism yields good objective scores but fails to optimize diversity effectively. With this finding in mind, we introduce HSEvo, an adaptive LLM-EPS framework that maintains a balance between diversity and convergence with a harmony search algorithm. Through experimentation, we find that HSEvo achieved high diversity indices and good objective scores while remaining cost-effective. These results underscore the importance of balancing exploration and exploitation and understanding heuristic search spaces in designing frameworks in LLM-EPS.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "129",
        "title": "Large Language Models and Code Security: A Systematic Literature Review",
        "author": [
            "Enna Basic",
            "Alberto Giaretta"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15004",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools for automating various programming tasks, including security-related ones, such as detecting and fixing vulnerabilities. Despite their promising capabilities, when required to produce or modify pre-existing code, LLMs could introduce vulnerabilities unbeknown to the programmer. When analyzing code, they could miss clear vulnerabilities or signal nonexistent ones. In this Systematic Literature Review (SLR), we aim to investigate both the security benefits and potential drawbacks of using LLMs for a variety of code-related tasks. In particular, first we focus on the types of vulnerabilities that could be introduced by LLMs, when used for producing code. Second, we analyze the capabilities of LLMs to detect and fix vulnerabilities, in any given code, and how the prompting strategy of choice impacts their performance in these two tasks. Last, we provide an in-depth analysis on how data poisoning attacks on LLMs can impact performance in the aforementioned tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "130",
        "title": "Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and Semantic Controls",
        "author": [
            "Riccardo Fosco Gramaccioni",
            "Christian Marinoni",
            "Emilian Postolache",
            "Marco Comunità",
            "Luca Cosmo",
            "Joshua D. Reiss",
            "Danilo Comminiello"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15023",
        "abstract": "Sound designers and Foley artists usually sonorize a scene, such as from a movie or video game, by manually annotating and sonorizing each action of interest in the video. In our case, the intent is to leave full creative control to sound designers with a tool that allows them to bypass the more repetitive parts of their work, thus being able to focus on the creative aspects of sound production. We achieve this presenting Stable-V2A, a two-stage model consisting of: an RMS-Mapper that estimates an envelope representative of the audio characteristics associated with the input video; and Stable-Foley, a diffusion model based on Stable Audio Open that generates audio semantically and temporally aligned with the target video. Temporal alignment is guaranteed by the use of the envelope as a ControlNet input, while semantic alignment is achieved through the use of sound representations chosen by the designer as cross-attention conditioning of the diffusion process. We train and test our model on Greatest Hits, a dataset commonly used to evaluate V2A models. In addition, to test our model on a case study of interest, we introduce Walking The Maps, a dataset of videos extracted from video games depicting animated characters walking in different locations. Samples and code available on our demo page at https://ispamm.github.io/Stable-V2A.",
        "tags": [
            "ControlNet",
            "Diffusion"
        ]
    },
    {
        "id": "131",
        "title": "DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT Space",
        "author": [
            "Mang Ning",
            "Mingxiao Li",
            "Jianlin Su",
            "Haozhe Jia",
            "Lanmiao Liu",
            "Martin Beneš",
            "Albert Ali Salah",
            "Itir Onal Ertugrul"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15032",
        "abstract": "This paper explores image modeling from the frequency space and introduces DCTdiff, an end-to-end diffusion generative paradigm that efficiently models images in the discrete cosine transform (DCT) space. We investigate the design space of DCTdiff and reveal the key design factors. Experiments on different frameworks (UViT, DiT), generation tasks, and various diffusion samplers demonstrate that DCTdiff outperforms pixel-based diffusion models regarding generative quality and training efficiency. Remarkably, DCTdiff can seamlessly scale up to high-resolution generation without using the latent diffusion paradigm. Finally, we illustrate several intriguing properties of DCT image modeling. For example, we provide a theoretical proof of why `image diffusion can be seen as spectral autoregression', bridging the gap between diffusion and autoregressive models. The effectiveness of DCTdiff and the introduced properties suggest a promising direction for image modeling in the frequency space. The code is at \\url{https://github.com/forever208/DCTdiff}.",
        "tags": [
            "DiT",
            "Diffusion"
        ]
    },
    {
        "id": "132",
        "title": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps",
        "author": [
            "Felix Friedrich",
            "Simone Tedeschi",
            "Patrick Schramowski",
            "Manuel Brack",
            "Roberto Navigli",
            "Huu Nguyen",
            "Bo Li",
            "Kristian Kersting"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15035",
        "abstract": "Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, following the detailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in the category crime_tax for Italian but remains safe in other languages. Similar differences can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure safe and responsible usage across diverse user communities.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "133",
        "title": "Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream Diffusion",
        "author": [
            "Zhifei Chen",
            "Tianshuo Xu",
            "Wenhang Ge",
            "Leyi Wu",
            "Dongyu Yan",
            "Jing He",
            "Luozhou Wang",
            "Lu Zeng",
            "Shunsi Zhang",
            "Yingcong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15050",
        "abstract": "Rendering and inverse rendering are pivotal tasks in both computer vision and graphics. The rendering equation is the core of the two tasks, as an ideal conditional distribution transfer function from intrinsic properties to RGB images. Despite achieving promising results of existing rendering methods, they merely approximate the ideal estimation for a specific scene and come with a high computational cost. Additionally, the inverse conditional distribution transfer is intractable due to the inherent ambiguity. To address these challenges, we propose a data-driven method that jointly models rendering and inverse rendering as two conditional generation tasks within a single diffusion framework. Inspired by UniDiffuser, we utilize two distinct time schedules to model both tasks, and with a tailored dual streaming module, we achieve cross-conditioning of two pre-trained diffusion models. This unified approach, named Uni-Renderer, allows the two processes to facilitate each other through a cycle-consistent constrain, mitigating ambiguity by enforcing consistency between intrinsic properties and rendered images. Combined with a meticulously prepared dataset, our method effectively decomposition of intrinsic properties and demonstrates a strong capability to recognize changes during rendering. We will open-source our training and inference code to the public, fostering further research and development in this area.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "134",
        "title": "ConfliBERT: A Language Model for Political Conflict",
        "author": [
            "Patrick T. Brandt",
            "Sultan Alsarra",
            "Vito J. D`Orazio",
            "Dagmar Heintze",
            "Latifur Khan",
            "Shreyas Meher",
            "Javier Osorio",
            "Marcus Sianan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15060",
        "abstract": "Conflict scholars have used rule-based approaches to extract information about political violence from news reports and texts. Recent Natural Language Processing developments move beyond rigid rule-based approaches. We review our recent ConfliBERT language model (Hu et al. 2022) to process political and violence related texts. The model can be used to extract actor and action classifications from texts about political conflict. When fine-tuned, results show that ConfliBERT has superior performance in accuracy, precision and recall over other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama 3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also hundreds of times faster than these more generalist LLMs. These results are illustrated using texts from the BBC, re3d, and the Global Terrorism Dataset (GTD).",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "135",
        "title": "Numerical analysis and simulation of lateral memristive devices: Schottky, ohmic, and multi-dimensional electrode models",
        "author": [
            "Dilara Abdel",
            "Maxime Herda",
            "Martin Ziegler",
            "Claire Chainais-Hillairet",
            "Benjamin Spetzler",
            "Patricio Farrell"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15065",
        "abstract": "In this paper, we present the numerical analysis and simulations of a multi-dimensional memristive device model. Memristive devices and memtransistors based on two-dimensional (2D) materials have demonstrated promising potential as components for next-generation artificial intelligence (AI) hardware and information technology. Our charge transport model describes the drift-diffusion of electrons, holes, and ionic defects self-consistently in an electric field. We incorporate two types of boundary models: ohmic and Schottky contacts. The coupled drift-diffusion partial differential equations are discretized using a physics-preserving Voronoi finite volume method. It relies on an implicit time-stepping scheme and the excess chemical potential flux approximation. We demonstrate that the fully discrete nonlinear scheme is unconditionally stable, preserving the free-energy structure of the continuous system and ensuring the non-negativity of carrier densities. Novel discrete entropy-dissipation inequalities for both boundary condition types in multiple dimensions allow us to prove the existence of discrete solutions. We perform multi-dimensional simulations to understand the impact of electrode configurations and device geometries, focusing on the hysteresis behavior in lateral 2D memristive devices. Three electrode configurations -- side, top, and mixed contacts -- are compared numerically for different geometries and boundary conditions. These simulations reveal the conditions under which a simplified one-dimensional electrode geometry can well represent the three electrode configurations. This work lays the foundations for developing accurate, efficient simulation tools for 2D memristive devices and memtransistors, offering tools and guidelines for their design and optimization in future applications.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "136",
        "title": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling",
        "author": [
            "Zihan Liu",
            "Yang Chen",
            "Mohammad Shoeybi",
            "Bryan Catanzaro",
            "Wei Ping"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15084",
        "abstract": "In this paper, we introduce AceMath, a suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose a supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted fine-tuning for the math domain using a carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72B-Instruct greatly outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, a comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present a systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. We will release model weights, training data, and evaluation benchmarks at: https://research.nvidia.com/labs/adlr/acemath",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "137",
        "title": "Nano-ESG: Extracting Corporate Sustainability Information from News Articles",
        "author": [
            "Fabian Billert",
            "Stefan Conrad"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15093",
        "abstract": "Determining the sustainability impact of companies is a highly complex subject which has garnered more and more attention over the past few years. Today, investors largely rely on sustainability-ratings from established rating-providers in order to analyze how responsibly a company acts. However, those ratings have recently been criticized for being hard to understand and nearly impossible to reproduce.\nAn independent way to find out about the sustainability practices of companies lies in the rich landscape of news article data. In this paper, we explore a different approach to identify key opportunities and challenges of companies in the sustainability domain. We present a novel dataset of more than 840,000 news articles which were gathered for major German companies between January 2023 and September 2024. By applying a mixture of Natural Language Processing techniques, we first identify relevant articles, before summarizing them and extracting their sustainability-related sentiment and aspect using Large Language Models (LLMs). Furthermore, we conduct an evaluation of the obtained data and determine that the LLM-produced answers are accurate. We release both datasets at https://github.com/Bailefan/Nano-ESG.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "138",
        "title": "A Full Transformer-based Framework for Automatic Pain Estimation using Videos",
        "author": [
            "Stefanos Gkikas",
            "Manolis Tsiknakis"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15095",
        "abstract": "The automatic estimation of pain is essential in designing an optimal pain management system offering reliable assessment and reducing the suffering of patients. In this study, we present a novel full transformer-based framework consisting of a Transformer in Transformer (TNT) model and a Transformer leveraging cross-attention and self-attention blocks. Elaborating on videos from the BioVid database, we demonstrate state-of-the-art performances, showing the efficacy, efficiency, and generalization capability across all the primary pain estimation tasks.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "139",
        "title": "Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering with Temporal Adaptability",
        "author": [
            "Xiangsen Chen",
            "Xuming Hu",
            "Nan Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15101",
        "abstract": "Retrieve-augmented generation (RAG) frameworks have emerged as a promising solution to multi-hop question answering(QA) tasks since it enables large language models (LLMs) to incorporate external knowledge and mitigate their inherent knowledge deficiencies. Despite this progress, existing RAG frameworks, which usually follows the retrieve-then-read paradigm, often struggle with multi-hop QA with temporal information since it has difficulty retrieving and synthesizing accurate time-related information. To address the challenge, this paper proposes a novel framework called review-then-refine, which aims to enhance LLM performance in multi-hop QA scenarios with temporal information. Our approach begins with a review phase, where decomposed sub-queries are dynamically rewritten with temporal information, allowing for subsequent adaptive retrieval and reasoning process. In addition, we implement adaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing the potential for hallucinations. In the subsequent refine phase, the LLM synthesizes the retrieved information from each sub-query along with its internal knowledge to formulate a coherent answer. Extensive experimental results across multiple datasets demonstrate the effectiveness of our proposed framework, highlighting its potential to significantly improve multi-hop QA capabilities in LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "140",
        "title": "Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture",
        "author": [
            "Thomas F Burns",
            "Tomoki Fukai",
            "Christopher J Earls"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15113",
        "abstract": "Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million parameters, focusing on attention head values, with results also indicating improved ICL performance at this larger and more naturalistic scale.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "141",
        "title": "Qwen2.5 Technical Report",
        "author": [
            "Qwen",
            "An Yang",
            "Baosong Yang",
            "Beichen Zhang",
            "Binyuan Hui",
            "Bo Zheng",
            "Bowen Yu",
            "Chengyuan Li",
            "Dayiheng Liu",
            "Fei Huang",
            "Haoran Wei",
            "Huan Lin",
            "Jian Yang",
            "Jianhong Tu",
            "Jianwei Zhang",
            "Jianxin Yang",
            "Jiaxi Yang",
            "Jingren Zhou",
            "Junyang Lin",
            "Kai Dang",
            "Keming Lu",
            "Keqin Bao",
            "Kexin Yang",
            "Le Yu",
            "Mei Li",
            "Mingfeng Xue",
            "Pei Zhang",
            "Qin Zhu",
            "Rui Men",
            "Runji Lin",
            "Tianhao Li",
            "Tingyu Xia",
            "Xingzhang Ren",
            "Xuancheng Ren",
            "Yang Fan",
            "Yang Su",
            "Yichang Zhang",
            "Yu Wan",
            "Yuqiong Liu",
            "Zeyu Cui",
            "Zhenru Zhang",
            "Zihan Qiu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15115",
        "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "142",
        "title": "Outcome-Refining Process Supervision for Code Generation",
        "author": [
            "Zhuohao Yu",
            "Weizheng Gu",
            "Yidong Wang",
            "Zhengran Zeng",
            "Jindong Wang",
            "Wei Ye",
            "Shikun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15118",
        "abstract": "Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: https://github.com/zhuohaoyu/ORPS",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "143",
        "title": "Parallelized Autoregressive Visual Generation",
        "author": [
            "Yuqing Wang",
            "Shuhuai Ren",
            "Zhijie Lin",
            "Yujin Han",
            "Haoyuan Guo",
            "Zhenheng Yang",
            "Difan Zou",
            "Jiashi Feng",
            "Xihui Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15119",
        "abstract": "Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "144",
        "title": "Adaptive Pruning for Large Language Models with Structural Importance Awareness",
        "author": [
            "Haotian Zheng",
            "Jinke Ren",
            "Yushan Sun",
            "Ruichen Zhang",
            "Wenbo Zhang",
            "Zhen Li",
            "Dusit Niyato",
            "Shuguang Cui",
            "Yatong Han"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15127",
        "abstract": "The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high computational and storage resource demands. To address this issue, we propose a novel LLM model pruning method, namely structurally-aware adaptive pruning (SAAP), to significantly reduce the computational and memory costs while maintaining model performance. We first define an adaptive importance fusion metric to evaluate the importance of all coupled structures in LLMs by considering their homoscedastic uncertainty. Then, we rank the importance of all modules to determine the specific layers that should be pruned to meet particular performance requirements. Furthermore, we develop a new group fine-tuning strategy to improve the inference efficiency of LLMs. Finally, we evaluate the proposed SAAP method on multiple LLMs across two common tasks, i.e., zero-shot classification and text generation. Experimental results show that our SAAP method outperforms several state-of-the-art baseline methods, achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and LLaMA-13B. Additionally, SAAP improves the token generation speed by 5%, showcasing its practical advantages in resource-constrained scenarios.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "Vicuna"
        ]
    },
    {
        "id": "145",
        "title": "Jet: A Modern Transformer-Based Normalizing Flow",
        "author": [
            "Alexander Kolesnikov",
            "André Susano Pinto",
            "Michael Tschannen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15129",
        "abstract": "In the past, normalizing generative flows have emerged as a promising class of generative models for natural images. This type of model has many modeling advantages: the ability to efficiently compute log-likelihood of the input data, fast generation and simple overall structure. Normalizing flows remained a topic of active research but later fell out of favor, as visual quality of the samples was not competitive with other model classes, such as GANs, VQ-VAE-based approaches or diffusion models. In this paper we revisit the design of the coupling-based normalizing flow models by carefully ablating prior design choices and using computational blocks based on the Vision Transformer architecture, not convolutional neural networks. As a result, we achieve state-of-the-art quantitative and qualitative performance with a much simpler architecture. While the overall visual quality is still behind the current state-of-the-art models, we argue that strong normalizing flow models can help advancing research frontier by serving as building components of more powerful generative models.",
        "tags": [
            "Diffusion",
            "Normalizing Flows",
            "Transformer",
            "VAE"
        ]
    },
    {
        "id": "146",
        "title": "Language Models as Continuous Self-Evolving Data Engineers",
        "author": [
            "Peidong Wang",
            "Ming Wang",
            "Zhiming Ma",
            "Xiaocui Yang",
            "Shi Feng",
            "Daling Wang",
            "Yifei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15151",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert-labeled data, setting an upper limit on the performance of LLMs. To address this issue, we propose a novel paradigm that enables LLMs to train itself by autonomously generating, cleaning, reviewing, and annotating data with preference information, named LANCE. Our approach demonstrates that LLMs can serve as continuous self-evolving data engineers, significantly reducing the time and cost of the post-training data construction process. Through iterative fine-tuning on different variants of the Qwen2, we validate the effectiveness of LANCE across various tasks, showing that it can continuously improve model performance and maintain high-quality data generation. Across eight benchmark dimensions, LANCE resulted in an average score enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This training paradigm with autonomous data construction not only reduces the reliance on human experts or external models but also ensures that the data aligns with human values and preferences, paving the way for the development of future superintelligent systems that can exceed human capabilities.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "147",
        "title": "Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned LLM",
        "author": [
            "Yatai Ji",
            "Jiacheng Zhang",
            "Jie Wu",
            "Shilong Zhang",
            "Shoufa Chen",
            "Chongjian GE",
            "Peize Sun",
            "Weifeng Chen",
            "Wenqi Shao",
            "Xuefeng Xiao",
            "Weilin Huang",
            "Ping Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15156",
        "abstract": "Text-to-video models have made remarkable advancements through optimization on high-quality text-video pairs, where the textual prompts play a pivotal role in determining quality of output videos. However, achieving the desired output often entails multiple revisions and iterative inference to refine user-provided prompts. Current automatic methods for refining prompts encounter challenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware when applied to text-to-video diffusion models. To address these problem, we introduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video, which excels in crafting Video-Centric, Labor-Free and Preference-Aligned prompts tailored to specific video diffusion model. Our approach involves a meticulously crafted two-stage optimization and alignment system. Initially, we conduct a reward-guided prompt evolution pipeline to automatically create optimal prompts pool and leverage them for supervised fine-tuning (SFT) of the LLM. Then multi-dimensional rewards are employed to generate pairwise data for the SFT model, followed by the direct preference optimization (DPO) algorithm to further facilitate preference alignment. Through extensive experimentation and comparative analyses, we validate the effectiveness of Prompt-A-Video across diverse generation models, highlighting its potential to push the boundaries of video generation.",
        "tags": [
            "Diffusion",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "148",
        "title": "OnlineVPO: Align Video Diffusion Model with Online Video-Centric Preference Optimization",
        "author": [
            "Jiacheng Zhang",
            "Jie Wu",
            "Weifeng Chen",
            "Yatai Ji",
            "Xuefeng Xiao",
            "Weilin Huang",
            "Kai Han"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15159",
        "abstract": "In recent years, the field of text-to-video (T2V) generation has made significant strides. Despite this progress, there is still a gap between theoretical advancements and practical application, amplified by issues like degraded image quality and flickering artifacts. Recent advancements in enhancing the video diffusion model (VDM) through feedback learning have shown promising results. However, these methods still exhibit notable limitations, such as misaligned feedback and inferior scalability. To tackle these issues, we introduce OnlineVPO, a more efficient preference learning approach tailored specifically for video diffusion models. Our method features two novel designs, firstly, instead of directly using image-based reward feedback, we leverage the video quality assessment (VQA) model trained on synthetic data as the reward model to provide distribution and modality-aligned feedback on the video diffusion model. Additionally, we introduce an online DPO algorithm to address the off-policy optimization and scalability issue in existing video preference learning frameworks. By employing the video reward model to offer concise video feedback on the fly, OnlineVPO offers effective and efficient preference guidance. Extensive experiments on the open-source video-diffusion model demonstrate OnlineVPO as a simple yet effective and more importantly scalable preference learning algorithm for video diffusion models, offering valuable insights for future advancements in this domain.",
        "tags": [
            "Diffusion",
            "Text-to-Video"
        ]
    },
    {
        "id": "149",
        "title": "SqueezeMe: Efficient Gaussian Avatars for VR",
        "author": [
            "Shunsuke Saito",
            "Stanislav Pidhorskyi",
            "Igor Santesteban",
            "Forrest Iandola",
            "Divam Gupta",
            "Anuj Pahuja",
            "Nemanja Bartolovic",
            "Frank Yu",
            "Emanuel Garbin",
            "Tomas Simon"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15171",
        "abstract": "Gaussian Splatting has enabled real-time 3D human avatars with unprecedented levels of visual quality. While previous methods require a desktop GPU for real-time inference of a single avatar, we aim to squeeze multiple Gaussian avatars onto a portable virtual reality headset with real-time drivable inference. We begin by training a previous work, Animatable Gaussians, on a high quality dataset captured with 512 cameras. The Gaussians are animated by controlling base set of Gaussians with linear blend skinning (LBS) motion and then further adjusting the Gaussians with a neural network decoder to correct their appearance. When deploying the model on a Meta Quest 3 VR headset, we find two major computational bottlenecks: the decoder and the rendering. To accelerate the decoder, we train the Gaussians in UV-space instead of pixel-space, and we distill the decoder to a single neural network layer. Further, we discover that neighborhoods of Gaussians can share a single corrective from the decoder, which provides an additional speedup. To accelerate the rendering, we develop a custom pipeline in Vulkan that runs on the mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently at 72 FPS on a VR headset. Demo videos are at https://forresti.github.io/squeezeme.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "150",
        "title": "Rethinking Uncertainty Estimation in Natural Language Generation",
        "author": [
            "Lukas Aichberger",
            "Kajetan Schweighofer",
            "Sepp Hochreiter"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15176",
        "abstract": "Large Language Models (LLMs) are increasingly employed in real-world applications, driving the need to evaluate the trustworthiness of their generated text. To this end, reliable uncertainty estimation is essential. Since current LLMs generate text autoregressively through a stochastic process, the same prompt can lead to varying outputs. Consequently, leading uncertainty estimation methods generate and analyze multiple output sequences to determine the LLM's uncertainty. However, generating output sequences is computationally expensive, making these methods impractical at scale. In this work, we inspect the theoretical foundations of the leading methods and explore new directions to enhance their computational efficiency. Building on the framework of proper scoring rules, we find that the negative log-likelihood of the most likely output sequence constitutes a theoretically grounded uncertainty measure. To approximate this alternative measure, we propose G-NLL, which has the advantage of being obtained using only a single output sequence generated by greedy decoding. This makes uncertainty estimation more efficient and straightforward, while preserving theoretical rigor. Empirical results demonstrate that G-NLL achieves state-of-the-art performance across various LLMs and tasks. Our work lays the foundation for efficient and reliable uncertainty estimation in natural language generation, challenging the necessity of more computationally involved methods currently leading the field.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "151",
        "title": "Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying",
        "author": [
            "Federico Castagna",
            "Isabel Sassoon",
            "Simon Parsons"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15177",
        "abstract": "Studies have underscored how, regardless of the recent breakthrough and swift advances in AI research, even state-of-the-art Large Language models (LLMs) continue to struggle when performing logical and mathematical reasoning. The results seem to suggest that LLMs still work as (highly advanced) data pattern identifiers, scoring poorly when attempting to generalise and solve reasoning problems the models have never previously seen or that are not close to samples presented in their training data. To address this compelling concern, this paper makes use of the notion of critical questions from the literature on argumentation theory, focusing in particular on Toulmin's model of argumentation. We show that employing these critical questions can improve the reasoning capabilities of LLMs. By probing the rationale behind the models' reasoning process, the LLM can assess whether some logical mistake is occurring and correct it before providing the final reply to the user prompt. The underlying idea is drawn from the gold standard of any valid argumentative procedure: the conclusion is valid if it is entailed by accepted premises. Or, to paraphrase such Aristotelian principle in a real-world approximation, characterised by incomplete information and presumptive logic, the conclusion is valid if not proved otherwise. This approach successfully steers the models' output through a reasoning pipeline, resulting in better performance against the baseline and its Chain-of-Thought (CoT) implementation. To this end, an extensive evaluation of the proposed approach on the MT-Bench Reasoning and Math tasks across a range of LLMs is provided.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "152",
        "title": "HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages",
        "author": [
            "Aman Chaturvedi",
            "Daniel Nichols",
            "Siddharth Singh",
            "Abhinav Bhatele"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15178",
        "abstract": "Large Language Model (LLM) based coding tools have been tremendously successful as software development assistants, yet they are often designed for general purpose programming tasks and perform poorly for more specialized domains such as high performance computing. Creating specialized models and tools for these domains is crucial towards gaining the benefits of LLMs in areas such as HPC. While previous work has explored HPC-specific models, LLMs still struggle to generate parallel code and it is not at all clear what hurdles are still holding back these LLMs and what must be done to overcome them. In this work, we conduct an in-depth study along the many axes of fine-tuning a specialized HPC LLM in order to better understand the challenges. Based on our findings we fine-tune and evaluate a specialized HPC LLM that is shown to be the best performing open-source code LLM for parallel code generation to date.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "153",
        "title": "Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning",
        "author": [
            "Simon Frieder",
            "Jonas Bayer",
            "Katherine M. Collins",
            "Julius Berner",
            "Jacob Loader",
            "András Juhász",
            "Fabian Ruehle",
            "Sean Welleck",
            "Gabriel Poesia",
            "Ryan-Rhys Griffiths",
            "Adrian Weller",
            "Anirudh Goyal",
            "Thomas Lukasiewicz",
            "Timothy Gowers"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15184",
        "abstract": "The suite of datasets commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings. These limitations include a restricted scope of mathematical complexity, typically not exceeding lower undergraduate-level mathematics, binary rating protocols and other issues, which makes comprehensive proof-based evaluation suites difficult. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or \"thought partners\"), necessitates a paradigm shift in the design of mathematical datasets and the evaluation criteria of mathematical ability: It is necessary to move away from result-based datasets (theorem statement to theorem proof) and convert the rich facets of mathematical research practice to data LLMs can train on. Examples of these are mathematical workflows (sequences of atomic, potentially subfield-dependent tasks that are often performed when creating new mathematics), which are an important part of the proof-discovery process. Additionally, we advocate for mathematical dataset developers to consider the concept of \"motivated proof\", introduced by G. Pólya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations. Lastly, we introduce math datasheets for datasets, extending the general, dataset-agnostic variants of datasheets: We provide a questionnaire designed specifically for math datasets that we urge dataset creators to include with their datasets. This will make creators aware of potential limitations of their datasets while at the same time making it easy for readers to assess it from the point of view of training and evaluating mathematical copilots.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "154",
        "title": "Tiled Diffusion",
        "author": [
            "Or Madar",
            "Ohad Fried"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15185",
        "abstract": "Image tiling -- the seamless connection of disparate images to create a coherent visual field -- is crucial for applications such as texture creation, video game asset development, and digital art. Traditionally, tiles have been constructed manually, a method that poses significant limitations in scalability and flexibility. Recent research has attempted to automate this process using generative models. However, current approaches primarily focus on tiling textures and manipulating models for single-image generation, without inherently supporting the creation of multiple interconnected tiles across diverse domains. This paper presents Tiled Diffusion, a novel approach that extends the capabilities of diffusion models to accommodate the generation of cohesive tiling patterns across various domains of image synthesis that require tiling. Our method supports a wide range of tiling scenarios, from self-tiling to complex many-to-many connections, enabling seamless integration of multiple images. Tiled Diffusion automates the tiling process, eliminating the need for manual intervention and enhancing creative possibilities in various applications, such as seamlessly tiling of existing images, tiled texture creation, and 360° synthesis.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "155",
        "title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation",
        "author": [
            "Weijia Shi",
            "Xiaochuang Han",
            "Chunting Zhou",
            "Weixin Liang",
            "Xi Victoria Lin",
            "Luke Zettlemoyer",
            "Lili Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15188",
        "abstract": "We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LlamaFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LlamaFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.",
        "tags": [
            "Diffusion",
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "156",
        "title": "AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation",
        "author": [
            "Moayed Haji-Ali",
            "Willi Menapace",
            "Aliaksandr Siarohin",
            "Ivan Skorokhodov",
            "Alper Canberk",
            "Kwot Sin Lee",
            "Vicente Ordonez",
            "Sergey Tulyakov"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15191",
        "abstract": "We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video generation that leverages the activations of frozen video and audio diffusion models for temporally-aligned cross-modal conditioning. The key to our framework is a Fusion Block that enables bidirectional information exchange between our backbone video and audio diffusion models through a temporally-aligned self attention operation. Unlike prior work that uses feature extractors pretrained for other tasks for the conditioning signal, AV-Link can directly leverage features obtained by the complementary modality in a single framework i.e. video features to generate audio, or audio features to generate video. We extensively evaluate our design choices and demonstrate the ability of our method to achieve synchronized and high-quality audiovisual content, showcasing its potential for applications in immersive media generation. Project Page: http://snap-research.github.io/AVLink/",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "157",
        "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark",
        "author": [
            "Qihao Zhao",
            "Yangyu Huang",
            "Tengchao Lv",
            "Lei Cui",
            "Qinzheng Sun",
            "Shaoguang Mao",
            "Xin Zhang",
            "Ying Xin",
            "Qiufeng Yin",
            "Scarlett Li",
            "Furu Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15194",
        "abstract": "Multiple-choice question (MCQ) datasets like Massive Multitask Language Understanding (MMLU) are widely used to evaluate the commonsense, understanding, and problem-solving abilities of large language models (LLMs). However, the open-source nature of these benchmarks and the broad sources of training data for LLMs have inevitably led to benchmark contamination, resulting in unreliable evaluation results. To alleviate this issue, we propose a contamination-free and more challenging MCQ benchmark called MMLU-CF. This benchmark reassesses LLMs' understanding of world knowledge by averting both unintentional and malicious data leakage. To avoid unintentional data leakage, we source data from a broader domain and design three decontamination rules. To prevent malicious data leakage, we divide the benchmark into validation and test sets with similar difficulty and subject distributions. The test set remains closed-source to ensure reliable results, while the validation set is publicly available to promote transparency and facilitate independent verification. Our evaluation of mainstream LLMs reveals that the powerful GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on the test set, which indicates the effectiveness of our approach in creating a more rigorous and contamination-free evaluation standard. The GitHub repository is available at https://github.com/microsoft/MMLU-CF and the dataset refers to https://huggingface.co/datasets/microsoft/MMLU-CF.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "158",
        "title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport",
        "author": [
            "Borui Zhang",
            "Wenzhao Zheng",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15195",
        "abstract": "Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "159",
        "title": "DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation",
        "author": [
            "Wang Zhao",
            "Yan-Pei Cao",
            "Jiale Xu",
            "Yuejiang Dong",
            "Ying Shan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15200",
        "abstract": "Procedural Content Generation (PCG) is powerful in creating high-quality 3D contents, yet controlling it to produce desired shapes is difficult and often requires extensive parameter tuning. Inverse Procedural Content Generation aims to automatically find the best parameters under the input condition. However, existing sampling-based and neural network-based methods still suffer from numerous sample iterations or limited controllability. In this work, we present DI-PCG, a novel and efficient method for Inverse PCG from general image conditions. At its core is a lightweight diffusion transformer model, where PCG parameters are directly treated as the denoising target and the observed images as conditions to control parameter generation. DI-PCG is efficient and effective. With only 7.6M network parameters and 30 GPU hours to train, it demonstrates superior performance in recovering parameters accurately, and generalizing well to in-the-wild images. Quantitative and qualitative experiment results validate the effectiveness of DI-PCG in inverse PCG and image-to-3D generation tasks. DI-PCG offers a promising approach for efficient inverse PCG and represents a valuable exploration step towards a 3D generation path that models how to construct a 3D asset using parametric models.",
        "tags": [
            "3D",
            "Diffusion",
            "Diffusion Transformer",
            "Image-to-3D",
            "Transformer"
        ]
    },
    {
        "id": "160",
        "title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks",
        "author": [
            "Yushi Bai",
            "Shangqing Tu",
            "Jiajie Zhang",
            "Hao Peng",
            "Xiaozhi Wang",
            "Xin Lv",
            "Shulin Cao",
            "Jiazheng Xu",
            "Lei Hou",
            "Yuxiao Dong",
            "Jie Tang",
            "Juanzi Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15204",
        "abstract": "This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at https://longbench2.github.io.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "161",
        "title": "FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching",
        "author": [
            "Sucheng Ren",
            "Qihang Yu",
            "Ju He",
            "Xiaohui Shen",
            "Alan Yuille",
            "Liang-Chieh Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15205",
        "abstract": "Autoregressive (AR) modeling has achieved remarkable success in natural language processing by enabling models to generate text with coherence and contextual understanding through next token prediction. Recently, in image generation, VAR proposes scale-wise autoregressive modeling, which extends the next token prediction to the next scale prediction, preserving the 2D structure of images. However, VAR encounters two primary challenges: (1) its complex and rigid scale design limits generalization in next scale prediction, and (2) the generator's dependence on a discrete tokenizer with the same complex scale structure restricts modularity and flexibility in updating the tokenizer. To address these limitations, we introduce FlowAR, a general next scale prediction method featuring a streamlined scale design, where each subsequent scale is simply double the previous one. This eliminates the need for VAR's intricate multi-scale residual tokenizer and enables the use of any off-the-shelf Variational AutoEncoder (VAE). Our simplified design enhances generalization in next scale prediction and facilitates the integration of Flow Matching for high-quality image synthesis. We validate the effectiveness of FlowAR on the challenging ImageNet-256 benchmark, demonstrating superior generation performance compared to previous methods. Codes will be available at \\url{https://github.com/OliverRensu/FlowAR}.",
        "tags": [
            "Flow Matching",
            "VAE"
        ]
    },
    {
        "id": "162",
        "title": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving",
        "author": [
            "Shuo Xing",
            "Chengyuan Qian",
            "Yuping Wang",
            "Hongyuan Hua",
            "Kexin Tian",
            "Yang Zhou",
            "Zhengzhong Tu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15208",
        "abstract": "Since the advent of Multimodal Large Language Models (MLLMs), they have made a significant impact across a wide range of real-world applications, particularly in Autonomous Driving (AD). Their ability to process complex visual data and reason about intricate driving scenarios has paved the way for a new paradigm in end-to-end AD systems. However, the progress of developing end-to-end models for AD has been slow, as existing fine-tuning methods demand substantial resources, including extensive computational power, large-scale datasets, and significant funding. Drawing inspiration from recent advancements in inference computing, we propose OpenEMMA, an open-source end-to-end framework based on MLLMs. By incorporating the Chain-of-Thought reasoning process, OpenEMMA achieves significant improvements compared to the baseline when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates effectiveness, generalizability, and robustness across a variety of challenging driving scenarios, offering a more efficient and effective approach to autonomous driving. We release all the codes in https://github.com/taco-group/OpenEMMA.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "163",
        "title": "Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation",
        "author": [
            "Hadi Alzayer",
            "Philipp Henzler",
            "Jonathan T. Barron",
            "Jia-Bin Huang",
            "Pratul P. Srinivasan",
            "Dor Verbin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15211",
        "abstract": "Reconstructing the geometry and appearance of objects from photographs taken in different environments is difficult as the illumination and therefore the object appearance vary across captured images. This is particularly challenging for more specular objects whose appearance strongly depends on the viewing direction. Some prior approaches model appearance variation across images using a per-image embedding vector, while others use physically-based rendering to recover the materials and per-image illumination. Such approaches fail at faithfully recovering view-dependent appearance given significant variation in input illumination and tend to produce mostly diffuse results. We present an approach that reconstructs objects from images taken under different illuminations by first relighting the images under a single reference illumination with a multiview relighting diffusion model and then reconstructing the object's geometry and appearance with a radiance field architecture that is robust to the small remaining inconsistencies among the relit images. We validate our proposed approach on both synthetic and real datasets and demonstrate that it greatly outperforms existing techniques at reconstructing high-fidelity appearance from images taken under extreme illumination variation. Moreover, our approach is particularly effective at recovering view-dependent \"shiny\" appearance which cannot be reconstructed by prior methods.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "164",
        "title": "Scaling 4D Representations",
        "author": [
            "João Carreira",
            "Dilara Gokay",
            "Michael King",
            "Chuhan Zhang",
            "Ignacio Rocco",
            "Aravindh Mahendran",
            "Thomas Albert Keck",
            "Joseph Heyward",
            "Skanda Koppula",
            "Etienne Pot",
            "Goker Erdogan",
            "Yana Hasson",
            "Yi Yang",
            "Klaus Greff",
            "Guillaume Le Moing",
            "Sjoerd van Steenkiste",
            "Daniel Zoran",
            "Drew A. Hudson",
            "Pedro Vélez",
            "Luisa Polanía",
            "Luke Friedman",
            "Chris Duvarney",
            "Ross Goroshin",
            "Kelsey Allen",
            "Jacob Walker",
            "Rishabh Kabra",
            "Eric Aboussouan",
            "Jennifer Sun",
            "Thomas Kipf",
            "Carl Doersch",
            "Viorica Pătrăucean",
            "Dima Damen",
            "Pauline Luc",
            "Mehdi S. M. Sajjadi",
            "Andrew Zisserman"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15212",
        "abstract": "Scaling has not yet been convincingly demonstrated for pure self-supervised learning from video. However, prior work has focused evaluations on semantic-related tasks $\\unicode{x2013}$ action classification, ImageNet classification, etc. In this paper we focus on evaluating self-supervised learning on non-semantic vision tasks that are more spatial (3D) and temporal (+1D = 4D), such as camera pose estimation, point and object tracking, and depth estimation. We show that by learning from very large video datasets, masked auto-encoding (MAE) with transformer video models actually scales, consistently improving performance on these 4D tasks, as model size increases from 20M all the way to the largest by far reported self-supervised video model $\\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with many recent image and video models demonstrates the benefits of scaling 4D representations.",
        "tags": [
            "3D",
            "Depth Estimation",
            "Pose Estimation",
            "Transformer"
        ]
    },
    {
        "id": "165",
        "title": "Flowing from Words to Pixels: A Framework for Cross-Modality Evolution",
        "author": [
            "Qihao Liu",
            "Xi Yin",
            "Alan Yuille",
            "Andrew Brown",
            "Mannat Singh"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15213",
        "abstract": "Diffusion models, and their generalization, flow matching, have had a remarkable impact on the field of media generation. Here, the conventional approach is to learn the complex mapping from a simple source distribution of Gaussian noise to the target media distribution. For cross-modal tasks such as text-to-image generation, this same mapping from noise to image is learnt whilst including a conditioning mechanism in the model. One key and thus far relatively unexplored feature of flow matching is that, unlike Diffusion models, they are not constrained for the source distribution to be noise. Hence, in this paper, we propose a paradigm shift, and ask the question of whether we can instead train flow matching models to learn a direct mapping from the distribution of one modality to the distribution of another, thus obviating the need for both the noise distribution and conditioning mechanism. We present a general and simple framework, CrossFlow, for cross-modal flow matching. We show the importance of applying Variational Encoders to the input data, and introduce a method to enable Classifier-free guidance. Surprisingly, for text-to-image, CrossFlow with a vanilla transformer without cross attention slightly outperforms standard flow matching, and we show that it scales better with training steps and model size, while also allowing for interesting latent arithmetic which results in semantically meaningful edits in the output space. To demonstrate the generalizability of our approach, we also show that CrossFlow is on par with or outperforms the state-of-the-art for various cross-modal / intra-modal mapping tasks, viz. image captioning, depth estimation, and image super-resolution. We hope this paper contributes to accelerating progress in cross-modal media generation.",
        "tags": [
            "Depth Estimation",
            "Diffusion",
            "Flow Matching",
            "Super Resolution",
            "Text-to-Image",
            "Transformer"
        ]
    },
    {
        "id": "166",
        "title": "LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis",
        "author": [
            "Hanlin Wang",
            "Hao Ouyang",
            "Qiuyu Wang",
            "Wen Wang",
            "Ka Leong Cheng",
            "Qifeng Chen",
            "Yujun Shen",
            "Limin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15214",
        "abstract": "The intuitive nature of drag-based interaction has led to its growing adoption for controlling object trajectories in image-to-video synthesis. Still, existing methods that perform dragging in the 2D space usually face ambiguity when handling out-of-plane movements. In this work, we augment the interaction with a new dimension, i.e., the depth dimension, such that users are allowed to assign a relative depth for each point on the trajectory. That way, our new interaction paradigm not only inherits the convenience from 2D dragging, but facilitates trajectory control in the 3D space, broadening the scope of creativity. We propose a pioneering method for 3D trajectory control in image-to-video synthesis by abstracting object masks into a few cluster points. These points, accompanied by the depth information and the instance information, are finally fed into a video diffusion model as the control signal. Extensive experiments validate the effectiveness of our approach, dubbed LeviTor, in precisely manipulating the object movements when producing photo-realistic videos from static images. Project page: https://ppetrichor.github.io/levitor.github.io/",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "167",
        "title": "UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency",
        "author": [
            "Enis Simsar",
            "Alessio Tonioni",
            "Yongqin Xian",
            "Thomas Hofmann",
            "Federico Tombari"
        ],
        "pdf": "https://arxiv.org/pdf/2412.15216",
        "abstract": "We propose an unsupervised model for instruction-based image editing that eliminates the need for ground-truth edited images during training. Existing supervised methods depend on datasets containing triplets of input image, edited image, and edit instruction. These are generated by either existing editing methods or human-annotations, which introduce biases and limit their generalization ability. Our method addresses these challenges by introducing a novel editing mechanism called Cycle Edit Consistency (CEC), which applies forward and backward edits in one training step and enforces consistency in image and attention spaces. This allows us to bypass the need for ground-truth edited images and unlock training for the first time on datasets comprising either real image-caption pairs or image-caption-edit triplets. We empirically show that our unsupervised technique performs better across a broader range of edits with high fidelity and precision. By eliminating the need for pre-existing datasets of triplets, reducing biases associated with supervised methods, and proposing CEC, our work represents a significant advancement in unblocking scaling of instruction-based image editing.",
        "tags": [
            "Image Editing"
        ]
    }
]