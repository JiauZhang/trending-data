[
    {
        "id": "1",
        "title": "Evaluating Computational Accuracy of Large Language Models in Numerical Reasoning Tasks for Healthcare Applications",
        "author": [
            "Arjun R. Malghan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13936",
        "abstract": "Large Language Models (LLMs) have emerged as transformative tools in the healthcare sector, demonstrating remarkable capabilities in natural language understanding and generation. However, their proficiency in numerical reasoning, particularly in high-stakes domains like in clinical applications, remains underexplored. Numerical reasoning is critical in healthcare applications, influencing patient outcomes, treatment planning, and resource allocation. This study investigates the computational accuracy of LLMs in numerical reasoning tasks within healthcare contexts. Using a curated dataset of 1,000 numerical problems, encompassing real-world scenarios such as dosage calculations and lab result interpretations, the performance of a refined LLM based on the GPT-3 architecture was evaluated. The methodology includes prompt engineering, integration of fact-checking pipelines, and application of regularization techniques to enhance model accuracy and generalization. Key metrics such as precision, recall, and F1-score were utilized to assess the model's efficacy. The results indicate an overall accuracy of 84.10%, with improved performance in straightforward numerical tasks and challenges in multi-step reasoning. The integration of a fact-checking pipeline improved accuracy by 11%, underscoring the importance of validation mechanisms. This research highlights the potential of LLMs in healthcare numerical reasoning and identifies avenues for further refinement to support critical decision-making in clinical environments. The findings aim to contribute to the development of reliable, interpretable, and contextually relevant AI tools for healthcare.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "2",
        "title": "Fanar: An Arabic-Centric Multimodal Generative AI Platform",
        "author": [
            "Fanar Team",
            "Ummar Abbas",
            "Mohammad Shahmeer Ahmad",
            "Firoj Alam",
            "Enes Altinisik",
            "Ehsannedin Asgari",
            "Yazan Boshmaf",
            "Sabri Boughorbel",
            "Sanjay Chawla",
            "Shammur Chowdhury",
            "Fahim Dalvi",
            "Kareem Darwish",
            "Nadir Durrani",
            "Mohamed Elfeky",
            "Ahmed Elmagarmid",
            "Mohamed Eltabakh",
            "Masoomali Fatehkia",
            "Anastasios Fragkopoulos",
            "Maram Hasanain",
            "Majd Hawasly",
            "Mus'ab Husaini",
            "Soon-Gyo Jung",
            "Ji Kim Lucas",
            "Walid Magdy",
            "Safa Messaoud",
            "Abubakr Mohamed",
            "Tasnim Mohiuddin",
            "Basel Mousi",
            "Hamdy Mubarak",
            "Ahmad Musleh",
            "Zan Naeem",
            "Mourad Ouzzani",
            "Dorde Popovic",
            "Amin Sadeghi",
            "Husrev Taha Sencar",
            "Mohammed Shinoy",
            "Omar Sinan",
            "Yifan Zhang",
            "Ahmed Ali",
            "Yassine El Kheir",
            "Xiaosong Ma",
            "Chaoyi Ruan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13944",
        "abstract": "We present Fanar, a platform for Arabic-centric multimodal generative AI systems, that supports language, speech and image generation tasks. At the heart of Fanar are Fanar Star and Fanar Prime, two highly capable Arabic Large Language Models (LLMs) that are best in the class on well established benchmarks for similar sized models. Fanar Star is a 7B (billion) parameter model that was trained from scratch on nearly 1 trillion clean and deduplicated Arabic, English and Code tokens. Fanar Prime is a 9B parameter model continually trained on the Gemma-2 9B base model on the same 1 trillion token set. Both models are concurrently deployed and designed to address different types of prompts transparently routed through a custom-built orchestrator. The Fanar platform provides many other capabilities including a customized Islamic Retrieval Augmented Generation (RAG) system for handling religious prompts, a Recency RAG for summarizing information about current or recent events that have occurred after the pre-training data cut-off date. The platform provides additional cognitive capabilities including in-house bilingual speech recognition that supports multiple Arabic dialects, voice and image generation that is fine-tuned to better reflect regional characteristics. Finally, Fanar provides an attribution service that can be used to verify the authenticity of fact based generated content.\nThe design, development, and implementation of Fanar was entirely undertaken at Hamad Bin Khalifa University's Qatar Computing Research Institute (QCRI) and was sponsored by Qatar's Ministry of Communications and Information Technology to enable sovereign AI technology development.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "3",
        "title": "Self-Explanation in Social AI Agents",
        "author": [
            "Rhea Basappa",
            "Mustafa Tekman",
            "Hong Lu",
            "Benjamin Faught",
            "Sandeep Kakar",
            "Ashok K. Goel"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13945",
        "abstract": "Social AI agents interact with members of a community, thereby changing the behavior of the community. For example, in online learning, an AI social assistant may connect learners and thereby enhance social interaction. These social AI assistants too need to explain themselves in order to enhance transparency and trust with the learners. We present a method of self-explanation that uses introspection over a self-model of an AI social assistant. The self-model is captured as a functional model that specifies how the methods of the agent use knowledge to achieve its tasks. The process of generating self-explanations uses Chain of Thought to reflect on the self-model and ChatGPT to provide explanations about its functioning. We evaluate the self-explanation of the AI social assistant for completeness and correctness. We also report on its deployment in a live class.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "4",
        "title": "Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks",
        "author": [
            "Diego Gosmar",
            "Deborah A. Dahl"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13946",
        "abstract": "Hallucinations remain a significant challenge in current Generative AI models, undermining trust in AI systems and their reliability. This study investigates how orchestrating multiple specialized Artificial Intelligent Agents can help mitigate such hallucinations, with a focus on systems leveraging Natural Language Processing (NLP) to facilitate seamless agent interactions. To achieve this, we design a pipeline that introduces over three hundred prompts, purposefully crafted to induce hallucinations, into a front-end agent. The outputs are then systematically reviewed and refined by second- and third-level agents, each employing distinct large language models and tailored strategies to detect unverified claims, incorporate explicit disclaimers, and clarify speculative content. Additionally, we introduce a set of novel Key Performance Indicators (KPIs) specifically designed to evaluate hallucination score levels. A dedicated fourth-level AI agent is employed to evaluate these KPIs, providing detailed assessments and ensuring accurate quantification of shifts in hallucination-related behaviors. A core component of this investigation is the use of the OVON (Open Voice Network) framework, which relies on universal NLP-based interfaces to transfer contextual information among agents. Through structured JSON messages, each agent communicates its assessment of the hallucination likelihood and the reasons underlying questionable content, thereby enabling the subsequent stage to refine the text without losing context. The results demonstrate that employing multiple specialized agents capable of interoperating with each other through NLP-based agentic frameworks can yield promising outcomes in hallucination mitigation, ultimately bolstering trust within the AI community.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "5",
        "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
        "author": [
            "Lilian Some",
            "Wenli Yang",
            "Michael Bain",
            "Byeong Kang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13947",
        "abstract": "The rapid development of artificial intelligence has brought about substantial advancements in the field. One promising direction is the integration of Large Language Models (LLMs) with structured knowledge-based systems. This approach aims to enhance AI capabilities by combining the generative language understanding of LLMs with the precise knowledge representation of structured systems. This survey explores the synergy between LLMs and knowledge bases, focusing on real-world applications and addressing associated technical, operational, and ethical challenges. Through a comprehensive literature review, the study identifies critical issues and evaluates existing solutions. The paper highlights the benefits of integrating generative AI with knowledge bases, including improved data contextualization, enhanced model accuracy, and better utilization of knowledge resources. The findings provide a detailed overview of the current state of research, identify key gaps, and offer actionable recommendations. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "6",
        "title": "Longitudinal Abuse and Sentiment Analysis of Hollywood Movie Dialogues using LLMs",
        "author": [
            "Rohitash Chandra",
            "Guoxiang Ren",
            "Group-H"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13948",
        "abstract": "Over the past decades, there has been an increasing concern about the prevalence of abusive and violent content in Hollywood movies. This study uses Large Language Models (LLMs) to explore the longitudinal abuse and sentiment analysis of Hollywood Oscar and blockbuster movie dialogues from 1950 to 2024. By employing fine-tuned LLMs, we analyze subtitles for over a thousand movies categorised into four genres to examine the trends and shifts in emotional and abusive content over the past seven decades. Our findings reveal significant temporal changes in movie dialogues, which reflect broader social and cultural influences. Overall, the emotional tendencies in the films are diverse, and the detection of abusive content also exhibits significant fluctuations. The results show a gradual rise in abusive content in recent decades, reflecting social norms and regulatory policy changes. Genres such as thrillers still present a higher frequency of abusive content that emphasises the ongoing narrative role of violence and conflict. At the same time, underlying positive emotions such as humour and optimism remain prevalent in most of the movies. Furthermore, the gradual increase of abusive content in movie dialogues has been significant over the last two decades, where Oscar-nominated movies overtook the top ten blockbusters.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "7",
        "title": "Can OpenAI o1 Reason Well in Ophthalmology? A 6,990-Question Head-to-Head Evaluation Study",
        "author": [
            "Sahana Srinivasan",
            "Xuguang Ai",
            "Minjie Zou",
            "Ke Zou",
            "Hyunjae Kim",
            "Thaddaeus Wai Soon Lo",
            "Krithi Pushpanathan",
            "Yiming Kong",
            "Anran Li",
            "Maxwell Singer",
            "Kai Jin",
            "Fares Antaki",
            "David Ziyou Chen",
            "Dianbo Liu",
            "Ron A. Adelman",
            "Qingyu Chen",
            "Yih Chung Tham"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13949",
        "abstract": "Question: What is the performance and reasoning ability of OpenAI o1 compared to other large language models in addressing ophthalmology-specific questions?\nFindings: This study evaluated OpenAI o1 and five LLMs using 6,990 ophthalmological questions from MedMCQA. O1 achieved the highest accuracy (0.88) and macro-F1 score but ranked third in reasoning capabilities based on text-generation metrics. Across subtopics, o1 ranked first in ``Lens'' and ``Glaucoma'' but second to GPT-4o in ``Corneal and External Diseases'', ``Vitreous and Retina'' and ``Oculoplastic and Orbital Diseases''. Subgroup analyses showed o1 performed better on queries with longer ground truth explanations.\nMeaning: O1's reasoning enhancements may not fully extend to ophthalmology, underscoring the need for domain-specific refinements to optimize performance in specialized fields like ophthalmology.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "8",
        "title": "The Dual-use Dilemma in LLMs: Do Empowering Ethical Capacities Make a Degraded Utility?",
        "author": [
            "Yiyi Zhang",
            "Xingyu Chen",
            "Kexin Chen",
            "Yuyang Du",
            "Xilin Dang",
            "Pheng-Ann Heng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13952",
        "abstract": "Recent years have witnessed extensive efforts to enhance Large Language Models (LLMs) across various domains, alongside growing attention to their ethical implications. However, a critical challenge remains largely overlooked: LLMs must balance between rejecting harmful requests for safety and accommodating legitimate ones for utility. This paper presents a Direct Preference Optimization (DPO) based alignment framework that achieves better overall performance by addressing this ethical-utility trade-off, using chemical domain applications as a proof-of-concept. Our alignment pipeline starts with a GPT-assisted three-phase data generation scheme, in which we create LibraChemQA, a chemical question-answering dataset comprising 31.6k triplet instances. By incorporating an innovative balanced seed in the data generation process, our framework systematically considers both legitimate and illegitimate requests. The framework also introduces a rephrasing mechanism for efficient data augmentation that enhances the model's chemical comprehension. We further develop a novel hybrid evaluation scheme with LLM judges for precise assessment of both safety and utility. Experimental results demonstrate our model's substantial improvements in overall performance where both safety and utility are considered - our resulting model, LibraChem, outperforms leading LLMs including Claude-3, GPT-4o, and LLaMA-3 by margins of 13.44%, 7.16%, and 7.10% respectively on our released benchmark.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "9",
        "title": "Redundancy Principles for MLLMs Benchmarks",
        "author": [
            "Zicheng Zhang",
            "Xiangyu Zhao",
            "Xinyu Fang",
            "Chunyi Li",
            "Xiaohong Liu",
            "Xiongkuo Min",
            "Haodong Duan",
            "Kai Chen",
            "Guangtao Zhai"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13953",
        "abstract": "With the rapid iteration of Multi-modality Large Language Models (MLLMs) and the evolving demands of the field, the number of benchmarks produced annually has surged into the hundreds. The rapid growth has inevitably led to significant redundancy among benchmarks. Therefore, it is crucial to take a step back and critically assess the current state of redundancy and propose targeted principles for constructing effective MLLM benchmarks. In this paper, we focus on redundancy from three key perspectives: 1) Redundancy of benchmark capability dimensions, 2) Redundancy in the number of test questions, and 3) Cross-benchmark redundancy within specific domains. Through the comprehensive analysis over hundreds of MLLMs' performance across more than 20 benchmarks, we aim to quantitatively measure the level of redundancy lies in existing MLLM evaluations, provide valuable insights to guide the future development of MLLM benchmarks, and offer strategies to refine and address redundancy issues effectively.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "10",
        "title": "Chat3GPP: An Open-Source Retrieval-Augmented Generation Framework for 3GPP Documents",
        "author": [
            "Long Huang",
            "Ming Zhao",
            "Limin Xiao",
            "Xiujun Zhang",
            "Jungang Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13954",
        "abstract": "The 3rd Generation Partnership Project (3GPP) documents is key standards in global telecommunications, while posing significant challenges for engineers and researchers in the telecommunications field due to the large volume and complexity of their contents as well as the frequent updates. Large language models (LLMs) have shown promise in natural language processing tasks, but their general-purpose nature limits their effectiveness in specific domains like telecommunications. To address this, we propose Chat3GPP, an open-source retrieval-augmented generation (RAG) framework tailored for 3GPP specifications. By combining chunking strategies, hybrid retrieval and efficient indexing methods, Chat3GPP can efficiently retrieve relevant information and generate accurate responses to user queries without requiring domain-specific fine-tuning, which is both flexible and scalable, offering significant potential for adapting to other technical standards beyond 3GPP. We evaluate Chat3GPP on two telecom-specific datasets and demonstrate its superior performance compared to existing methods, showcasing its potential for downstream tasks like protocol generation and code automation.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "11",
        "title": "Guided Persona-based AI Surveys: Can we replicate personal mobility preferences at scale using LLMs?",
        "author": [
            "Ioannis Tzachristas",
            "Santhanakrishnan Narayanan",
            "Constantinos Antoniou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13955",
        "abstract": "This study explores the potential of Large Language Models (LLMs) to generate artificial surveys, with a focus on personal mobility preferences in Germany. By leveraging LLMs for synthetic data creation, we aim to address the limitations of traditional survey methods, such as high costs, inefficiency and scalability challenges. A novel approach incorporating \"Personas\" - combinations of demographic and behavioural attributes - is introduced and compared to five other synthetic survey methods, which vary in their use of real-world data and methodological complexity. The MiD 2017 dataset, a comprehensive mobility survey in Germany, serves as a benchmark to assess the alignment of synthetic data with real-world patterns. The results demonstrate that LLMs can effectively capture complex dependencies between demographic attributes and preferences while offering flexibility to explore hypothetical scenarios. This approach presents valuable opportunities for transportation planning and social science research, enabling scalable, cost-efficient and privacy-preserving data generation.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "12",
        "title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models",
        "author": [
            "Qinggang Zhang",
            "Shengyuan Chen",
            "Yuanchen Bei",
            "Zheng Yuan",
            "Huachi Zhou",
            "Zijin Hong",
            "Junnan Dong",
            "Hao Chen",
            "Yi Chang",
            "Xiao Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13958",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks, yet their application to specialized domains remains challenging due to the need for deep expertise. Retrieval-augmented generation (RAG) has emerged as a promising solution to customize LLMs for professional fields by seamlessly integrating external knowledge bases, enabling real-time access to domain-specific expertise during inference. Despite its potential, traditional RAG systems, based on flat text retrieval, face three critical challenges: (i) complex query understanding in professional contexts, (ii) difficulties in knowledge integration across distributed sources, and (iii) system efficiency bottlenecks at scale. This survey presents a systematic analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new paradigm that revolutionizes domain-specific LLM applications. GraphRAG addresses traditional RAG limitations through three key innovations: (i) graph-structured knowledge representation that explicitly captures entity relationships and domain hierarchies, (ii) efficient graph-based retrieval techniques that enable context-preserving knowledge retrieval with multihop reasoning ability, and (iii) structure-aware knowledge integration algorithms that leverage retrieved knowledge for accurate and logical coherent generation of LLMs. In this survey, we systematically analyze the technical foundations of GraphRAG and examine current implementations across various professional domains, identifying key technical challenges and promising research directions. All the related resources of GraphRAG, including research papers, open-source data, and projects, are collected for the community in \\textcolor{blue}{\\url{https://github.com/DEEP-PolyU/Awesome-GraphRAG}}.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "13",
        "title": "Assisting Mathematical Formalization with A Learning-based Premise Retriever",
        "author": [
            "Yicheng Tao",
            "Haotian Liu",
            "Shanwen Wang",
            "Hongteng Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13959",
        "abstract": "Premise selection is a crucial yet challenging step in mathematical formalization, especially for users with limited experience. Due to the lack of available formalization projects, existing approaches that leverage language models often suffer from data scarcity. In this work, we introduce an innovative method for training a premise retriever to support the formalization of mathematics. Our approach employs a BERT model to embed proof states and premises into a shared latent space. The retrieval model is trained within a contrastive learning framework and incorporates a domain-specific tokenizer along with a fine-grained similarity computation method. Experimental results show that our model is highly competitive compared to existing baselines, achieving strong performance while requiring fewer computational resources. Performance is further enhanced through the integration of a re-ranking module. To streamline the formalization process, we will release a search engine that enables users to query Mathlib theorems directly using proof states, significantly improving accessibility and efficiency. Codes are available at https://github.com/ruc-ai4math/Premise-Retrieval.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "14",
        "title": "Advancing the Understanding and Evaluation of AR-Generated Scenes: When Vision-Language Models Shine and Stumble",
        "author": [
            "Lin Duan",
            "Yanming Xiu",
            "Maria Gorlatova"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13964",
        "abstract": "Augmented Reality (AR) enhances the real world by integrating virtual content, yet ensuring the quality, usability, and safety of AR experiences presents significant challenges. Could Vision-Language Models (VLMs) offer a solution for the automated evaluation of AR-generated scenes? Could Vision-Language Models (VLMs) offer a solution for the automated evaluation of AR-generated scenes? In this study, we evaluate the capabilities of three state-of-the-art commercial VLMs -- GPT, Gemini, and Claude -- in identifying and describing AR scenes. For this purpose, we use DiverseAR, the first AR dataset specifically designed to assess VLMs' ability to analyze virtual content across a wide range of AR scene complexities. Our findings demonstrate that VLMs are generally capable of perceiving and describing AR scenes, achieving a True Positive Rate (TPR) of up to 93\\% for perception and 71\\% for description. While they excel at identifying obvious virtual objects, such as a glowing apple, they struggle when faced with seamlessly integrated content, such as a virtual pot with realistic shadows. Our results highlight both the strengths and the limitations of VLMs in understanding AR scenarios. We identify key factors affecting VLM performance, including virtual content placement, rendering quality, and physical plausibility. This study underscores the potential of VLMs as tools for evaluating the quality of AR experiences.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "15",
        "title": "ZKLoRA: Efficient Zero-Knowledge Proofs for LoRA Verification",
        "author": [
            "Bidhan Roy",
            "Peter Potash",
            "Marcos Villagra"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13965",
        "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted method for customizing large-scale language models. In distributed, untrusted training environments, an open source base model user may want to use LoRA weights created by an external contributor, leading to two requirements: (1) the base model user must confirm that the LoRA weights are effective when paired with the intended base model, and (2) the LoRA contributor must keep their proprietary weights private until compensation is assured.\nWe present ZKLoRA, a zero-knowledge verification protocol that relies on succinct proofs and our novel Multi-Party Inference procedure to verify LoRA-base model compatibility without exposing LoRA weights. ZKLoRA produces deterministic correctness guarantees and validates each LoRA module in only 1-2 seconds on state-of-the-art large language models. This low-latency approach enables nearly real-time verification and promotes secure collaboration among geographically decentralized teams and contract-based training pipelines. The protocol ensures that the delivered LoRA module works as claimed, safeguarding the contributor's intellectual property while providing the base model user with verification of compatibility and lineage.",
        "tags": [
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "16",
        "title": "InsTex: Indoor Scenes Stylized Texture Synthesis",
        "author": [
            "Yunfan Zhang",
            "Zhiwei Xiong",
            "Zhiqi Shen",
            "Guosheng Lin",
            "Hao Wang",
            "Nicolas Vun"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13969",
        "abstract": "Generating high-quality textures for 3D scenes is crucial for applications in interior design, gaming, and augmented/virtual reality (AR/VR). Although recent advancements in 3D generative models have enhanced content creation, significant challenges remain in achieving broad generalization and maintaining style consistency across multiple viewpoints. Current methods, such as 2D diffusion models adapted for 3D texturing, suffer from lengthy processing times and visual artifacts, while approaches driven by 3D data often fail to generalize effectively. To overcome these challenges, we introduce InsTex, a two-stage architecture designed to generate high-quality, style-consistent textures for 3D indoor scenes. InsTex utilizes depth-to-image diffusion priors in a coarse-to-fine pipeline, first generating multi-view images with a pre-trained 2D diffusion model and subsequently refining the textures for consistency. Our method supports both textual and visual prompts, achieving state-of-the-art results in visual quality and quantitative metrics, and demonstrates its effectiveness across various 3D texturing applications.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "17",
        "title": "GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic Gaussian Splatting",
        "author": [
            "Junzhe Jiang",
            "Chun Gu",
            "Yurui Chen",
            "Li Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13971",
        "abstract": "LiDAR novel view synthesis (NVS) has emerged as a novel task within LiDAR simulation, offering valuable simulated point cloud data from novel viewpoints to aid in autonomous driving systems. However, existing LiDAR NVS methods typically rely on neural radiance fields (NeRF) as their 3D representation, which incurs significant computational costs in both training and rendering. Moreover, NeRF and its variants are designed for symmetrical scenes, making them ill-suited for driving scenarios. To address these challenges, we propose GS-LiDAR, a novel framework for generating realistic LiDAR point clouds with panoramic Gaussian splatting. Our approach employs 2D Gaussian primitives with periodic vibration properties, allowing for precise geometric reconstruction of both static and dynamic elements in driving scenarios. We further introduce a novel panoramic rendering technique with explicit ray-splat intersection, guided by panoramic LiDAR supervision. By incorporating intensity and ray-drop spherical harmonic (SH) coefficients into the Gaussian primitives, we enhance the realism of the rendered point clouds. Extensive experiments on KITTI-360 and nuScenes demonstrate the superiority of our method in terms of quantitative metrics, visual quality, as well as training and rendering efficiency.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "18",
        "title": "3DGS$^2$: Near Second-order Converging 3D Gaussian Splatting",
        "author": [
            "Lei Lan",
            "Tianjia Shao",
            "Zixuan Lu",
            "Yu Zhang",
            "Chenfanfu Jiang",
            "Yin Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13975",
        "abstract": "3D Gaussian Splatting (3DGS) has emerged as a mainstream solution for novel view synthesis and 3D reconstruction. By explicitly encoding a 3D scene using a collection of Gaussian kernels, 3DGS achieves high-quality rendering with superior efficiency. As a learning-based approach, 3DGS training has been dealt with the standard stochastic gradient descent (SGD) method, which offers at most linear convergence. Consequently, training often requires tens of minutes, even with GPU acceleration. This paper introduces a (near) second-order convergent training algorithm for 3DGS, leveraging its unique properties. Our approach is inspired by two key observations. First, the attributes of a Gaussian kernel contribute independently to the image-space loss, which endorses isolated and local optimization algorithms. We exploit this by splitting the optimization at the level of individual kernel attributes, analytically constructing small-size Newton systems for each parameter group, and efficiently solving these systems on GPU threads. This achieves Newton-like convergence per training image without relying on the global Hessian. Second, kernels exhibit sparse and structured coupling across input images. This property allows us to effectively utilize spatial information to mitigate overshoot during stochastic training. Our method converges an order faster than standard GPU-based 3DGS training, requiring over $10\\times$ fewer iterations while maintaining or surpassing the quality of the compared with the SGD-based 3DGS reconstructions.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "19",
        "title": "Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful Content Moderation Using Large Language Models",
        "author": [
            "Akash Bonagiri",
            "Lucen Li",
            "Rajvardhan Oak",
            "Zeerak Babar",
            "Magdalena Wojcieszak",
            "Anshuman Chhabra"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13976",
        "abstract": "The prevalence of harmful content on social media platforms poses significant risks to users and society, necessitating more effective and scalable content moderation strategies. Current approaches rely on human moderators, supervised classifiers, and large volumes of training data, and often struggle with scalability, subjectivity, and the dynamic nature of harmful content (e.g., violent content, dangerous challenge trends, etc.). To bridge these gaps, we utilize Large Language Models (LLMs) to undertake few-shot dynamic content moderation via in-context learning. Through extensive experiments on multiple LLMs, we demonstrate that our few-shot approaches can outperform existing proprietary baselines (Perspective and OpenAI Moderation) as well as prior state-of-the-art few-shot learning methods, in identifying harm. We also incorporate visual information (video thumbnails) and assess if different multimodal techniques improve model performance. Our results underscore the significant benefits of employing LLM based methods for scalable and dynamic harmful content moderation online.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "20",
        "title": "Chain of Grounded Objectives: Bridging Process and Goal-oriented Prompting for Code Generation",
        "author": [
            "Sangyeop Yeo",
            "Seung-won Hwang",
            "Yu-Seung Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13978",
        "abstract": "The use of Large Language Models (LLMs) for code generation has gained significant attention in recent years. Existing methods often aim to improve the quality of generated code by incorporating additional contextual information or guidance into input prompts. Many of these approaches adopt sequential reasoning strategies, mimicking human-like step-by-step thinking. However, such strategies may constrain flexibility, as they do not always align with the structured characteristics of programming languages. This paper introduces the Chain of Grounded Objectives (CGO), a method that embeds functional objectives into input prompts to enhance code generation. By leveraging appropriately structured objectives as input and avoiding explicit sequential procedures, CGO adapts effectively to the structured nature of programming tasks. Empirical evaluations demonstrate that CGO effectively enhances code generation, addressing limitations of existing approaches.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "21",
        "title": "Attribute-based Visual Reprogramming for Image Classification with CLIP",
        "author": [
            "Chengyi Cai",
            "Zesheng Ye",
            "Lei Feng",
            "Jianzhong Qi",
            "Feng Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13982",
        "abstract": "Visual reprogramming (VR) reuses pre-trained vision models for downstream image classification tasks by adding trainable noise patterns to inputs. When applied to vision-language models (e.g., CLIP), existing VR approaches follow the same pipeline used in vision models (e.g., ResNet, ViT), where ground-truth class labels are inserted into fixed text templates to guide the optimization of VR patterns. This label-based approach, however, overlooks the rich information and diverse attribute-guided textual representations that CLIP can exploit, which may lead to the misclassification of samples. In this paper, we propose Attribute-based Visual Reprogramming (AttrVR) for CLIP, utilizing descriptive attributes (DesAttrs) and distinctive attributes (DistAttrs), which respectively represent common and unique feature descriptions for different classes. Besides, as images of the same class may reflect different attributes after VR, AttrVR iteratively refines patterns using the $k$-nearest DesAttrs and DistAttrs for each image sample, enabling more dynamic and sample-specific optimization. Theoretically, AttrVR is shown to reduce intra-class variance and increase inter-class separation. Empirically, it achieves superior performance in 12 downstream tasks for both ViT-based and ResNet-based CLIP. The success of AttrVR facilitates more effective integration of VR from unimodal vision models into vision-language models. Our code is available at https://github.com/tmlr-group/AttrVR.",
        "tags": [
            "CLIP",
            "ViT"
        ]
    },
    {
        "id": "22",
        "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models",
        "author": [
            "Yang Fan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13983",
        "abstract": "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the issue of data contamination has become increasingly severe, leading to potential overestimation of model performance during evaluation. To address this, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data evaluation method aimed at mitigating the impact of data contamination on evaluation reliability. AdEval extracts key knowledge points and main ideas to align dynamically generated questions with static data's core concepts. It also leverages online search to provide detailed explanations of related knowledge points, thereby creating high-quality evaluation samples with robust knowledge support. Furthermore, AdEval incorporates mechanisms to control the number and complexity of questions, enabling dynamic alignment and flexible adjustment. This ensures that the generated questions align with the complexity of static data while supporting varied complexity levels. Based on Bloom's taxonomy, AdEval conducts a multi-dimensional evaluation of LLMs across six cognitive levels: remembering, understanding, applying, analyzing, evaluating, and creating. Experimental results on multiple datasets demonstrate that AdEval effectively reduces the impact of data contamination on evaluation outcomes, enhancing both the fairness and reliability of the evaluation process.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "23",
        "title": "OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting",
        "author": [
            "Xing Hu",
            "Yuan Cheng",
            "Dawei Yang",
            "Zukang Xu",
            "Zhihang Yuan",
            "Jiangyong Yu",
            "Chen Xu",
            "Zhe Jiang",
            "Sifan Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13987",
        "abstract": "Post-training quantization (PTQ) has emerged as a widely adopted technique for compressing and accelerating Large Language Models (LLMs). The major challenge in LLM quantization is that uneven and heavy-tailed data distributions can expand the quantization range, thereby reducing bit precision for most values. Recent methods attempt to eliminate outliers and balance inter-channel differences by employing linear transformations; however, they remain heuristic and are often overlook optimizing the data distribution across the entire quantization http://space.In this paper, we introduce Quantization Space Utilization Rate (QSUR), a novel metric that effectively assesses the quantizability of transformed data by measuring the space utilization of the data in the quantization space. We complement QSUR with mathematical derivations that examine the effects and limitations of various transformations, guiding our development of Orthogonal and Scaling Transformation-based Quantization (OSTQuant). OSQuant employs a learnable equivalent transformation, consisting of an orthogonal transformation and a scaling transformation, to optimize the distributions of weights and activations across the entire quantization space. Futhermore, we propose the KL-Top loss function, designed to mitigate noise during optimization while retaining richer semantic information within the limited calibration data imposed by PTQ. OSTQuant outperforms existing work on various LLMs and benchmarks. In the W4-only setting, it retains 99.5\\% of the floating-point accuracy. In the more challenging W4A4KV4 configuration, OSTQuant reduces the performance gap by 32\\% on the LLaMA-3-8B model compared to state-of-the-art methods. \\href{https://github.com/BrotherHappy/OSTQuant}{https://github.com/BrotherHappy/OSTQuant}.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "24",
        "title": "FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting",
        "author": [
            "Wenzhen Yue",
            "Yong Liu",
            "Xianghua Ying",
            "Bowei Xing",
            "Ruohao Guo",
            "Ji Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13989",
        "abstract": "This paper presents \\textbf{FreEformer}, a simple yet effective model that leverages a \\textbf{Fre}quency \\textbf{E}nhanced Trans\\textbf{former} for multivariate time series forecasting. Our work is based on the assumption that the frequency spectrum provides a global perspective on the composition of series across various frequencies and is highly suitable for robust representation learning. Specifically, we first convert time series into the complex frequency domain using the Discrete Fourier Transform (DFT). The Transformer architecture is then applied to the frequency spectra to capture cross-variate dependencies, with the real and imaginary parts processed independently. However, we observe that the vanilla attention matrix exhibits a low-rank characteristic, thus limiting representation diversity. This could be attributed to the inherent sparsity of the frequency domain and the strong-value-focused nature of Softmax in vanilla attention. To address this, we enhance the vanilla attention mechanism by introducing an additional learnable matrix to the original attention matrix, followed by row-wise L1 normalization. Theoretical analysis~demonstrates that this enhanced attention mechanism improves both feature diversity and gradient flow. Extensive experiments demonstrate that FreEformer consistently outperforms state-of-the-art models on eighteen real-world benchmarks covering electricity, traffic, weather, healthcare and finance. Notably, the enhanced attention mechanism also consistently improves the performance of state-of-the-art Transformer-based forecasters.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "25",
        "title": "CAPRAG: A Large Language Model Solution for Customer Service and Automatic Reporting using Vector and Graph Retrieval-Augmented Generation",
        "author": [
            "Hamza Landolsi",
            "Kais Letaief",
            "Nizar Taghouti",
            "Ines Abdeljaoued-Tej"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13993",
        "abstract": "The introduction of new features and services in the banking sector often overwhelms customers, creating an opportunity for banks to enhance user experience through financial chatbots powered by large language models (LLMs). We initiated an AI agent designed to provide customers with relevant information about banking services and insights from annual reports. We proposed a hybrid Customer Analysis Pipeline Retrieval-Augmented Generation (CAPRAG) that effectively addresses both relationship-based and contextual queries, thereby improving customer engagement in the digital banking landscape. To implement this, we developed a processing pipeline to refine text data, which we utilized in two main frameworks: Vector RAG and Graph RAG. This dual approach enables us to populate both vector and graph databases with processed data for efficient retrieval. The Cypher query component is employed to effectively query the graph database. When a user submits a query, it is first expanded by a query expansion module before being routed to construct a final query from the hybrid Knowledge Base (KB). This final query is then sent to an open-source LLM for response generation. Overall, our innovative, designed to international banks, serves bank's customers in an increasingly complex digital environment, enhancing clarity and accessibility of information.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "26",
        "title": "Predictive Learning in Energy-based Models with Attractor Structures",
        "author": [
            "Xingsi Dong",
            "Pengxiang Yuan",
            "Si Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13997",
        "abstract": "Predictive models are highly advanced in understanding the mechanisms of brain function. Recent advances in machine learning further underscore the power of prediction for optimal representation in learning. However, there remains a gap in creating a biologically plausible model that explains how the neural system achieves prediction. In this paper, we introduce a framework that employs an energy-based model (EBM) to capture the nuanced processes of predicting observation after action within the neural system, encompassing prediction, learning, and inference. We implement the EBM with a hierarchical structure and integrate a continuous attractor neural network for memory, constructing a biologically plausible model. In experimental evaluations, our model demonstrates efficacy across diverse scenarios. The range of actions includes eye movement, motion in environments, head turning, and static observation while the environment changes. Our model not only makes accurate predictions for environments it was trained on, but also provides reasonable predictions for unseen environments, matching the performances of machine learning methods in multiple tasks. We hope that this study contributes to a deep understanding of how the neural system performs prediction.",
        "tags": [
            "Energy-Based Models"
        ]
    },
    {
        "id": "27",
        "title": "Framework for Progressive Knowledge Fusion in Large Language Models Through Structured Conceptual Redundancy Analysis",
        "author": [
            "Joseph Sakau",
            "Evander Kozlowski",
            "Roderick Thistledown",
            "Basil Steinberger"
        ],
        "pdf": "https://arxiv.org/pdf/2501.13999",
        "abstract": "The organization of latent knowledge within large-scale models poses unique challenges when addressing overlapping representations and optimizing contextual accuracy. Conceptual redundancies embedded across layers often result in inefficiencies that affect both computational demands and task-specific outcomes. A framework was proposed to restructure these redundancies through advanced clustering techniques and dynamic thresholding, ensuring that critical semantic relationships are preserved while removing unnecessary overlaps. Evaluations revealed improved memory efficiency and faster inference times, alongside better alignment in latent knowledge clusters that enhanced interpretability. Improvements in error rates and adversarial robustness suggest that restructuring redundancies has broader implications for increasing model reliability across diverse applications. Comparative analyses highlighted reductions in resource consumption and notable gains in performance, particularly in translation and summarization tasks. Energy metrics demonstrated significant savings during training phases, further validating the practicality of the approach for real-world deployments. Representational fidelity was also enhanced, with latent space evaluations indicating better cluster alignment and higher semantic consistency. The methodology bridges a key gap in model optimization through directly addressing redundancies at the structural level. Its application opens avenues for scalable, efficient, and contextually aware systems that can adapt to complex, domain-specific tasks without compromising on performance.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "28",
        "title": "Local Control Networks (LCNs): Optimizing Flexibility in Neural Network Data Pattern Capture",
        "author": [
            "Hy Nguyen",
            "Duy Khoa Pham",
            "Srikanth Thudumu",
            "Hung Du",
            "Rajesh Vasa",
            "Kon Mouzakis"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14000",
        "abstract": "The widespread use of Multi-layer perceptrons (MLPs) often relies on a fixed activation function (e.g., ReLU, Sigmoid, Tanh) for all nodes within the hidden layers. While effective in many scenarios, this uniformity may limit the networks ability to capture complex data patterns. We argue that employing the same activation function at every node is suboptimal and propose leveraging different activation functions at each node to increase flexibility and adaptability. To achieve this, we introduce Local Control Networks (LCNs), which leverage B-spline functions to enable distinct activation curves at each node. Our mathematical analysis demonstrates the properties and benefits of LCNs over conventional MLPs. In addition, we demonstrate that more complex architectures, such as Kolmogorov-Arnold Networks (KANs), are unnecessary in certain scenarios, and LCNs can be a more efficient alternative. Empirical experiments on various benchmarks and datasets validate our theoretical findings. In computer vision tasks, LCNs achieve marginal improvements over MLPs and outperform KANs by approximately 5\\%, while also being more computationally efficient than KANs. In basic machine learning tasks, LCNs show a 1\\% improvement over MLPs and a 0.6\\% improvement over KANs. For symbolic formula representation tasks, LCNs perform on par with KANs, with both architectures outperforming MLPs. Our findings suggest that diverse activations at the node level can lead to improved performance and efficiency.",
        "tags": [
            "Kolmogorov-Arnold Networks"
        ]
    },
    {
        "id": "29",
        "title": "Advancing Math Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages",
        "author": [
            "Zui Chen",
            "Tianqiao Liu",
            "Mi Tian",
            "Qing Tong",
            "Weiqi Luo",
            "Zitao Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14002",
        "abstract": "Advancements in LLMs have significantly expanded their capabilities across various domains. However, mathematical reasoning remains a challenging area, prompting the development of math-specific LLMs. These models typically follow a two-stage training paradigm: pre-training with math-related corpora and post-training with problem datasets for SFT. Despite these efforts, the improvements in mathematical reasoning achieved through continued pre-training (CPT) are often less significant compared to those obtained via SFT. This study addresses this discrepancy by exploring alternative strategies during the pre-training phase, focusing on the use of problem-solving data over general mathematical corpora. We investigate three primary research questions: (1) Can problem-solving data enhance the model's mathematical reasoning capabilities more effectively than general mathematical corpora during CPT? (2) Are synthetic data from the same source equally effective, and which synthesis methods are most efficient? (3) How do the capabilities developed from the same problem-solving data differ between the CPT and SFT stages, and what factors contribute to these differences? Our findings indicate that problem-solving data significantly enhances the model's mathematical capabilities compared to general mathematical corpora. We also identify effective data synthesis methods, demonstrating that the tutorship amplification synthesis method achieves the best performance. Furthermore, while SFT facilitates instruction-following abilities, it underperforms compared to CPT with the same data, which can be partially attributed to its poor learning capacity for hard multi-step problem-solving data. These insights provide valuable guidance for optimizing the mathematical reasoning capabilities of LLMs, culminating in our development of a powerful mathematical base model called JiuZhang-8B.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "30",
        "title": "ME-CPT: Multi-Task Enhanced Cross-Temporal Point Transformer for Urban 3D Change Detection",
        "author": [
            "Luqi Zhang",
            "Haiping Wang",
            "Chong Liu",
            "Zhen Dong",
            "Bisheng Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14004",
        "abstract": "The point clouds collected by the Airborne Laser Scanning (ALS) system provide accurate 3D information of urban land covers. By utilizing multi-temporal ALS point clouds, semantic changes in urban area can be captured, demonstrating significant potential in urban planning, emergency management, and infrastructure maintenance. Existing 3D change detection methods struggle to efficiently extract multi-class semantic information and change features, still facing the following challenges: (1) the difficulty of accurately modeling cross-temporal point clouds spatial relationships for effective change feature extraction; (2) class imbalance of change samples which hinders distinguishability of semantic features; (3) the lack of real-world datasets for 3D semantic change detection. To resolve these challenges, we propose the Multi-task Enhanced Cross-temporal Point Transformer (ME-CPT) network. ME-CPT establishes spatiotemporal correspondences between point cloud across different epochs and employs attention mechanisms to jointly extract semantic change features, facilitating information exchange and change comparison. Additionally, we incorporate a semantic segmentation task and through the multi-task training strategy, further enhance the distinguishability of semantic features, reducing the impact of class imbalance in change types. Moreover, we release a 22.5 $km^2$ 3D semantic change detection dataset, offering diverse scenes for comprehensive evaluation. Experiments on multiple datasets show that the proposed MT-CPT achieves superior performance compared to existing state-of-the-art methods. The source code and dataset will be released upon acceptance at \\url{https://github.com/zhangluqi0209/ME-CPT}.",
        "tags": [
            "3D",
            "Detection",
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "31",
        "title": "Scalable and Explainable Verification of Image-based Neural Network Controllers for Autonomous Vehicles",
        "author": [
            "Aditya Parameshwaran",
            "Yue Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14009",
        "abstract": "Existing formal verification methods for image-based neural network controllers in autonomous vehicles often struggle with high-dimensional inputs, computational inefficiency, and a lack of explainability. These challenges make it difficult to ensure safety and reliability, as processing high-dimensional image data is computationally intensive and neural networks are typically treated as black boxes. To address these issues, we propose \\textbf{SEVIN} (Scalable and Explainable Verification of Image-Based Neural Network Controllers), a framework that leverages a Variational Autoencoders (VAE) to encode high-dimensional images into a lower-dimensional, explainable latent space. By annotating latent variables with corresponding control actions, we generate convex polytopes that serve as structured input spaces for verification, significantly reducing computational complexity and enhancing scalability. Integrating the VAE's decoder with the neural network controller allows for formal and robustness verification using these explainable polytopes. Our approach also incorporates robustness verification under real-world perturbations by augmenting the dataset and retraining the VAE to capture environmental variations. Experimental results demonstrate that SEVIN achieves efficient and scalable verification while providing explainable insights into controller behavior, bridging the gap between formal verification techniques and practical applications in safety-critical systems.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "32",
        "title": "INDIGO+: A Unified INN-Guided Probabilistic Diffusion Algorithm for Blind and Non-Blind Image Restoration",
        "author": [
            "Di You",
            "Pier Luigi Dragotti"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14014",
        "abstract": "Generative diffusion models are becoming one of the most popular prior in image restoration (IR) tasks due to their remarkable ability to generate realistic natural images. Despite achieving satisfactory results, IR methods based on diffusion models present several limitations. First of all, most non-blind approaches require an analytical expression of the degradation model to guide the sampling process. Secondly, most existing blind approaches rely on families of pre-defined degradation models for training their deep networks. The above issues limit the flexibility of these approaches and so their ability to handle real-world degradation tasks. In this paper, we propose a novel INN-guided probabilistic diffusion algorithm for non-blind and blind image restoration, namely INDIGO and BlindINDIGO, which combines the merits of the perfect reconstruction property of invertible neural networks (INN) with the strong generative capabilities of pre-trained diffusion models. Specifically, we train the forward process of the INN to simulate an arbitrary degradation process and use the inverse to obtain an intermediate image that we use to guide the reverse diffusion sampling process through a gradient step. We also introduce an initialization strategy, to further improve the performance and inference speed of our algorithm. Experiments demonstrate that our algorithm obtains competitive results compared with recently leading methods both quantitatively and visually on synthetic and real-world low-quality images.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "33",
        "title": "Leveraging Large Language Models to Analyze Emotional and Contextual Drivers of Teen Substance Use in Online Discussions",
        "author": [
            "Jianfeng Zhu",
            "Ruoming Jin",
            "Hailong Jiang",
            "Yulan Wang",
            "Xinyu Zhang",
            "Karin G. Coifman"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14037",
        "abstract": "Adolescence is a critical stage often linked to risky behaviors, including substance use, with significant developmental and public health implications. Social media provides a lens into adolescent self-expression, but interpreting emotional and contextual signals remains complex. This study applies Large Language Models (LLMs) to analyze adolescents' social media posts, uncovering emotional patterns (e.g., sadness, guilt, fear, joy) and contextual factors (e.g., family, peers, school) related to substance use. Heatmap and machine learning analyses identified key predictors of substance use-related posts. Negative emotions like sadness and guilt were significantly more frequent in substance use contexts, with guilt acting as a protective factor, while shame and peer influence heightened substance use risk. Joy was more common in non-substance use discussions. Peer influence correlated strongly with sadness, fear, and disgust, while family and school environments aligned with non-substance use. Findings underscore the importance of addressing emotional vulnerabilities and contextual influences, suggesting that collaborative interventions involving families, schools, and communities can reduce risk factors and foster healthier adolescent development.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "34",
        "title": "LLM-guided Instance-level Image Manipulation with Diffusion U-Net Cross-Attention Maps",
        "author": [
            "Andrey Palaev",
            "Adil Khan",
            "Syed M. Ahsan Kazmi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14046",
        "abstract": "The advancement of text-to-image synthesis has introduced powerful generative models capable of creating realistic images from textual prompts. However, precise control over image attributes remains challenging, especially at the instance level. While existing methods offer some control through fine-tuning or auxiliary information, they often face limitations in flexibility and accuracy. To address these challenges, we propose a pipeline leveraging Large Language Models (LLMs), open-vocabulary detectors, cross-attention maps and intermediate activations of diffusion U-Net for instance-level image manipulation. Our method detects objects mentioned in the prompt and present in the generated image, enabling precise manipulation without extensive training or input masks. By incorporating cross-attention maps, our approach ensures coherence in manipulated images while controlling object positions. Our method enables precise manipulations at the instance level without fine-tuning or auxiliary information such as masks or bounding boxes. Code is available at https://github.com/Palandr123/DiffusionU-NetLLM",
        "tags": [
            "Diffusion",
            "LLMs",
            "Large Language Models",
            "Text-to-Image"
        ]
    },
    {
        "id": "35",
        "title": "LLMs are Vulnerable to Malicious Prompts Disguised as Scientific Language",
        "author": [
            "Yubin Ge",
            "Neeraja Kirtane",
            "Hao Peng",
            "Dilek Hakkani-TÃ¼r"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14073",
        "abstract": "As large language models (LLMs) have been deployed in various real-world settings, concerns about the harm they may propagate have grown. Various jailbreaking techniques have been developed to expose the vulnerabilities of these models and improve their safety. This work reveals that many state-of-the-art proprietary and open-source LLMs are vulnerable to malicious requests hidden behind scientific language. Specifically, our experiments with GPT4o, GPT4o-mini, GPT-4, LLama3-405B-Instruct, Llama3-70B-Instruct, Cohere, Gemini models on the StereoSet data demonstrate that, the models' biases and toxicity substantially increase when prompted with requests that deliberately misinterpret social science and psychological studies as evidence supporting the benefits of stereotypical biases. Alarmingly, these models can also be manipulated to generate fabricated scientific arguments claiming that biases are beneficial, which can be used by ill-intended actors to systematically jailbreak even the strongest models like GPT. Our analysis studies various factors that contribute to the models' vulnerabilities to malicious requests in academic language. Mentioning author names and venues enhances the persuasiveness of some models, and the bias scores can increase as dialogues progress. Our findings call for a more careful investigation on the use of scientific data in the training of LLMs.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "36",
        "title": "Enhancing Biomedical Relation Extraction with Directionality",
        "author": [
            "Po-Ting Lai",
            "Chih-Hsuan Wei",
            "Shubo Tian",
            "Robert Leaman",
            "Zhiyong Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14079",
        "abstract": "Biological relation networks contain rich information for understanding the biological mechanisms behind the relationship of entities such as genes, proteins, diseases, and chemicals. The vast growth of biomedical literature poses significant challenges updating the network knowledge. The recent Biomedical Relation Extraction Dataset (BioRED) provides valuable manual annotations, facilitating the develop-ment of machine-learning and pre-trained language model approaches for automatically identifying novel document-level (inter-sentence context) relationships. Nonetheless, its annotations lack directionality (subject/object) for the entity roles, essential for studying complex biological networks. Herein we annotate the entity roles of the relationships in the BioRED corpus and subsequently propose a novel multi-task language model with soft-prompt learning to jointly identify the relationship, novel findings, and entity roles. Our results in-clude an enriched BioRED corpus with 10,864 directionality annotations. Moreover, our proposed method outperforms existing large language models such as the state-of-the-art GPT-4 and Llama-3 on two benchmarking tasks. Our source code and dataset are available at https://github.com/ncbi-nlp/BioREDirect.",
        "tags": [
            "GPT",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "37",
        "title": "5G LDPC Linear Transformer for Channel Decoding",
        "author": [
            "Mario Hernandez",
            "Fernando Pinero"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14102",
        "abstract": "This work introduces a novel, fully differentiable linear-time complexity transformer decoder and a transformer decoder to correct 5G New Radio (NR) LDPC. We propose a scalable approach to decode linear block codes with $O(n)$ complexity rather than $O(n^2)$ for regular transformers. The architectures' performances are compared to Belief Propagation (BP), the production-level decoding algorithm used for 5G New Radio (NR) LDPC codes. We achieve bit error rate performance that matches a regular Transformer decoder and surpases one iteration BP, also achieving competitive time performance against BP, even for larger block codes. We utilize Sionna, Nvidia's 5G & 6G physical layer research software, for reproducible results.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "38",
        "title": "MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning",
        "author": [
            "Joshua Davis",
            "Thomas Sounack",
            "Kate Sciacca",
            "Jessie M Brain",
            "Brigitte N Durieux",
            "Nicole D Agaronnik",
            "Charlotta Lindvall"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14105",
        "abstract": "Extracting sections from clinical notes is crucial for downstream analysis but is challenging due to variability in formatting and labor-intensive nature of manual sectioning. While proprietary large language models (LLMs) have shown promise, privacy concerns limit their accessibility. This study develops a pipeline for automated note sectioning using open-source LLMs, focusing on three sections: History of Present Illness, Interval History, and Assessment and Plan. We fine-tuned three open-source LLMs to extract sections using a curated dataset of 487 progress notes, comparing results relative to proprietary models (GPT-4o, GPT-4o mini). Internal and external validity were assessed via precision, recall and F1 score. Fine-tuned Llama 3.1 8B outperformed GPT-4o (F1=0.92). On the external validity test set, performance remained high (F1= 0.85). Fine-tuned open-source LLMs can surpass proprietary models in clinical note sectioning, offering advantages in cost, performance, and accessibility.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "39",
        "title": "Autonomous Structural Memory Manipulation for Large Language Models Using Hierarchical Embedding Augmentation",
        "author": [
            "Derek Yotheringhay",
            "Alistair Kirkland",
            "Humphrey Kirkbride",
            "Josiah Whitesteeple"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14119",
        "abstract": "Transformative innovations in model architectures have introduced hierarchical embedding augmentation as a means to redefine the representation of tokens through multi-level semantic structures, offering enhanced adaptability to complex linguistic inputs. Autonomous structural memory manipulation further advances this paradigm through dynamic memory reallocation mechanisms that prioritize critical contextual features while suppressing less relevant information, enabling scalable and efficient performance across diverse tasks. Experimental results reveal substantial improvements in computational efficiency, with marked reductions in processing overhead for longer input sequences, achieved through memory reorganization strategies that adapt to evolving contextual requirements. Hierarchical embeddings not only improved contextual alignment but also facilitated task generalization by capturing relationships at varying semantic granularities, ensuring coherence across layers without introducing significant computational redundancies. Comparative analysis against baseline models demonstrated unique advantages in accuracy, efficiency, and interpretability, particularly in tasks requiring complex contextual understanding or domain-specific adaptability. The ability to dynamically adjust token representations and memory configurations contributed to the model's robustness under varied and unpredictable input conditions. Applications benefiting from these advancements include multi-domain generalization, interactive systems, and scenarios involving real-time decision-making, where traditional static memory architectures often face limitations. The proposed methodology combines advanced embedding and memory management strategies into a cohesive framework that addresses scalability challenges while preserving task-specific relevance.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "40",
        "title": "Test-Time Code-Switching for Cross-lingual Aspect Sentiment Triplet Extraction",
        "author": [
            "Dongming Sheng",
            "Kexin Han",
            "Hao Li",
            "Yan Zhang",
            "Yucheng Huang",
            "Jun Lang",
            "Wenqiang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14144",
        "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is a thriving research area with impressive outcomes being achieved on high-resource languages. However, the application of cross-lingual transfer to the ASTE task has been relatively unexplored, and current code-switching methods still suffer from term boundary detection issues and out-of-dictionary problems. In this study, we introduce a novel Test-Time Code-SWitching (TT-CSW) framework, which bridges the gap between the bilingual training phase and the monolingual test-time prediction. During training, a generative model is developed based on bilingual code-switched training data and can produce bilingual ASTE triplets for bilingual inputs. In the testing stage, we employ an alignment-based code-switching technique for test-time augmentation. Extensive experiments on cross-lingual ASTE datasets validate the effectiveness of our proposed method. We achieve an average improvement of 3.7% in terms of weighted-averaged F1 in four datasets with different languages. Additionally, we set a benchmark using ChatGPT and GPT-4, and demonstrate that even smaller generative models fine-tuned with our proposed TT-CSW framework surpass ChatGPT and GPT-4 by 14.2% and 5.0% respectively.",
        "tags": [
            "ChatGPT",
            "Detection",
            "GPT"
        ]
    },
    {
        "id": "41",
        "title": "HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting",
        "author": [
            "Javier Yu",
            "Timothy Chen",
            "Mac Schwager"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14147",
        "abstract": "3D Gaussian Splatting offers expressive scene reconstruction, modeling a broad range of visual, geometric, and semantic information. However, efficient real-time map reconstruction with data streamed from multiple robots and devices remains a challenge. To that end, we propose HAMMER, a server-based collaborative Gaussian Splatting method that leverages widely available ROS communication infrastructure to generate 3D, metric-semantic maps from asynchronous robot data-streams with no prior knowledge of initial robot positions and varying on-device pose estimators. HAMMER consists of (i) a frame alignment module that transforms local SLAM poses and image data into a global frame and requires no prior relative pose knowledge, and (ii) an online module for training semantic 3DGS maps from streaming data. HAMMER handles mixed perception modes, adjusts automatically for variations in image pre-processing among different devices, and distills CLIP semantic codes into the 3D scene for open-vocabulary language queries. In our real-world experiments, HAMMER creates higher-fidelity maps (2x) compared to competing baselines and is useful for downstream tasks, such as semantic goal-conditioned navigation (e.g., ``go to the couch\"). Accompanying content available at http://hammer-project.github.io.",
        "tags": [
            "3D",
            "CLIP",
            "Gaussian Splatting",
            "Robot",
            "SLAM"
        ]
    },
    {
        "id": "42",
        "title": "RL + Transformer = A General-Purpose Problem Solver",
        "author": [
            "Micah Rentschler",
            "Jesse Roberts"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14176",
        "abstract": "What if artificial intelligence could not only solve problems for which it was trained but also learn to teach itself to solve new problems (i.e., meta-learn)? In this study, we demonstrate that a pre-trained transformer fine-tuned with reinforcement learning over multiple episodes develops the ability to solve problems that it has never encountered before - an emergent ability called In-Context Reinforcement Learning (ICRL). This powerful meta-learner not only excels in solving unseen in-distribution environments with remarkable sample efficiency, but also shows strong performance in out-of-distribution environments. In addition, we show that it exhibits robustness to the quality of its training data, seamlessly stitches together behaviors from its context, and adapts to non-stationary environments. These behaviors demonstrate that an RL-trained transformer can iteratively improve upon its own solutions, making it an excellent general-purpose problem solver.",
        "tags": [
            "RL",
            "Transformer"
        ]
    },
    {
        "id": "43",
        "title": "GeoSim.AI: AI assistants for numerical simulations in geomechanics",
        "author": [
            "Yared W. Bekele"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14186",
        "abstract": "The ability to accomplish tasks via natural language instructions is one of the most efficient forms of interaction between humans and technology. This efficiency has been translated into practical applications with generative AI tools now allowing users to get things done through natural language queries. The emergence of advanced Large Language Models (LLMs) marks a pivotal shift in this direction. With ongoing advancements in the field of generative AI, integrating natural language commands into sophisticated technical fields in science and engineering is becoming increasingly feasible. This paper introduces http://GeoSim.AI - a suite of AI assistants for numerical simulations in geomechanics - thereby demonstrating the transformative potential of generative AI in geotechnical engineering. We investigate how AI assistants powered by LLMs can streamline the process of creating complex simulation inputs and interpreting results by translating natural language instructions or image inputs into precise technical commands and scripts. This approach aims to bridge the gap between human intent and the intricate requirements of numerical modeling tools, potentially revolutionizing how researchers and engineers interact with simulation software. We present demonstrations involving AI assistants for performing slope stability analyses in various software packages. The demonstrations highlight the potential of this technology to significantly enhance productivity and accessibility in computational geomechanics. http://GeoSim.AI is under active development, continuously expanding the suite of AI assistants for various numerical simulation problems in geotechnical engineering.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "44",
        "title": "Distributed Multi-Agent Coordination Using Multi-Modal Foundation Models",
        "author": [
            "Saaduddin Mahmud",
            "Dorian Benhamou Goldfajn",
            "Shlomo Zilberstein"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14189",
        "abstract": "Distributed Constraint Optimization Problems (DCOPs) offer a powerful framework for multi-agent coordination but often rely on labor-intensive, manual problem construction. To address this, we introduce VL-DCOPs, a framework that takes advantage of large multimodal foundation models (LFMs) to automatically generate constraints from both visual and linguistic instructions. We then introduce a spectrum of agent archetypes for solving VL-DCOPs: from a neuro-symbolic agent that delegates some of the algorithmic decisions to an LFM, to a fully neural agent that depends entirely on an LFM for coordination. We evaluate these agent archetypes using state-of-the-art LLMs (large language models) and VLMs (vision language models) on three novel VL-DCOP tasks and compare their respective advantages and drawbacks. Lastly, we discuss how this work extends to broader frontier challenges in the DCOP literature.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "45",
        "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement Learning-based Model Caching and Inference Offloading",
        "author": [
            "Minrui Xu",
            "Dusit Niyato",
            "Christopher G. Brinton"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14205",
        "abstract": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks and few-shot learning on complex reasoning tasks. However, resource-limited mobile edge networks struggle to support long-context LLM serving for LLM agents during multi-round interactions with users. Unlike stateless computation offloading and static service offloading in edge computing, optimizing LLM serving at edge servers is challenging because LLMs continuously learn from context which raises accuracy, latency, and resource consumption dynamics. In this paper, we propose a joint model caching and inference offloading framework that utilizes test-time deep reinforcement learning (T2DRL) to optimize deployment and execution strategies for long-context LLM serving. In this framework, we analyze the performance convergence and design an optimization problem considering the utilization of context windows in LLMs. Furthermore, the T2DRL algorithm can learn in both the training phase and the testing phase to proactively manage cached models and service requests and adapt to context changes and usage patterns during execution. To further enhance resource allocation efficiency, we propose a double Dutch auction (DDA) mechanism, which dynamically matches supply and demand while maximizing social welfare. Finally, experimental results demonstrate that the T2DRL algorithm can reduce system costs by at least 30% compared to baselines while guaranteeing the performance of LLM agents in real-world perception and reasoning tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "46",
        "title": "You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations",
        "author": [
            "Huayi Zhou",
            "Ruixiang Wang",
            "Yunxin Tai",
            "Yueci Deng",
            "Guiliang Liu",
            "Kui Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14208",
        "abstract": "Bimanual robotic manipulation is a long-standing challenge of embodied intelligence due to its characteristics of dual-arm spatial-temporal coordination and high-dimensional action spaces. Previous studies rely on pre-defined action taxonomies or direct teleoperation to alleviate or circumvent these issues, often making them lack simplicity, versatility and scalability. Differently, we believe that the most effective and efficient way for teaching bimanual manipulation is learning from human demonstrated videos, where rich features such as spatial-temporal positions, dynamic postures, interaction states and dexterous transitions are available almost for free. In this work, we propose the YOTO (You Only Teach Once), which can extract and then inject patterns of bimanual actions from as few as a single binocular observation of hand movements, and teach dual robot arms various complex tasks. Furthermore, based on keyframes-based motion trajectories, we devise a subtle solution for rapidly generating training demonstrations with diverse variations of manipulated objects and their locations. These data can then be used to learn a customized bimanual diffusion policy (BiDP) across diverse scenes. In experiments, YOTO achieves impressive performance in mimicking 5 intricate long-horizon bimanual tasks, possesses strong generalization under different visual and spatial conditions, and outperforms existing visuomotor imitation learning methods in accuracy and efficiency. Our project link is https://hnuzhy.github.io/projects/YOTO.",
        "tags": [
            "Diffusion",
            "Robot"
        ]
    },
    {
        "id": "47",
        "title": "PuzzleGPT: Emulating Human Puzzle-Solving Ability for Time and Location Prediction",
        "author": [
            "Hammad Ayyubi",
            "Xuande Feng",
            "Junzhang Liu",
            "Xudong Lin",
            "Zhecan Wang",
            "Shih-Fu Chang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14210",
        "abstract": "The task of predicting time and location from images is challenging and requires complex human-like puzzle-solving ability over different clues. In this work, we formalize this ability into core skills and implement them using different modules in an expert pipeline called PuzzleGPT. PuzzleGPT consists of a perceiver to identify visual clues, a reasoner to deduce prediction candidates, a combiner to combinatorially combine information from different clues, a web retriever to get external knowledge if the task can't be solved locally, and a noise filter for robustness. This results in a zero-shot, interpretable, and robust approach that records state-of-the-art performance on two datasets -- TARA and WikiTilo. PuzzleGPT outperforms large VLMs such as BLIP-2, InstructBLIP, LLaVA, and even GPT-4V, as well as automatically generated reasoning pipelines like VisProg, by at least 32% and 38%, respectively. It even rivals or surpasses finetuned models.",
        "tags": [
            "GPT",
            "LLaVA"
        ]
    },
    {
        "id": "48",
        "title": "Top Ten Challenges Towards Agentic Neural Graph Databases",
        "author": [
            "Jiaxin Bai",
            "Zihao Wang",
            "Yukun Zhou",
            "Hang Yin",
            "Weizhi Fei",
            "Qi Hu",
            "Zheye Deng",
            "Jiayang Cheng",
            "Tianshi Zheng",
            "Hong Ting Tsang",
            "Yisen Gao",
            "Zhongwei Xie",
            "Yufei Li",
            "Lixin Fan",
            "Binhang Yuan",
            "Wei Wang",
            "Lei Chen",
            "Xiaofang Zhou",
            "Yangqiu Song"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14224",
        "abstract": "Graph databases (GDBs) like Neo4j and TigerGraph excel at handling interconnected data but lack advanced inference capabilities. Neural Graph Databases (NGDBs) address this by integrating Graph Neural Networks (GNNs) for predictive analysis and reasoning over incomplete or noisy data. However, NGDBs rely on predefined queries and lack autonomy and adaptability. This paper introduces Agentic Neural Graph Databases (Agentic NGDBs), which extend NGDBs with three core functionalities: autonomous query construction, neural query execution, and continuous learning. We identify ten key challenges in realizing Agentic NGDBs: semantic unit representation, abductive reasoning, scalable query execution, and integration with foundation models like large language models (LLMs). By addressing these challenges, Agentic NGDBs can enable intelligent, self-improving systems for modern data-driven applications, paving the way for adaptable and autonomous data management solutions.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "49",
        "title": "Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game",
        "author": [
            "Rong Ye",
            "Yongxin Zhang",
            "Yikai Zhang",
            "Haoyu Kuang",
            "Zhongyu Wei",
            "Peng Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14225",
        "abstract": "Achieving Artificial General Intelligence (AGI) requires AI agents that can not only make stratigic decisions but also engage in flexible and meaningful communication. Inspired by Wittgenstein's language game theory in Philosophical Investigations, we propose that language agents can learn through in-context interaction rather than traditional multi-stage frameworks that separate decision-making from language expression. Using Werewolf, a social deduction game that tests language understanding, strategic interaction, and adaptability, we develop the Multi-agent Kahneman & Tversky's Optimization (MaKTO). MaKTO engages diverse models in extensive gameplay to generate unpaired desirable and unacceptable responses, then employs KTO to refine the model's decision-making process. In 9-player Werewolf games, MaKTO achieves a 61% average win rate across various models, outperforming GPT-4o and two-stage RL agents by relative improvements of 23.0% and 10.9%, respectively. Notably, MaKTO also demonstrates human-like performance, winning 60% against expert players and showing only 49% detectability in Turing-style blind tests. These results showcase MaKTO's superior decision-making, strategic adaptation, and natural language generation in complex social deduction games.",
        "tags": [
            "GPT",
            "RL"
        ]
    },
    {
        "id": "50",
        "title": "Micro-macro Wavelet-based Gaussian Splatting for 3D Reconstruction from Unconstrained Images",
        "author": [
            "Yihui Li",
            "Chengxin Lv",
            "Hongyu Yang",
            "Di Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14231",
        "abstract": "3D reconstruction from unconstrained image collections presents substantial challenges due to varying appearances and transient occlusions. In this paper, we introduce Micro-macro Wavelet-based Gaussian Splatting (MW-GS), a novel approach designed to enhance 3D reconstruction by disentangling scene representations into global, refined, and intrinsic components. The proposed method features two key innovations: Micro-macro Projection, which allows Gaussian points to capture details from feature maps across multiple scales with enhanced diversity; and Wavelet-based Sampling, which leverages frequency domain information to refine feature representations and significantly improve the modeling of scene appearances. Additionally, we incorporate a Hierarchical Residual Fusion Network to seamlessly integrate these features. Extensive experiments demonstrate that MW-GS delivers state-of-the-art rendering performance, surpassing existing methods.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "51",
        "title": "Humanity's Last Exam",
        "author": [
            "Long Phan",
            "Alice Gatti",
            "Ziwen Han",
            "Nathaniel Li",
            "Josephina Hu",
            "Hugh Zhang",
            "Sean Shi",
            "Michael Choi",
            "Anish Agrawal",
            "Arnav Chopra",
            "Adam Khoja",
            "Ryan Kim",
            "Jason Hausenloy",
            "Oliver Zhang",
            "Mantas Mazeika",
            "Daron Anderson",
            "Tung Nguyen",
            "Mobeen Mahmood",
            "Fiona Feng",
            "Steven Y. Feng",
            "Haoran Zhao",
            "Michael Yu",
            "Varun Gangal",
            "Chelsea Zou",
            "Zihan Wang",
            "Jessica P. Wang",
            "Pawan Kumar",
            "Oleksandr Pokutnyi",
            "Robert Gerbicz",
            "Serguei Popov",
            "John-Clark Levin",
            "Mstyslav Kazakov",
            "Johannes Schmitt",
            "Geoff Galgon",
            "Alvaro Sanchez",
            "Yongki Lee",
            "Will Yeadon",
            "Scott Sauers",
            "Marc Roth",
            "Chidozie Agu",
            "SÃ¸ren Riis",
            "Fabian Giska",
            "Saiteja Utpala",
            "Zachary Giboney",
            "Gashaw M. Goshu",
            "Joan of Arc Xavier",
            "Sarah-Jane Crowson",
            "Mohinder Maheshbhai Naiya",
            "Noah Burns",
            "Lennart Finke",
            "Zerui Cheng",
            "Hyunwoo Park",
            "Francesco Fournier-Facio",
            "John Wydallis",
            "Mark Nandor",
            "Ankit Singh",
            "Tim Gehrunger",
            "Jiaqi Cai",
            "Ben McCarty",
            "Darling Duclosel",
            "Jungbae Nam",
            "Jennifer Zampese",
            "Ryan G. Hoerr",
            "Aras Bacho",
            "Gautier Abou Loume",
            "Abdallah Galal",
            "Hangrui Cao",
            "Alexis C Garretson",
            "Damien Sileo",
            "Qiuyu Ren",
            "Doru Cojoc",
            "Pavel Arkhipov",
            "Usman Qazi",
            "Lianghui Li",
            "Sumeet Motwani",
            "Christian Schroeder de Witt",
            "Edwin Taylor",
            "Johannes Veith",
            "Eric Singer",
            "Taylor D. Hartman",
            "Paolo Rissone",
            "Jaehyeok Jin",
            "Jack Wei Lun Shi",
            "Chris G. Willcocks",
            "Joshua Robinson",
            "Aleksandar Mikov",
            "Ameya Prabhu",
            "Longke Tang",
            "Xavier Alapont",
            "Justine Leon Uro",
            "Kevin Zhou",
            "Emily de Oliveira Santos",
            "Andrey Pupasov Maksimov",
            "Edward Vendrow",
            "Kengo Zenitani",
            "Julien Guillod",
            "Yuqi Li",
            "Joshua Vendrow",
            "Vladyslav Kuchkin",
            "Ng Ze-An"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14249",
        "abstract": "Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 3,000 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "52",
        "title": "Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors",
        "author": [
            "Yi Zhao",
            "Youzhi Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14250",
        "abstract": "Large language models (LLMs) are widely used in real-world applications, raising concerns about their safety and trustworthiness. While red-teaming with jailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus primarily on single-turn attacks, overlooking the multi-turn strategies used by real-world adversaries. Existing multi-turn methods rely on static patterns or predefined logical chains, failing to account for the dynamic strategies during attacks. We propose Siren, a learning-based multi-turn attack framework designed to simulate real-world human jailbreak behaviors. Siren consists of three stages: (1) training set construction utilizing Turn-Level LLM feedback (Turn-MF), (2) post-training attackers with supervised fine-tuning (SFT) and direct preference optimization (DPO), and (3) interactions between the attacking and target LLMs. Experiments demonstrate that Siren achieves an attack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against Gemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o, significantly outperforming single-turn baselines. Moreover, Siren with a 7B-scale model achieves performance comparable to a multi-turn baseline that leverages GPT-4o as the attacker, while requiring fewer turns and employing decomposition strategies that are better semantically aligned with attack goals. We hope Siren inspires the development of stronger defenses against advanced multi-turn jailbreak attacks under realistic scenarios. Code is available at https://github.com/YiyiyiZhao/siren. Warning: This paper contains potentially harmful text.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "53",
        "title": "C2SaferRust: Transforming C Projects into Safer Rust with NeuroSymbolic Techniques",
        "author": [
            "Vikram Nitin",
            "Rahul Krishna",
            "Luiz Lemos do Valle",
            "Baishakhi Ray"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14257",
        "abstract": "In recent years, there has been a lot of interest in converting C code to Rust, to benefit from the memory and thread safety guarantees of Rust. C2Rust is a rule-based system that can automatically convert C code to functionally identical Rust, but the Rust code that it produces is non-idiomatic, i.e., makes extensive use of unsafe Rust, a subset of the language that doesn't have memory or thread safety guarantees. At the other end of the spectrum are LLMs, which produce idiomatic Rust code, but these have the potential to make mistakes and are constrained in the length of code they can process. In this paper, we present C2SaferRust, a novel approach to translate C to Rust that combines the strengths of C2Rust and LLMs. We first use C2Rust to convert C code to non-idiomatic, unsafe Rust. We then decompose the unsafe Rust code into slices that can be individually translated to safer Rust by an LLM. After processing each slice, we run end-to-end test cases to verify that the code still functions as expected. We also contribute a benchmark of 7 real-world programs, translated from C to unsafe Rust using C2Rust. Each of these programs also comes with end-to-end test cases. On this benchmark, we are able to reduce the number of raw pointers by up to 38%, and reduce the amount of unsafe code by up to 28%, indicating an increase in safety. The resulting programs still pass all test cases. C2SaferRust also shows convincing gains in performance against two previous techniques for making Rust code safer.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "54",
        "title": "TrajFlow: A Generative Framework for Occupancy Density Estimation Using Normalizing Flows",
        "author": [
            "Mitch Kosieradzki",
            "Seongjin Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14266",
        "abstract": "In transportation systems and autonomous vehicles, intelligent agents must understand the future motion of traffic participants to effectively plan motion trajectories. At the same time, the motion of traffic participants is inherently uncertain. In this paper, we propose TrajFlow, a generative framework for estimating the occupancy density of traffic participants. Our framework utilizes a causal encoder to extract semantically meaningful embeddings of the observed trajectory, as well as a normalizing flow to decode these embeddings and determine the most likely future location of traffic participants at some time point in the future. Our formulation differs from existing approaches because we model the marginal distribution of spatial locations instead of the joint distribution of unobserved trajectories. The advantages of a marginal formulation are numerous. First, we demonstrate that the marginal formulation produces higher accuracy on challenging trajectory forecasting benchmarks. Second, the marginal formulation allows for a fully continuous sampling of future locations. Finally, marginal densities are better suited for downstream tasks as they allow for the computation of per-agent motion trajectories and occupancy grids, the two most commonly used representations for motion forecasting. We present a novel architecture based entirely on neural differential equations as an implementation of this framework and provide ablations to demonstrate the advantages of a continuous implementation over a more traditional discrete neural network based approach. The code is available at https://github.com/kosieram21/TrajFlow .",
        "tags": [
            "Normalizing Flows"
        ]
    },
    {
        "id": "55",
        "title": "Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation",
        "author": [
            "Sadegh Mahdavi",
            "Muchen Li",
            "Kaiwen Liu",
            "Christos Thrampoulidis",
            "Leonid Sigal",
            "Renjie Liao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14275",
        "abstract": "Advances in Large Language Models (LLMs) have sparked interest in their ability to solve Olympiad-level math problems. However, the training and evaluation of these models are constrained by the limited size and quality of available datasets, as creating large-scale data for such advanced problems requires extensive effort from human experts. In addition, current benchmarks are prone to contamination, leading to unreliable evaluations. In this paper, we present an automated pipeline that leverages the rich resources of the Art of Problem Solving (AoPS) forum, which predominantly features Olympiad-level problems and community-driven solutions. Using open-source LLMs, we develop a method to extract question-answer pairs from the forum, resulting in AoPS-Instruct, a dataset of more than 600,000 high-quality QA pairs. Our experiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their reasoning abilities across various benchmarks. Moreover, we build an automatic pipeline that introduces LiveAoPSBench, an evolving evaluation set with timestamps, derived from the latest forum data, providing a contamination-resistant benchmark for assessing LLM performance. Notably, we observe a significant decline in LLM performance over time, suggesting their success on older examples may stem from pre-training exposure rather than true reasoning ability. Our work presents a scalable approach to creating and maintaining large-scale, high-quality datasets for advanced math reasoning, offering valuable insights into the capabilities and limitations of LLMs in this domain. Our benchmark and code is available at https://github.com/DSL-Lab/aops",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "56",
        "title": "Dense-SfM: Structure from Motion with Dense Consistent Matching",
        "author": [
            "JongMin Lee",
            "Sungjoo Yoo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14277",
        "abstract": "We present Dense-SfM, a novel Structure from Motion (SfM) framework designed for dense and accurate 3D reconstruction from multi-view images. Sparse keypoint matching, which traditional SfM methods often rely on, limits both accuracy and point density, especially in texture-less areas. Dense-SfM addresses this limitation by integrating dense matching with a Gaussian Splatting (GS) based track extension which gives more consistent, longer feature tracks. To further improve reconstruction accuracy, Dense-SfM is equipped with a multi-view kernelized matching module leveraging transformer and Gaussian Process architectures, for robust track refinement across multi-views. Evaluations on the ETH3D and Texture-Poor SfM datasets show that Dense-SfM offers significant improvements in accuracy and density over state-of-the-art methods.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Transformer"
        ]
    },
    {
        "id": "57",
        "title": "A Comprehensive Framework for Semantic Similarity Detection Using Transformer Architectures and Enhanced Ensemble Techniques",
        "author": [
            "Lifu Gao",
            "Qi Zhang",
            "Ziwei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14288",
        "abstract": "Detecting AI-generated text, especially in short-context documents, is difficult because there is not enough context for accurate classification. This paper presents a new teacher-student model that uses domain adaptation and data augmentation to solve these problems. The teacher model, which combines DeBERTa-v3-large and Mamba-790m, learns semantic knowledge through domain-specific fine-tuning. The student model handles short-context text more efficiently. The system uses a Mean Squared Error (MSE) loss function to guide the student's learning, improving both accuracy and efficiency. Also, data augmentation methods like spelling correction and error injection make the model more robust. Experimental results show that this approach works better than baseline methods, proving its usefulness for real-time AI-generated text detection and other text classification tasks.",
        "tags": [
            "Detection",
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "58",
        "title": "Advances in Temporal Point Processes: Bayesian, Deep, and LLM Approaches",
        "author": [
            "Feng Zhou",
            "Quyu Kong",
            "Yixuan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14291",
        "abstract": "Temporal point processes (TPPs) are stochastic process models used to characterize event sequences occurring in continuous time. Traditional statistical TPPs have a long-standing history, with numerous models proposed and successfully applied across diverse domains. In recent years, advances in deep learning have spurred the development of neural TPPs, enabling greater flexibility and expressiveness in capturing complex temporal dynamics. The emergence of large language models (LLMs) has further sparked excitement, offering new possibilities for modeling and analyzing event sequences by leveraging their rich contextual understanding. This survey presents a comprehensive review of recent research on TPPs from three perspectives: Bayesian, deep learning, and LLM approaches. We begin with a review of the fundamental concepts of TPPs, followed by an in-depth discussion of model design and parameter estimation techniques in these three frameworks. We also revisit classic application areas of TPPs to highlight their practical relevance. Finally, we outline challenges and promising directions for future research.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "59",
        "title": "Examining Alignment of Large Language Models through Representative Heuristics: The Case of Political Stereotypes",
        "author": [
            "Sullam Jeoung",
            "Yubin Ge",
            "Haohan Wang",
            "Jana Diesner"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14294",
        "abstract": "Examining the alignment of large language models (LLMs) has become increasingly important, particularly when these systems fail to operate as intended. This study explores the challenge of aligning LLMs with human intentions and values, with specific focus on their political inclinations. Previous research has highlighted LLMs' propensity to display political leanings, and their ability to mimic certain political parties' stances on various issues. However, the extent and conditions under which LLMs deviate from empirical positions have not been thoroughly examined. To address this gap, our study systematically investigates the factors contributing to LLMs' deviations from empirical positions on political issues, aiming to quantify these deviations and identify the conditions that cause them.\nDrawing on cognitive science findings related to representativeness heuristics -- where individuals readily recall the representative attribute of a target group in a way that leads to exaggerated beliefs -- we scrutinize LLM responses through this heuristics lens. We conduct experiments to determine how LLMs exhibit stereotypes by inflating judgments in favor of specific political parties. Our results indicate that while LLMs can mimic certain political parties' positions, they often exaggerate these positions more than human respondents do. Notably, LLMs tend to overemphasize representativeness to a greater extent than humans. This study highlights the susceptibility of LLMs to representativeness heuristics, suggeseting potential vulnerabilities to political stereotypes. We propose prompt-based mitigation strategies that demonstrate effectiveness in reducing the influence of representativeness in LLM responses.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "60",
        "title": "Multi-stage Large Language Model Pipelines Can Outperform GPT-4o in Relevance Assessment",
        "author": [
            "Julian A. Schnabel",
            "Johanne R. Trippas",
            "Falk Scholer",
            "Danula Hettiachchi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14296",
        "abstract": "The effectiveness of search systems is evaluated using relevance labels that indicate the usefulness of documents for specific queries and users. While obtaining these relevance labels from real users is ideal, scaling such data collection is challenging. Consequently, third-party annotators are employed, but their inconsistent accuracy demands costly auditing, training, and monitoring. We propose an LLM-based modular classification pipeline that divides the relevance assessment task into multiple stages, each utilising different prompts and models of varying sizes and capabilities. Applied to TREC Deep Learning (TREC-DL), one of our approaches showed an 18.4% Krippendorff's $\\alpha$ accuracy increase over OpenAI's GPT-4o mini while maintaining a cost of about 0.2 USD per million input tokens, offering a more efficient and scalable solution for relevance assessment. This approach beats the baseline performance of GPT-4o (5 USD). With a pipeline approach, even the accuracy of the GPT-4o flagship model, measured in $\\alpha$, could be improved by 9.7%.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "61",
        "title": "Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large Language Model on Knowledge Graph",
        "author": [
            "Xujian Liang",
            "Zhaoquan Gu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14300",
        "abstract": "Graph Retrieval Augmented Generation (GRAG) is a novel paradigm that takes the naive RAG system a step further by integrating graph information, such as knowledge graph (KGs), into large-scale language models (LLMs) to mitigate hallucination. However, existing GRAG still encounter limitations: 1) simple paradigms usually fail with the complex problems due to the narrow and shallow correlations capture from KGs 2) methods of strong coupling with KGs tend to be high computation cost and time consuming if the graph is dense. In this paper, we propose the Fast Think-on-Graph (FastToG), an innovative paradigm for enabling LLMs to think ``community by community\" within KGs. To do this, FastToG employs community detection for deeper correlation capture and two stages community pruning - coarse and fine pruning for faster retrieval. Furthermore, we also develop two Community-to-Text methods to convert the graph structure of communities into textual form for better understanding by LLMs. Experimental results demonstrate the effectiveness of FastToG, showcasing higher accuracy, faster reasoning, and better explainability compared to the previous works.",
        "tags": [
            "Detection",
            "LLMs",
            "RAG"
        ]
    },
    {
        "id": "62",
        "title": "MASTER: A Multi-Agent System with LLM Specialized MCTS",
        "author": [
            "Bingzheng Gan",
            "Yufan Zhao",
            "Tianyi Zhang",
            "Jing Huang",
            "Yusu Li",
            "Shu Xian Teo",
            "Changwang Zhang",
            "Wei Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14304",
        "abstract": "Large Language Models (LLM) are increasingly being explored for problem-solving tasks. However, their strategic planning capability is often viewed with skepticism. Recent studies have incorporated the Monte Carlo Tree Search (MCTS) algorithm to augment the planning capacity of LLM. Despite its potential, MCTS relies on extensive sampling simulations to approximate the true reward distribution, leading to two primary issues. Firstly, MCTS is effective for tasks like the Game of Go, where simulation results can yield objective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such as question answering, the result of a simulation is the answer to the question, which cannot obtain an objective reward without the ground truth. Secondly, obtaining statistically significant reward estimations typically requires a sample size exceeding 30 simulations, resulting in excessive token usage and time consumption. To address these challenges, we present Multi-Agent System with Tactical Execution and Reasoning using LLM Specialized MCTS (MASTER), a novel framework that coordinates agent recruitment and communication using LLM specialized MCTS. This system autonomously adjusts the number of agents based on task complexity and ensures focused communication among them. Comprehensive experiments across various tasks demonstrate the effectiveness of our proposed framework. It achieves 76% accuracy on HotpotQA and 80% on WebShop, setting new state-of-the-art performance on these datasets.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "63",
        "title": "Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video",
        "author": [
            "Xiaohao Xu",
            "Tianyi Zhang",
            "Shibo Zhao",
            "Xiang Li",
            "Sibo Wang",
            "Yongqi Chen",
            "Ye Li",
            "Bhiksha Raj",
            "Matthew Johnson-Roberson",
            "Sebastian Scherer",
            "Xiaonan Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14319",
        "abstract": "We aim to redefine robust ego-motion estimation and photorealistic 3D reconstruction by addressing a critical limitation: the reliance on noise-free data in existing models. While such sanitized conditions simplify evaluation, they fail to capture the unpredictable, noisy complexities of real-world environments. Dynamic motion, sensor imperfections, and synchronization perturbations lead to sharp performance declines when these models are deployed in practice, revealing an urgent need for frameworks that embrace and excel under real-world noise. To bridge this gap, we tackle three core challenges: scalable data generation, comprehensive benchmarking, and model robustness enhancement. First, we introduce a scalable noisy data synthesis pipeline that generates diverse datasets simulating complex motion, sensor imperfections, and synchronization errors. Second, we leverage this pipeline to create Robust-Ego3D, a benchmark rigorously designed to expose noise-induced performance degradation, highlighting the limitations of current learning-based methods in ego-motion accuracy and 3D reconstruction quality. Third, we propose Correspondence-guided Gaussian Splatting (CorrGS), a novel test-time adaptation method that progressively refines an internal clean 3D representation by aligning noisy observations with rendered RGB-D frames from clean 3D map, enhancing geometric alignment and appearance restoration through visual correspondence. Extensive experiments on synthetic and real-world data demonstrate that CorrGS consistently outperforms prior state-of-the-art methods, particularly in scenarios involving rapid motion and dynamic illumination.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "64",
        "title": "Assessing Large Language Models in Comprehending and Verifying Concurrent Programs across Memory Models",
        "author": [
            "Ridhi Jain",
            "Rahul Purandare"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14326",
        "abstract": "As concurrent programming becomes increasingly prevalent, effectively identifying and addressing concurrency issues such as data races and deadlocks is critical. This study evaluates the performance of several leading large language models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini, and Mistral-AI's Large2, in understanding and analyzing concurrency issues within software programs. Given that relaxed memory models, such as Total Store Order (TSO) and Partial Store Order (PSO), are widely implemented and adapted in modern systems, supported even by commodity architectures like ARM and x86, our evaluation focuses not only on sequentially consistent memory models but also on these relaxed memory models. Specifically, we assess two main aspects: the models' capacity to detect concurrency problems under a sequentially consistent memory model and their ability to verify the correctness conditions of concurrent programs across both sequentially consistent and relaxed memory models. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests designed to evaluate Total Store Order (TSO) and Partial Store Order (PSO) memory models. The experimental results reveal that GPT-4, GPT-4o, and Mistral-AI's Large2 demonstrate a robust understanding of concurrency issues, effectively identifying data races and deadlocks when assessed under a sequentially consistent memory model. However, despite its superior performance, all selected LLMs face significant challenges verifying program correctness under relaxed memory models. These LLMs exhibit limitations in accurately capturing memory ordering constraints, and their current capabilities fall short in verifying even small programs in these complex scenarios.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "65",
        "title": "Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts",
        "author": [
            "ClÃ©ment Desroches",
            "Martin Chauvin",
            "Louis Ladan",
            "Caroline Vateau",
            "Simon Gosset",
            "Philippe Cordier"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14334",
        "abstract": "The rapid growth of artificial intelligence (AI), particularly Large Language Models (LLMs), has raised concerns regarding its global environmental impact that extends beyond greenhouse gas emissions to include consideration of hardware fabrication and end-of-life processes. The opacity from major providers hinders companies' abilities to evaluate their AI-related environmental impacts and achieve net-zero http://targets.In this paper, we propose a methodology to estimate the environmental impact of a company's AI portfolio, providing actionable insights without necessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results confirm that large generative AI models consume up to 4600x more energy than traditional models. Our modelling approach, which accounts for increased AI usage, hardware computing efficiency, and changes in electricity mix in line with IPCC scenarios, forecasts AI electricity use up to 2030. Under a high adoption scenario, driven by widespread Generative AI and agents adoption associated to increasingly complex models and frameworks, AI electricity use is projected to rise by a factor of http://24.4.Mitigating the environmental impact of Generative AI by 2030 requires coordinated efforts across the AI value chain. Isolated measures in hardware efficiency, model efficiency, or grid improvements alone are insufficient. We advocate for standardized environmental assessment frameworks, greater transparency from the all actors of the value chain and the introduction of a \"Return on Environment\" metric to align AI development with net-zero goals.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "66",
        "title": "Low-rank Prompt Interaction for Continual Vision-Language Retrieval",
        "author": [
            "Weicai Yan",
            "Ye Wang",
            "Wang Lin",
            "Zirun Guo",
            "Zhou Zhao",
            "Tao Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14369",
        "abstract": "Research on continual learning in multi-modal tasks has been receiving increasing attention. However, most existing work overlooks the explicit cross-modal and cross-task interactions. In this paper, we innovatively propose the Low-rank Prompt Interaction (LPI) to address this general problem of multi-modal understanding, which considers both cross-modal and cross-task interactions. Specifically, as for the former, we employ multi-modal correlation modules for corresponding Transformer layers. Considering that the training parameters scale to the number of layers and tasks, we propose low-rank interaction-augmented decomposition to avoid memory explosion while enhancing the cross-modal association through sharing and separating common-specific low-rank factors. In addition, due to the multi-modal semantic differences carried by the low-rank initialization, we adopt hierarchical low-rank contrastive learning to ensure training robustness. As for the latter, we initially employ a visual analysis and identify that different tasks have clear distinctions in proximity. Therefore, we introduce explicit task contrastive constraints in the prompt learning process based on task semantic distances. Experiments on two retrieval tasks show performance improvements with the introduction of a minimal number of parameters, demonstrating the effectiveness of our method. Code is available at https://github.com/Kelvin-ywc/LPI.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "67",
        "title": "DRESSing Up LLM: Efficient Stylized Question-Answering via Style Subspace Editing",
        "author": [
            "Xinyu Ma",
            "Yifeng Xu",
            "Yang Lin",
            "Tianlong Wang",
            "Xu Chu",
            "Xin Gao",
            "Junfeng Zhao",
            "Yasha Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14371",
        "abstract": "We introduce DRESS, a novel approach for generating stylized large language model (LLM) responses through representation editing. Existing methods like prompting and fine-tuning are either insufficient for complex style adaptation or computationally expensive, particularly in tasks like NPC creation or character role-playing. Our approach leverages the over-parameterized nature of LLMs to disentangle a style-relevant subspace within the model's representation space to conduct representation editing, ensuring a minimal impact on the original semantics. By applying adaptive editing strengths, we dynamically adjust the steering vectors in the style subspace to maintain both stylistic fidelity and semantic integrity. We develop two stylized QA benchmark datasets to validate the effectiveness of DRESS, and the results demonstrate significant improvements compared to baseline methods such as prompting and ITI. In short, DRESS is a lightweight, train-free solution for enhancing LLMs with flexible and effective style control, making it particularly useful for developing stylized conversational agents. Codes and benchmark datasets are available at https://github.com/ArthurLeoM/DRESS-LLM.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "68",
        "title": "DeepFlow: Serverless Large Language Model Serving at Scale",
        "author": [
            "Junhao Hu",
            "Jiang Xu",
            "Yulong He",
            "Yuetao Chen",
            "Gengyuan Dan",
            "Zhixia Liu",
            "Baoquan Zhang",
            "Shining Wan",
            "Zhiyu Dong",
            "Hao Xu",
            "Zhihao Ren",
            "Jiang Liu",
            "Jie Meng",
            "Chao He",
            "Tao Xie",
            "Dayun Lin",
            "Qin Zhang",
            "Yue Yu",
            "Hao Feng",
            "Xusheng Chen",
            "Yizhou Shan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14417",
        "abstract": "This paper introduces DeepFlow, a scalable and serverless AI platform designed to efficiently serve large language models (LLMs) at scale in cloud environments. DeepFlow addresses key challenges such as resource allocation, serving efficiency, and cold start latencies through four main design components. First, it uses a simple serverless abstraction called the request-job-task model, which helps manage AI workloads across post-training and model serving tasks. Second, it builds an in-house serving engine FlowServe using a microkernel-inspired design, NPU-centric execution, and SPMD-based parallelism to optimize LLM serving. The system also includes novel scheduling policies tailored for both PD-disaggregated and PD-colocated configurations. With optimizations like pre-warmed pods, DRAM pre-loading, and NPU-fork, DeepFlow can scale up to 64 instances in seconds. DeepFlow has been in production for over a year, operating on a large Ascend NPU cluster and providing industrystandard APIs for fine-tuning, agent serving, and model serving to our customers.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "69",
        "title": "Central schemes for systems of non-local balance laws",
        "author": [
            "Sanjibanee Sudha",
            "Jan Friedrich",
            "Samala Rathan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14425",
        "abstract": "We present numerical approaches to approximate the solutions of systems of non-local balance laws. In particular, we derive a non-staggered central scheme based on the well-known Nessyahu-Tadmor scheme and show that it preserves the positivity of solutions. To reduce the numerical diffusion, we then consider a non-local version of the Kurganov-Tadmor scheme. For both schemes, an appropriate approximation of the non-local term is crucial to maintain a second-order accuracy. Numerical examples validate our theory and demonstrate its applicability to various systems of non-local problems.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "70",
        "title": "GraphBC: Improving LLMs for Better Graph Data Processing",
        "author": [
            "Xu Chu",
            "Hanlin Xue",
            "Zhijie Tan",
            "Bingce Wang",
            "Tong Mo",
            "Weiping Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14427",
        "abstract": "The success of Large Language Models (LLMs) in various domains has led researchers to apply them to graph-related problems by converting graph data into natural language text. However, unlike graph data, natural language inherently has sequential order. We observe that when the order of nodes or edges in the natural language description of a graph is shuffled, despite describing the same graph, model performance fluctuates between high performance and random guessing. Additionally, due to the limited input context length of LLMs, current methods typically randomly sample neighbors of target nodes as representatives of their neighborhood, which may not always be effective for accurate reasoning. To address these gaps, we introduce GraphBC. This novel model framework features an Order Selector Module to ensure proper serialization order of the graph and a Subgraph Sampling Module to sample subgraphs with better structure for better reasoning. Furthermore, we propose Graph CoT obtained through distillation, and enhance LLM's reasoning and zero-shot learning capabilities for graph tasks through instruction tuning. Experiments on multiple datasets for node classification and graph question-answering demonstrate that GraphBC improves LLMs' performance and generalization ability on graph tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "71",
        "title": "Optimizing Human Pose Estimation Through Focused Human and Joint Regions",
        "author": [
            "Yingying Jiao",
            "Zhigang Wang",
            "Zhenguang Liu",
            "Shaojing Fan",
            "Sifan Wu",
            "Zheqi Wu",
            "Zhuoyue Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14439",
        "abstract": "Human pose estimation has given rise to a broad spectrum of novel and compelling applications, including action recognition, sports analysis, as well as surveillance. However, accurate video pose estimation remains an open challenge. One aspect that has been overlooked so far is that existing methods learn motion clues from all pixels rather than focusing on the target human body, making them easily misled and disrupted by unimportant information such as background changes or movements of other people. Additionally, while the current Transformer-based pose estimation methods has demonstrated impressive performance with global modeling, they struggle with local context perception and precise positional identification. In this paper, we try to tackle these challenges from three aspects: (1) We propose a bilayer Human-Keypoint Mask module that performs coarse-to-fine visual token refinement, which gradually zooms in on the target human body and keypoints while masking out unimportant figure regions. (2) We further introduce a novel deformable cross attention mechanism and a bidirectional separation strategy to adaptively aggregate spatial and temporal motion clues from constrained surrounding contexts. (3) We mathematically formulate the deformable cross attention, constraining that the model focuses solely on the regions centered at the target person body. Empirically, our method achieves state-of-the-art performance on three large-scale benchmark datasets. A remarkable highlight is that our method achieves an 84.8 mean Average Precision (mAP) on the challenging wrist joint, which significantly outperforms the 81.5 mAP achieved by the current state-of-the-art method on the PoseTrack2017 dataset.",
        "tags": [
            "Pose Estimation",
            "Transformer"
        ]
    },
    {
        "id": "72",
        "title": "Understanding and Mitigating Gender Bias in LLMs via Interpretable Neuron Editing",
        "author": [
            "Zeping Yu",
            "Sophia Ananiadou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14457",
        "abstract": "Large language models (LLMs) often exhibit gender bias, posing challenges for their safe deployment. Existing methods to mitigate bias lack a comprehensive understanding of its mechanisms or compromise the model's core capabilities. To address these issues, we propose the CommonWords dataset, to systematically evaluate gender bias in LLMs. Our analysis reveals pervasive bias across models and identifies specific neuron circuits, including gender neurons and general neurons, responsible for this behavior. Notably, editing even a small number of general neurons can disrupt the model's overall capabilities due to hierarchical neuron interactions. Based on these insights, we propose an interpretable neuron editing method that combines logit-based and causal-based strategies to selectively target biased neurons. Experiments on five LLMs demonstrate that our method effectively reduces gender bias while preserving the model's original capabilities, outperforming existing fine-tuning and editing approaches. Our findings contribute a novel dataset, a detailed analysis of bias mechanisms, and a practical solution for mitigating gender bias in LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "73",
        "title": "Visual-Lidar Map Alignment for Infrastructure Inspections",
        "author": [
            "Jake McLaughlin",
            "Nicholas Charron",
            "Sriram Narasimhan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14486",
        "abstract": "Routine and repetitive infrastructure inspections present safety, efficiency, and consistency challenges as they are performed manually, often in challenging or hazardous environments. They can also introduce subjectivity and errors into the process, resulting in undesirable outcomes. Simultaneous localization and mapping (SLAM) presents an opportunity to generate high-quality 3D maps that can be used to extract accurate and objective inspection data. Yet, many SLAM algorithms are limited in their ability to align 3D maps from repeated inspections in GPS-denied settings automatically. This limitation hinders practical long-term asset health assessments by requiring tedious manual alignment for data association across scans from previous inspections. This paper introduces a versatile map alignment algorithm leveraging both visual and lidar data for improved place recognition robustness and presents an infrastructure-focused dataset tailored for consecutive inspections. By detaching map alignment from SLAM, our approach enhances infrastructure inspection pipelines, supports monitoring asset degradation over time, and invigorates SLAM research by permitting exploration beyond existing multi-session SLAM algorithms.",
        "tags": [
            "3D",
            "SLAM"
        ]
    },
    {
        "id": "74",
        "title": "RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques",
        "author": [
            "Zhengyang Tang",
            "Ziniu Li",
            "Zhenyang Xiao",
            "Tian Ding",
            "Ruoyu Sun",
            "Benyou Wang",
            "Dayiheng Liu",
            "Fei Huang",
            "Tianyu Liu",
            "Bowen Yu",
            "Junyang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14492",
        "abstract": "Critiques are important for enhancing the performance of Large Language Models (LLMs), enabling both self-improvement and constructive feedback for others by identifying flaws and suggesting improvements. However, evaluating the critique capabilities of LLMs presents a significant challenge due to the open-ended nature of the task. In this work, we introduce a new benchmark designed to assess the critique capabilities of LLMs. Unlike existing benchmarks, which typically function in an open-loop fashion, our approach employs a closed-loop methodology that evaluates the quality of corrections generated from critiques. Moreover, the benchmark incorporates features such as self-critique, cross-critique, and iterative critique, which are crucial for distinguishing the abilities of advanced reasoning models from more classical ones. We implement this benchmark using eight challenging reasoning tasks. We have several interesting findings. First, despite demonstrating comparable performance in direct chain-of-thought generation, classical LLMs significantly lag behind the advanced reasoning-based model o1-mini across all critique scenarios. Second, in self-critique and iterative critique settings, classical LLMs may even underperform relative to their baseline capabilities. We hope that this benchmark will serve as a valuable resource to guide future advancements. The code and data are available at \\url{https://github.com/tangzhy/RealCritic}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "75",
        "title": "Evaluating and Improving Graph to Text Generation with Large Language Models",
        "author": [
            "Jie He",
            "Yijun Yang",
            "Wanqiu Long",
            "Deyi Xiong",
            "Victor Gutierrez Basulto",
            "Jeff Z. Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14497",
        "abstract": "Large language models (LLMs) have demonstrated immense potential across various tasks. However, research for exploring and improving the capabilities of LLMs in interpreting graph structures remains limited. To address this gap, we conduct a comprehensive evaluation of prompting current open-source LLMs on graph-to-text generation tasks. Although we explored the optimal prompting strategies and proposed a novel and effective diversity-difficulty-based few-shot sample selection method, we found that the improvements from tuning-free approaches were incremental, as LLMs struggle with planning on complex graphs, particularly those with a larger number of triplets. To further improve LLMs in planning with graph sequences and grounding in truth, we introduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks: reordering and attribution. Through extensive automatic and human evaluations, we demonstrate significant improvements in the quality of generated text from both few-shot learning and fine-tuning perspectives using the PlanGTG dataset. Our study paves the way for new research directions in graph-to-text generation. PlanGTG datasets can be found in https://github.com/probe2/kg_text.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "76",
        "title": "Automated Assignment Grading with Large Language Models: Insights From a Bioinformatics Course",
        "author": [
            "Pavlin G. PoliÄar",
            "Martin Å pendl",
            "TomaÅ¾ Curk",
            "BlaÅ¾ Zupan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14499",
        "abstract": "Providing students with individualized feedback through assignments is a cornerstone of education that supports their learning and development. Studies have shown that timely, high-quality feedback plays a critical role in improving learning outcomes. However, providing personalized feedback on a large scale in classes with large numbers of students is often impractical due to the significant time and effort required. Recent advances in natural language processing and large language models (LLMs) offer a promising solution by enabling the efficient delivery of personalized feedback. These technologies can reduce the workload of course staff while improving student satisfaction and learning outcomes. Their successful implementation, however, requires thorough evaluation and validation in real classrooms. We present the results of a practical evaluation of LLM-based graders for written assignments in the 2024/25 iteration of the Introduction to Bioinformatics course at the University of Ljubljana. Over the course of the semester, more than 100 students answered 36 text-based questions, most of which were automatically graded using LLMs. In a blind study, students received feedback from both LLMs and human teaching assistants without knowing the source, and later rated the quality of the feedback. We conducted a systematic evaluation of six commercial and open-source LLMs and compared their grading performance with human teaching assistants. Our results show that with well-designed prompts, LLMs can achieve grading accuracy and feedback quality comparable to human graders. Our results also suggest that open-source LLMs perform as well as commercial LLMs, allowing schools to implement their own grading systems while maintaining privacy.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "77",
        "title": "Training-Free Style and Content Transfer by Leveraging U-Net Skip Connections in Stable Diffusion 2.*",
        "author": [
            "Ludovica Schaerf",
            "Andrea Alfarano",
            "Fabrizio Silvestri",
            "Leonardo Impett"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14524",
        "abstract": "Despite significant recent advances in image generation with diffusion models, their internal latent representations remain poorly understood. Existing works focus on the bottleneck layer (h-space) of Stable Diffusion's U-Net or leverage the cross-attention, self-attention, or decoding layers. Our model, SkipInject takes advantage of U-Net's skip connections. We conduct thorough analyses on the role of the skip connections and find that the residual connections passed by the third encoder block carry most of the spatial information of the reconstructed image, splitting the content from the style. We show that injecting the representations from this block can be used for text-based editing, precise modifications, and style transfer. We compare our methods state-of-the-art style transfer and image editing methods and demonstrate that our method obtains the best content alignment and optimal structural preservation tradeoff.",
        "tags": [
            "Diffusion",
            "Image Editing",
            "Style Transfer"
        ]
    },
    {
        "id": "78",
        "title": "Idiom Detection in Sorani Kurdish Texts",
        "author": [
            "Skala Kamaran Omer",
            "Hossein Hassani"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14528",
        "abstract": "Idiom detection using Natural Language Processing (NLP) is the computerized process of recognizing figurative expressions within a text that convey meanings beyond the literal interpretation of the words. While idiom detection has seen significant progress across various languages, the Kurdish language faces a considerable research gap in this area despite the importance of idioms in tasks like machine translation and sentiment analysis. This study addresses idiom detection in Sorani Kurdish by approaching it as a text classification task using deep learning techniques. To tackle this, we developed a dataset containing 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse contexts. Using this dataset, we developed and evaluated three deep learning models: KuBERT-based transformer sequence classification, a Recurrent Convolutional Neural Network (RCNN), and a BiLSTM model with an attention mechanism. The evaluations revealed that the transformer model, the fine-tuned BERT, consistently outperformed the others, achieving nearly 99% accuracy while the RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the effectiveness of Transformer-based architectures in low-resource languages like Kurdish. This research provides a dataset, three optimized models, and insights into idiom detection, laying a foundation for advancing Kurdish NLP.",
        "tags": [
            "BERT",
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "79",
        "title": "Trick-GS: A Balanced Bag of Tricks for Efficient Gaussian Splatting",
        "author": [
            "Anil Armagan",
            "Albert SaÃ -Garriga",
            "Bruno Manganelli",
            "Mateusz Nowak",
            "Mehmet Kerim Yucel"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14534",
        "abstract": "Gaussian splatting (GS) for 3D reconstruction has become quite popular due to their fast training, inference speeds and high quality reconstruction. However, GS-based reconstructions generally consist of millions of Gaussians, which makes them hard to use on computationally constrained devices such as smartphones. In this paper, we first propose a principled analysis of advances in efficient GS methods. Then, we propose Trick-GS, which is a careful combination of several strategies including (1) progressive training with resolution, noise and Gaussian scales, (2) learning to prune and mask primitives and SH bands by their significance, and (3) accelerated GS training framework. Trick-GS takes a large step towards resource-constrained GS, where faster run-time, smaller and faster-convergence of models is of paramount concern. Our results on three datasets show that Trick-GS achieves up to 2x faster training, 40x smaller disk size and 2x faster rendering speed compared to vanilla GS, while having comparable accuracy.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "80",
        "title": "Rethinking Encoder-Decoder Flow Through Shared Structures",
        "author": [
            "Frederik Laboyrie",
            "Mehmet Kerim Yucel",
            "Albert Saa-Garriga"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14535",
        "abstract": "Dense prediction tasks have enjoyed a growing complexity of encoder architectures, decoders, however, have remained largely the same. They rely on individual blocks decoding intermediate feature maps sequentially. We introduce banks, shared structures that are used by each decoding block to provide additional context in the decoding process. These structures, through applying them via resampling and feature fusion, improve performance on depth estimation for state-of-the-art transformer-based architectures on natural and synthetic images whilst training on large-scale datasets.",
        "tags": [
            "Depth Estimation",
            "Transformer"
        ]
    },
    {
        "id": "81",
        "title": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic Reasoning",
        "author": [
            "Benjamin Callewaert",
            "Simon Vandevelde",
            "Joost Vennekens"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14540",
        "abstract": "A recent approach to neurosymbolic reasoning is to explicitly combine the strengths of large language models (LLMs) and symbolic solvers to tackle complex reasoning tasks. However, current approaches face significant limitations, including poor generalizability due to task-specific prompts, inefficiencies caused by the lack of separation between knowledge and queries, and restricted inferential capabilities. These shortcomings hinder their scalability and applicability across diverse domains. In this paper, we introduce VERUS-LM, a novel framework designed to address these challenges. VERUS-LM employs a generic prompting mechanism, clearly separates domain knowledge from queries, and supports a wide range of different logical reasoning tasks. This framework enhances adaptability, reduces computational cost, and allows for richer forms of reasoning, such as optimization and constraint satisfaction. We show that our approach succeeds in diverse reasoning on a novel dataset, markedly outperforming LLMs. Additionally, our system achieves competitive results on common reasoning benchmarks when compared to other state-of-the-art approaches, and significantly surpasses them on the difficult AR-LSAT dataset. By pushing the boundaries of hybrid reasoning, VERUS-LM represents a significant step towards more versatile neurosymbolic AI systems",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "82",
        "title": "Leveraging ChatGPT's Multimodal Vision Capabilities to Rank Satellite Images by Poverty Level: Advancing Tools for Social Science Research",
        "author": [
            "Hamid Sarmadi",
            "Ola Hall",
            "Thorsteinn RÃ¶gnvaldsson",
            "Mattias Ohlsson"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14546",
        "abstract": "This paper investigates the novel application of Large Language Models (LLMs) with vision capabilities to analyze satellite imagery for village-level poverty prediction. Although LLMs were originally designed for natural language understanding, their adaptability to multimodal tasks, including geospatial analysis, has opened new frontiers in data-driven research. By leveraging advancements in vision-enabled LLMs, we assess their ability to provide interpretable, scalable, and reliable insights into human poverty from satellite images. Using a pairwise comparison approach, we demonstrate that ChatGPT can rank satellite images based on poverty levels with accuracy comparable to domain experts. These findings highlight both the promise and the limitations of LLMs in socioeconomic research, providing a foundation for their integration into poverty assessment workflows. This study contributes to the ongoing exploration of unconventional data sources for welfare analysis and opens pathways for cost-effective, large-scale poverty monitoring.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "83",
        "title": "ZETA: Leveraging Z-order Curves for Efficient Top-k Attention",
        "author": [
            "Qiuhao Zeng",
            "Jerry Huang",
            "Peng Lu",
            "Gezheng Xu",
            "Boxing Chen",
            "Charles Ling",
            "Boyu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14577",
        "abstract": "Over recent years, the Transformer has become a fundamental building block for sequence modeling architectures. Yet at its core is the use of self-attention, whose memory and computational cost grow quadratically with the sequence length $N$, rendering it prohibitively expensive for long sequences. A promising approach is top-$k$ attention, which selects only the $k$ most relevant tokens and achieves performance comparable to vanilla self-attention while significantly reducing space and computational demands. However, causal masks require the current query token to only attend to past tokens, preventing the existing top-$k$ attention method from efficiently searching for the most relevant tokens in parallel, thereby limiting training efficiency. In this work, we propose ZETA, leveraging \\textbf{Z}-Order Curves for \\textbf{E}fficient \\textbf{T}op-$k$ \\textbf{A}ttention, to enable parallel querying of past tokens for entire sequences. % in both space and time complexity of $\\mathcal{O}(N \\log N)$. We first theoretically show that the choice of key and query dimensions involves a trade-off between the curse of dimensionality and the preservation of relative distances after projection. In light of this insight, we propose reducing the dimensionality of keys and queries in contrast to values and further leverage $Z$-order curves to map low-dimensional keys and queries into \\emph{one}-dimensional space, which permits parallel sorting, thereby largely improving the efficiency for top-$k$ token selection. Experimental results demonstrate that ZETA matches the performance of standard attention on the synthetic \\textsc{Multi-Query Associative Recall} task and outperforms attention and its variants on \\textsc{Long Range Arena} and \\textsc{WikiText-103} language modeling.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "84",
        "title": "Extracting Problem Structure with LLMs for Optimized SAT Local Search",
        "author": [
            "AndrÃ© Schilder",
            "Stefan Szeider"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14630",
        "abstract": "Local search preprocessing makes Conflict-Driven Clause Learning (CDCL) solvers faster by providing high-quality starting points and modern SAT solvers have incorporated this technique into their preprocessing steps. However, these tools rely on basic strategies that miss the structural patterns in problems. We present a method that applies Large Language Models (LLMs) to analyze Python-based encoding code. This reveals hidden structural patterns in how problems convert into SAT. Our method automatically generates specialized local search algorithms that find these patterns and use them to create strong initial assignments. This works for any problem instance from the same encoding type. Our tests show encouraging results, achieving faster solving times compared to baseline preprocessing systems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "85",
        "title": "Recommending Actionable Strategies: A Semantic Approach to Integrating Analytical Frameworks with Decision Heuristics",
        "author": [
            "Renato Ghisellini",
            "Remo Pareschi",
            "Marco Pedroni",
            "Giovanni Battista Raggi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14634",
        "abstract": "We present a novel approach for recommending actionable strategies by integrating strategic frameworks with decision heuristics through semantic analysis. While strategy frameworks provide systematic models for assessment and planning, and decision heuristics encode experiential knowledge,these traditions have historically remained separate. Our methodology bridges this gap using advanced natural language processing (NLP), demonstrated through integrating frameworks like the 6C model with the Thirty-Six Stratagems. The approach employs vector space representations and semantic similarity calculations to map framework parameters to heuristic patterns, supported by a computational architecture that combines deep semantic processing with constrained use of Large Language Models. By processing both primary content and secondary elements (diagrams, matrices) as complementary linguistic representations, we demonstrate effectiveness through corporate strategy case studies. The methodology generalizes to various analytical frameworks and heuristic sets, culminating in a plug-and-play architecture for generating recommender systems that enable cohesive integration of strategic frameworks and decision heuristics into actionable guidance.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "86",
        "title": "SyncAnimation: A Real-Time End-to-End Framework for Audio-Driven Human Pose and Talking Head Animation",
        "author": [
            "Yujian Liu",
            "Shidang Xu",
            "Jing Guo",
            "Dingbin Wang",
            "Zairan Wang",
            "Xianfeng Tan",
            "Xiaoli Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14646",
        "abstract": "Generating talking avatar driven by audio remains a significant challenge. Existing methods typically require high computational costs and often lack sufficient facial detail and realism, making them unsuitable for applications that demand high real-time performance and visual quality. Additionally, while some methods can synchronize lip movement, they still face issues with consistency between facial expressions and upper body movement, particularly during silent periods. In this paper, we introduce SyncAnimation, the first NeRF-based method that achieves audio-driven, stable, and real-time generation of speaking avatar by combining generalized audio-to-pose matching and audio-to-expression synchronization. By integrating AudioPose Syncer and AudioEmotion Syncer, SyncAnimation achieves high-precision poses and expression generation, progressively producing audio-synchronized upper body, head, and lip shapes. Furthermore, the High-Synchronization Human Renderer ensures seamless integration of the head and upper body, and achieves audio-sync lip. The project page can be found at https://syncanimation.github.io",
        "tags": [
            "NeRF",
            "Talking Head"
        ]
    },
    {
        "id": "87",
        "title": "Investigating the (De)Composition Capabilities of Large Language Models in Natural-to-Formal Language Conversion",
        "author": [
            "Ziyao Xu",
            "Houfeng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14649",
        "abstract": "To achieve generalized and robust natural-to-formal language conversion (N2F), large language models (LLMs) need to have strong capabilities of decomposition and composition in N2F when faced with an unfamiliar formal language and be able to cope with compositional gaps and counter-intuitive symbolic names. To investigate whether LLMs have this set of basic capabilities in N2F, we propose the DEDC framework. This framework semi-automatically performs sample and task construction, allowing decoupled evaluation of the set of decomposition and composition capabilities of LLMs in N2F. Based on this framework, we evaluate and analyze the most advanced LLMs, and the main findings include that: (1) the LLMs are deficient in both decomposition and composition; (2) the LLMs show a wide coverage of error types that can be attributed to deficiencies in natural language understanding and the learning and use of symbolic systems; (3) compositional gaps and counter-intuitive symbolic names both affect the decomposition and composition of the LLMs. Our work provides a new perspective for investigating the basic capabilities of decomposition and composition of LLMs in N2F. The detailed analysis of deficiencies and attributions can help subsequent improvements of LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "88",
        "title": "Predictive Position Estimation for Remote Surgery under Packet Loss Using the Informer Framework",
        "author": [
            "Muhammad Hanif Lashari",
            "Shakil Ahmed",
            "Wafa Batayneh",
            "Ashfaq Khokhar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14664",
        "abstract": "Accurate and real-time position estimation of the robotic arm on the patient's side is crucial for the success of remote robotic surgery in Tactile Internet environments. This paper proposes a predictive approach using the computationally efficient Transformer-based Informer model for position estimation, combined with a Four-State Hidden Markov Model (4-State HMM) to simulate realistic packet loss scenarios. The method effectively addresses network-induced delays, jitter, and packet loss, ensuring reliable performance in remote robotic surgery. The study evaluates the Informer model on the JIGSAWS dataset, demonstrating its capability to handle sequential data challenges caused by network uncertainties. Key features, including ProbSparse attention and a generative-style decoder, enhance prediction accuracy, computational speed, and memory efficiency. Results indicate that the proposed method achieves over 90 percent accuracy across varying network conditions. Furthermore, the Informer framework outperforms traditional models such as TCN, RNN, and LSTM, highlighting its suitability for real-time remote surgery applications.",
        "tags": [
            "RNN",
            "Transformer"
        ]
    },
    {
        "id": "89",
        "title": "State Space Models for Extractive Summarization in Low Resource Scenarios",
        "author": [
            "Nisrine Ait Khayi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14673",
        "abstract": "Extractive summarization involves selecting the most relevant sentences from a text. Recently, researchers have focused on advancing methods to improve state-of-the-art results in low-resource settings. Motivated by these advancements, we propose the MPoincareSum method. This method applies the Mamba state space model to generate the semantics of reviews and sentences, which are then concatenated. A Poincare compression is used to select the most meaningful features, followed by the application of a linear layer to predict sentence relevance based on the corresponding review. Finally, we paraphrase the relevant sentences to create the final summary. To evaluate the effectiveness of MPoincareSum, we conducted extensive experiments using the Amazon review dataset. The performance of the method was assessed using ROUGE scores. The experimental results demonstrate that MPoincareSum outperforms several existing approaches in the literature",
        "tags": [
            "Mamba",
            "State Space Models"
        ]
    },
    {
        "id": "90",
        "title": "MatAnyone: Stable Video Matting with Consistent Memory Propagation",
        "author": [
            "Peiqing Yang",
            "Shangchen Zhou",
            "Jixin Zhao",
            "Qingyi Tao",
            "Chen Change Loy"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14677",
        "abstract": "Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To address this, we propose MatAnyone, a robust framework tailored for target-assigned video matting. Specifically, building on a memory-based paradigm, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively integrates memory from the previous frame. This ensures semantic stability in core regions while preserving fine-grained details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, boosting matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust and accurate video matting results in diverse real-world scenarios, outperforming existing methods.",
        "tags": [
            "Matting",
            "Segmentation"
        ]
    },
    {
        "id": "91",
        "title": "A Predictive Approach for Enhancing Accuracy in Remote Robotic Surgery Using Informer Model",
        "author": [
            "Muhammad Hanif Lashari",
            "Shakil Ahmed",
            "Wafa Batayneh",
            "Ashfaq Khokhar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14678",
        "abstract": "Precise and real-time estimation of the robotic arm's position on the patient's side is essential for the success of remote robotic surgery in Tactile Internet (TI) environments. This paper presents a prediction model based on the Transformer-based Informer framework for accurate and efficient position estimation. Additionally, it combines a Four-State Hidden Markov Model (4-State HMM) to simulate realistic packet loss scenarios. The proposed approach addresses challenges such as network delays, jitter, and packet loss to ensure reliable and precise operation in remote surgical applications. The method integrates the optimization problem into the Informer model by embedding constraints such as energy efficiency, smoothness, and robustness into its training process using a differentiable optimization layer. The Informer framework uses features such as ProbSparse attention, attention distilling, and a generative-style decoder to focus on position-critical features while maintaining a low computational complexity of O(L log L). The method is evaluated using the JIGSAWS dataset, achieving a prediction accuracy of over 90 percent under various network scenarios. A comparison with models such as TCN, RNN, and LSTM demonstrates the Informer framework's superior performance in handling position prediction and meeting real-time requirements, making it suitable for Tactile Internet-enabled robotic surgery.",
        "tags": [
            "RNN",
            "Transformer"
        ]
    },
    {
        "id": "92",
        "title": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for Efficient Spherical Manifold Representation",
        "author": [
            "Rongzhao He",
            "Weihao Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14679",
        "abstract": "Attention-based methods have demonstrated exceptional performance in modelling long-range dependencies on spherical cortical surfaces, surpassing traditional Geometric Deep Learning (GDL) models. However, their extensive inference time and high memory demands pose challenges for application to large datasets with limited computing resources. Inspired by the state space model in computer vision, we introduce the attention-free Vision Mamba (Vim) to spherical surfaces, presenting a domain-agnostic architecture for analyzing data on spherical manifolds. Our method achieves surface patching by representing spherical data as a sequence of triangular patches derived from a subdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on multiple neurodevelopmental phenotype regression tasks using cortical surface metrics from neonatal brains. Experimental results demonstrate that SiM outperforms both attention- and GDL-based methods, delivering 4.8 times faster inference and achieving 91.7% lower memory consumption compared to the Surface Vision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity analysis further underscores the potential of SiM to identify subtle cognitive developmental patterns. The code is available at https://github.com/Rongzhao-He/surface-vision-mamba.",
        "tags": [
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "93",
        "title": "An Empirical Study on LLM-based Classification of Requirements-related Provisions in Food-safety Regulations",
        "author": [
            "Shabnam Hassani",
            "Mehrdad Sabetzadeh",
            "Daniel Amyot"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14683",
        "abstract": "As Industry 4.0 transforms the food industry, the role of software in achieving compliance with food-safety regulations is becoming increasingly critical. Food-safety regulations, like those in many legal domains, have largely been articulated in a technology-independent manner to ensure their longevity and broad applicability. However, this approach leaves a gap between the regulations and the modern systems and software increasingly used to implement them. In this article, we pursue two main goals. First, we conduct a Grounded Theory study of food-safety regulations and develop a conceptual characterization of food-safety concepts that closely relate to systems and software requirements. Second, we examine the effectiveness of two families of large language models (LLMs) -- BERT and GPT -- in automatically classifying legal provisions based on requirements-related food-safety concepts. Our results show that: (a) when fine-tuned, the accuracy differences between the best-performing models in the BERT and GPT families are relatively small. Nevertheless, the most powerful model in our experiments, GPT-4o, still achieves the highest accuracy, with an average Precision of 89% and an average Recall of 87%; (b) few-shot learning with GPT-4o increases Recall to 97% but decreases Precision to 65%, suggesting a trade-off between fine-tuning and few-shot learning; (c) despite our training examples being drawn exclusively from Canadian regulations, LLM-based classification performs consistently well on test provisions from the US, indicating a degree of generalizability across regulatory jurisdictions; and (d) for our classification task, LLMs significantly outperform simpler baselines constructed using long short-term memory (LSTM) networks and automatic keyword extraction.",
        "tags": [
            "BERT",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "94",
        "title": "Rethinking Table Instruction Tuning",
        "author": [
            "Naihao Deng",
            "Rada Mihalcea"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14693",
        "abstract": "Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. However, existing research has overlooked the impact of hyperparameter choices and lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. In this paper, we evaluate these abilities in existing table LLMs, and reveal significant declines in both out-of-domain table understanding and general capabilities compared to their base models. Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities. Contrary to the existing table instruction-tuning works, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities. Based on our findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "95",
        "title": "NLP-based assessment of prescription appropriateness from Italian referrals",
        "author": [
            "Vittorio Torri",
            "Annamaria Bottelli",
            "Michele Ercolanoni",
            "Olivia Leoni",
            "Francesca Ieva"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14701",
        "abstract": "Objective: This study proposes a Natural Language Processing pipeline to evaluate prescription appropriateness in Italian referrals, where reasons for prescriptions are recorded only as free text, complicating automated comparisons with guidelines. The pipeline aims to derive, for the first time, a comprehensive summary of the reasons behind these referrals and a quantification of their appropriateness. While demonstrated in a specific case study, the approach is designed to generalize to other types of examinations.\nMethods: Leveraging embeddings from a transformer-based model, the proposed approach clusters referral texts, maps clusters to labels, and aligns these labels with existing guidelines. We present a case study on a dataset of 496,971 referrals, consisting of all referrals for venous echocolordopplers of the lower limbs between 2019 and 2021 in the Lombardy Region. A sample of 1,000 referrals was manually annotated to validate the results.\nResults: The pipeline exhibited high performance for referrals' reasons (Prec=92.43%, Rec=83.28%) and excellent results for referrals' appropriateness (Prec=93.58%, Rec=91.52%) on the annotated subset. Analysis of the entire dataset identified clusters matching guideline-defined reasons - both appropriate and inappropriate - as well as clusters not addressed in the guidelines. Overall, 34.32% of referrals were marked as appropriate, 34.07% inappropriate, 14.37% likely inappropriate, and 17.24% could not be mapped to guidelines.\nConclusions: The proposed pipeline effectively assessed prescription appropriateness across a large dataset, serving as a valuable tool for health authorities. Findings have informed the Lombardy Region's efforts to strengthen recommendations and reduce the burden of inappropriate referrals.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "96",
        "title": "The Karp Dataset",
        "author": [
            "Mason DiCicco",
            "Eamon Worden",
            "Conner Olsen",
            "Nikhil Gangaram",
            "Daniel Reichman",
            "Neil Heffernan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14705",
        "abstract": "Understanding the mathematical reasoning capabilities of Large Language Models (LLMs) is a central topic in the study of artificial intelligence. This new domain necessitates the creation of datasets of reasoning tasks for both training and benchmarking the performance of LLMs. To this end, we introduce the Karp dataset: The first dataset composed of detailed proofs of NP-completeness reductions. The reductions vary in difficulty, ranging from simple exercises of undergraduate courses to more challenging reductions from academic papers. We compare the performance of state-of-the-art models on this task and demonstrate the effect of fine-tuning with the Karp dataset on reasoning capacity.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "97",
        "title": "FlexiGPT: Pruning and Extending Large Language Models with Low-Rank Weight Sharing",
        "author": [
            "James Seale Smith",
            "Chi-Heng Lin",
            "Shikhar Tuli",
            "Haris Jeelani",
            "Shangqian Gao",
            "Yilin Shen",
            "Hongxia Jin",
            "Yen-Chang Hsu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14713",
        "abstract": "The rapid proliferation of large language models (LLMs) in natural language processing (NLP) has created a critical need for techniques that enable efficient deployment on memory-constrained devices without compromising performance. We present a method to prune LLMs that selectively prunes model blocks based on an importance score and replaces them with a low-parameter replacement strategy. Specifically, we propose a principled metric to replace each pruned block using a weight-sharing mechanism that leverages unpruned counterparts from the model and block-specific low-rank adapters. Furthermore, we facilitate the learning of these replacement blocks with output feature normalization and an adapter initialization scheme built on low-rank SVD reconstructions. Empirical evaluations demonstrate substantial performance gains over existing methods, achieving state-of-the-art performance on 5/6 benchmarks for a compression rate of 30% and 6/6 benchmarks for a compression rate of 40%. We also demonstrate that our approach can extend smaller models, boosting performance on 6/6 benchmarks using only ~0.3% tokens of extended training with minimal additional parameter costs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "98",
        "title": "Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models",
        "author": [
            "Naihao Deng",
            "Sheng Zhang",
            "Henghui Zhu",
            "Shuaichen Chang",
            "Jiani Zhang",
            "Alexander Hanbo Li",
            "Chung-Wei Hang",
            "Hideo Kobayashi",
            "Yiqun Hu",
            "Patrick Ng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14717",
        "abstract": "Recent advances in natural language processing have leveraged instruction tuning to enhance Large Language Models (LLMs) for table-related tasks. However, previous works train different base models with different training data, lacking an apples-to-apples comparison across the result table LLMs. To address this, we fine-tune base models from the Mistral, OLMo, and Phi families on existing public training datasets. Our replication achieves performance on par with or surpassing existing table LLMs, establishing new state-of-the-art performance on Hitab, a table question-answering dataset. More importantly, through systematic out-of-domain evaluation, we decouple the contributions of training data and the base model, providing insight into their individual impacts. In addition, we assess the effects of table-specific instruction tuning on general-purpose benchmarks, revealing trade-offs between specialization and generalization.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "99",
        "title": "CDI: Blind Image Restoration Fidelity Evaluation based on Consistency with Degraded Image",
        "author": [
            "Xiaojun Tang",
            "Jingru Wang",
            "Guangwei Huang",
            "Guannan Chen",
            "Rui Zheng",
            "Lian Huai",
            "Yuyu Liu",
            "Xingqun Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14264",
        "abstract": "Recent advancements in Blind Image Restoration (BIR) methods, based on Generative Adversarial Networks and Diffusion Models, have significantly improved visual quality. However, they present significant challenges for Image Quality Assessment (IQA), as the existing Full-Reference IQA methods often rate images with high perceptual quality poorly. In this paper, we reassess the Solution Non-Uniqueness and Degradation Indeterminacy issues of BIR, and propose constructing a specific BIR IQA system. In stead of directly comparing a restored image with a reference image, the BIR IQA evaluates fidelity by calculating the Consistency with Degraded Image (CDI). Specifically, we propose a wavelet domain Reference Guided CDI algorithm, which can acquire the consistency with a degraded image for various types without requiring knowledge of degradation parameters. The supported degradation types include down sampling, blur, noise, JPEG and complex combined degradations etc. In addition, we propose a Reference Agnostic CDI, enabling BIR fidelity evaluation without reference images. Finally, in order to validate the rationality of CDI, we create a new Degraded Images Switch Display Comparison Dataset (DISDCD) for subjective evaluation of BIR fidelity. Experiments conducted on DISDCD verify that CDI is markedly superior to common Full Reference IQA methods for BIR fidelity evaluation. The source code and the DISDCD dataset will be publicly available shortly.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "100",
        "title": "Characteristic-Specific Partial Fine-Tuning for Efficient Emotion and Speaker Adaptation in Codec Language Text-to-Speech Models",
        "author": [
            "Tianrui Wang",
            "Meng Ge",
            "Cheng Gong",
            "Chunyu Qiang",
            "Haoyu Wang",
            "Zikang Huang",
            "Yu Jiang",
            "Xiaobao Wang",
            "Xie Chen",
            "Longbiao Wang",
            "Jianwu Dang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14273",
        "abstract": "Recently, emotional speech generation and speaker cloning have garnered significant interest in text-to-speech (TTS). With the open-sourcing of codec language TTS models trained on massive datasets with large-scale parameters, adapting these general pre-trained TTS models to generate speech with specific emotional expressions and target speaker characteristics has become a topic of great attention. Common approaches, such as full and adapter-based fine-tuning, often overlook the specific contributions of model parameters to emotion and speaker control. Treating all parameters uniformly during fine-tuning, especially when the target data has limited content diversity compared to the pre-training corpus, results in slow training speed and an increased risk of catastrophic forgetting. To address these challenges, we propose a characteristic-specific partial fine-tuning strategy, short as CSP-FT. First, we use a weighted-sum approach to analyze the contributions of different Transformer layers in a pre-trained codec language TTS model for emotion and speaker control in the generated speech. We then selectively fine-tune the layers with the highest and lowest characteristic-specific contributions to generate speech with target emotional expression and speaker identity. Experimental results demonstrate that our method achieves performance comparable to, or even surpassing, full fine-tuning in generating speech with specific emotional expressions and speaker identities. Additionally, CSP-FT delivers approximately 2x faster training speeds, fine-tunes only around 8% of parameters, and significantly reduces catastrophic forgetting. Furthermore, we show that codec language TTS models perform competitively with self-supervised models in speaker identification and emotion classification tasks, offering valuable insights for developing universal speech processing models.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "101",
        "title": "Scene Understanding Enabled Semantic Communication with Open Channel Coding",
        "author": [
            "Zhe Xiang",
            "Fei Yu",
            "Quan Deng",
            "Yuandi Li",
            "Zhiguo Wan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14520",
        "abstract": "As communication systems transition from symbol transmission to conveying meaningful information, sixth-generation (6G) networks emphasize semantic communication. This approach prioritizes high-level semantic information, improving robustness and reducing redundancy across modalities like text, speech, and images. However, traditional semantic communication faces limitations, including static coding strategies, poor generalization, and reliance on task-specific knowledge bases that hinder adaptability. To overcome these challenges, we propose a novel system combining scene understanding, Large Language Models (LLMs), and open channel coding, named \\textbf{OpenSC}. Traditional systems rely on fixed domain-specific knowledge bases, limiting their ability to generalize. Our open channel coding approach leverages shared, publicly available knowledge, enabling flexible, adaptive encoding. This dynamic system reduces reliance on static task-specific data, enhancing adaptability across diverse tasks and environments. Additionally, we use scene graphs for structured semantic encoding, capturing object relationships and context to improve tasks like Visual Question Answering (VQA). Our approach selectively encodes key semantic elements, minimizing redundancy and improving transmission efficiency. Experimental results show significant improvements in both semantic understanding and efficiency, advancing the potential of adaptive, generalizable semantic communication in 6G networks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "102",
        "title": "Diffusion based Text-to-Music Generationwith Global and Local Text based Conditioning",
        "author": [
            "Jisi Zhang",
            "Pablo Peso Parada",
            "Md Asif Jalal",
            "Karthikeyan Saravanan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.14680",
        "abstract": "Diffusion based Text-To-Music (TTM) models generate music corresponding to text descriptions. Typically UNet based diffusion models condition on text embeddings generated from a pre-trained large language model or from a cross-modality audio-language representation model. This work proposes a diffusion based TTM, in which the UNet is conditioned on both (i) a uni-modal language model (e.g., T5) via cross-attention and (ii) a cross-modal audio-language representation model (e.g., CLAP) via Feature-wise Linear Modulation (FiLM). The diffusion model is trained to exploit both a local text representation from the T5 and a global representation from the CLAP. Furthermore, we propose modifications that extract both global and local representations from the T5 through pooling mechanisms that we call mean pooling and self-attention pooling. This approach mitigates the need for an additional encoder (e.g., CLAP) to extract a global representation, thereby reducing the number of model parameters. Our results show that incorporating the CLAP global embeddings to the T5 local embeddings enhances text adherence (KL=1.47) compared to a baseline model solely relying on the T5 local embeddings (KL=1.54). Alternatively, extracting global text embeddings directly from the T5 local embeddings through the proposed mean pooling approach yields superior generation quality (FAD=1.89) while exhibiting marginally inferior text adherence (KL=1.51) against the model conditioned on both CLAP and T5 text embeddings (FAD=1.94 and KL=1.47). Our proposed solution is not only efficient but also compact in terms of the number of parameters required.",
        "tags": [
            "Diffusion"
        ]
    }
]