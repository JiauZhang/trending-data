[
    {
        "id": "1",
        "title": "LABIIUM: AI-Enhanced Zero-configuration Measurement Automation System",
        "author": [
            "Emmanuel A. Olowe",
            "Danial Chitnis"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16172",
        "abstract": "The complexity of laboratory environments requires solutions that simplify instrument interaction and enhance measurement automation. Traditional tools often require configuration, software, and programming skills, creating barriers to productivity. Previous approaches, including dedicated software suites and custom scripts, frequently fall short in providing user-friendly solutions that align with programming practices. We present LABIIUM, an AI-enhanced, zero-configuration measurement automation system designed to streamline experimental workflows and improve user productivity. LABIIUM integrates an AI assistant powered by Large Language Models (LLMs) to generate code. LABIIUM's Lab-Automation-Measurement Bridges (LAMBs) enable seamless instrument connectivity using standard tools such as VSCode and Python, eliminating setup overhead. To demonstrate its capabilities, we conducted experiments involving the measurement of the parametric transfer curve of a simple two-transistor inverting amplifier with a current source load. The AI assistant was evaluated using different prompt scenarios and compared with multiple models, including Claude Sonnet 3.5, Gemini Pro 1.5, and GPT-4o. An expert solution implementing the Gradient-Weighted Adaptive Stochastic Sampling (GWASS) method was used as a baseline. The solutions generated by the AI assistant were compared with the expert solution and a uniform linear sweep baseline with 10,000 points. The graph results show that the LLMs were able to successfully complete the most basic uniform sweep, but LLMs were unable to develop adaptive sweeping algorithms to compete with GWASS. The evaluation underscores LABIIUM's ability to enhance laboratory productivity and support digital transformation in research and industry, and emphasizes the future work required to improve LLM performance in Electronic Measurement Science Tasks.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "2",
        "title": "Efficient VoIP Communications through LLM-based Real-Time Speech Reconstruction and Call Prioritization for Emergency Services",
        "author": [
            "Danush Venkateshperumal",
            "Rahman Abdul Rafi",
            "Shakil Ahmed",
            "Ashfaq Khokhar"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16176",
        "abstract": "Emergency communication systems face disruptions due to packet loss, bandwidth constraints, poor signal quality, delays, and jitter in VoIP systems, leading to degraded real-time service quality. Victims in distress often struggle to convey critical information due to panic, speech disorders, and background noise, further complicating dispatchers' ability to assess situations accurately. Staffing shortages in emergency centers exacerbate delays in coordination and assistance. This paper proposes leveraging Large Language Models (LLMs) to address these challenges by reconstructing incomplete speech, filling contextual gaps, and prioritizing calls based on severity. The system integrates real-time transcription with Retrieval-Augmented Generation (RAG) to generate contextual responses, using Twilio and AssemblyAI APIs for seamless implementation. Evaluation shows high precision, favorable BLEU and ROUGE scores, and alignment with real-world needs, demonstrating the model's potential to optimize emergency response workflows and prioritize critical cases effectively.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "3",
        "title": "Mining Math Conjectures from LLMs: A Pruning Approach",
        "author": [
            "Jake Chuharski",
            "Elias Rojas Collins",
            "Mark Meringolo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16177",
        "abstract": "We present a novel approach to generating mathematical conjectures using Large Language Models (LLMs). Focusing on the solubilizer, a relatively recent construct in group theory, we demonstrate how LLMs such as ChatGPT, Gemini, and Claude can be leveraged to generate conjectures. These conjectures are pruned by allowing the LLMs to generate counterexamples. Our results indicate that LLMs are capable of producing original conjectures that, while not groundbreaking, are either plausible or falsifiable via counterexamples, though they exhibit limitations in code execution.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "4",
        "title": "Decoding Poultry Vocalizations -- Natural Language Processing and Transformer Models for Semantic and Emotional Analysis",
        "author": [
            "Venkatraman Manikandan",
            "Suresh Neethirajan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16182",
        "abstract": "Deciphering the acoustic language of chickens offers new opportunities in animal welfare and ecological informatics. Their subtle vocal signals encode health conditions, emotional states, and dynamic interactions within ecosystems. Understanding the semantics of these calls provides a valuable tool for interpreting their functional vocabulary and clarifying how each sound serves a specific purpose in social and environmental contexts. We apply advanced Natural Language Processing and transformer based models to translate bioacoustic data into meaningful insights. Our method integrates Wave2Vec 2.0 for raw audio feature extraction with a fine tuned Bidirectional Encoder Representations from Transformers model, pretrained on a broad corpus of animal sounds and adapted to poultry tasks. This pipeline decodes poultry vocalizations into interpretable categories including distress calls, feeding signals, and mating vocalizations, revealing emotional nuances often overlooked by conventional analyses. Achieving 92 percent accuracy in classifying key vocalization types, our approach demonstrates the feasibility of real time automated monitoring of flock health and stress. By tracking this functional vocabulary, farmers can respond proactively to environmental or behavioral changes, improving poultry welfare, reducing stress related productivity losses, and supporting more sustainable farm management. Beyond agriculture, this research enhances our understanding of computational ecology. Accessing the semantic foundation of animal calls may indicate biodiversity, environmental stressors, and species interactions, informing integrative ecosystem level decision making.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "5",
        "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using Locality-Sensitive Hashing",
        "author": [
            "Minghui Liu",
            "Tahseen Rabbani",
            "Tony O'Halloran",
            "Ananth Sankaralingam",
            "Mary-Anne Hartley",
            "Brian Gravelle",
            "Furong Huang",
            "Cornelia Fermüller",
            "Yiannis Aloimonos"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16187",
        "abstract": "Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce LSH-E, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. LSH-E quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, LSH-E makes these decisions pre-attention, thereby reducing computational costs. Additionally, LSH-E is dynamic - at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that LSH-E can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "6",
        "title": "A Decade of Deep Learning: A Survey on The Magnificent Seven",
        "author": [
            "Dilshod Azizov",
            "Muhammad Arslan Manzoor",
            "Velibor Bojkovic",
            "Yingxu Wang",
            "Zixiao Wang",
            "Zangir Iklassov",
            "Kailong Zhao",
            "Liang Li",
            "Siwei Liu",
            "Yu Zhong",
            "Wei Liu",
            "Shangsong Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16188",
        "abstract": "Deep learning has fundamentally reshaped the landscape of artificial intelligence over the past decade, enabling remarkable achievements across diverse domains. At the heart of these developments lie multi-layered neural network architectures that excel at automatic feature extraction, leading to significant improvements in machine learning tasks. To demystify these advances and offer accessible guidance, we present a comprehensive overview of the most influential deep learning algorithms selected through a broad-based survey of the field. Our discussion centers on pivotal architectures, including Residual Networks, Transformers, Generative Adversarial Networks, Variational Autoencoders, Graph Neural Networks, Contrastive Language-Image Pre-training, and Diffusion models. We detail their historical context, highlight their mathematical foundations and algorithmic principles, and examine subsequent variants, extensions, and practical considerations such as training methodologies, normalization techniques, and learning rate schedules. Beyond historical and technical insights, we also address their applications, challenges, and potential research directions. This survey aims to serve as a practical manual for both newcomers seeking an entry point into cutting-edge deep learning methods and experienced researchers transitioning into this rapidly evolving domain.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "7",
        "title": "Machine Learning-Based Automated Assessment of Intracorporeal Suturing in Laparoscopic Fundoplication",
        "author": [
            "Shekhar Madhav Khairnar",
            "Huu Phong Nguyen",
            "Alexis Desir",
            "Carla Holcomb",
            "Daniel J. Scott",
            "Ganesh Sankaranarayanan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16195",
        "abstract": "Automated assessment of surgical skills using artificial intelligence (AI) provides trainees with instantaneous feedback. After bimanual tool motions are captured, derived kinematic metrics are reliable predictors of performance in laparoscopic tasks. Implementing automated tool tracking requires time-intensive human annotation. We developed AI-based tool tracking using the Segment Anything Model (SAM) to eliminate the need for human annotators. Here, we describe a study evaluating the usefulness of our tool tracking model in automated assessment during a laparoscopic suturing task in the fundoplication procedure. An automated tool tracking model was applied to recorded videos of Nissen fundoplication on porcine bowel. Surgeons were grouped as novices (PGY1-2) and experts (PGY3-5, attendings). The beginning and end of each suturing step were segmented, and motions of the left and right tools were extracted. A low-pass filter with a 24 Hz cut-off frequency removed noise. Performance was assessed using supervised and unsupervised models, and an ablation study compared results. Kinematic features--RMS velocity, RMS acceleration, RMS jerk, total path length, and Bimanual Dexterity--were extracted and analyzed using Logistic Regression, Random Forest, Support Vector Classifier, and XGBoost. PCA was performed for feature reduction. For unsupervised learning, a Denoising Autoencoder (DAE) model with classifiers, such as a 1-D CNN and traditional models, was trained. Data were extracted for 28 participants (9 novices, 19 experts). Supervised learning with PCA and Random Forest achieved an accuracy of 0.795 and an F1 score of 0.778. The unsupervised 1-D CNN achieved superior results with an accuracy of 0.817 and an F1 score of 0.806, eliminating the need for kinematic feature computation. We demonstrated an AI model capable of automated performance classification, independent of human annotation.",
        "tags": [
            "SAM",
            "Segment Anything"
        ]
    },
    {
        "id": "8",
        "title": "CLIP-RLDrive: Human-Aligned Autonomous Driving via CLIP-Based Reward Shaping in Reinforcement Learning",
        "author": [
            "Erfan Doroudian",
            "Hamid Taghavifar"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16201",
        "abstract": "This paper presents CLIP-RLDrive, a new reinforcement learning (RL)-based framework for improving the decision-making of autonomous vehicles (AVs) in complex urban driving scenarios, particularly in unsignalized intersections. To achieve this goal, the decisions for AVs are aligned with human-like preferences through Contrastive Language-Image Pretraining (CLIP)-based reward shaping. One of the primary difficulties in RL scheme is designing a suitable reward model, which can often be challenging to achieve manually due to the complexity of the interactions and the driving scenarios. To deal with this issue, this paper leverages Vision-Language Models (VLMs), particularly CLIP, to build an additional reward model based on visual and textual cues.",
        "tags": [
            "CLIP",
            "RL"
        ]
    },
    {
        "id": "9",
        "title": "Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation",
        "author": [
            "Yiping Wang",
            "Xuehai He",
            "Kuan Wang",
            "Luyao Ma",
            "Jianwei Yang",
            "Shuohang Wang",
            "Simon Shaolei Du",
            "Yelong Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16211",
        "abstract": "The current state-of-the-art video generative models can produce commercial-grade videos with highly realistic details. However, they still struggle to coherently present multiple sequential events in the stories specified by the prompts, which is foreseeable an essential capability for future long video generation scenarios. For example, top T2V generative models still fail to generate a video of the short simple story 'how to put an elephant into a refrigerator.' While existing detail-oriented benchmarks primarily focus on fine-grained metrics like aesthetic quality and spatial-temporal consistency, they fall short of evaluating models' abilities to handle event-level story presentation. To address this gap, we introduce StoryEval, a story-oriented benchmark specifically designed to assess text-to-video (T2V) models' story-completion capabilities. StoryEval features 423 prompts spanning 7 classes, each representing short stories composed of 2-4 consecutive events. We employ advanced vision-language models, such as GPT-4V and LLaVA-OV-Chat-72B, to verify the completion of each event in the generated videos, applying a unanimous voting method to enhance reliability. Our methods ensure high alignment with human evaluations, and the evaluation of 11 models reveals its challenge, with none exceeding an average story-completion rate of 50%. StoryEval provides a new benchmark for advancing T2V models and highlights the challenges and opportunities in developing next-generation solutions for coherent story-driven video generation.",
        "tags": [
            "GPT",
            "LLaVA",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "10",
        "title": "ManiVideo: Generating Hand-Object Manipulation Video with Dexterous and Generalizable Grasping",
        "author": [
            "Youxin Pang",
            "Ruizhi Shao",
            "Jiajun Zhang",
            "Hanzhang Tu",
            "Yun Liu",
            "Boyao Zhou",
            "Hongwen Zhang",
            "Yebin Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16212",
        "abstract": "In this paper, we introduce ManiVideo, a novel method for generating consistent and temporally coherent bimanual hand-object manipulation videos from given motion sequences of hands and objects. The core idea of ManiVideo is the construction of a multi-layer occlusion (MLO) representation that learns 3D occlusion relationships from occlusion-free normal maps and occlusion confidence maps. By embedding the MLO structure into the UNet in two forms, the model enhances the 3D consistency of dexterous hand-object manipulation. To further achieve the generalizable grasping of objects, we integrate Objaverse, a large-scale 3D object dataset, to address the scarcity of video data, thereby facilitating the learning of extensive object consistency. Additionally, we propose an innovative training strategy that effectively integrates multiple datasets, supporting downstream tasks such as human-centric hand-object manipulation video generation. Through extensive experiments, we demonstrate that our approach not only achieves video generation with plausible hand-object interaction and generalizable objects, but also outperforms existing SOTA methods.",
        "tags": [
            "3D",
            "Video Generation"
        ]
    },
    {
        "id": "11",
        "title": "Zero-Shot Image Moderation in Google Ads with LLM-Assisted Textual Descriptions and Cross-modal Co-embeddings",
        "author": [
            "Enming Luo",
            "Wei Qiao",
            "Katie Warren",
            "Jingxiang Li",
            "Eric Xiao",
            "Krishna Viswanathan",
            "Yuan Wang",
            "Yintao Liu",
            "Jimin Li",
            "Ariel Fuxman"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16215",
        "abstract": "We present a scalable and agile approach for ads image content moderation at Google, addressing the challenges of moderating massive volumes of ads with diverse content and evolving policies. The proposed method utilizes human-curated textual descriptions and cross-modal text-image co-embeddings to enable zero-shot classification of policy violating ads images, bypassing the need for extensive supervised training data and human labeling. By leveraging large language models (LLMs) and user expertise, the system generates and refines a comprehensive set of textual descriptions representing policy guidelines. During inference, co-embedding similarity between incoming images and the textual descriptions serves as a reliable signal for policy violation detection, enabling efficient and adaptable ads content moderation. Evaluation results demonstrate the efficacy of this framework in significantly boosting the detection of policy violating content.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "12",
        "title": "GraphLoRA: Empowering LLMs Fine-Tuning via Graph Collaboration of MoE",
        "author": [
            "Ting Bai",
            "Yue Yu",
            "Le Huang",
            "Zenan Xu",
            "Zhe Zhao",
            "Chuan Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16216",
        "abstract": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method that has been widely adopted in various downstream applications of LLMs. Together with the Mixture-of-Expert (MoE) technique, fine-tuning approaches have shown remarkable improvements in model capability. However, the coordination of multiple experts in existing studies solely relies on the weights assigned by the simple router function. Lack of communication and collaboration among experts exacerbate the instability of LLMs due to the imbalance load problem of MoE. To address this issue, we propose a novel MoE graph-based LLM fine-tuning framework GraphLoRA, in which a graph router function is designed to capture the collaboration signals among experts by graph neural networks (GNNs). GraphLoRA enables all experts to understand input knowledge and share information from neighbor experts by aggregating operations. Besides, to enhance each expert's capability and their collaborations, we design two novel coordination strategies: the Poisson distribution-based distinction strategy and the Normal distribution-based load balance strategy. Extensive experiments on four real-world datasets demonstrate the effectiveness of our GraphLoRA in parameter-efficient fine-tuning of LLMs, showing the benefits of facilitating collaborations of multiple experts in the graph router of GraphLoRA.",
        "tags": [
            "LLMs",
            "LoRA"
        ]
    },
    {
        "id": "13",
        "title": "GNN-Transformer Cooperative Architecture for Trustworthy Graph Contrastive Learning",
        "author": [
            "Jianqing Liang",
            "Xinkai Wei",
            "Min Chen",
            "Yuan Liu",
            "Zhiqiang Wang",
            "Jiye Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16218",
        "abstract": "Graph contrastive learning (GCL) has become a hot topic in the field of graph representation learning. In contrast to traditional supervised learning relying on a large number of labels, GCL exploits augmentation strategies to generate multiple views and positive/negative pairs, both of which greatly influence the performance. Unfortunately, commonly used random augmentations may disturb the underlying semantics of graphs. Moreover, traditional GNNs, a type of widely employed encoders in GCL, are inevitably confronted with over-smoothing and over-squashing problems. To address these issues, we propose GNN-Transformer Cooperative Architecture for Trustworthy Graph Contrastive Learning (GTCA), which inherits the advantages of both GNN and Transformer, incorporating graph topology to obtain comprehensive graph representations. Theoretical analysis verifies the trustworthiness of the proposed method. Extensive experiments on benchmark datasets demonstrate state-of-the-art empirical performance.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "14",
        "title": "GALOT: Generative Active Learning via Optimizable Zero-shot Text-to-image Generation",
        "author": [
            "Hanbin Hong",
            "Shenao Yan",
            "Shuya Feng",
            "Yan Yan",
            "Yuan Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16227",
        "abstract": "Active Learning (AL) represents a crucial methodology within machine learning, emphasizing the identification and utilization of the most informative samples for efficient model training. However, a significant challenge of AL is its dependence on the limited labeled data samples and data distribution, resulting in limited performance. To address this limitation, this paper integrates the zero-shot text-to-image (T2I) synthesis and active learning by designing a novel framework that can efficiently train a machine learning (ML) model sorely using the text description. Specifically, we leverage the AL criteria to optimize the text inputs for generating more informative and diverse data samples, annotated by the pseudo-label crafted from text, then served as a synthetic dataset for active learning. This approach reduces the cost of data collection and annotation while increasing the efficiency of model training by providing informative training samples, enabling a novel end-to-end ML task from text description to vision models. Through comprehensive evaluations, our framework demonstrates consistent and significant improvements over traditional AL methods.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "15",
        "title": "Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models",
        "author": [
            "Konstantin Donhauser",
            "Kristina Ulicna",
            "Gemma Elyse Moran",
            "Aditya Ravuri",
            "Kian Kenyon-Dean",
            "Cian Eastwood",
            "Jason Hartford"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16247",
        "abstract": "Dictionary learning (DL) has emerged as a powerful interpretability tool for large language models. By extracting known concepts (e.g., Golden-Gate Bridge) from human-interpretable data (e.g., text), sparse DL can elucidate a model's inner workings. In this work, we ask if DL can also be used to discover unknown concepts from less human-interpretable scientific data (e.g., cell images), ultimately enabling modern approaches to scientific discovery. As a first step, we use DL algorithms to study microscopy foundation models trained on multi-cell image data, where little prior knowledge exists regarding which high-level concepts should arise. We show that sparse dictionaries indeed extract biologically-meaningful concepts such as cell type and genetic perturbation type. We also propose a new DL algorithm, Iterative Codebook Feature Learning~(ICFL), and combine it with a pre-processing step that uses PCA whitening from a control dataset. In our experiments, we demonstrate that both ICFL and PCA improve the selectivity of extracted features compared to TopK sparse autoencoders.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "16",
        "title": "Interactive Scene Authoring with Specialized Generative Primitives",
        "author": [
            "Clément Jambon",
            "Changwoon Choi",
            "Dongsu Zhang",
            "Olga Sorkine-Hornung",
            "Young Min Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16253",
        "abstract": "Generating high-quality 3D digital assets often requires expert knowledge of complex design tools. We introduce Specialized Generative Primitives, a generative framework that allows non-expert users to author high-quality 3D scenes in a seamless, lightweight, and controllable manner. Each primitive is an efficient generative model that captures the distribution of a single exemplar from the real world. With our framework, users capture a video of an environment, which we turn into a high-quality and explicit appearance model thanks to 3D Gaussian Splatting. Users then select regions of interest guided by semantically-aware features. To create a generative primitive, we adapt Generative Cellular Automata to single-exemplar training and controllable generation. We decouple the generative task from the appearance model by operating on sparse voxels and we recover a high-quality output with a subsequent sparse patch consistency step. Each primitive can be trained within 10 minutes and used to author new scenes interactively in a fully compositional manner. We showcase interactive sessions where various primitives are extracted from real-world scenes and controlled to create 3D assets and scenes in a few minutes. We also demonstrate additional capabilities of our primitives: handling various 3D representations to control generation, transferring appearances, and editing geometries.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "17",
        "title": "PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models",
        "author": [
            "Zhuomeng Zhang",
            "Fangqi Li",
            "Chong Di",
            "Shilin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16257",
        "abstract": "Current text-to-image (T2I) diffusion models can produce high-quality images, and malicious users who are authorized to use the model only for benign purposes might modify their models to generate images that result in harmful social impacts. Therefore, it is essential to verify the integrity of T2I diffusion models, especially when they are deployed as black-box services. To this end, considering the randomness within the outputs of generative models and the high costs in interacting with them, we capture modifications to the model through the differences in the distributions of the features of generated images. We propose a novel prompt selection algorithm based on learning automaton for efficient and accurate integrity verification of T2I diffusion models. Extensive experiments demonstrate the effectiveness, stability, accuracy and generalization of our algorithm against existing integrity violations compared with baselines. To the best of our knowledge, this paper is the first work addressing the integrity verification of T2I diffusion models, which paves the way to copyright discussions and protections for artificial intelligence applications in practice.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "18",
        "title": "Inference Scaling vs Reasoning: An Empirical Analysis of Compute-Optimal LLM Problem-Solving",
        "author": [
            "Marwan AbdElhameed",
            "Pavly Halim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16260",
        "abstract": "Recent advances in large language models (LLMs) have predominantly focused on maximizing accuracy and reasoning capabilities, often overlooking crucial computational efficiency considerations. While this approach has yielded impressive accuracy improvements, it has led to methods that may be impractical for real-world deployment due to computational overhead and latency constraints. This paper investigates the potential synergy between reasoning enhancement and computational efficiency by analyzing the integration of two contrasting approaches: Quiet-STaR (Self-Taught Reasoner) and REBASE (REward BAlanced SEarch). Through comprehensive empirical analysis using the Mistral-7B model on the GSM8K dataset, we demonstrate that while each method excels in its primary objective-Quiet-STaR achieving superior accuracy (32.03%) despite high computational cost (554.66s runtime, 12.73T FLOPs), and REBASE providing exceptional efficiency (8.47s runtime, 2.35T FLOPs) while maintaining baseline-comparable accuracy (10.94%)-their integration reveals fundamental challenges in reconciling reasoning depth with computational efficiency. The combined approach unexpectedly results in degraded performance (9.38% accuracy, 143.66s runtime), highlighting critical insights about the complex interplay between reasoning enhancement and efficiency optimization in LLMs. Our findings illuminate the need for novel architectures and algorithms specifically designed to bridge the gap between these competing objectives, while providing concrete directions for future research in compute-efficient reasoning methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "19",
        "title": "MetaScientist: A Human-AI Synergistic Framework for Automated Mechanical Metamaterial Design",
        "author": [
            "Jingyuan Qi",
            "Zian Jia",
            "Minqian Liu",
            "Wangzhi Zhan",
            "Junkai Zhang",
            "Xiaofei Wen",
            "Jingru Gan",
            "Jianpeng Chen",
            "Qin Liu",
            "Mingyu Derek Ma",
            "Bangzheng Li",
            "Haohui Wang",
            "Adithya Kulkarni",
            "Muhao Chen",
            "Dawei Zhou",
            "Ling Li",
            "Wei Wang",
            "Lifu Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16270",
        "abstract": "The discovery of novel mechanical metamaterials, whose properties are dominated by their engineered structures rather than chemical composition, is a knowledge-intensive and resource-demanding process. To accelerate the design of novel metamaterials, we present MetaScientist, a human-in-the-loop system that integrates advanced AI capabilities with expert oversight with two primary phases: (1) hypothesis generation, where the system performs complex reasoning to generate novel and scientifically sound hypotheses, supported with domain-specific foundation models and inductive biases retrieved from existing literature; (2) 3D structure synthesis, where a 3D structure is synthesized with a novel 3D diffusion model based on the textual hypothesis and refined it with a LLM-based refinement model to achieve better structure properties. At each phase, domain experts iteratively validate the system outputs, and provide feedback and supplementary materials to ensure the alignment of the outputs with scientific principles and human preferences. Through extensive evaluation from human scientists, MetaScientist is able to deliver novel and valid mechanical metamaterial designs that have the potential to be highly impactful in the metamaterial field.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "20",
        "title": "Mapping the Mind of an Instruction-based Image Editing using SMILE",
        "author": [
            "Zeinab Dehghani",
            "Koorosh Aslansefat",
            "Adil Khan",
            "Adín Ramírez Rivera",
            "Franky George",
            "Muhammad Khalid"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16277",
        "abstract": "Despite recent advancements in Instruct-based Image Editing models for generating high-quality images, they are known as black boxes and a significant barrier to transparency and user trust. To solve this issue, we introduce SMILE (Statistical Model-agnostic Interpretability with Local Explanations), a novel model-agnostic for localized interpretability that provides a visual heatmap to clarify the textual elements' influence on image-generating models. We applied our method to various Instruction-based Image Editing models like Pix2Pix, Image2Image-turbo and Diffusers-Inpaint and showed how our model can improve interpretability and reliability. Also, we use stability, accuracy, fidelity, and consistency metrics to evaluate our method. These findings indicate the exciting potential of model-agnostic interpretability for reliability and trustworthiness in critical applications such as healthcare and autonomous driving while encouraging additional investigation into the significance of interpretability in enhancing dependable image editing models.",
        "tags": [
            "Image Editing"
        ]
    },
    {
        "id": "21",
        "title": "A numerical study of a PDE-ODE system with a stochastic dynamical boundary condition: a nonlinear model for sulphation phenomena",
        "author": [
            "Francesca Arceci",
            "Daniela Morale",
            "Stefania Ugolini"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16307",
        "abstract": "We investigate the qualitative behaviour of the solutions of a stochastic boundary value problem on the half-line for a nonlinear system of parabolic reaction-diffusion equations, from a numerical point of view. The model describes the chemical aggression of calcium carbonate stones under the attack of sulphur dioxide. The dynamical boundary condition is given by a Pearson diffusion, which is original in the context of the degradation of cultural heritage. We first discuss a scheme based on the Lamperti transformation for the stochastic differential equation to preserve the boundary and a splitting strategy for the partial differential equation based on recent theoretical results. Positiveness, boundedness, and stability are stated. The impact of boundary noise on the solution and its qualitative behaviour both in the slow and fast regimes is discussed in several numerical experiments.",
        "tags": [
            "Diffusion",
            "ODE"
        ]
    },
    {
        "id": "22",
        "title": "HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases",
        "author": [
            "Meng-Chieh Lee",
            "Qi Zhu",
            "Costas Mavromatis",
            "Zhen Han",
            "Soji Adeshina",
            "Vassilis N. Ioannidis",
            "Huzefa Rangwala",
            "Christos Faloutsos"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16311",
        "abstract": "Given a semi-structured knowledge base (SKB), where text documents are interconnected by relations, how can we effectively retrieve relevant information to answer user questions? Retrieval-Augmented Generation (RAG) retrieves documents to assist large language models (LLMs) in question answering; while Graph RAG (GRAG) uses structured knowledge bases as its knowledge source. However, many questions require both textual and relational information from SKB - referred to as \"hybrid\" questions - which complicates the retrieval process and underscores the need for a hybrid retrieval method that leverages both information. In this paper, through our empirical analysis, we identify key insights that show why existing methods may struggle with hybrid question answering (HQA) over SKB. Based on these insights, we propose HybGRAG for HQA consisting of a retriever bank and a critic module, with the following advantages: (1) Agentic, it automatically refines the output by incorporating feedback from the critic module, (2) Adaptive, it solves hybrid questions requiring both textual and relational information with the retriever bank, (3) Interpretable, it justifies decision making with intuitive refinement path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In experiments on the STaRK benchmark, HybGRAG achieves significant performance gains, with an average relative improvement in Hit@1 of 51%.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "23",
        "title": "Towards Safe and Honest AI Agents with Neural Self-Other Overlap",
        "author": [
            "Marc Carauleanu",
            "Michael Vaiana",
            "Judd Rosenblatt",
            "Cameron Berg",
            "Diogo Schwerz de Lucena"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16325",
        "abstract": "As AI systems increasingly make critical decisions, deceptive AI poses a significant challenge to trust and safety. We present Self-Other Overlap (SOO) fine-tuning, a promising approach in AI Safety that could substantially improve our ability to build honest artificial intelligence. Inspired by cognitive neuroscience research on empathy, SOO aims to align how AI models represent themselves and others. Our experiments on LLMs with 7B, 27B, and 78B parameters demonstrate SOO's efficacy: deceptive responses of Mistral-7B-Instruct-v0.2 dropped from 73.6% to 17.2% with no observed reduction in general task performance, while in Gemma-2-27b-it and CalmeRys-78B-Orpo-v0.1 deceptive responses were reduced from 100% to 9.3% and 2.7%, respectively, with a small impact on capabilities. In reinforcement learning scenarios, SOO-trained agents showed significantly reduced deceptive behavior. SOO's focus on contrastive self and other-referencing observations offers strong potential for generalization across AI architectures. While current applications focus on language models and simple RL environments, SOO could pave the way for more trustworthy AI in broader domains. Ethical implications and long-term effects warrant further investigation, but SOO represents a significant step forward in AI safety research.",
        "tags": [
            "LLMs",
            "RL"
        ]
    },
    {
        "id": "24",
        "title": "When Worse is Better: Navigating the compression-generation tradeoff in visual tokenization",
        "author": [
            "Vivek Ramanujan",
            "Kushal Tirumala",
            "Armen Aghajanyan",
            "Luke Zettlemoyer",
            "Ali Farhadi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16326",
        "abstract": "Current image generation methods, such as latent diffusion and discrete token-based generation, depend on a two-stage training approach. In stage 1, an auto-encoder is trained to compress an image into a latent space; in stage 2, a generative model is trained to learn a distribution over that latent space. Most work focuses on maximizing stage 1 performance independent of stage 2, assuming better reconstruction always leads to better generation. However, we show this is not strictly true. Smaller stage 2 models can benefit from more compressed stage 1 latents even if reconstruction performance worsens, showing a fundamental trade-off between compression and generation modeling capacity. To better optimize this trade-off, we introduce Causally Regularized Tokenization (CRT), which uses knowledge of the stage 2 generation modeling procedure to embed useful inductive biases in stage 1 latents. This regularization makes stage 1 reconstruction performance worse, but makes stage 2 generation performance better by making the tokens easier to model: we are able to improve compute efficiency 2-3$\\times$ over baseline and match state-of-the-art discrete autoregressive ImageNet generation (2.18 FID) with less than half the tokens per image (256 vs. 576) and a fourth the total model parameters (775M vs. 3.1B) as the previous SOTA (LlamaGen).",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "25",
        "title": "DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment",
        "author": [
            "Cijo Jose",
            "Théo Moutakanni",
            "Dahyun Kang",
            "Federico Baldassarre",
            "Timothée Darcet",
            "Hu Xu",
            "Daniel Li",
            "Marc Szafraniec",
            "Michaël Ramamonjisoa",
            "Maxime Oquab",
            "Oriane Siméoni",
            "Huy V. Vo",
            "Patrick Labatut",
            "Piotr Bojanowski"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16334",
        "abstract": "Self-supervised visual foundation models produce powerful embeddings that achieve remarkable performance on a wide range of downstream tasks. However, unlike vision-language models such as CLIP, self-supervised visual features are not readily aligned with language, hindering their adoption in open-vocabulary tasks. Our method, named http://dino.txt, unlocks this new ability for DINOv2, a widely used self-supervised visual encoder. We build upon the LiT training strategy, which trains a text encoder to align with a frozen vision model but leads to unsatisfactory results on dense tasks. We propose several key ingredients to improve performance on both global and dense tasks, such as concatenating the [CLS] token with the patch average to train the alignment and curating data using both text and image modalities. With these, we successfully train a CLIP-like model with only a fraction of the computational cost compared to CLIP while achieving state-of-the-art results in zero-shot classification and open-vocabulary semantic segmentation.",
        "tags": [
            "CLIP",
            "Segmentation"
        ]
    },
    {
        "id": "26",
        "title": "SOUS VIDE: Cooking Visual Drone Navigation Policies in a Gaussian Splatting Vacuum",
        "author": [
            "JunEn Low",
            "Maximilian Adang",
            "Javier Yu",
            "Keiko Nagami",
            "Mac Schwager"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16346",
        "abstract": "We propose a new simulator, training approach, and policy architecture, collectively called SOUS VIDE, for end-to-end visual drone navigation. Our trained policies exhibit zero-shot sim-to-real transfer with robust real-world performance using only on-board perception and computation. Our simulator, called FiGS, couples a computationally simple drone dynamics model with a high visual fidelity Gaussian Splatting scene reconstruction. FiGS can quickly simulate drone flights producing photorealistic images at up to 130 fps. We use FiGS to collect 100k-300k observation-action pairs from an expert MPC with privileged state and dynamics information, randomized over dynamics parameters and spatial disturbances. We then distill this expert MPC into an end-to-end visuomotor policy with a lightweight neural architecture, called SV-Net. SV-Net processes color image, optical flow and IMU data streams into low-level body rate and thrust commands at 20Hz onboard a drone. Crucially, SV-Net includes a Rapid Motor Adaptation (RMA) module that adapts at runtime to variations in drone dynamics. In a campaign of 105 hardware experiments, we show SOUS VIDE policies to be robust to 30% mass variations, 40 m/s wind gusts, 60% changes in ambient brightness, shifting or removing objects from the scene, and people moving aggressively through the drone's visual field. Code, data, and experiment videos can be found on our project page: https://stanfordmsl.github.io/SousVide/.",
        "tags": [
            "Gaussian Splatting",
            "MPC"
        ]
    },
    {
        "id": "27",
        "title": "Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context",
        "author": [
            "Nilanjana Das",
            "Edward Raff",
            "Manas Gaur"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16359",
        "abstract": "Previous research on LLM vulnerabilities often relied on nonsensical adversarial prompts, which were easily detectable by automated methods. We address this gap by focusing on human-readable adversarial prompts, a more realistic and potent threat. Our key contributions are situation-driven attacks leveraging movie scripts to create contextually relevant, human-readable prompts that successfully deceive LLMs, adversarial suffix conversion to transform nonsensical adversarial suffixes into meaningful text, and AdvPrompter with p-nucleus sampling, a method to generate diverse, human-readable adversarial suffixes, improving attack efficacy in models like GPT-3.5 and Gemma 7B. Our findings demonstrate that LLMs can be tricked by sophisticated adversaries into producing harmful responses with human-readable adversarial prompts and that there exists a scope for improvement when it comes to robust LLMs.",
        "tags": [
            "GPT",
            "LLMs"
        ]
    },
    {
        "id": "28",
        "title": "A High-Quality Text-Rich Image Instruction Tuning Dataset via Hybrid Instruction Generation",
        "author": [
            "Shijie Zhou",
            "Ruiyi Zhang",
            "Yufan Zhou",
            "Changyou Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16364",
        "abstract": "Large multimodal models still struggle with text-rich images because of inadequate training data. Self-Instruct provides an annotation-free way for generating instruction data, but its quality is poor, as multimodal alignment remains a hurdle even for the largest models. In this work, we propose LLaVAR-2, to enhance multimodal alignment for text-rich images through hybrid instruction generation between human annotators and large language models. Specifically, it involves detailed image captions from human annotators, followed by the use of these annotations in tailored text prompts for GPT-4o to curate a dataset. It also implements several mechanisms to filter out low-quality data, and the resulting dataset comprises 424k high-quality pairs of instructions. Empirical results show that models fine-tuned on this dataset exhibit impressive enhancements over those trained with self-instruct data.",
        "tags": [
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "29",
        "title": "A Herd of Young Mastodonts: the User-Centered Footprints of Newcomers After Twitter Acquisition",
        "author": [
            "Francesco Di Cursi",
            "Chiara Boldrini",
            "Andrea Passarella",
            "Marco Conti"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16383",
        "abstract": "The tremendous success of major Online Social Networks (OSNs) platforms has raised increasing concerns about negative phenomena, such as mass control, fake news, and echo chambers. In addition, the increasingly strict control over users' data by platform owners questions their trustworthiness as open interaction tools. These trends and, notably, the recent drastic change in X (formerly Twitter) policies and data accessibility through public APIs, have fuelled significant migration of users towards Fediverse platforms (primarily Mastodon). In this work, we provide an initial analysis of the microscopic properties of Mastodon users' social structures. Specifically, according to the Ego network model, we analyse interaction patterns between a large set of users (egos) and the other users they interact with (alters) to characterise the properties of those users' ego networks. As was observed previously in other OSNs, we found a quite regular structure compatible with the reference Dunbar's Ego Network model. Quite interestingly, our results show clear signs of ego network formation during the initial diffusion of a social networking tool, coherent with the recent surge of Mastodon activity. Therefore, our analysis motivates the use of Mastodon as an open \"big data microscope\" to characterise human social behaviour, making it a prime candidate to replace those OSN platforms that, unfortunately, cannot be used anymore for this purpose.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "30",
        "title": "Ethics and Technical Aspects of Generative AI Models in Digital Content Creation",
        "author": [
            "Atahan Karagoz"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16389",
        "abstract": "Generative AI models like GPT-4o and DALL-E 3 are reshaping digital content creation, offering industries tools to generate diverse and sophisticated text and images with remarkable creativity and efficiency. This paper examines both the capabilities and challenges of these models within creative workflows. While they deliver high performance in generating content with creativity, diversity, and technical precision, they also raise significant ethical concerns. Our study addresses two key research questions: (a) how these models perform in terms of creativity, diversity, accuracy, and computational efficiency, and (b) the ethical risks they present, particularly concerning bias, authenticity, and potential misuse. Through a structured series of experiments, we analyze their technical performance and assess the ethical implications of their outputs, revealing that although generative models enhance creative processes, they often reflect biases from their training data and carry ethical vulnerabilities that require careful oversight. This research proposes ethical guidelines to support responsible AI integration into industry practices, fostering a balance between innovation and ethical integrity.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "31",
        "title": "Application of Multimodal Large Language Models in Autonomous Driving",
        "author": [
            "Md Robiul Islam"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16410",
        "abstract": "In this era of technological advancements, several cutting-edge techniques are being implemented to enhance Autonomous Driving (AD) systems, focusing on improving safety, efficiency, and adaptability in complex driving environments. However, AD still faces some problems including performance limitations. To address this problem, we conducted an in-depth study on implementing the Multi-modal Large Language Model. We constructed a Virtual Question Answering (VQA) dataset to fine-tune the model and address problems with the poor performance of MLLM on AD. We then break down the AD decision-making process by scene understanding, prediction, and decision-making. Chain of Thought has been used to make the decision more perfectly. Our experiments and detailed analysis of Autonomous Driving give an idea of how important MLLM is for AD.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "32",
        "title": "InfoTech Assistant : A Multimodal Conversational Agent for InfoTechnology Web Portal Queries",
        "author": [
            "Sai Surya Gadiraju",
            "Duoduo Liao",
            "Akhila Kudupudi",
            "Santosh Kasula",
            "Charitha Chalasani"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16412",
        "abstract": "This pilot study presents the development of the InfoTech Assistant, a domain-specific, multimodal chatbot engineered to address queries in bridge evaluation and infrastructure technology. By integrating web data scraping, large language models (LLMs), and Retrieval-Augmented Generation (RAG), the InfoTech Assistant provides accurate and contextually relevant responses. Data, including textual descriptions and images, are sourced from publicly available documents on the InfoTechnology website and organized in JSON format to facilitate efficient querying. The architecture of the system includes an HTML-based interface and a Flask back end connected to the Llama 3.1 model via LLM Studio. Evaluation results show approximately 95 percent accuracy on domain-specific tasks, with high similarity scores confirming the quality of response matching. This RAG-enhanced setup enables the InfoTech Assistant to handle complex, multimodal queries, offering both textual and visual information in its responses. The InfoTech Assistant demonstrates strong potential as a dependable tool for infrastructure professionals, delivering high accuracy and relevance in its domain-specific outputs.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "33",
        "title": "Transport Quasi-Monte Carlo",
        "author": [
            "Sifan Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16416",
        "abstract": "Quasi-Monte Carlo (QMC) is a powerful method for evaluating high-dimensional integrals. However, its use is typically limited to distributions where direct sampling is straightforward, such as the uniform distribution on the unit hypercube or the Gaussian distribution. For general target distributions with potentially unnormalized densities, leveraging the low-discrepancy property of QMC to improve accuracy remains challenging. We propose training a transport map to push forward the uniform distribution on the unit hypercube to approximate the target distribution. Inspired by normalizing flows, the transport map is constructed as a composition of simple, invertible transformations. To ensure that RQMC achieves its superior error rate, the transport map must satisfy specific regularity conditions. We introduce a flexible parametrization for the transport map that not only meets these conditions but is also expressive enough to model complex distributions. Our theoretical analysis establishes that the proposed transport QMC estimator achieves faster convergence rates than standard Monte Carlo, under mild and easily verifiable growth conditions on the integrand. Numerical experiments confirm the theoretical results, demonstrating the effectiveness of the proposed method in Bayesian inference tasks.",
        "tags": [
            "Normalizing Flows"
        ]
    },
    {
        "id": "34",
        "title": "Revisiting MLLMs: An In-Depth Analysis of Image Classification Abilities",
        "author": [
            "Huan Liu",
            "Lingyu Xiao",
            "Jiangjiang Liu",
            "Xiaofan Li",
            "Ze Feng",
            "Sen Yang",
            "Jingdong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16418",
        "abstract": "With the rapid advancement of Multimodal Large Language Models (MLLMs), a variety of benchmarks have been introduced to evaluate their capabilities. While most evaluations have focused on complex tasks such as scientific comprehension and visual reasoning, little attention has been given to assessing their fundamental image classification abilities. In this paper, we address this gap by thoroughly revisiting the MLLMs with an in-depth analysis of image classification. Specifically, building on established datasets, we examine a broad spectrum of scenarios, from general classification tasks (e.g., ImageNet, ObjectNet) to more fine-grained categories such as bird and food classification. Our findings reveal that the most recent MLLMs can match or even outperform CLIP-style vision-language models on several datasets, challenging the previous assumption that MLLMs are bad at image classification \\cite{VLMClassifier}. To understand the factors driving this improvement, we conduct an in-depth analysis of the network architecture, data selection, and training recipe used in public MLLMs. Our results attribute this success to advancements in language models and the diversity of training data sources. Based on these observations, we further analyze and attribute the potential reasons to conceptual knowledge transfer and enhanced exposure of target concepts, respectively. We hope our findings will offer valuable insights for future research on MLLMs and their evaluation in image classification tasks.",
        "tags": [
            "CLIP",
            "Large Language Models"
        ]
    },
    {
        "id": "35",
        "title": "Beyond End-to-End VLMs: Leveraging Intermediate Text Representations for Superior Flowchart Understanding",
        "author": [
            "Junyi Ye",
            "Ankan Dash",
            "Wenpeng Yin",
            "Guiling Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16420",
        "abstract": "Flowcharts are typically presented as images, driving the trend of using vision-language models (VLMs) for end-to-end flowchart understanding. However, two key challenges arise: (i) Limited controllability--users have minimal influence over the downstream task, as they can only modify input images, while the training of VLMs is often out of reach for most researchers. (ii) Lack of explainability--it is difficult to trace VLM errors to specific causes, such as failures in visual encoding or reasoning. We propose TextFlow, addressing aforementioned issues with two stages: (i) Vision Textualizer--which generates textual representations from flowchart images; and (ii) Textual Reasoner--which performs question-answering based on the text representations. TextFlow offers three key advantages: (i) users can select the type of text representations (e.g., Graphviz, Mermaid, PlantUML), or further convert them into executable graph object to call tools, enhancing performance and controllability; (ii) it improves explainability by helping to attribute errors more clearly to visual or textual processing components; and (iii) it promotes the modularization of the solution, such as allowing advanced LLMs to be used in the Reasoner stage when VLMs underperform in end-to-end fashion. Experiments on the FlowVQA and FlowLearn benchmarks demonstrate TextFlow's state-of-the-art performance as well as its robustness. All code is publicly available.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "36",
        "title": "Technical Report: Small Language Model for Japanese Clinical and Medicine",
        "author": [
            "Shogo Watanabe"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16423",
        "abstract": "This report presents a small language model (SLM) for Japanese clinical and medicine, named NCVC-slm-1. This 1B parameters model was trained using Japanese text classified to be of high-quality. Moreover, NCVC-slm-1 was augmented with respect to clinical and medicine content that includes the variety of diseases, drugs, and examinations. Using a carefully designed pre-processing, a specialized morphological analyzer and tokenizer, this small and light-weight model performed not only to generate text but also indicated the feasibility of understanding clinical and medicine text. In comparison to other large language models, a fine-tuning NCVC-slm-1 demonstrated the highest scores on 6 tasks of total 8 on JMED-LLM. According to this result, SLM indicated the feasibility of performing several downstream tasks in the field of clinical and medicine. Hopefully, NCVC-slm-1 will be contributed to develop and accelerate the field of clinical and medicine for a bright future.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "37",
        "title": "LearnLM: Improving Gemini for Learning",
        "author": [
            "LearnLM Team Google",
            "Abhinit Modi",
            "Aditya Srikanth Veerubhotla",
            "Aliya Rysbek",
            "Andrea Huber",
            "Brett Wiltshire",
            "Brian Veprek",
            "Daniel Gillick",
            "Daniel Kasenberg",
            "Derek Ahmed",
            "Irina Jurenka",
            "James Cohan",
            "Jennifer She",
            "Julia Wilkowski",
            "Kaiz Alarakyia",
            "Kevin McKee",
            "Lisa Wang",
            "Markus Kunesch",
            "Mike Schaekermann",
            "Miruna Pîslar",
            "Nikhil Joshi",
            "Parsa Mahmoudieh",
            "Paul Jhun",
            "Sara Wiltberger",
            "Shakir Mohamed",
            "Shashank Agarwal",
            "Shubham Milind Phal",
            "Sun Jae Lee",
            "Theofilos Strinopoulos",
            "Wei-Jen Ko",
            "Amy Wang",
            "Ankit Anand",
            "Avishkar Bhoopchand",
            "Dan Wild",
            "Divya Pandya",
            "Filip Bar",
            "Garth Graham",
            "Holger Winnemoeller",
            "Mahvish Nagda",
            "Prateek Kolhar",
            "Renee Schneider",
            "Shaojian Zhu",
            "Stephanie Chan",
            "Steve Yadlowsky",
            "Viknesh Sounderajah",
            "Yannis Assael"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16429",
        "abstract": "Today's generative AI systems are tuned to present information by default rather than engage users in service of learning as a human tutor would. To address the wide range of potential education use cases for these systems, we reframe the challenge of injecting pedagogical behavior as one of \\textit{pedagogical instruction following}, where training and evaluation examples include system-level instructions describing the specific pedagogy attributes present or desired in subsequent model turns. This framing avoids committing our models to any particular definition of pedagogy, and instead allows teachers or developers to specify desired model behavior. It also clears a path to improving Gemini models for learning -- by enabling the addition of our pedagogical data to post-training mixtures -- alongside their rapidly expanding set of capabilities. Both represent important changes from our initial tech report. We show how training with pedagogical instruction following produces a LearnLM model (available on Google AI Studio) that is preferred substantially by expert raters across a diverse set of learning scenarios, with average preference strengths of 31\\% over GPT-4o, 11\\% over Claude 3.5, and 13\\% over the Gemini 1.5 Pro model LearnLM was based on.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "38",
        "title": "Object Detection Approaches to Identifying Hand Images with High Forensic Values",
        "author": [
            "Thanh Thi Nguyen",
            "Campbell Wilson",
            "Imad Khan",
            "Janis Dalins"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16431",
        "abstract": "Forensic science plays a crucial role in legal investigations, and the use of advanced technologies, such as object detection based on machine learning methods, can enhance the efficiency and accuracy of forensic analysis. Human hands are unique and can leave distinct patterns, marks, or prints that can be utilized for forensic examinations. This paper compares various machine learning approaches to hand detection and presents the application results of employing the best-performing model to identify images of significant importance in forensic contexts. We fine-tune YOLOv8 and vision transformer-based object detection models on four hand image datasets, including the 11k hands dataset with our own bounding boxes annotated by a semi-automatic approach. Two YOLOv8 variants, i.e., YOLOv8 nano (YOLOv8n) and YOLOv8 extra-large (YOLOv8x), and two vision transformer variants, i.e., DEtection TRansformer (DETR) and Detection Transformers with Assignment (DETA), are employed for the experiments. Experimental results demonstrate that the YOLOv8 models outperform DETR and DETA on all datasets. The experiments also show that YOLOv8 approaches result in superior performance compared with existing hand detection methods, which were based on YOLOv3 and YOLOv4 models. Applications of our fine-tuned YOLOv8 models for identifying hand images (or frames in a video) with high forensic values produce excellent results, significantly reducing the time required by forensic experts. This implies that our approaches can be implemented effectively for real-world applications in forensics or related fields.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "39",
        "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
        "author": [
            "Saurabh Agarwal",
            "Anyong Mao",
            "Aditya Akella",
            "Shivaram Venkataraman"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16434",
        "abstract": "Large Language Models (LLMs) are increasingly being deployed in applications such as chatbots, code editors, and conversational agents. A key feature of LLMs is their ability to engage in multi-turn interactions with humans or external tools, enabling a wide range of tasks. Each new request in a multi-turn interaction depends on the intermediate state, specifically the key-value (K,V) caches, from previous requests in the ongoing interaction. Existing serving engines either recompute the K,V caches or offload them to main memory. Profiling reveals that recomputation can result in over 99% of processed tokens being redundant. On the other hand, offloading K,V caches from GPU memory makes inference serving stateful, leading to load imbalances across the cluster. To address these challenges, we developed SYMPHONY. SYMPHONY leverages the observation that multi-turn work loads provide additional hints that allow K,V caches to be migrated off the critical serving path. By utilizing these hints, SYMPHONY dynamically migrates K,V caches to enable finegrained scheduling of inference requests. Our experiments demonstrate that SYMPHONY can handle over 8x the number of requests compared to state-of-the-art baselines, with a similar latency profile.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "40",
        "title": "Has LLM Reached the Scaling Ceiling Yet? Unified Insights into LLM Regularities and Constraints",
        "author": [
            "Charles Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16443",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their scalability raises a critical question: Have we reached the scaling ceiling? This paper addresses this pivotal question by developing a unified theoretical framework that integrates mathematical and statistical insights to explain the scaling dynamics of LLMs. We present: 1. Central Limit Theorem (CLT) for Hidden Representations: We show that noise in hidden representations scales inversely with context size, explaining stabilization effects and the limits of context length improvements. 2. Bias-Variance Decomposition: We decompose next-token prediction loss into irreducible entropy, capacity-driven bias, and finite sample variance, revealing trade-offs where scaling yields diminishing returns. 3. Emergent SNR Thresholds: By defining signal-to-noise ratio (SNR), we quantify how capabilities emerge abruptly once SNR surpasses a threshold, offering insights into when scaling becomes less effective. Through this framework, we conclude that while LLMs have not reached an absolute scaling ceiling, practical constraints are increasingly prominent: diminishing returns, resource inefficiencies, and data limitations. Future progress will require a shift from brute-force scaling to innovations in architecture, data quality, and training paradigms. This work provides a roadmap for guiding the efficient development of next-generation LLMs and advancing the field beyond traditional scaling strategies.\nKeywords: Large Language Models; Scaling Ceiling; Central Limit Theorem; Bias-Variance Trade-Off; Signal-to-Noise Ratio; Emergent Capabilities",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "41",
        "title": "Sensitive Image Classification by Vision Transformers",
        "author": [
            "Hanxian He",
            "Campbell Wilson",
            "Thanh Thi Nguyen",
            "Janis Dalins"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16446",
        "abstract": "When it comes to classifying child sexual abuse images, managing similar inter-class correlations and diverse intra-class correlations poses a significant challenge. Vision transformer models, unlike conventional deep convolutional network models, leverage a self-attention mechanism to capture global interactions among contextual local elements. This allows them to navigate through image patches effectively, avoiding incorrect correlations and reducing ambiguity in attention maps, thus proving their efficacy in computer vision tasks. Rather than directly analyzing child sexual abuse data, we constructed two datasets: one comprising clean and pornographic images and another with three classes, which additionally include images indicative of pornography, sourced from Reddit and Google Open Images data. In our experiments, we also employ an adult content image benchmark dataset. These datasets served as a basis for assessing the performance of vision transformer models in pornographic image classification. In our study, we conducted a comparative analysis between various popular vision transformer models and traditional pre-trained ResNet models. Furthermore, we compared them with established methods for sensitive image detection such as attention and metric learning based CNN and Bumble. The findings demonstrated that vision transformer networks surpassed the benchmark pre-trained models, showcasing their superior classification and detection capabilities in this task.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "42",
        "title": "Correcting Large Language Model Behavior via Influence Function",
        "author": [
            "Han Zhang",
            "Zhuo Zhang",
            "Yi Zhang",
            "Yuanzhao Zhai",
            "Hanyang Peng",
            "Yu Lei",
            "Yue Yu",
            "Hui Wang",
            "Bin Liang",
            "Lin Gui",
            "Ruifeng Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16451",
        "abstract": "Recent advancements in AI alignment techniques have significantly improved the alignment of large language models (LLMs) with static human preferences. However, the dynamic nature of human preferences can render some prior training data outdated or even erroneous, ultimately causing LLMs to deviate from contemporary human preferences and societal norms. Existing methodologies, whether they involve the curation of new data for continual alignment or the manual correction of outdated data for re-alignment, demand costly human resources. To address this challenge, we propose a novel approach, Large Language Model Behavior Correction with Influence Function Recall and Post-Training (LANCET), which requires no human involvement. LANCET consists of two phases: (1) using influence functions to identify the training data that significantly impact undesirable model outputs, and (2) applying an Influence function-driven Bregman Optimization (IBO) technique to adjust the model's behavior based on these influence distributions. Our experiments demonstrate that LANCET effectively and efficiently correct inappropriate behaviors of LLMs. Furthermore, LANCET can outperform methods that rely on collecting human preferences, and it enhances the interpretability of learning human preferences within LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "43",
        "title": "The Evolving Usage of GenAI by Computing Students",
        "author": [
            "Irene Hou",
            "Hannah Vy Nguyen",
            "Owen Man",
            "Stephen MacNeil"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16453",
        "abstract": "Help-seeking is a critical aspect of learning and problem-solving for computing students. Recent research has shown that many students are aware of generative AI (GenAI) tools; however, there are gaps in the extent and effectiveness of how students use them. With over two years of widespread GenAI usage, it is crucial to understand whether students' help-seeking behaviors with these tools have evolved and how. This paper presents findings from a repeated cross-sectional survey conducted among computing students across North American universities (n=95). Our results indicate shifts in GenAI usage patterns. In 2023, 34.1% of students (n=47) reported never using ChatGPT for help, ranking it fourth after online searches, peer support, and class forums. By 2024, this figure dropped sharply to 6.3% (n=48), with ChatGPT nearly matching online search as the most commonly used help resource. Despite this growing prevalence, there has been a decline in students' hourly and daily usage of GenAI tools, which may be attributed to a common tendency to underestimate usage frequency. These findings offer new insights into the evolving role of GenAI in computing education, highlighting its increasing acceptance and solidifying its position as a key help resource.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "44",
        "title": "FACTS: Fine-Grained Action Classification for Tactical Sports",
        "author": [
            "Christopher Lai",
            "Jason Mo",
            "Haotian Xia",
            "Yuan-fang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16454",
        "abstract": "Classifying fine-grained actions in fast-paced, close-combat sports such as fencing and boxing presents unique challenges due to the complexity, speed, and nuance of movements. Traditional methods reliant on pose estimation or fancy sensor data often struggle to capture these dynamics accurately. We introduce FACTS, a novel transformer-based approach for fine-grained action recognition that processes raw video data directly, eliminating the need for pose estimation and the use of cumbersome body markers and sensors. FACTS achieves state-of-the-art performance, with 90% accuracy on fencing actions and 83.25% on boxing actions. Additionally, we present a new publicly available dataset featuring 8 detailed fencing actions, addressing critical gaps in sports analytics resources. Our findings enhance training, performance analysis, and spectator engagement, setting a new benchmark for action classification in tactical sports.",
        "tags": [
            "Pose Estimation",
            "Transformer"
        ]
    },
    {
        "id": "45",
        "title": "Research on Violent Text Detection System Based on BERT-fasttext Model",
        "author": [
            "Yongsheng Yang",
            "Xiaoying Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16455",
        "abstract": "In the digital age of today, the internet has become an indispensable platform for people's lives, work, and information exchange. However, the problem of violent text proliferation in the network environment has arisen, which has brought about many negative effects. In view of this situation, it is particularly important to build an effective system for cutting off violent text. The study of violent text cutting off based on the BERT-fasttext model has significant meaning. BERT is a pre-trained language model with strong natural language understanding ability, which can deeply mine and analyze text semantic information; Fasttext itself is an efficient text classification tool with low complexity and good effect, which can quickly provide basic judgments for text processing. By combining the two and applying them to the system for cutting off violent text, on the one hand, it can accurately identify violent text, and on the other hand, it can efficiently and reasonably cut off the content, preventing harmful information from spreading freely on the network. Compared with the single BERT model and fasttext, the accuracy was improved by 0.7% and 0.8%, respectively. Through this model, it is helpful to purify the network environment, maintain the health of network information, and create a positive, civilized, and harmonious online communication space for netizens, driving the development of social networking, information dissemination, and other aspects in a more benign direction.",
        "tags": [
            "BERT",
            "Detection"
        ]
    },
    {
        "id": "46",
        "title": "Transducer-Llama: Integrating LLMs into Streamable Transducer-based Speech Recognition",
        "author": [
            "Keqi Deng",
            "Jinxi Guo",
            "Yingyi Ma",
            "Niko Moritz",
            "Philip C. Woodland",
            "Ozlem Kalinli",
            "Mike Seltzer"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16464",
        "abstract": "While large language models (LLMs) have been applied to automatic speech recognition (ASR), the task of making the model streamable remains a challenge. This paper proposes a novel model architecture, Transducer-Llama, that integrates LLMs into a Factorized Transducer (FT) model, naturally enabling streaming capabilities. Furthermore, given that the large vocabulary of LLMs can cause data sparsity issue and increased training costs for spoken language systems, this paper introduces an efficient vocabulary adaptation technique to align LLMs with speech system vocabularies. The results show that directly optimizing the FT model with a strong pre-trained LLM-based predictor using the RNN-T loss yields some but limited improvements over a smaller pre-trained LM predictor. Therefore, this paper proposes a weak-to-strong LM swap strategy, using a weak LM predictor during RNN-T loss training and then replacing it with a strong LLM. After LM replacement, the minimum word error rate (MWER) loss is employed to finetune the integration of the LLM predictor with the Transducer-Llama model. Experiments on the LibriSpeech and large-scale multi-lingual LibriSpeech corpora show that the proposed streaming Transducer-Llama approach gave a 17% relative WER reduction (WERR) over a strong FT baseline and a 32% WERR over an RNN-T baseline.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "RNN"
        ]
    },
    {
        "id": "47",
        "title": "The Road to Artificial SuperIntelligence: A Comprehensive Survey of Superalignment",
        "author": [
            "HyunJin Kim",
            "Xiaoyuan Yi",
            "Jing Yao",
            "Jianxun Lian",
            "Muhua Huang",
            "Shitong Duan",
            "JinYeong Bak",
            "Xing Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16468",
        "abstract": "The emergence of large language models (LLMs) has sparked the possibility of about Artificial Superintelligence (ASI), a hypothetical AI system surpassing human intelligence. However, existing alignment paradigms struggle to guide such advanced AI systems. Superalignment, the alignment of AI systems with human values and safety requirements at superhuman levels of capability aims to addresses two primary goals -- scalability in supervision to provide high-quality guidance signals and robust governance to ensure alignment with human values. In this survey, we examine scalable oversight methods and potential solutions for superalignment. Specifically, we explore the concept of ASI, the challenges it poses, and the limitations of current alignment paradigms in addressing the superalignment problem. Then we review scalable oversight methods for superalignment. Finally, we discuss the key challenges and propose pathways for the safe and continual improvement of ASI systems. By comprehensively reviewing the current literature, our goal is provide a systematical introduction of existing methods, analyze their strengths and limitations, and discuss potential future directions.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "48",
        "title": "Chained Tuning Leads to Biased Forgetting",
        "author": [
            "Megan Ung",
            "Alicia Sun",
            "Samuel J. Bell",
            "Bhaktipriya Radharapu",
            "Levent Sagun",
            "Adina Williams"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16469",
        "abstract": "Large language models (LLMs) are often fine-tuned for use on downstream tasks, though this can degrade capabilities learned during previous training. This phenomenon, often referred to as catastrophic forgetting, has important potential implications for the safety of deployed models. In this work, we first show that models trained on downstream tasks forget their safety tuning to a greater extent than models trained in the opposite http://order.Second, we show that forgetting disproportionately impacts safety information about certain groups. To quantify this phenomenon, we define a new metric we term biased forgetting. We conduct a systematic evaluation of the effects of task ordering on forgetting and apply mitigations that can help the model recover from the forgetting observed. We hope our findings can better inform methods for chaining the finetuning of LLMs in continual learning settings to enable training of safer and less toxic models.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "49",
        "title": "\"ScatSpotter\" 2024 -- A Distributed Dog Poop Detection Dataset",
        "author": [
            "Jon Crall"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16473",
        "abstract": "We introduce a new -- currently 42 gigabyte -- ``living'' dataset of phone images of dog feces, annotated with manually drawn or AI-assisted polygon labels. There are 6k full resolution images and 4k detailed polygon annotations. The collection and annotation of images started in late 2020 and the dataset grows by roughly 1GB a month. We train VIT and MaskRCNN baseline models to explore the difficulty of the dataset. The best model achieves a pixelwise average precision of 0.858 on a 691-image validation set and 0.847 on a small independently captured 30-image contributor test set. The most recent snapshot of dataset is made publicly available through three different distribution methods: one centralized (Girder) and two decentralized (IPFS and BitTorrent). We study of the trade-offs between distribution methods and discuss the feasibility of each with respect to reliably sharing open scientific data. The code to reproduce the experiments is hosted on GitHub, and the data is published under the Creative Commons Attribution 4.0 International license. Model weights are made publicly available with the dataset. Experimental hardware, time, energy, and emissions are quantified.",
        "tags": [
            "Detection",
            "ViT"
        ]
    },
    {
        "id": "50",
        "title": "When Can Proxies Improve the Sample Complexity of Preference Learning?",
        "author": [
            "Yuchen Zhu",
            "Daniel Augusto de Souza",
            "Zhengyan Shi",
            "Mengyue Yang",
            "Pasquale Minervini",
            "Alexander D'Amour",
            "Matt J. Kusner"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16475",
        "abstract": "We address the problem of reward hacking, where maximising a proxy reward does not necessarily increase the true reward. This is a key concern for Large Language Models (LLMs), as they are often fine-tuned on human preferences that may not accurately reflect a true objective. Existing work uses various tricks such as regularisation, tweaks to the reward model, and reward hacking detectors, to limit the influence that such proxy preferences have on a model. Luckily, in many contexts such as medicine, education, and law, a sparse amount of expert data is often available. In these cases, it is often unclear whether the addition of proxy data can improve policy learning. We outline a set of sufficient conditions on proxy feedback that, if satisfied, indicate that proxy data can provably improve the sample complexity of learning the ground truth policy. These conditions can inform the data collection process for specific tasks. The result implies a parameterisation for LLMs that achieves this improved sample complexity. We detail how one can adapt existing architectures to yield this improved sample complexity.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "51",
        "title": "Query Quantized Neural SLAM",
        "author": [
            "Sijia Jiang",
            "Jing Hua",
            "Zhizhong Han"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16476",
        "abstract": "Neural implicit representations have shown remarkable abilities in jointly modeling geometry, color, and camera poses in simultaneous localization and mapping (SLAM). Current methods use coordinates, positional encodings, or other geometry features as input to query neural implicit functions for signed distances and color which produce rendering errors to drive the optimization in overfitting image observations. However, due to the run time efficiency requirement in SLAM systems, we are merely allowed to conduct optimization on each frame in few iterations, which is far from enough for neural networks to overfit these queries. The underfitting usually results in severe drifts in camera tracking and artifacts in reconstruction. To resolve this issue, we propose query quantized neural SLAM which uses quantized queries to reduce variations of input for much easier and faster overfitting a frame. To this end, we quantize a query into a discrete representation with a set of codes, and only allow neural networks to observe a finite number of variations. This allows neural networks to become increasingly familiar with these codes after overfitting more and more previous frames. Moreover, we also introduce novel initialization, losses, and argumentation to stabilize the optimization with significant uncertainty in the early optimization stage, constrain the optimization space, and estimate camera poses more accurately. We justify the effectiveness of each design and report visual and numerical comparisons on widely used benchmarks to show our superiority over the latest methods in both reconstruction and camera tracking.",
        "tags": [
            "SLAM"
        ]
    },
    {
        "id": "52",
        "title": "Enhancing Nighttime Vehicle Detection with Day-to-Night Style Transfer and Labeling-Free Augmentation",
        "author": [
            "Yunxiang Yang",
            "Hao Zhen",
            "Yongcan Huang",
            "Jidong J. Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16478",
        "abstract": "Existing deep learning-based object detection models perform well under daytime conditions but face significant challenges at night, primarily because they are predominantly trained on daytime images. Additionally, training with nighttime images presents another challenge: even human annotators struggle to accurately label objects in low-light conditions. This issue is particularly pronounced in transportation applications, such as detecting vehicles and other objects of interest on rural roads at night, where street lighting is often absent, and headlights may introduce undesirable glare. This study addresses these challenges by introducing a novel framework for labeling-free data augmentation, leveraging CARLA-generated synthetic data for day-to-night image style transfer. Specifically, the framework incorporates the Efficient Attention Generative Adversarial Network for realistic day-to-night style transfer and uses CARLA-generated synthetic nighttime images to help the model learn vehicle headlight effects. To evaluate the efficacy of the proposed framework, we fine-tuned the YOLO11 model with an augmented dataset specifically curated for rural nighttime environments, achieving significant improvements in nighttime vehicle detection. This novel approach is simple yet effective, offering a scalable solution to enhance AI-based detection systems in low-visibility environments and extend the applicability of object detection models to broader real-world contexts.",
        "tags": [
            "Detection",
            "Style Transfer"
        ]
    },
    {
        "id": "53",
        "title": "Flash3D: Super-scaling Point Transformers through Joint Hardware-Geometry Locality",
        "author": [
            "Liyan Chen",
            "Gregory P. Meyer",
            "Zaiwei Zhang",
            "Eric M. Wolff",
            "Paul Vernaza"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16481",
        "abstract": "Recent efforts recognize the power of scale in 3D learning (e.g. PTv3) and attention mechanisms (e.g. FlashAttention). However, current point cloud backbones fail to holistically unify geometric locality, attention mechanisms, and GPU architectures in one view. In this paper, we introduce Flash3D Transformer, which aligns geometric locality and GPU tiling through a principled locality mechanism based on Perfect Spatial Hashing (PSH). The common alignment with GPU tiling naturally fuses our PSH locality mechanism with FlashAttention at negligible extra cost. This mechanism affords flexible design choices throughout the backbone that result in superior downstream task results. Flash3D outperforms state-of-the-art PTv3 results on benchmark datasets, delivering a 2.25x speed increase and 2.4x memory efficiency boost. This efficiency enables scaling to wider attention scopes and larger models without additional overhead. Such scaling allows Flash3D to achieve even higher task accuracies than PTv3 under the same compute budget.",
        "tags": [
            "3D",
            "Transformer"
        ]
    },
    {
        "id": "54",
        "title": "Trusted Mamba Contrastive Network for Multi-View Clustering",
        "author": [
            "Jian Zhu",
            "Xin Zou",
            "Lei Liu",
            "Zhangmin Huang",
            "Ying Zhang",
            "Chang Tang",
            "Li-Rong Dai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16487",
        "abstract": "Multi-view clustering can partition data samples into their categories by learning a consensus representation in an unsupervised way and has received more and more attention in recent years. However, there is an untrusted fusion problem. The reasons for this problem are as follows: 1) The current methods ignore the presence of noise or redundant information in the view; 2) The similarity of contrastive learning comes from the same sample rather than the same cluster in deep multi-view clustering. It causes multi-view fusion in the wrong direction. This paper proposes a novel multi-view clustering network to address this problem, termed as Trusted Mamba Contrastive Network (TMCN). Specifically, we present a new Trusted Mamba Fusion Network (TMFN), which achieves a trusted fusion of multi-view data through a selective mechanism. Moreover, we align the fused representation and the view-specific representation using the Average-similarity Contrastive Learning (AsCL) module. AsCL increases the similarity of view presentation from the same cluster, not merely from the same sample. Extensive experiments show that the proposed method achieves state-of-the-art results in deep multi-view clustering tasks.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "55",
        "title": "ImagePiece: Content-aware Re-tokenization for Efficient Image Recognition",
        "author": [
            "Seungdong Yoa",
            "Seungjun Lee",
            "Hyeseung Cho",
            "Bumsoo Kim",
            "Woohyung Lim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16491",
        "abstract": "Vision Transformers (ViTs) have achieved remarkable success in various computer vision tasks. However, ViTs have a huge computational cost due to their inherent reliance on multi-head self-attention (MHSA), prompting efforts to accelerate ViTs for practical applications. To this end, recent works aim to reduce the number of tokens, mainly focusing on how to effectively prune or merge them. Nevertheless, since ViT tokens are generated from non-overlapping grid patches, they usually do not convey sufficient semantics, making it incompatible with efficient ViTs. To address this, we propose ImagePiece, a novel re-tokenization strategy for Vision Transformers. Following the MaxMatch strategy of NLP tokenization, ImagePiece groups semantically insufficient yet locally coherent tokens until they convey meaning. This simple retokenization is highly compatible with previous token reduction methods, being able to drastically narrow down relevant tokens, enhancing the inference speed of DeiT-S by 54% (nearly 1.5$\\times$ faster) while achieving a 0.39% improvement in ImageNet classification accuracy. For hyper-speed inference scenarios (with 251% acceleration), our approach surpasses other baselines by an accuracy over 8%.",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "56",
        "title": "Follow-Your-MultiPose: Tuning-Free Multi-Character Text-to-Video Generation via Pose Guidance",
        "author": [
            "Beiyuan Zhang",
            "Yue Ma",
            "Chunlei Fu",
            "Xinyang Song",
            "Zhenan Sun",
            "Ziqiang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16495",
        "abstract": "Text-editable and pose-controllable character video generation is a challenging but prevailing topic with practical applications. However, existing approaches mainly focus on single-object video generation with pose guidance, ignoring the realistic situation that multi-character appear concurrently in a scenario. To tackle this, we propose a novel multi-character video generation framework in a tuning-free manner, which is based on the separated text and pose guidance. Specifically, we first extract character masks from the pose sequence to identify the spatial position for each generating character, and then single prompts for each character are obtained with LLMs for precise text guidance. Moreover, the spatial-aligned cross attention and multi-branch control module are proposed to generate fine grained controllable multi-character video. The visualized results of generating video demonstrate the precise controllability of our method for multi-character generation. We also verify the generality of our method by applying it to various personalized T2I models. Moreover, the quantitative results show that our approach achieves superior performance compared with previous works.",
        "tags": [
            "LLMs",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "57",
        "title": "VSFormer: Value and Shape-Aware Transformer with Prior-Enhanced Self-Attention for Multivariate Time Series Classification",
        "author": [
            "Wenjie Xi",
            "Rundong Zuo",
            "Alejandro Alvarez",
            "Jie Zhang",
            "Byron Choi",
            "Jessica Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16515",
        "abstract": "Multivariate time series classification is a crucial task in data mining, attracting growing research interest due to its broad applications. While many existing methods focus on discovering discriminative patterns in time series, real-world data does not always present such patterns, and sometimes raw numerical values can also serve as discriminative features. Additionally, the recent success of Transformer models has inspired many studies. However, when applying to time series classification, the self-attention mechanisms in Transformer models could introduce classification-irrelevant features, thereby compromising accuracy. To address these challenges, we propose a novel method, VSFormer, that incorporates both discriminative patterns (shape) and numerical information (value). In addition, we extract class-specific prior information derived from supervised information to enrich the positional encoding and provide classification-oriented self-attention learning, thereby enhancing its effectiveness. Extensive experiments on all 30 UEA archived datasets demonstrate the superior performance of our method compared to SOTA models. Through ablation studies, we demonstrate the effectiveness of the improved encoding layer and the proposed self-attention mechanism. Finally, We provide a case study on a real-world time series dataset without discriminative patterns to interpret our model.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "58",
        "title": "HammerBench: Fine-Grained Function-Calling Evaluation in Real Mobile Device Scenarios",
        "author": [
            "Jun Wang",
            "Jiamu Zhou",
            "Muning Wen",
            "Xiaoyun Mo",
            "Haoyu Zhang",
            "Qiqiang Lin",
            "Cheng Jin",
            "Xihuai Wang",
            "Weinan Zhang",
            "Qiuying Peng",
            "Jun Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16516",
        "abstract": "Evaluating the capabilities of large language models (LLMs) in human-LLM interactions remains challenging due to the inherent complexity and openness of dialogue processes. This paper introduces HammerBench, a novel benchmarking framework designed to assess the function-calling ability of LLMs more effectively in such interactions. We model a wide range of real-world user scenarios on mobile devices, encompassing imperfect instructions, diverse question-answer trajectories, intent/argument shifts, and the use of external individual information through pronouns. To construct the corresponding datasets, we propose a comprehensive pipeline that involves LLM-generated data and multiple rounds of human validation, ensuring high data quality. Additionally, we decompose the conversations into function-calling snapshots, enabling a fine-grained evaluation of each turn. We evaluate several popular LLMs using HammerBench and highlight different performance aspects. Our empirical findings reveal that errors in parameter naming constitute the primary factor behind conversation failures across different data types.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "59",
        "title": "Text2midi: Generating Symbolic Music from Captions",
        "author": [
            "Keshav Bhandari",
            "Abhinaba Roy",
            "Kyra Wang",
            "Geeta Puri",
            "Simon Colton",
            "Dorien Herremans"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16526",
        "abstract": "This paper introduces text2midi, an end-to-end model to generate MIDI files from textual descriptions. Leveraging the growing popularity of multimodal generative approaches, text2midi capitalizes on the extensive availability of textual data and the success of large language models (LLMs). Our end-to-end system harnesses the power of LLMs to generate symbolic music in the form of MIDI files. Specifically, we utilize a pretrained LLM encoder to process captions, which then condition an autoregressive transformer decoder to produce MIDI sequences that accurately reflect the provided descriptions. This intuitive and user-friendly method significantly streamlines the music creation process by allowing users to generate music pieces using text prompts. We conduct comprehensive empirical evaluations, incorporating both automated and human studies, that show our model generates MIDI files of high quality that are indeed controllable by text captions that may include music theory terms such as chords, keys, and tempo. We release the code and music samples on our demo page (https://github.com/AMAAI-Lab/Text2midi) for users to interact with text2midi.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "60",
        "title": "From Creation to Curriculum: Examining the role of generative AI in Arts Universities",
        "author": [
            "Atticus Sims"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16531",
        "abstract": "The age of Artificial Intelligence (AI) is marked by its transformative \"generative\" capabilities, distinguishing it from prior iterations. This burgeoning characteristic of AI has enabled it to produce new and original content, inherently showcasing its creative prowess. This shift challenges and requires a recalibration in the realm of arts education, urging a departure from established pedagogies centered on human-driven image creation. The paper meticulously addresses the integration of AI tools, with a spotlight on Stable Diffusion (SD), into university arts curricula. Drawing from practical insights gathered from workshops conducted in July 2023, which culminated in an exhibition of AI-driven artworks, the paper aims to provide a roadmap for seamlessly infusing these tools into academic settings. Given their recent emergence, the paper delves into a comprehensive overview of such tools, emphasizing the intricate dance between artists, developers, and researchers in the open-source AI art world. This discourse extends to the challenges and imperatives faced by educational institutions. It presents a compelling case for the swift adoption of these avant-garde tools, underscoring the paramount importance of equipping students with the competencies required to thrive in an AI-augmented artistic landscape.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "61",
        "title": "Self-guided Knowledgeable Network of Thoughts: Amplifying Reasoning with Large Language Models",
        "author": [
            "Chao-Chi Chen",
            "Chin-Yuan Yeh",
            "Hsi-Wen Chen",
            "De-Nian Yang",
            "Ming-Syan Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16533",
        "abstract": "We introduce Knowledgeable Network of Thoughts (kNoT): a prompt scheme that advances the capabilities of large language models (LLMs) beyond existing paradigms like Chain-of-Thought (CoT), Tree of Thoughts (ToT), and Graph of Thoughts (GoT). The key innovation of kNoT is the LLM Workflow Template (LWT), which allows for an executable plan to be specified by LLMs for LLMs. LWT allows these plans to be arbitrary networks, where single-step LLM operations are nodes, and edges correspond to message passing between these steps. Furthermore, LWT supports selection of individual elements through indexing, facilitating kNoT to produce intricate plans where each LLM operation can be limited to elementary operations, greatly enhancing reliability over extended task sequences. We demonstrate that kNoT significantly outperforms the state of the art on six use cases, while reducing the need for extensive prompt engineering. For instance, kNoT finds 92% accuracy for sorting 32 numbers over 12% and 31% for ToT and GoT, while utilizing up to 84.4% and 87.3% less task-specific prompts, respectively.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "62",
        "title": "Accelerating Private Large Transformers Inference through Fine-grained Collaborative Computation",
        "author": [
            "Yuntian Chen",
            "Zhanyong Tang",
            "Tianpei Lu",
            "Bingsheng Zhang",
            "Zhiying Shi",
            "Zheng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16537",
        "abstract": "Homomorphic encryption (HE) and secret sharing (SS) enable computations on encrypted data, providing significant privacy benefits for large transformer-based models (TBM) in sensitive sectors like medicine and finance. However, private TBM inference incurs significant costs due to the coarse-grained application of HE and SS. We present FASTLMPI, a new approach to accelerate private TBM inference through fine-grained computation optimization. Specifically, through the fine-grained co-design of homomorphic encryption and secret sharing, FASTLMPI achieves efficient protocols for matrix multiplication, SoftMax, LayerNorm, and GeLU. In addition, FASTLMPI introduces a precise segmented approximation technique for differentiable non-linear, improving its fitting accuracy while maintaining a low polynomial degree. Compared to solution BOLT (S\\&P'24), \\SystemName shows a remarkable 54\\% to 64\\% decrease in runtime and an impressive 72.2\\% reduction in communication costs.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "63",
        "title": "Mathematics and Machine Creativity: A Survey on Bridging Mathematics with AI",
        "author": [
            "Shizhe Liang",
            "Wei Zhang",
            "Tianyang Zhong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16543",
        "abstract": "This paper presents a comprehensive survey on the applications of artificial intelligence (AI) in mathematical research, highlighting the transformative role AI has begun to play in this domain. Traditionally, AI advancements have heavily relied on theoretical foundations from fields like mathematics and statistics. However, recent developments in AI, particularly in reinforcement learning (RL) and large language models (LLMs), have demonstrated the potential for AI to contribute back to mathematics, offering flexible algorithmic frameworks and powerful inductive reasoning capabilities that support various aspects of mathematical research. This survey aims to establish a bridge between AI and mathematics, providing insights into the mutual benefits and fostering deeper interdisciplinary understanding.\nIn particular, we argue that while current AI and LLMs may struggle with complex deductive reasoning, their inherent creativity holds significant potential to support and inspire mathematical research. This creative capability, often overlooked, could be the key to unlocking new perspectives and methodologies in mathematics. Furthermore, we address the lack of cross-disciplinary communication: mathematicians may not fully comprehend the latest advances in AI, while AI researchers frequently prioritize benchmarks and standardized testing over AI's applications in frontier mathematical research. This paper seeks to close that gap, offering a detailed exploration of AI's basic knowledge, its strengths, and its emerging applications in the mathematical sciences.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RL"
        ]
    },
    {
        "id": "64",
        "title": "Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models",
        "author": [
            "Zhisong Zhang",
            "Yan Wang",
            "Xinting Huang",
            "Tianqing Fang",
            "Hongming Zhang",
            "Chenlong Deng",
            "Shuaiyi Li",
            "Dong Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16545",
        "abstract": "Large language models have shown remarkable performance across a wide range of language tasks, owing to their exceptional capabilities in context modeling. The most commonly used method of context modeling is full self-attention, as seen in standard decoder-only Transformers. Although powerful, this method can be inefficient for long sequences and may overlook inherent input structures. To address these problems, an alternative approach is parallel context encoding, which splits the context into sub-pieces and encodes them parallelly. Because parallel patterns are not encountered during training, naively applying parallel encoding leads to performance degradation. However, the underlying reasons and potential mitigations are unclear. In this work, we provide a detailed analysis of this issue and identify that unusually high attention entropy can be a key factor. Furthermore, we adopt two straightforward methods to reduce attention entropy by incorporating attention sinks and selective mechanisms. Experiments on various tasks reveal that these methods effectively lower irregular attention entropy and narrow performance gaps. We hope this study can illuminate ways to enhance context modeling mechanisms.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "65",
        "title": "ActPC-Chem: Discrete Active Predictive Coding for Goal-Guided Algorithmic Chemistry as a Potential Cognitive Kernel for Hyperon & PRIMUS-Based AGI",
        "author": [
            "Ben Goertzel"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16547",
        "abstract": "We explore a novel paradigm (labeled ActPC-Chem) for biologically inspired, goal-guided artificial intelligence (AI) centered on a form of Discrete Active Predictive Coding (ActPC) operating within an algorithmic chemistry of rewrite rules. ActPC-Chem is envisioned as a foundational \"cognitive kernel\" for advanced cognitive architectures, such as the OpenCog Hyperon system, incorporating essential elements of the PRIMUS cognitive architecture. The central thesis is that general-intelligence-capable cognitive structures and dynamics can emerge in a system where both data and models are represented as evolving patterns of metagraph rewrite rules, and where prediction errors, intrinsic and extrinsic rewards, and semantic constraints guide the continual reorganization and refinement of these rules. Using a virtual \"robot bug\" thought experiment, we illustrate how such a system might self-organize to handle challenging tasks involving delayed and context-dependent rewards, integrating causal rule inference (AIRIS) and probabilistic logical abstraction (PLN) to discover and exploit conceptual patterns and causal constraints. Next, we describe how continuous predictive coding neural networks, which excel at handling noisy sensory data and motor control signals, can be coherently merged with the discrete ActPC substrate. Finally, we outline how these ideas might be extended to create a transformer-like architecture that foregoes traditional backpropagation in favor of rule-based transformations guided by ActPC. This layered architecture, supplemented with AIRIS and PLN, promises structured, multi-modal, and logically consistent next-token predictions and narrative sequences.",
        "tags": [
            "Robot",
            "Transformer"
        ]
    },
    {
        "id": "66",
        "title": "Diffusion Prior Interpolation for Flexibility Real-World Face Super-Resolution",
        "author": [
            "Jiarui Yang",
            "Tao Dai",
            "Yufei Zhu",
            "Naiqi Li",
            "Jinmin Li",
            "Shutao Xia"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16552",
        "abstract": "Diffusion models represent the state-of-the-art in generative modeling. Due to their high training costs, many works leverage pre-trained diffusion models' powerful representations for downstream tasks, such as face super-resolution (FSR), through fine-tuning or prior-based methods. However, relying solely on priors without supervised training makes it challenging to meet the pixel-level accuracy requirements of discrimination task. Although prior-based methods can achieve high fidelity and high-quality results, ensuring consistency remains a significant challenge. In this paper, we propose a masking strategy with strong and weak constraints and iterative refinement for real-world FSR, termed Diffusion Prior Interpolation (DPI). We introduce conditions and constraints on consistency by masking different sampling stages based on the structural characteristics of the face. Furthermore, we propose a condition Corrector (CRT) to establish a reciprocal posterior sampling process, enhancing FSR performance by mutual refinement of conditions and samples. DPI can balance consistency and diversity and can be seamlessly integrated into pre-trained models. In extensive experiments conducted on synthetic and real datasets, along with consistency validation in face recognition, DPI demonstrates superiority over SOTA FSR methods. The code is available at \\url{https://github.com/JerryYann/DPI}.",
        "tags": [
            "Diffusion",
            "Super Resolution"
        ]
    },
    {
        "id": "67",
        "title": "Semantics Prompting Data-Free Quantization for Low-Bit Vision Transformers",
        "author": [
            "Yunshan Zhong",
            "Yuyao Zhou",
            "Yuxin Zhang",
            "Shen Li",
            "Yong Li",
            "Fei Chao",
            "Zhanpeng Zeng",
            "Rongrong Ji"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16553",
        "abstract": "Data-free quantization (DFQ), which facilitates model quantization without real data to address increasing concerns about data security, has garnered significant attention within the model compression community. Recently, the unique architecture of vision transformers (ViTs) has driven the development of specialized DFQ techniques. However, we observe that the synthetic images from existing methods suffer from the deficient semantics issue compared to real images, thereby compromising performance. Motivated by this, we propose SPDFQ, a Semantics Prompting Data-Free Quantization method for ViTs. First, SPDFQ incorporates Attention Priors Alignment (APA), which uses randomly generated attention priors to enhance the semantics of synthetic images. Second, SPDFQ introduces Multi-Semantic Reinforcement (MSR), which utilizes localized patch optimization to prompt efficient parameterization and diverse semantics in synthetic images. Finally, SPDFQ employs Softlabel Learning (SL), where soft learning targets are adapted to encourage more complex semantics and accommodate images augmented by MSR. Experimental results demonstrate that SPDFQ significantly outperforms existing methods. For instance, SPDFQ achieves a 15.52% increase in top-1 accuracy on ImageNet for W4A4 ViT-B",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "68",
        "title": "Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models",
        "author": [
            "Yanxu Mao",
            "Peipei Liu",
            "Tiehan Cui",
            "Congying Liu",
            "Datao You"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16555",
        "abstract": "Large language models (LLMs) are widely applied in various fields of society due to their powerful reasoning, understanding, and generation capabilities. However, the security issues associated with these models are becoming increasingly severe. Jailbreaking attacks, as an important method for detecting vulnerabilities in LLMs, have been explored by researchers who attempt to induce these models to generate harmful content through various attack methods. Nevertheless, existing jailbreaking methods face numerous limitations, such as excessive query counts, limited coverage of jailbreak modalities, low attack success rates, and simplistic evaluation methods. To overcome these constraints, this paper proposes a multimodal jailbreaking method: JMLLM. This method integrates multiple strategies to perform comprehensive jailbreak attacks across text, visual, and auditory modalities. Additionally, we contribute a new and comprehensive dataset for multimodal jailbreaking research: TriJail, which includes jailbreak prompts for all three modalities. Experiments on the TriJail dataset and the benchmark dataset AdvBench, conducted on 13 popular LLMs, demonstrate advanced attack success rates and significant reduction in time overhead.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "69",
        "title": "Improving FIM Code Completions via Context & Curriculum Based Learning",
        "author": [
            "Hitesh Sagtani",
            "Rishabh Mehrotra",
            "Beyang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16589",
        "abstract": "Fill-in-the-Middle (FIM) models play a vital role in code completion tasks, leveraging both prefix and suffix context to provide more accurate and contextually relevant suggestions. This paper presents approaches to improve FIM code completion while addressing the challenge of maintaining low latency for real-time coding assistance. We enhance FIM code completion by incorporating context and curriculum examples in the training process. We identify patterns where completion suggestions fail more frequently, revealing complexities that smaller language models struggle with. To address these challenges, we develop a curriculum dataset by extracting hard-to-complete patterns from code repositories and generate context examples using semantic and static analysis tools (e.g. TSC compiler). We fine-tune various sized models, including StarCoder and DeepSeek, on this enhanced dataset. Our evaluation encompasses three key dimensions: the Santa Coder FIM task, the Amazon CCEval benchmark, and a new Multi-Line Infilling evaluation benchmark derived from SWE-bench. Comprehensive ablation studies across multiple model sizes reveal that while all fine-tuned models show improvements, the performance gains are more pronounced for smaller parameter models and incorporating difficult-to-complete examples, as part of curriculum learning, improves the code completion performance. This finding is particularly significant given the latency constraints of code completion tasks. While larger models like GPT and Claude perform well in multi-line completions but are prohibitively challenging to use given high latency, and our fine-tuned models achieve a balance between performance and latency. Finally, we validate our approach through online A/B testing, demonstrating tangible improvements in Completion Acceptance Rate (CAR) and Completion Persistence Rate (CPR), with zero latency impact.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "70",
        "title": "Designing LLM-Based Voice-Control for Surgical Augmented Reality Navigation System in Pancreatic Surgery",
        "author": [
            "Hamraz Javaheri",
            "Omid Ghamarnejad",
            "Paul Lukowicz",
            "Gregor Alexander Stavrou",
            "akob Karolus"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16597",
        "abstract": "Wearable Augmented Reality (AR) technologies are gaining recognition for their potential to transform surgical navigation systems. As these technologies evolve, selecting the right interaction method to control the system becomes crucial. Our work introduces a voice-controlled user interface (VCUI) for surgical AR assistance systems (ARAS), designed for pancreatic surgery, that integrates Large Language Models (LLMs). Employing a mixed-method research approach, we assessed the usability of our LLM-based design in both simulated surgical tasks and during pancreatic surgeries, comparing its performance against conventional VCUI for surgical ARAS using speech commands. Our findings demonstrated the usability of our proposed LLM-based VCUI, yielding a significantly lower task completion time and cognitive workload compared to speech commands. Additionally, qualitative insights from interviews with surgeons aligned with the quantitative data, revealing a strong preference for the LLM-based VCUI. Surgeons emphasized its intuitiveness and highlighted the potential of LLM-based VCUI in expediting decision-making in surgical environments.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "71",
        "title": "V\"Mean\"ba: Visual State Space Models only need 1 hidden dimension",
        "author": [
            "Tien-Yu Chi",
            "Hung-Yueh Chiang",
            "Chi-Chih Chang",
            "Ning-Chi Huang",
            "Kai-Chiang Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16602",
        "abstract": "Vision transformers dominate image processing tasks due to their superior performance. However, the quadratic complexity of self-attention limits the scalability of these systems and their deployment on resource-constrained devices. State Space Models (SSMs) have emerged as a solution by introducing a linear recurrence mechanism, which reduces the complexity of sequence modeling from quadratic to linear. Recently, SSMs have been extended to high-resolution vision tasks. Nonetheless, the linear recurrence mechanism struggles to fully utilize matrix multiplication units on modern hardware, resulting in a computational bottleneck. We address this issue by introducing \\textit{VMeanba}, a training-free compression method that eliminates the channel dimension in SSMs using mean operations. Our key observation is that the output activations of SSM blocks exhibit low variances across channels. Our \\textit{VMeanba} leverages this property to optimize computation by averaging activation maps across the channel to reduce the computational overhead without compromising accuracy. Evaluations on image classification and semantic segmentation tasks demonstrate that \\textit{VMeanba} achieves up to a 1.12x speedup with less than a 3\\% accuracy loss. When combined with 40\\% unstructured pruning, the accuracy drop remains under 3\\%.",
        "tags": [
            "SSMs",
            "Segmentation",
            "State Space Models"
        ]
    },
    {
        "id": "72",
        "title": "OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional Images with Editable Capabilities",
        "author": [
            "Suyoung Lee",
            "Jaeyoung Chung",
            "Kihoon Kim",
            "Jaeyoo Huh",
            "Gunhee Lee",
            "Minsoo Lee",
            "Kyoung Mu Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16604",
        "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) models have gained significant popularity due to their ability to generate scenes immediately without needing per-scene optimization. Although omnidirectional images are getting more popular since they reduce the computation for image stitching to composite a holistic scene, existing feed-forward models are only designed for perspective images. The unique optical properties of omnidirectional images make it difficult for feature encoders to correctly understand the context of the image and make the Gaussian non-uniform in space, which hinders the image quality synthesized from novel views. We propose OmniSplat, a pioneering work for fast feed-forward 3DGS generation from a few omnidirectional images. We introduce Yin-Yang grid and decompose images based on it to reduce the domain gap between omnidirectional and perspective images. The Yin-Yang grid can use the existing CNN structure as it is, but its quasi-uniform characteristic allows the decomposed image to be similar to a perspective image, so it can exploit the strong prior knowledge of the learned feed-forward network. OmniSplat demonstrates higher reconstruction accuracy than existing feed-forward networks trained on perspective images. Furthermore, we enhance the segmentation consistency between omnidirectional images by leveraging attention from the encoder of OmniSplat, providing fast and clean 3DGS editing results.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Segmentation"
        ]
    },
    {
        "id": "73",
        "title": "Concept Guided Co-saliency Objection Detection",
        "author": [
            "Jiayi Zhu",
            "Qing Guo",
            "Felix Juefei-Xu",
            "Yihao Huang",
            "Yang Liu",
            "Geguang Pu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16609",
        "abstract": "The task of co-saliency object detection (Co-SOD) seeks to identify common, salient objects across a collection of images by examining shared visual features. However, traditional Co-SOD methods often encounter limitations when faced with diverse object variations (e.g., different postures) and irrelevant background elements that introduce noise. To address these challenges, we propose ConceptCoSOD, a novel concept-guided approach that leverages text semantic information to enhance Co-SOD performance by guiding the model to focus on consistent object features. Through rethinking Co-SOD as an (image-text)-to-image task instead of an image-to-image task, ConceptCoSOD first captures shared semantic concepts within an image group and then uses them as guidance for precise object segmentation in complex scenarios. Experimental results on three benchmark datasets and six corruptions reveal that ConceptCoSOD significantly improves detection accuracy, especially in challenging settings with considerable background distractions and object variability.",
        "tags": [
            "Detection",
            "Segmentation",
            "Text-to-Image"
        ]
    },
    {
        "id": "74",
        "title": "Automated Classification of Cybercrime Complaints using Transformer-based Language Models for Hinglish Texts",
        "author": [
            "Nanda Rani",
            "Divyanshu Singh",
            "Bikash Saha",
            "Sandeep Kumar Shukla"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16614",
        "abstract": "The rise in cybercrime and the complexity of multilingual and code-mixed complaints present significant challenges for law enforcement and cybersecurity agencies. These organizations need automated, scalable methods to identify crime types, enabling efficient processing and prioritization of large complaint volumes. Manual triaging is inefficient, and traditional machine learning methods fail to capture the semantic and contextual nuances of textual cybercrime complaints. Moreover, the lack of publicly available datasets and privacy concerns hinder the research to present robust solutions. To address these challenges, we propose a framework for automated cybercrime complaint classification. The framework leverages Hinglish-adapted transformers, such as HingBERT and HingRoBERTa, to handle code-mixed inputs effectively. We employ the real-world dataset provided by Indian Cybercrime Coordination Centre (I4C) during CyberGuard AI Hackathon 2024. We employ GenAI open source model-based data augmentation method to address class imbalance. We also employ privacy-aware preprocessing to ensure compliance with ethical standards while maintaining data integrity. Our solution achieves significant performance improvements, with HingRoBERTa attaining an accuracy of 74.41% and an F1-score of 71.49%. We also develop ready-to-use tool by integrating Django REST backend with a modern frontend. The developed tool is scalable and ready for real-world deployment in platforms like the National Cyber Crime Reporting Portal. This work bridges critical gaps in cybercrime complaint management, offering a scalable, privacy-conscious, and adaptable solution for modern cybersecurity challenges.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "75",
        "title": "Topology-Aware 3D Gaussian Splatting: Leveraging Persistent Homology for Optimized Structural Integrity",
        "author": [
            "Tianqi Shen",
            "Shaohua Liu",
            "Jiaqi Feng",
            "Ziye Ma",
            "Ning An"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16619",
        "abstract": "Gaussian Splatting (GS) has emerged as a crucial technique for representing discrete volumetric radiance fields. It leverages unique parametrization to mitigate computational demands in scene optimization. This work introduces Topology-Aware 3D Gaussian Splatting (Topology-GS), which addresses two key limitations in current approaches: compromised pixel-level structural integrity due to incomplete initial geometric coverage, and inadequate feature-level integrity from insufficient topological constraints during optimization. To overcome these limitations, Topology-GS incorporates a novel interpolation strategy, Local Persistent Voronoi Interpolation (LPVI), and a topology-focused regularization term based on persistent barcodes, named PersLoss. LPVI utilizes persistent homology to guide adaptive interpolation, enhancing point coverage in low-curvature areas while preserving topological structure. PersLoss aligns the visual perceptual similarity of rendered images with ground truth by constraining distances between their topological features. Comprehensive experiments on three novel-view synthesis benchmarks demonstrate that Topology-GS outperforms existing methods in terms of PSNR, SSIM, and LPIPS metrics, while maintaining efficient memory usage. This study pioneers the integration of topology with 3D-GS, laying the groundwork for future research in this area.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "76",
        "title": "A Large-scale Empirical Study on Fine-tuning Large Language Models for Unit Testing",
        "author": [
            "Ye Shang",
            "Quanjun Zhang",
            "Chunrong Fang",
            "Siqi Gu",
            "Jianyi Zhou",
            "Zhenyu Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16620",
        "abstract": "Unit testing plays a pivotal role in software development, improving software quality and reliability. However, generating effective test cases manually is time-consuming, prompting interest in unit testing research. Recently, Large Language Models (LLMs) have shown potential in various unit testing tasks, including test generation, assertion generation, and test evolution, but existing studies are limited in scope and lack a systematic evaluation of the effectiveness of LLMs.\nTo bridge this gap, we present a large-scale empirical study on fine-tuning LLMs for unit testing. Our study involves three unit testing tasks, five benchmarks, eight evaluation metrics, and 37 popular LLMs across various architectures and sizes, consuming over 3,000 NVIDIA A100 GPU hours. We focus on three key research questions: (1) the performance of LLMs compared to state-of-the-art methods, (2) the impact of different factors on LLM performance, and (3) the effectiveness of fine-tuning versus prompt engineering. Our findings reveal that LLMs outperform existing state-of-the-art approaches on all three unit testing tasks across nearly all metrics, highlighting the potential of fine-tuning LLMs in unit testing tasks. Furthermore, large-scale, decoder-only models achieve the best results across tasks, while encoder-decoder models perform better under the same parameter scale. Additionally, the comparison of the performance between fine-tuning and prompt engineering approaches reveals the considerable potential capability of the prompt engineering approach in unit testing tasks. We then discuss the concerned issues on the test generation task, including data leakage issues, bug detection capabilities, and metrics comparisons. Finally, we further pinpoint carious practical guidelines for LLM-based approaches to unit testing tasks in the near future.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "77",
        "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
        "author": [
            "Junyu Wang",
            "Zizhen Lin",
            "Tianrui Wang",
            "Meng Ge",
            "Longbiao Wang",
            "Jianwu Dang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16626",
        "abstract": "In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73.",
        "tags": [
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "78",
        "title": "POEX: Policy Executable Embodied AI Jailbreak Attacks",
        "author": [
            "Xuancun Lu",
            "Zhengxian Huang",
            "Xinfeng Li",
            "Xiaoyu ji",
            "Wenyuan Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16633",
        "abstract": "The integration of large language models (LLMs) into the planning module of Embodied Artificial Intelligence (Embodied AI) systems has greatly enhanced their ability to translate complex user instructions into executable policies. In this paper, we demystified how traditional LLM jailbreak attacks behave in the Embodied AI context. We conducted a comprehensive safety analysis of the LLM-based planning module of embodied AI systems against jailbreak attacks. Using the carefully crafted Harmful-RLbench, we accessed 20 open-source and proprietary LLMs under traditional jailbreak attacks, and highlighted two key challenges when adopting the prior jailbreak techniques to embodied AI contexts: (1) The harmful text output by LLMs does not necessarily induce harmful policies in Embodied AI context, and (2) even we can generate harmful policies, we have to guarantee they are executable in practice. To overcome those challenges, we propose Policy Executable (POEX) jailbreak attacks, where harmful instructions and optimized suffixes are injected into LLM-based planning modules, leading embodied AI to perform harmful actions in both simulated and physical environments. Our approach involves constraining adversarial suffixes to evade detection and fine-tuning a policy evaluater to improve the executability of harmful policies. We conducted extensive experiments on both a robotic arm embodied AI platform and simulators, to validate the attack and policy success rates on 136 harmful instructions from Harmful-RLbench. Our findings expose serious safety vulnerabilities in LLM-based planning modules, including the ability of POEX to be transferred across models. Finally, we propose mitigation strategies, such as safety-constrained prompts, pre- and post-planning checks, to address these vulnerabilities and ensure the safe deployment of embodied AI in real-world settings.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "79",
        "title": "L3TC: Leveraging RWKV for Learned Lossless Low-Complexity Text Compression",
        "author": [
            "Junxuan Zhang",
            "Zhengxue Cheng",
            "Yan Zhao",
            "Shihao Wang",
            "Dajiang Zhou",
            "Guo Lu",
            "Li Song"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16642",
        "abstract": "Learning-based probabilistic models can be combined with an entropy coder for data compression. However, due to the high complexity of learning-based models, their practical application as text compressors has been largely overlooked. To address this issue, our work focuses on a low-complexity design while maintaining compression performance. We introduce a novel Learned Lossless Low-complexity Text Compression method (L3TC). Specifically, we conduct extensive experiments demonstrating that RWKV models achieve the fastest decoding speed with a moderate compression ratio, making it the most suitable backbone for our method. Second, we propose an outlier-aware tokenizer that uses a limited vocabulary to cover frequent tokens while allowing outliers to bypass the prediction and encoding. Third, we propose a novel high-rank reparameterization strategy that enhances the learning capability during training without increasing complexity during inference. Experimental results validate that our method achieves 48\\% bit saving compared to gzip compressor. Besides, \\emph{L3TC} offers compression performance comparable to other learned compressors, with a $50\\times$ reduction in model parameters. More importantly, \\emph{L3TC} is the fastest among all learned compressors, providing real-time decoding speeds up to megabytes per second.",
        "tags": [
            "RWKV"
        ]
    },
    {
        "id": "80",
        "title": "TimeRAG: BOOSTING LLM Time Series Forecasting via Retrieval-Augmented Generation",
        "author": [
            "Silin Yang",
            "Dong Wang",
            "Haoqi Zheng",
            "Ruochun Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16643",
        "abstract": "Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97% on average.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "81",
        "title": "Internalized Self-Correction for Large Language Models",
        "author": [
            "Nishanth Upadhyaya",
            "Raghavendra Sridharamurthy"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16653",
        "abstract": "In this article, we introduce 'Internalized Self-Correction' (InSeC) for large language models (LLMs). While many approaches exist for self-reflection at inference time, we propose a novel method that combines ideas from negative sampling, self-reflection during training, and inference time. InSeC allows LLMs to correct themselves by introducing mistakes and their corresponding corrections during training, thereby converting the learning process into a true supervised learning task with both positive and negative examples. This approach can be extended to improve instruction following and correct hallucinations or incorrect sentences generated by LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "82",
        "title": "Generalizable Articulated Object Perception with Superpoints",
        "author": [
            "Qiaojun Yu",
            "Ce Hao",
            "Xibin Yuan",
            "Li Zhang",
            "Liu Liu",
            "Yukang Huo",
            "Rohit Agarwal",
            "Cewu Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16656",
        "abstract": "Manipulating articulated objects with robotic arms is challenging due to the complex kinematic structure, which requires precise part segmentation for efficient manipulation. In this work, we introduce a novel superpoint-based perception method designed to improve part segmentation in 3D point clouds of articulated objects. We propose a learnable, part-aware superpoint generation technique that efficiently groups points based on their geometric and semantic similarities, resulting in clearer part boundaries. Furthermore, by leveraging the segmentation capabilities of the 2D foundation model SAM, we identify the centers of pixel regions and select corresponding superpoints as candidate query points. Integrating a query-based transformer decoder further enhances our method's ability to achieve precise part segmentation. Experimental results on the GAPartNet dataset show that our method outperforms existing state-of-the-art approaches in cross-category part segmentation, achieving AP50 scores of 77.9% for seen categories (4.4% improvement) and $39.3\\%$ for unseen categories (11.6% improvement), with superior results in 5 out of 9 part categories for seen objects and outperforming all previous methods across all part categories for unseen objects.",
        "tags": [
            "3D",
            "SAM",
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "83",
        "title": "Label Privacy in Split Learning for Large Models with Parameter-Efficient Training",
        "author": [
            "Philip Zmushko",
            "Marat Mansurov",
            "Ruslan Svirschevski",
            "Denis Kuznedelev",
            "Max Ryabinin",
            "Aleksandr Beznosikov"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16669",
        "abstract": "As deep learning models become larger and more expensive, many practitioners turn to fine-tuning APIs. These web services allow fine-tuning a model between two parties: the client that provides the data, and the server that hosts the model. While convenient, these APIs raise a new concern: the data of the client is at risk of privacy breach during the training procedure. This challenge presents an important practical case of vertical federated learning, where the two parties perform parameter-efficient fine-tuning (PEFT) of a large model. In this study, we systematically search for a way to fine-tune models over an API while keeping the labels private. We analyze the privacy of LoRA, a popular approach for parameter-efficient fine-tuning when training over an API. Using this analysis, we propose P$^3$EFT, a multi-party split learning algorithm that takes advantage of existing PEFT properties to maintain privacy at a lower performance overhead. To validate our algorithm, we fine-tune DeBERTa-v2-XXLarge, Flan-T5 Large and LLaMA-2 7B using LoRA adapters on a range of NLP tasks. We find that P$^3$EFT is competitive with existing privacy-preserving methods in multi-party and two-party setups while having higher accuracy.",
        "tags": [
            "LLaMA",
            "LoRA"
        ]
    },
    {
        "id": "84",
        "title": "Two-in-One: Unified Multi-Person Interactive Motion Generation by Latent Diffusion Transformer",
        "author": [
            "Boyuan Li",
            "Xihua Wang",
            "Ruihua Song",
            "Wenbing Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16670",
        "abstract": "Multi-person interactive motion generation, a critical yet under-explored domain in computer character animation, poses significant challenges such as intricate modeling of inter-human interactions beyond individual motions and generating two motions with huge differences from one text condition. Current research often employs separate module branches for individual motions, leading to a loss of interaction information and increased computational demands. To address these challenges, we propose a novel, unified approach that models multi-person motions and their interactions within a single latent space. Our approach streamlines the process by treating interactive motions as an integrated data point, utilizing a Variational AutoEncoder (VAE) for compression into a unified latent space, and performing a diffusion process within this space, guided by the natural language conditions. Experimental results demonstrate our method's superiority over existing approaches in generation quality, performing text condition in particular when motions have significant asymmetry, and accelerating the generation efficiency while preserving high quality.",
        "tags": [
            "Diffusion",
            "Diffusion Transformer",
            "Transformer",
            "VAE"
        ]
    },
    {
        "id": "85",
        "title": "VAST 1.0: A Unified Framework for Controllable and Consistent Video Generation",
        "author": [
            "Chi Zhang",
            "Yuanzhi Liang",
            "Xi Qiu",
            "Fangqiu Yi",
            "Xuelong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16677",
        "abstract": "Generating high-quality videos from textual descriptions poses challenges in maintaining temporal coherence and control over subject motion. We propose VAST (Video As Storyboard from Text), a two-stage framework to address these challenges and enable high-quality video generation. In the first stage, StoryForge transforms textual descriptions into detailed storyboards, capturing human poses and object layouts to represent the structural essence of the scene. In the second stage, VisionForge generates videos from these storyboards, producing high-quality videos with smooth motion, temporal consistency, and spatial coherence. By decoupling text understanding from video generation, VAST enables precise control over subject dynamics and scene composition. Experiments on the VBench benchmark demonstrate that VAST outperforms existing methods in both visual quality and semantic expression, setting a new standard for dynamic and coherent video generation.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "86",
        "title": "The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents",
        "author": [
            "Feiran Jia",
            "Tong Wu",
            "Xin Qin",
            "Anna Squicciarini"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16682",
        "abstract": "Large Language Model (LLM) agents are increasingly being deployed as conversational assistants capable of performing complex real-world tasks through tool integration. This enhanced ability to interact with external systems and process various data sources, while powerful, introduces significant security vulnerabilities. In particular, indirect prompt injection attacks pose a critical threat, where malicious instructions embedded within external data sources can manipulate agents to deviate from user intentions. While existing defenses based on rule constraints, source spotlighting, and authentication protocols show promise, they struggle to maintain robust security while preserving task functionality. We propose a novel and orthogonal perspective that reframes agent security from preventing harmful actions to ensuring task alignment, requiring every agent action to serve user objectives. Based on this insight, we develop Task Shield, a test-time defense mechanism that systematically verifies whether each instruction and tool call contributes to user-specified goals. Through experiments on the AgentDojo benchmark, we demonstrate that Task Shield reduces attack success rates (2.07\\%) while maintaining high task utility (69.79\\%) on GPT-4o.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "87",
        "title": "NILE: Internal Consistency Alignment in Large Language Models",
        "author": [
            "Minda Hu",
            "Qiyuan Zhang",
            "Yufei Wang",
            "Bowei He",
            "Hongru Wang",
            "Jingyan Zhou",
            "Liangyou Li",
            "Yasheng Wang",
            "Chen Ma",
            "Irwin King"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16686",
        "abstract": "As a crucial step to enhance LLMs alignment with human intentions, Instruction Fine-Tuning (IFT) has a high demand on dataset quality. However, existing IFT datasets often contain knowledge that is inconsistent with LLMs' internal knowledge learned from the pre-training phase, which can greatly affect the efficacy of IFT. To address this issue, we introduce NILE (iNternal consIstency aLignmEnt) framework, aimed at optimizing IFT datasets to unlock LLMs' capability further. NILE operates by eliciting target pre-trained LLM's internal knowledge corresponding to instruction data. The internal knowledge is leveraged to revise the answer in IFT datasets. Additionally, we propose a novel Internal Consistency Filtering (ICF) method to filter training samples, ensuring its high consistency with LLM's internal knowledge. Our experiments demonstrate that NILE-aligned IFT datasets sharply boost LLM performance across multiple LLM ability evaluation datasets, achieving up to 66.6% gain on Arena-Hard and 68.5% on Alpaca-Eval V2. Further analysis confirms that each component of the NILE}framework contributes to these substantial performance improvements, and provides compelling evidence that dataset consistency with pre-trained internal knowledge is pivotal for maximizing LLM potential.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "88",
        "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
        "author": [
            "Majd Zayyad",
            "Yossi Adi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16689",
        "abstract": "The integration of retrieval-augmented techniques with LLMs has shown promise in improving performance across various domains. However, their utility in tasks requiring advanced reasoning, such as generating and evaluating mathematical statements and proofs, remains underexplored. This study explores the use of Lean, a programming language for writing mathematical proofs, to populate the knowledge corpus used by RAG systems. We hope for this to lay the foundation to exploring different methods of using RAGs to improve the performance of LLMs in advanced logical reasoning tasks.",
        "tags": [
            "LLMs",
            "RAG"
        ]
    },
    {
        "id": "89",
        "title": "FAP-CD: Fairness-Driven Age-Friendly Community Planning via Conditional Diffusion Generation",
        "author": [
            "Jinlin Li",
            "Xintong Li",
            "Xiao Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16699",
        "abstract": "As global populations age rapidly, incorporating age-specific considerations into urban planning has become essential to addressing the urgent demand for age-friendly built environments and ensuring sustainable urban development. However, current practices often overlook these considerations, resulting in inadequate and unevenly distributed elderly services in cities. There is a pressing need for equitable and optimized urban renewal strategies to support effective age-friendly planning. To address this challenge, we propose a novel framework, Fairness-driven Age-friendly community Planning via Conditional Diffusion generation (FAP-CD). FAP-CD leverages a conditioned graph denoising diffusion probabilistic model to learn the joint probability distribution of aging facilities and their spatial relationships at a fine-grained regional level. Our framework generates optimized facility distributions by iteratively refining noisy graphs, conditioned on the needs of the elderly during the diffusion process. Key innovations include a demand-fairness pre-training module that integrates community demand features and facility characteristics using an attention mechanism and min-max optimization, ensuring equitable service distribution across regions. Additionally, a discrete graph structure captures walkable accessibility within regional road networks, guiding model sampling. To enhance information integration, we design a graph denoising network with an attribute augmentation module and a hybrid graph message aggregation module, combining local and global node and edge information. Empirical results across multiple metrics demonstrate the effectiveness of FAP-CD in balancing age-friendly needs with regional equity, achieving an average improvement of 41% over competitive baseline models.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "90",
        "title": "TCAQ-DM: Timestep-Channel Adaptive Quantization for Diffusion Models",
        "author": [
            "Haocheng Huang",
            "Jiaxin Chen",
            "Jinyang Guo",
            "Ruiyi Zhan",
            "Yunhong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16700",
        "abstract": "Diffusion models have achieved remarkable success in the image and video generation tasks. Nevertheless, they often require a large amount of memory and time overhead during inference, due to the complex network architecture and considerable number of timesteps for iterative diffusion. Recently, the post-training quantization (PTQ) technique has proved a promising way to reduce the inference cost by quantizing the float-point operations to low-bit ones. However, most of them fail to tackle with the large variations in the distribution of activations across distinct channels and timesteps, as well as the inconsistent of input between quantization and inference on diffusion models, thus leaving much room for improvement. To address the above issues, we propose a novel method dubbed Timestep-Channel Adaptive Quantization for Diffusion Models (TCAQ-DM). Specifically, we develop a timestep-channel joint reparameterization (TCR) module to balance the activation range along both the timesteps and channels, facilitating the successive reconstruction procedure. Subsequently, we employ a dynamically adaptive quantization (DAQ) module that mitigate the quantization error by selecting an optimal quantizer for each post-Softmax layers according to their specific types of distributions. Moreover, we present a progressively aligned reconstruction (PAR) strategy to mitigate the bias caused by the input mismatch. Extensive experiments on various benchmarks and distinct diffusion models demonstrate that the proposed method substantially outperforms the state-of-the-art approaches in most cases, especially yielding comparable FID metrics to the full precision model on CIFAR-10 in the W6A6 setting, while enabling generating available images in the W4A4 settings.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "91",
        "title": "Towards More Robust Retrieval-Augmented Generation: Evaluating RAG Under Adversarial Poisoning Attacks",
        "author": [
            "Jinyan Su",
            "Jin Peng Zhou",
            "Zhengxin Zhang",
            "Preslav Nakov",
            "Claire Cardie"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16708",
        "abstract": "Retrieval-Augmented Generation (RAG) systems have emerged as a promising solution to mitigate LLM hallucinations and enhance their performance in knowledge-intensive domains. However, these systems are vulnerable to adversarial poisoning attacks, where malicious passages injected into retrieval databases can mislead the model into generating factually incorrect outputs. In this paper, we investigate both the retrieval and the generation components of RAG systems to understand how to enhance their robustness against such attacks. From the retrieval perspective, we analyze why and how the adversarial contexts are retrieved and assess how the quality of the retrieved passages impacts downstream generation. From a generation perspective, we evaluate whether LLMs' advanced critical thinking and internal knowledge capabilities can be leveraged to mitigate the impact of adversarial contexts, i.e., using skeptical prompting as a self-defense mechanism. Our experiments and findings provide actionable insights into designing safer and more resilient retrieval-augmented frameworks, paving the way for their reliable deployment in real-world applications.",
        "tags": [
            "LLMs",
            "RAG"
        ]
    },
    {
        "id": "92",
        "title": "GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space",
        "author": [
            "Souhaib Attaiki",
            "Paul Guerrero",
            "Duygu Ceylan",
            "Niloy J. Mitra",
            "Maks Ovsjanikov"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16717",
        "abstract": "We train a feed-forward text-to-3D diffusion generator for human characters using only single-view 2D data for supervision. Existing 3D generative models cannot yet match the fidelity of image or video generative models. State-of-the-art 3D generators are either trained with explicit 3D supervision and are thus limited by the volume and diversity of existing 3D data. Meanwhile, generators that can be trained with only 2D data as supervision typically produce coarser results, cannot be text-conditioned, or must revert to test-time optimization. We observe that GAN- and diffusion-based generators have complementary qualities: GANs can be trained efficiently with 2D supervision to produce high-quality 3D objects but are hard to condition on text. In contrast, denoising diffusion models can be conditioned efficiently but tend to be hard to train with only 2D supervision. We introduce GANFusion, which starts by generating unconditional triplane features for 3D data using a GAN architecture trained with only single-view 2D data. We then generate random samples from the GAN, caption them, and train a text-conditioned diffusion model that directly learns to sample from the space of good triplane features that can be decoded into 3D objects.",
        "tags": [
            "3D",
            "Diffusion",
            "GAN",
            "Text-to-3D"
        ]
    },
    {
        "id": "93",
        "title": "Large Language Models Compression via Low-Rank Feature Distillation",
        "author": [
            "Yaya Sy",
            "Christophe Cerisara",
            "Irina Illina"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16719",
        "abstract": "Current LLM structured pruning methods involve two steps: (1) compressing with calibration data and (2) continued pretraining on billions of tokens to recover the lost performance. This costly second step is needed as the first step significantly impacts performance. Previous studies have found that pretrained Transformer weights aren't inherently low-rank, unlike their activations, which may explain this performance drop. Based on this observation, we introduce a one-shot compression method that locally distills low-rank weights. We accelerate convergence by initializing the low-rank weights with SVD and using a joint loss that combines teacher and student activations. We reduce memory requirements by applying local gradient updates only. Our approach can compress Mixtral-8x7B within minutes on a single A100 GPU, removing 10 billion parameters while maintaining over 95% of the original performance. Phi-2 3B can be compressed by 40% using only 13 million calibration tokens into a small model that competes with recent models of similar size. We show our method generalizes well to non-transformer architectures: Mamba-3B can be compressed by 20% while maintaining 99% of its performance.",
        "tags": [
            "Large Language Models",
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "94",
        "title": "Argumentation Computation with Large Language Models : A Benchmark Study",
        "author": [
            "Zhaoqun Li",
            "Xiaotong Fang",
            "Chen Chen",
            "Mengze Li",
            "Beishui Liao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16725",
        "abstract": "In recent years, large language models (LLMs) have made significant advancements in neuro-symbolic computing. However, the combination of LLM with argumentation computation remains an underexplored domain, despite its considerable potential for real-world applications requiring defeasible reasoning. In this paper, we aim to investigate the capability of LLMs in determining the extensions of various abstract argumentation semantics. To achieve this, we develop and curate a benchmark comprising diverse abstract argumentation frameworks, accompanied by detailed explanations of algorithms for computing extensions. Subsequently, we fine-tune LLMs on the proposed benchmark, focusing on two fundamental extension-solving tasks. As a comparative baseline, LLMs are evaluated using a chain-of-thought approach, where they struggle to accurately compute semantics. In the experiments, we demonstrate that the process explanation plays a crucial role in semantics computation learning. Models trained with explanations show superior generalization accuracy compared to those trained solely with question-answer pairs. Furthermore, by leveraging the self-explanation capabilities of LLMs, our approach provides detailed illustrations that mitigate the lack of transparency typically associated with neural networks. Our findings contribute to the broader understanding of LLMs' potential in argumentation computation, offering promising avenues for further research in this domain.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "95",
        "title": "LUCES-MV: A Multi-View Dataset for Near-Field Point Light Source Photometric Stereo",
        "author": [
            "Fotios Logothetis",
            "Ignas Budvytis",
            "Stephan Liwicki",
            "Roberto Cipolla"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16737",
        "abstract": "The biggest improvements in Photometric Stereo (PS) field has recently come from adoption of differentiable volumetric rendering techniques such as NeRF or Neural SDF achieving impressive reconstruction error of 0.2mm on DiLiGenT-MV benchmark. However, while there are sizeable datasets for environment lit objects such as Digital Twin Catalogue (DTS), there are only several small Photometric Stereo datasets which often lack challenging objects (simple, smooth, untextured) and practical, small form factor (near-field) light setup.\nTo address this, we propose LUCES-MV, the first real-world, multi-view dataset designed for near-field point light source photometric stereo. Our dataset includes 15 objects with diverse materials, each imaged under varying light conditions from an array of 15 LEDs positioned 30 to 40 centimeters from the camera center. To facilitate transparent end-to-end evaluation, our dataset provides not only ground truth normals and ground truth object meshes and poses but also light and camera calibration images.\nWe evaluate state-of-the-art near-field photometric stereo algorithms, highlighting their strengths and limitations across different material and shape complexities. LUCES-MV dataset offers an important benchmark for developing more robust, accurate and scalable real-world Photometric Stereo based 3D reconstruction methods.",
        "tags": [
            "3D",
            "NeRF"
        ]
    },
    {
        "id": "96",
        "title": "KKANs: Kurkova-Kolmogorov-Arnold Networks and Their Learning Dynamics",
        "author": [
            "Juan Diego Toscano",
            "Li-Lian Wang",
            "George Em Karniadakis"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16738",
        "abstract": "Inspired by the Kolmogorov-Arnold representation theorem and Kurkova's principle of using approximate representations, we propose the Kurkova-Kolmogorov-Arnold Network (KKAN), a new two-block architecture that combines robust multi-layer perceptron (MLP) based inner functions with flexible linear combinations of basis functions as outer functions. We first prove that KKAN is a universal approximator, and then we demonstrate its versatility across scientific machine-learning applications, including function regression, physics-informed machine learning (PIML), and operator-learning frameworks. The benchmark results show that KKANs outperform MLPs and the original Kolmogorov-Arnold Networks (KANs) in function approximation and operator learning tasks and achieve performance comparable to fully optimized MLPs for PIML. To better understand the behavior of the new representation models, we analyze their geometric complexity and learning dynamics using information bottleneck theory, identifying three universal learning stages, fitting, transition, and diffusion, across all types of architectures. We find a strong correlation between geometric complexity and signal-to-noise ratio (SNR), with optimal generalization achieved during the diffusion stage. Additionally, we propose self-scaled residual-based attention weights to maintain high SNR dynamically, ensuring uniform convergence and prolonged learning.",
        "tags": [
            "Diffusion",
            "Kolmogorov-Arnold Networks"
        ]
    },
    {
        "id": "97",
        "title": "Business Analysis: User Attitude Evaluation and Prediction Based on Hotel User Reviews and Text Mining",
        "author": [
            "Ruochun Zhao",
            "Yue Hao",
            "Xuechen Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16744",
        "abstract": "In the post-pandemic era, the hotel industry plays a crucial role in economic recovery, with consumer sentiment increasingly influencing market trends. This study utilizes advanced natural language processing (NLP) and the BERT model to analyze user reviews, extracting insights into customer satisfaction and guiding service improvements. By transforming reviews into feature vectors, the BERT model accurately classifies emotions, uncovering patterns of satisfaction and dissatisfaction. This approach provides valuable data for hotel management, helping them refine service offerings and improve customer experiences. From a financial perspective, understanding sentiment is vital for predicting market performance, as shifts in consumer sentiment often correlate with stock prices and overall industry performance. Additionally, the study addresses data imbalance in sentiment analysis, employing techniques like oversampling and undersampling to enhance model robustness. The results offer actionable insights not only for the hotel industry but also for financial analysts, aiding in market forecasts and investment decisions. This research highlights the potential of sentiment analysis to drive business growth, improve financial outcomes, and enhance competitive advantage in the dynamic tourism and hospitality sectors, thereby contributing to the broader economic landscape.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "98",
        "title": "ViM-Disparity: Bridging the Gap of Speed, Accuracy and Memory for Disparity Map Generation",
        "author": [
            "Maheswar Bora",
            "Tushar Anand",
            "Saurabh Atreya",
            "Aritra Mukherjee",
            "Abhijit Das"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16745",
        "abstract": "In this work we propose a Visual Mamba (ViM) based architecture, to dissolve the existing trade-off for real-time and accurate model with low computation overhead for disparity map generation (DMG). Moreover, we proposed a performance measure that can jointly evaluate the inference speed, computation overhead and the accurateness of a DMG model.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "99",
        "title": "Unpacking Political Bias in Large Language Models: Insights Across Topic Polarization",
        "author": [
            "Kaiqi Yang",
            "Hang Li",
            "Yucheng Chu",
            "Yuping Lin",
            "Tai-Quan Peng",
            "Hui Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16746",
        "abstract": "Large Language Models (LLMs) have been widely used to generate responses on social topics due to their world knowledge and generative capabilities. Beyond reasoning and generation performance, political bias is an essential issue that warrants attention. Political bias, as a universal phenomenon in human society, may be transferred to LLMs and distort LLMs' behaviors of information acquisition and dissemination with humans, leading to unequal access among different groups of people. To prevent LLMs from reproducing and reinforcing political biases, and to encourage fairer LLM-human interactions, comprehensively examining political bias in popular LLMs becomes urgent and crucial.\nIn this study, we systematically measure the political biases in a wide range of LLMs, using a curated set of questions addressing political bias in various contexts. Our findings reveal distinct patterns in how LLMs respond to political topics. For highly polarized topics, most LLMs exhibit a pronounced left-leaning bias. Conversely, less polarized topics elicit greater consensus, with similar response patterns across different LLMs. Additionally, we analyze how LLM characteristics, including release date, model scale, and region of origin affect political bias. The results indicate political biases evolve with model scale and release date, and are also influenced by regional factors of LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "100",
        "title": "Solving Inverse Problems via Diffusion Optimal Control",
        "author": [
            "Henry Li",
            "Marcus Pereira"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16748",
        "abstract": "Existing approaches to diffusion-based inverse problem solvers frame the signal recovery task as a probabilistic sampling episode, where the solution is drawn from the desired posterior distribution. This framework suffers from several critical drawbacks, including the intractability of the conditional likelihood function, strict dependence on the score network approximation, and poor $\\mathbf{x}_0$ prediction quality. We demonstrate that these limitations can be sidestepped by reframing the generative process as a discrete optimal control episode. We derive a diffusion-based optimal controller inspired by the iterative Linear Quadratic Regulator (iLQR) algorithm. This framework is fully general and able to handle any differentiable forward measurement operator, including super-resolution, inpainting, Gaussian deblurring, nonlinear deblurring, and even highly nonlinear neural classifiers. Furthermore, we show that the idealized posterior sampling equation can be recovered as a special case of our algorithm. We then evaluate our method against a selection of neural inverse problem solvers, and establish a new baseline in image reconstruction with inverse problems.",
        "tags": [
            "Diffusion",
            "Super Resolution"
        ]
    },
    {
        "id": "101",
        "title": "Non-Asymptotic Error Analysis of Subspace Identification for Deterministic Systems",
        "author": [
            "Shuai Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16761",
        "abstract": "This paper is concerned with the perturbation error analysis of the widely-used Subspace Identification Methods (SIM) for n-dimensional discrete-time Multiple-Input Multiple-Output (MIMO) Linear Time-Invariant (LTI) systems with m outputs, based on finite input/output sample data. Using a single input/output trajectory, we provide non-asymptotic upper bounds on the perturbation errors for the system matrices in state-space models as well as the system poles for two unified algorithms, offering a unified perspective across various SIM variants. Furthermore, we prove that SIMs become ill-conditioned for MIMO systems when the ratio n/m is large, regardless of system parameters. Numerical experiments are conducted to validate the ill-conditionedness of SIMs.",
        "tags": [
            "State Space Models"
        ]
    },
    {
        "id": "102",
        "title": "Paraformer: Parameterization of Sub-grid Scale Processes Using Transformers",
        "author": [
            "Shuochen Wang",
            "Nishant Yadav",
            "Auroop R. Ganguly"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16763",
        "abstract": "One of the major sources of uncertainty in the current generation of Global Climate Models (GCMs) is the representation of sub-grid scale physical processes. Over the years, a series of deep-learning-based parameterization schemes have been developed and tested on both idealized and real-geography GCMs. However, datasets on which previous deep-learning models were trained either contain limited variables or have low spatial-temporal coverage, which can not fully simulate the parameterization process. Additionally, these schemes rely on classical architectures while the latest attention mechanism used in Transformer models remains unexplored in this field. In this paper, we propose Paraformer, a \"memory-aware\" Transformer-based model on ClimSim, the largest dataset ever created for climate parameterization. Our results demonstrate that the proposed model successfully captures the complex non-linear dependencies in the sub-grid scale variables and outperforms classical deep-learning architectures. This work highlights the applicability of the attenuation mechanism in this field and provides valuable insights for developing future deep-learning-based climate parameterization schemes.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "103",
        "title": "SilVar: Speech Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization",
        "author": [
            "Tan-Hanh Pham",
            "Hoang-Nam Le",
            "Phu-Vinh Nguyen",
            "Chris Ngo",
            "Truong-Son Hy"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16771",
        "abstract": "Visual Language Models have demonstrated remarkable capabilities across tasks, including visual question answering and image captioning. However, most models rely on text-based instructions, limiting their effectiveness in human-machine interactions. Moreover, the quality of language models depends on reasoning and prompting techniques, such as COT, which remain underexplored when using speech instructions. To address these challenges, we propose SilVar, a novel end-to-end multimodal model that uses speech instructions for reasoning in visual question answering. In addition, we investigate reasoning techniques with levels including conversational, simple, and complex speech instruction. SilVar is built upon CLIP, Whisper, and LLaMA 3.1-8B, enabling intuitive interactions by allowing users to provide verbal or text instructions. To this end, we introduce a dataset designed to challenge models with speech-based reasoning tasks for object localization. This dataset enhances the model ability to process and explain visual scenes from spoken input, moving beyond object recognition to reasoning-based interactions. The experiments show that SilVar achieves SOTA performance on the MMMU and ScienceQA benchmarks despite the challenge of speech-based instructions. We believe SilVar will inspire next-generation multimodal reasoning models, toward expert artificial general intelligence. Our code and dataset are available here.",
        "tags": [
            "CLIP",
            "LLaMA"
        ]
    },
    {
        "id": "104",
        "title": "Assessing Social Alignment: Do Personality-Prompted Large Language Models Behave Like Humans?",
        "author": [
            "Ivan Zakazov",
            "Mikolaj Boronski",
            "Lorenzo Drudi",
            "Robert West"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16772",
        "abstract": "The ongoing revolution in language modelling has led to various novel applications, some of which rely on the emerging \"social abilities\" of large language models (LLMs). Already, many turn to the new \"cyber friends\" for advice during pivotal moments of their lives and trust them with their deepest secrets, implying that accurate shaping of LLMs' \"personalities\" is paramount. Leveraging the vast diversity of data on which LLMs are pretrained, state-of-the-art approaches prompt them to adopt a particular personality. We ask (i) if personality-prompted models behave (i.e. \"make\" decisions when presented with a social situation) in line with the ascribed personality, and (ii) if their behavior can be finely controlled. We use classic psychological experiments - the Milgram Experiment and the Ultimatum Game - as social interaction testbeds and apply personality prompting to GPT-3.5/4/4o-mini/4o. Our experiments reveal failure modes of the prompt-based modulation of the models' \"behavior\", thus challenging the feasibility of personality prompting with today's LLMs.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "105",
        "title": "RoomPainter: View-Integrated Diffusion for Consistent Indoor Scene Texturing",
        "author": [
            "Zhipeng Huang",
            "Wangbo Yu",
            "Xinhua Cheng",
            "ChengShu Zhao",
            "Yunyang Ge",
            "Mingyi Guo",
            "Li Yuan",
            "Yonghong Tian"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16778",
        "abstract": "Indoor scene texture synthesis has garnered significant interest due to its important potential applications in virtual reality, digital media, and creative arts. Existing diffusion model-based researches either rely on per-view inpainting techniques, which are plagued by severe cross-view inconsistencies and conspicuous seams, or they resort to optimization-based approaches that entail substantial computational overhead. In this work, we present RoomPainter, a framework that seamlessly integrates efficiency and consistency to achieve high-fidelity texturing of indoor scenes. The core of RoomPainter features a zero-shot technique that effectively adapts a 2D diffusion model for 3D-consistent texture synthesis, along with a two-stage generation strategy that ensures both global and local consistency. Specifically, we introduce Attention-Guided Multi-View Integrated Sampling (MVIS) combined with a neighbor-integrated attention mechanism for zero-shot texture map generation. Using the MVIS, we firstly generate texture map for the entire room to ensure global consistency, then adopt its variant, namely an attention-guided multi-view integrated repaint sampling (MVRS) to repaint individual instances within the room, thereby further enhancing local consistency. Experiments demonstrate that RoomPainter achieves superior performance for indoor scene texture synthesis in visual quality, global consistency, and generation efficiency.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "106",
        "title": "SubData: A Python Library to Collect and Combine Datasets for Evaluating LLM Alignment on Downstream Tasks",
        "author": [
            "Leon Fröhling",
            "Pietro Bernardelle",
            "Gianluca Demartini"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16783",
        "abstract": "With the release of ever more capable large language models (LLMs), researchers in NLP and related disciplines have started to explore the usability of LLMs for a wide variety of different annotation tasks. Very recently, a lot of this attention has shifted to tasks that are subjective in nature. Given that the latest generations of LLMs have digested and encoded extensive knowledge about different human subpopulations and individuals, the hope is that these models can be trained, tuned or prompted to align with a wide range of different human perspectives. While researchers already evaluate the success of this alignment via surveys and tests, there is a lack of resources to evaluate the alignment on what oftentimes matters the most in NLP; the actual downstream tasks. To fill this gap we present SubData, a Python library that offers researchers working on topics related to subjectivity in annotation tasks a convenient way of collecting, combining and using a range of suitable datasets.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "107",
        "title": "Quantum-Like Contextuality in Large Language Models",
        "author": [
            "Kin Ian Lo",
            "Mehrnoosh Sadrzadeh",
            "Shane Mansfield"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16806",
        "abstract": "Contextuality is a distinguishing feature of quantum mechanics and there is growing evidence that it is a necessary condition for quantum advantage. In order to make use of it, researchers have been asking whether similar phenomena arise in other domains. The answer has been yes, e.g. in behavioural sciences. However, one has to move to frameworks that take some degree of signalling into account. Two such frameworks exist: (1) a signalling-corrected sheaf theoretic model, and (2) the Contextuality-by-Default (CbD) framework. This paper provides the first large scale experimental evidence for a yes answer in natural language. We construct a linguistic schema modelled over a contextual quantum scenario, instantiate it in the Simple English Wikipedia and extract probability distributions for the instances using the large language model BERT. This led to the discovery of 77,118 sheaf-contextual and 36,938,948 CbD contextual instances. We proved that the contextual instances came from semantically similar words, by deriving an equation between degrees of contextuality and Euclidean distances of BERT's embedding vectors. A regression model further reveals that Euclidean distance is indeed the best statistical predictor of contextuality. Our linguistic schema is a variant of the co-reference resolution challenge. These results are an indication that quantum methods may be advantageous in language tasks.",
        "tags": [
            "BERT",
            "Large Language Models"
        ]
    },
    {
        "id": "108",
        "title": "GeoTexDensifier: Geometry-Texture-Aware Densification for High-Quality Photorealistic 3D Gaussian Splatting",
        "author": [
            "Hanqing Jiang",
            "Xiaojun Xiang",
            "Han Sun",
            "Hongjie Li",
            "Liyang Zhou",
            "Xiaoyu Zhang",
            "Guofeng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16809",
        "abstract": "3D Gaussian Splatting (3DGS) has recently attracted wide attentions in various areas such as 3D navigation, Virtual Reality (VR) and 3D simulation, due to its photorealistic and efficient rendering performance. High-quality reconstrution of 3DGS relies on sufficient splats and a reasonable distribution of these splats to fit real geometric surface and texture details, which turns out to be a challenging problem. We present GeoTexDensifier, a novel geometry-texture-aware densification strategy to reconstruct high-quality Gaussian splats which better comply with the geometric structure and texture richness of the scene. Specifically, our GeoTexDensifier framework carries out an auxiliary texture-aware densification method to produce a denser distribution of splats in fully textured areas, while keeping sparsity in low-texture regions to maintain the quality of Gaussian point cloud. Meanwhile, a geometry-aware splitting strategy takes depth and normal priors to guide the splitting sampling and filter out the noisy splats whose initial positions are far from the actual geometric surfaces they aim to fit, under a Validation of Depth Ratio Change checking. With the help of relative monocular depth prior, such geometry-aware validation can effectively reduce the influence of scattered Gaussians to the final rendering quality, especially in regions with weak textures or without sufficient training views. The texture-aware densification and geometry-aware splitting strategies are fully combined to obtain a set of high-quality Gaussian splats. We experiment our GeoTexDensifier framework on various datasets and compare our Novel View Synthesis results to other state-of-the-art 3DGS approaches, with detailed quantitative and qualitative evaluations to demonstrate the effectiveness of our method in producing more photorealistic 3DGS models.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "109",
        "title": "An Exploration of Pattern Mining with ChatGPT",
        "author": [
            "Michael Weiss"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16814",
        "abstract": "This paper takes an exploratory approach to examine the use of ChatGPT for pattern mining. It proposes an eight-step collaborative process that combines human insight with AI capabilities to extract patterns from known uses. The paper offers a practical demonstration of this process by creating a pattern language for integrating Large Language Models (LLMs) with data sources and tools. LLMs, such as ChatGPT, are a new class of AI models that have been trained on large amounts of text, and can create new content, including text, images, or video. The paper also argues for adding affordances of the underlying components as a new element of pattern descriptions. The primary audience of the paper includes pattern writers interested in pattern mining using LLMs.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "110",
        "title": "Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers",
        "author": [
            "Haoran You",
            "Connelly Barnes",
            "Yuqian Zhou",
            "Yan Kang",
            "Zhenbang Du",
            "Wei Zhou",
            "Lingzhi Zhang",
            "Yotam Nitzan",
            "Xiaoyang Liu",
            "Zhe Lin",
            "Eli Shechtman",
            "Sohrab Amirghodsi",
            "Yingyan Celine Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16822",
        "abstract": "Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image generation quality but suffer from high latency and memory inefficiency, making them difficult to deploy on resource-constrained devices. One key efficiency bottleneck is that existing DiTs apply equal computation across all regions of an image. However, not all image tokens are equally important, and certain localized areas require more computation, such as objects. To address this, we propose DiffRatio-MoD, a dynamic DiT inference framework with differentiable compression ratios, which automatically learns to dynamically route computation across layers and timesteps for each image token, resulting in Mixture-of-Depths (MoD) efficient DiT models. Specifically, DiffRatio-MoD integrates three features: (1) A token-level routing scheme where each DiT layer includes a router that is jointly fine-tuned with model weights to predict token importance scores. In this way, unimportant tokens bypass the entire layer's computation; (2) A layer-wise differentiable ratio mechanism where different DiT layers automatically learn varying compression ratios from a zero initialization, resulting in large compression ratios in redundant layers while others remain less compressed or even uncompressed; (3) A timestep-wise differentiable ratio mechanism where each denoising timestep learns its own compression ratio. The resulting pattern shows higher ratios for noisier timesteps and lower ratios as the image becomes clearer. Extensive experiments on both text-to-image and inpainting tasks show that DiffRatio-MoD effectively captures dynamism across token, layer, and timestep axes, achieving superior trade-offs between generation quality and efficiency compared to prior works.",
        "tags": [
            "DiT",
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "111",
        "title": "Visual Prompting with Iterative Refinement for Design Critique Generation",
        "author": [
            "Peitong Duan",
            "Chin-Yi Chen",
            "Bjoern Hartmann",
            "Yang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16829",
        "abstract": "Feedback is crucial for every design process, such as user interface (UI) design, and automating design critiques can significantly improve the efficiency of the design workflow. Although existing multimodal large language models (LLMs) excel in many tasks, they often struggle with generating high-quality design critiques -- a complex task that requires producing detailed design comments that are visually grounded in a given design's image. Building on recent advancements in iterative refinement of text output and visual prompting methods, we propose an iterative visual prompting approach for UI critique that takes an input UI screenshot and design guidelines and generates a list of design comments, along with corresponding bounding boxes that map each comment to a specific region in the screenshot. The entire process is driven completely by LLMs, which iteratively refine both the text output and bounding boxes using few-shot samples tailored for each step. We evaluated our approach using Gemini-1.5-pro and GPT-4o, and found that human experts generally preferred the design critiques generated by our pipeline over those by the baseline, with the pipeline reducing the gap from human performance by 50% for one rating metric. To assess the generalizability of our approach to other multimodal tasks, we applied our pipeline to open-vocabulary object and attribute detection, and experiments showed that our method also outperformed the baseline.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "112",
        "title": "RealisID: Scale-Robust and Fine-Controllable Identity Customization via Local and Global Complementation",
        "author": [
            "Zhaoyang Sun",
            "Fei Du",
            "Weihua Chen",
            "Fan Wang",
            "Yaxiong Chen",
            "Yi Rong",
            "Shengwu Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16832",
        "abstract": "Recently, the success of text-to-image synthesis has greatly advanced the development of identity customization techniques, whose main goal is to produce realistic identity-specific photographs based on text prompts and reference face images. However, it is difficult for existing identity customization methods to simultaneously meet the various requirements of different real-world applications, including the identity fidelity of small face, the control of face location, pose and expression, as well as the customization of multiple persons. To this end, we propose a scale-robust and fine-controllable method, namely RealisID, which learns different control capabilities through the cooperation between a pair of local and global branches. Specifically, by using cropping and up-sampling operations to filter out face-irrelevant information, the local branch concentrates the fine control of facial details and the scale-robust identity fidelity within the face region. Meanwhile, the global branch manages the overall harmony of the entire image. It also controls the face location by taking the location guidance as input. As a result, RealisID can benefit from the complementarity of these two branches. Finally, by implementing our branches with two different variants of ControlNet, our method can be easily extended to handle multi-person customization, even only trained on single-person datasets. Extensive experiments and ablation studies indicate the effectiveness of RealisID and verify its ability in fulfilling all the requirements mentioned above.",
        "tags": [
            "ControlNet",
            "Text-to-Image"
        ]
    },
    {
        "id": "113",
        "title": "Online Learning from Strategic Human Feedback in LLM Fine-Tuning",
        "author": [
            "Shugang Hao",
            "Lingjie Duan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16834",
        "abstract": "Reinforcement learning from human feedback (RLHF) has become an essential step in fine-tuning large language models (LLMs) to align them with human preferences. However, human labelers are selfish and have diverse preferences. They may strategically misreport their online feedback to influence the system's aggregation towards their own preferences. Current practice simply averages labelers' feedback per time and fails to identify the most accurate human labeler, leading to linear regret $\\mathcal{O}(T)$ for $T$ time slots. To our best knowledge, we are the first to study online learning mechanisms against strategic human labelers in the LLM fine-tuning process. We formulate a new dynamic Bayesian game and dynamically adjust human labelers' weights in the preference aggregation, ensuring their truthful feedback and sublinear regret $\\mathcal{O}(T^{1/2})$. Simulation results demonstrate our mechanism's great advantages over the existing benchmark schemes.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "114",
        "title": "Adaptive User Interface Generation Through Reinforcement Learning: A Data-Driven Approach to Personalization and Optimization",
        "author": [
            "Qi Sun",
            "Yayun Xue",
            "Zhijun Song"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16837",
        "abstract": "This study introduces an adaptive user interface generation technology, emphasizing the role of Human-Computer Interaction (HCI) in optimizing user experience. By focusing on enhancing the interaction between users and intelligent systems, this approach aims to automatically adjust interface layouts and configurations based on user feedback, streamlining the design process. Traditional interface design involves significant manual effort and struggles to meet the evolving personalized needs of users. Our proposed system integrates adaptive interface generation with reinforcement learning and intelligent feedback mechanisms to dynamically adjust the user interface, better accommodating individual usage patterns. In the experiment, the OpenAI CLIP Interactions dataset was utilized to verify the adaptability of the proposed method, using click-through rate (CTR) and user retention rate (RR) as evaluation metrics. The findings highlight the system's ability to deliver flexible and personalized interface solutions, providing a novel and effective approach for user interaction design and ultimately enhancing HCI through continuous learning and adaptation.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "115",
        "title": "Ask-Before-Detection: Identifying and Mitigating Conformity Bias in LLM-Powered Error Detector for Math Word Problem Solutions",
        "author": [
            "Hang Li",
            "Tianlong Xu",
            "Kaiqi Yang",
            "Yucheng Chu",
            "Yanling Chen",
            "Yichi Song",
            "Qingsong Wen",
            "Hui Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16838",
        "abstract": "The rise of large language models (LLMs) offers new opportunities for automatic error detection in education, particularly for math word problems (MWPs). While prior studies demonstrate the promise of LLMs as error detectors, they overlook the presence of multiple valid solutions for a single MWP. Our preliminary analysis reveals a significant performance gap between conventional and alternative solutions in MWPs, a phenomenon we term conformity bias in this work. To mitigate this bias, we introduce the Ask-Before-Detect (AskBD) framework, which generates adaptive reference solutions using LLMs to enhance error detection. Experiments on 200 examples of GSM8K show that AskBD effectively mitigates bias and improves performance, especially when combined with reasoning-enhancing techniques like chain-of-thought prompting.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "116",
        "title": "Sim911: Towards Effective and Equitable 9-1-1 Dispatcher Training with an LLM-Enabled Simulation",
        "author": [
            "Zirong Chen",
            "Elizabeth Chason",
            "Noah Mladenovski",
            "Erin Wilson",
            "Kristin Mullen",
            "Stephen Martini",
            "Meiyi Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16844",
        "abstract": "Emergency response services are vital for enhancing public safety by safeguarding the environment, property, and human lives. As frontline members of these services, 9-1-1 dispatchers have a direct impact on response times and the overall effectiveness of emergency operations. However, traditional dispatcher training methods, which rely on role-playing by experienced personnel, are labor-intensive, time-consuming, and often neglect the specific needs of underserved communities. To address these challenges, we introduce Sim911, the first training simulation for 9-1-1 dispatchers powered by Large Language Models (LLMs). Sim911 enhances training through three key technical innovations: (1) knowledge construction, which utilizes archived 9-1-1 call data to generate simulations that closely mirror real-world scenarios; (2) context-aware controlled generation, which employs dynamic prompts and vector bases to ensure that LLM behavior aligns with training objectives; and (3) validation with looped correction, which filters out low-quality responses and refines the system performance.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "117",
        "title": "GME: Improving Universal Multimodal Retrieval by Multimodal LLMs",
        "author": [
            "Xin Zhang",
            "Yanzhao Zhang",
            "Wen Xie",
            "Mingxin Li",
            "Ziqi Dai",
            "Dingkun Long",
            "Pengjun Xie",
            "Meishan Zhang",
            "Wenjie Li",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16855",
        "abstract": "Universal Multimodal Retrieval (UMR) aims to enable search across various modalities using a unified model, where queries and candidates can consist of pure text, images, or a combination of both. Previous work has attempted to adopt multimodal large language models (MLLMs) to realize UMR using only text data. However, our preliminary experiments demonstrate that more diverse multimodal training data can further unlock the potential of MLLMs. Despite its effectiveness, the existing multimodal training data is highly imbalanced in terms of modality, which motivates us to develop a training data synthesis pipeline and construct a large-scale, high-quality fused-modal training dataset. Based on the synthetic training data, we develop the General Multimodal Embedder (GME), an MLLM-based dense retriever designed for UMR. Furthermore, we construct a comprehensive UMR Benchmark (UMRB) to evaluate the effectiveness of our approach. Experimental results show that our method achieves state-of-the-art performance among existing UMR methods. Last, we provide in-depth analyses of model scaling, training strategies, and perform ablation studies on both the model and synthetic data.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "118",
        "title": "Adversarial Diffusion Model for Unsupervised Domain-Adaptive Semantic Segmentation",
        "author": [
            "Jongmin Yu",
            "Zhongtian Sun",
            "Shan Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16859",
        "abstract": "Semantic segmentation requires labour-intensive labelling tasks to obtain the supervision signals, and because of this issue, it is encouraged that using domain adaptation, which transfers information from the existing labelled source domains to unlabelled or weakly labelled target domains, is essential. However, it is intractable to find a well-generalised representation which can describe two domains due to probabilistic or geometric difference between the two domains. This paper presents a novel method, the Conditional and Inter-coder Connected Latent Diffusion (CICLD) based Semantic Segmentation Model, to advance unsupervised domain adaptation (UDA) for semantic segmentation tasks. Leveraging the strengths of latent diffusion models and adversarial learning, our method effectively bridges the gap between synthetic and real-world imagery. CICLD incorporates a conditioning mechanism to improve contextual understanding during segmentation and an inter-coder connection to preserve fine-grained details and spatial hierarchies. Additionally, adversarial learning aligns latent feature distributions across source, mixed, and target domains, further enhancing generalisation. Extensive experiments are conducted across three benchmark datasets-GTA5, Synthia, and Cityscape-shows that CICLD outperforms state-of-the-art UDA methods. Notably, the proposed method achieves a mean Intersection over Union (mIoU) of 74.4 for the GTA5 to Cityscape UDA setting and 67.2 mIoU for the Synthia to Cityscape UDA setting. This project is publicly available on 'https://github.com/andreYoo/CICLD'.",
        "tags": [
            "Diffusion",
            "Segmentation"
        ]
    },
    {
        "id": "119",
        "title": "CoF: Coarse to Fine-Grained Image Understanding for Multi-modal Large Language Models",
        "author": [
            "Yeyuan Wang",
            "Dehong Gao",
            "Bin Li",
            "Rujiao Long",
            "Lei Yi",
            "Xiaoyan Cai",
            "Libin Yang",
            "Jinxia Zhang",
            "Shanqing Yu",
            "Qi Xuan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16869",
        "abstract": "The impressive performance of Large Language Model (LLM) has prompted researchers to develop Multi-modal LLM (MLLM), which has shown great potential for various multi-modal tasks. However, current MLLM often struggles to effectively address fine-grained multi-modal challenges. We argue that this limitation is closely linked to the models' visual grounding capabilities. The restricted spatial awareness and perceptual acuity of visual encoders frequently lead to interference from irrelevant background information in images, causing the models to overlook subtle but crucial details. As a result, achieving fine-grained regional visual comprehension becomes difficult. In this paper, we break down multi-modal understanding into two stages, from Coarse to Fine (CoF). In the first stage, we prompt the MLLM to locate the approximate area of the answer. In the second stage, we further enhance the model's focus on relevant areas within the image through visual prompt engineering, adjusting attention weights of pertinent regions. This, in turn, improves both visual grounding and overall performance in downstream tasks. Our experiments show that this approach significantly boosts the performance of baseline models, demonstrating notable generalization and effectiveness. Our CoF approach is available online at https://github.com/Gavin001201/CoF.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "120",
        "title": "Teaching LLMs to Refine with Tools",
        "author": [
            "Dian Yu",
            "Yuheng Zhang",
            "Jiahao Xu",
            "Tian Liang",
            "Linfeng Song",
            "Zhaopeng Tu",
            "Haitao Mi",
            "Dong Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16871",
        "abstract": "Large language models (LLMs) can refine their responses based on feedback, enabling self-improvement through iterative training or test-time refinement. However, existing methods predominantly focus on refinement within the same reasoning format, which may lead to non-correcting behaviors. We propose CaP, a novel approach that uses external tools to refine chain-of-thought (CoT) responses generated by the same or other LLMs. CaP employs a two-stage training process: supervised fine-tuning followed by preference optimization with DPO variants. Our observations highlight the critical role of preference optimization in enabling effective refinement. Additionally, we compare several sampling strategies to leverage CoT and tools at inference time. Experimental results demonstrate CaP's potential for effective cross-reasoning refinement and efficient inference.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "121",
        "title": "Reconsidering SMT Over NMT for Closely Related Languages: A Case Study of Persian-Hindi Pair",
        "author": [
            "Waisullah Yousofi",
            "Pushpak Bhattacharyya"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16877",
        "abstract": "This paper demonstrates that Phrase-Based Statistical Machine Translation (PBSMT) can outperform Transformer-based Neural Machine Translation (NMT) in moderate-resource scenarios, specifically for structurally similar languages, like the Persian-Hindi pair. Despite the Transformer architecture's typical preference for large parallel corpora, our results show that PBSMT achieves a BLEU score of 66.32, significantly exceeding the Transformer-NMT score of 53.7 on the same dataset. Additionally, we explore variations of the SMT architecture, including training on Romanized text and modifying the word order of Persian sentences to match the left-to-right (LTR) structure of Hindi. Our findings highlight the importance of choosing the right architecture based on language pair characteristics and advocate for SMT as a high-performing alternative, even in contexts commonly dominated by NMT.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "122",
        "title": "Learning to Generate Gradients for Test-Time Adaptation via Test-Time Training Layers",
        "author": [
            "Qi Deng",
            "Shuaicheng Niu",
            "Ronghao Zhang",
            "Yaofo Chen",
            "Runhao Zeng",
            "Jian Chen",
            "Xiping Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16901",
        "abstract": "Test-time adaptation (TTA) aims to fine-tune a trained model online using unlabeled testing data to adapt to new environments or out-of-distribution data, demonstrating broad application potential in real-world scenarios. However, in this optimization process, unsupervised learning objectives like entropy minimization frequently encounter noisy learning signals. These signals produce unreliable gradients, which hinder the model ability to converge to an optimal solution quickly and introduce significant instability into the optimization process. In this paper, we seek to resolve these issues from the perspective of optimizer design. Unlike prior TTA using manually designed optimizers like SGD, we employ a learning-to-optimize approach to automatically learn an optimizer, called Meta Gradient Generator (MGG). Specifically, we aim for MGG to effectively utilize historical gradient information during the online optimization process to optimize the current model. To this end, in MGG, we design a lightweight and efficient sequence modeling layer -- gradient memory layer. It exploits a self-supervised reconstruction loss to compress historical gradient information into network parameters, thereby enabling better memorization ability over a long-term adaptation process. We only need a small number of unlabeled samples to pre-train MGG, and then the trained MGG can be deployed to process unseen samples. Promising results on ImageNet-C, R, Sketch, and A indicate that our method surpasses current state-of-the-art methods with fewer updates, less data, and significantly shorter adaptation iterations. Compared with a previous SOTA method SAR, we achieve 7.4% accuracy improvement and 4.2 times faster adaptation speed on ImageNet-C.",
        "tags": [
            "Test-Time Training"
        ]
    },
    {
        "id": "123",
        "title": "Temporal-Frequency State Space Duality: An Efficient Paradigm for Speech Emotion Recognition",
        "author": [
            "Jiaqi Zhao",
            "Fei Wang",
            "Kun Li",
            "Yanyan Wei",
            "Shengeng Tang",
            "Shu Zhao",
            "Xiao Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16904",
        "abstract": "Speech Emotion Recognition (SER) plays a critical role in enhancing user experience within human-computer interaction. However, existing methods are overwhelmed by temporal domain analysis, overlooking the valuable envelope structures of the frequency domain that are equally important for robust emotion recognition. To overcome this limitation, we propose TF-Mamba, a novel multi-domain framework that captures emotional expressions in both temporal and frequency http://dimensions.Concretely, we propose a temporal-frequency mamba block to extract temporal- and frequency-aware emotional features, achieving an optimal balance between computational efficiency and model expressiveness. Besides, we design a Complex Metric-Distance Triplet (CMDT) loss to enable the model to capture representative emotional clues for SER. Extensive experiments on the IEMOCAP and MELD datasets show that TF-Mamba surpasses existing methods in terms of model size and latency, providing a more practical solution for future SER applications.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "124",
        "title": "Self-Corrected Flow Distillation for Consistent One-Step and Few-Step Text-to-Image Generation",
        "author": [
            "Quan Dao",
            "Hao Phung",
            "Trung Dao",
            "Dimitris Metaxas",
            "Anh Tran"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16906",
        "abstract": "Flow matching has emerged as a promising framework for training generative models, demonstrating impressive empirical performance while offering relative ease of training compared to diffusion-based models. However, this method still requires numerous function evaluations in the sampling process. To address these limitations, we introduce a self-corrected flow distillation method that effectively integrates consistency models and adversarial training within the flow-matching framework. This work is a pioneer in achieving consistent generation quality in both few-step and one-step sampling. Our extensive experiments validate the effectiveness of our method, yielding superior results both quantitatively and qualitatively on CelebA-HQ and zero-shot benchmarks on the COCO dataset. Our implementation is released at https://github.com/VinAIResearch/SCFlow",
        "tags": [
            "Consistency Models",
            "Diffusion",
            "Flow Matching",
            "Text-to-Image"
        ]
    },
    {
        "id": "125",
        "title": "Map Imagination Like Blind Humans: Group Diffusion Model for Robotic Map Generation",
        "author": [
            "Qijin Song",
            "Weibang Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16908",
        "abstract": "Can robots imagine or generate maps like humans do, especially when only limited information can be perceived like blind people? To address this challenging task, we propose a novel group diffusion model (GDM) based architecture for robots to generate point cloud maps with very limited input http://information.Inspired from the blind humans' natural capability of imagining or generating mental maps, the proposed method can generate maps without visual perception data or depth data. With additional limited super-sparse spatial positioning data, like the extra contact-based positioning information the blind individuals can obtain, the map generation quality can be improved even http://more.Experiments on public datasets are conducted, and the results indicate that our method can generate reasonable maps solely based on path data, and produce even more refined maps upon incorporating exiguous LiDAR http://data.Compared to conventional mapping approaches, our novel method significantly mitigates sensor dependency, enabling the robots to imagine and generate elementary maps without heavy onboard sensory devices.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "126",
        "title": "FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation",
        "author": [
            "Tianyun Zhong",
            "Chao Liang",
            "Jianwen Jiang",
            "Gaojie Lin",
            "Jiaqi Yang",
            "Zhou Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16915",
        "abstract": "Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage http://fadavatar.github.io.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "127",
        "title": "TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction",
        "author": [
            "Xuying Zhang",
            "Yutong Liu",
            "Yangguang Li",
            "Renrui Zhang",
            "Yufei Liu",
            "Kai Wang",
            "Wanli Ouyang",
            "Zhiwei Xiong",
            "Peng Gao",
            "Qibin Hou",
            "Ming-Ming Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16919",
        "abstract": "We present TAR3D, a novel framework that consists of a 3D-aware Vector Quantized-Variational AutoEncoder (VQ-VAE) and a Generative Pre-trained Transformer (GPT) to generate high-quality 3D assets. The core insight of this work is to migrate the multimodal unification and promising learning capabilities of the next-token prediction paradigm to conditional 3D object generation. To achieve this, the 3D VQ-VAE first encodes a wide range of 3D shapes into a compact triplane latent space and utilizes a set of discrete representations from a trainable codebook to reconstruct fine-grained geometries under the supervision of query point occupancy. Then, the 3D GPT, equipped with a custom triplane position embedding called TriPE, predicts the codebook index sequence with prefilling prompt tokens in an autoregressive manner so that the composition of 3D geometries can be modeled part by part. Extensive experiments on ShapeNet and Objaverse demonstrate that TAR3D can achieve superior generation quality over existing methods in text-to-3D and image-to-3D tasks",
        "tags": [
            "3D",
            "GPT",
            "Image-to-3D",
            "Text-to-3D",
            "Transformer",
            "VAE"
        ]
    },
    {
        "id": "128",
        "title": "Enhancing Supply Chain Transparency in Emerging Economies Using Online Contents and LLMs",
        "author": [
            "Bohan Jin",
            "Qianyou Sun",
            "Lihua Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16922",
        "abstract": "In the current global economy, supply chain transparency plays a pivotal role in ensuring this security by enabling companies to monitor supplier performance and fostering accountability and responsibility. Despite the advancements in supply chain relationship datasets like Bloomberg and FactSet, supply chain transparency remains a significant challenge in emerging economies due to issues such as information asymmetry and institutional gaps in regulation. This study proposes a novel approach to enhance supply chain transparency in emerging economies by leveraging online content and large language models (LLMs). We develop a Supply Chain Knowledge Graph Mining System that integrates advanced LLMs with web crawler technology to automatically collect and analyze supply chain information. The system's effectiveness is validated through a case study focusing on the semiconductor supply chain, a domain that has recently gained significant attention due to supply chain risks. Our results demonstrate that the proposed system provides greater applicability for emerging economies, such as mainland China, complementing the data gaps in existing datasets. However, challenges including the accurate estimation of monetary and material flows, the handling of time series data, synonyms disambiguation, and mitigating biases from online contents still remains. Future research should focus on addressing these issues to further enhance the system's capabilities and broaden its application to other emerging economies and industries.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "129",
        "title": "Leveraging Consistent Spatio-Temporal Correspondence for Robust Visual Odometry",
        "author": [
            "Zhaoxing Zhang",
            "Junda Cheng",
            "Gangwei Xu",
            "Xiaoxiang Wang",
            "Can Zhang",
            "Xin Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16923",
        "abstract": "Recent approaches to VO have significantly improved performance by using deep networks to predict optical flow between video frames. However, existing methods still suffer from noisy and inconsistent flow matching, making it difficult to handle challenging scenarios and long-sequence estimation. To overcome these challenges, we introduce Spatio-Temporal Visual Odometry (STVO), a novel deep network architecture that effectively leverages inherent spatio-temporal cues to enhance the accuracy and consistency of multi-frame flow matching. With more accurate and consistent flow matching, STVO can achieve better pose estimation through the bundle adjustment (BA). Specifically, STVO introduces two innovative components: 1) the Temporal Propagation Module that utilizes multi-frame information to extract and propagate temporal cues across adjacent frames, maintaining temporal consistency; 2) the Spatial Activation Module that utilizes geometric priors from the depth maps to enhance spatial consistency while filtering out excessive noise and incorrect matches. Our STVO achieves state-of-the-art performance on TUM-RGBD, EuRoc MAV, ETH3D and KITTI Odometry benchmarks. Notably, it improves accuracy by 77.8% on ETH3D benchmark and 38.9% on KITTI Odometry benchmark over the previous best methods.",
        "tags": [
            "Flow Matching",
            "Pose Estimation"
        ]
    },
    {
        "id": "130",
        "title": "GSemSplat: Generalizable Semantic 3D Gaussian Splatting from Uncalibrated Image Pairs",
        "author": [
            "Xingrui Wang",
            "Cuiling Lan",
            "Hanxin Zhu",
            "Zhibo Chen",
            "Yan Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16932",
        "abstract": "Modeling and understanding the 3D world is crucial for various applications, from augmented reality to robotic navigation. Recent advancements based on 3D Gaussian Splatting have integrated semantic information from multi-view images into Gaussian primitives. However, these methods typically require costly per-scene optimization from dense calibrated images, limiting their practicality. In this paper, we consider the new task of generalizable 3D semantic field modeling from sparse, uncalibrated image pairs. Building upon the Splatt3R architecture, we introduce GSemSplat, a framework that learns open-vocabulary semantic representations linked to 3D Gaussians without the need for per-scene optimization, dense image collections or calibration. To ensure effective and reliable learning of semantic features in 3D space, we employ a dual-feature approach that leverages both region-specific and context-aware semantic features as supervision in the 2D space. This allows us to capitalize on their complementary strengths. Experimental results on the ScanNet++ dataset demonstrate the effectiveness and superiority of our approach compared to the traditional scene-specific method. We hope our work will inspire more research into generalizable 3D understanding.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "131",
        "title": "Prompting Large Language Models with Rationale Heuristics for Knowledge-based Visual Question Answering",
        "author": [
            "Zhongjian Hu",
            "Peng Yang",
            "Bing Li",
            "Fengyuan Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16936",
        "abstract": "Recently, Large Language Models (LLMs) have been used for knowledge-based Visual Question Answering (VQA). Despite the encouraging results of previous studies, prior methods prompt LLMs to predict answers directly, neglecting intermediate thought processes. We argue that prior methods do not sufficiently activate the capacities of LLMs. We propose a framework called PLRH that Prompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH prompts LLMs with Chain of Thought (CoT) to generate rationale heuristics, i.e., intermediate thought processes, and then leverages the rationale heuristics to inspire LLMs to predict answers. Experiments show that our approach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA and A-OKVQA, respectively.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "132",
        "title": "BloomCoreset: Fast Coreset Sampling using Bloom Filters for Fine-Grained Self-Supervised Learning",
        "author": [
            "Prajwal Singh",
            "Gautam Vashishtha",
            "Indra Deep Mastan",
            "Shanmuganathan Raman"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16942",
        "abstract": "The success of deep learning in supervised fine-grained recognition for domain-specific tasks relies heavily on expert annotations. The Open-Set for fine-grained Self-Supervised Learning (SSL) problem aims to enhance performance on downstream tasks by strategically sampling a subset of images (the Core-Set) from a large pool of unlabeled data (the Open-Set). In this paper, we propose a novel method, BloomCoreset, that significantly reduces sampling time from Open-Set while preserving the quality of samples in the coreset. To achieve this, we utilize Bloom filters as an innovative hashing mechanism to store both low- and high-level features of the fine-grained dataset, as captured by Open-CLIP, in a space-efficient manner that enables rapid retrieval of the coreset from the Open-Set. To show the effectiveness of the sampled coreset, we integrate the proposed method into the state-of-the-art fine-grained SSL framework, SimCore [1]. The proposed algorithm drastically outperforms the sampling strategy of the baseline in SimCore [1] with a $98.5\\%$ reduction in sampling time with a mere $0.83\\%$ average trade-off in accuracy calculated across $11$ downstream datasets.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "133",
        "title": "A Career Interview Dialogue System using Large Language Model-based Dynamic Slot Generation",
        "author": [
            "Ekai Hashimoto",
            "Mikio Nakano",
            "Takayoshi Sakurai",
            "Shun Shiramatsu",
            "Toshitake Komazaki",
            "Shiho Tsuchiya"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16943",
        "abstract": "This study aims to improve the efficiency and quality of career interviews conducted by nursing managers. To this end, we have been developing a slot-filling dialogue system that engages in pre-interviews to collect information on staff careers as a preparatory step before the actual interviews. Conventional slot-filling-based interview dialogue systems have limitations in the flexibility of information collection because the dialogue progresses based on predefined slot sets. We therefore propose a method that leverages large language models (LLMs) to dynamically generate new slots according to the flow of the dialogue, achieving more natural conversations. Furthermore, we incorporate abduction into the slot generation process to enable more appropriate and effective slot generation. To validate the effectiveness of the proposed method, we conducted experiments using a user simulator. The results suggest that the proposed method using abduction is effective in enhancing both information-collecting capabilities and the naturalness of the dialogue.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "134",
        "title": "Linguistics-Vision Monotonic Consistent Network for Sign Language Production",
        "author": [
            "Xu Wang",
            "Shengeng Tang",
            "Peipei Song",
            "Shuo Wang",
            "Dan Guo",
            "Richang Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16944",
        "abstract": "Sign Language Production (SLP) aims to generate sign videos corresponding to spoken language sentences, where the conversion of sign Glosses to Poses (G2P) is the key step. Due to the cross-modal semantic gap and the lack of word-action correspondence labels for strong supervision alignment, the SLP suffers huge challenges in linguistics-vision consistency. In this work, we propose a Transformer-based Linguistics-Vision Monotonic Consistent Network (LVMCN) for SLP, which constrains fine-grained cross-modal monotonic alignment and coarse-grained multimodal semantic consistency in language-visual cues through Cross-modal Semantic Aligner (CSA) and Multimodal Semantic Comparator (MSC). In the CSA, we constrain the implicit alignment between corresponding gloss and pose sequences by computing the cosine similarity association matrix between cross-modal feature sequences (i.e., the order consistency of fine-grained sign glosses and actions). As for MSC, we construct multimodal triplets based on paired and unpaired samples in batch data. By pulling closer the corresponding text-visual pairs and pushing apart the non-corresponding text-visual pairs, we constrain the semantic co-occurrence degree between corresponding gloss and pose sequences (i.e., the semantic consistency of coarse-grained textual sentences and sign videos). Extensive experiments on the popular PHOENIX14T benchmark show that the LVMCN outperforms the state-of-the-art.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "135",
        "title": "Aristotle: Mastering Logical Reasoning with A Logic-Complete Decompose-Search-Resolve Framework",
        "author": [
            "Jundong Xu",
            "Hao Fei",
            "Meng Luo",
            "Qian Liu",
            "Liangming Pan",
            "William Yang Wang",
            "Preslav Nakov",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16953",
        "abstract": "In the context of large language models (LLMs), current advanced reasoning methods have made impressive strides in various reasoning tasks. However, when it comes to logical reasoning tasks, major challenges remain in both efficacy and efficiency. This is rooted in the fact that these systems fail to fully leverage the inherent structure of logical tasks throughout the reasoning processes such as decomposition, search, and resolution. To address this, we propose a logic-complete reasoning framework, Aristotle, with three key components: Logical Decomposer, Logical Search Router, and Logical Resolver. In our framework, symbolic expressions and logical rules are comprehensively integrated into the entire reasoning process, significantly alleviating the bottlenecks of logical reasoning, i.e., reducing sub-task complexity, minimizing search errors, and resolving logical contradictions. The experimental results on several datasets demonstrate that Aristotle consistently outperforms state-of-the-art reasoning frameworks in both accuracy and efficiency, particularly excelling in complex logical reasoning scenarios. We will open-source all our code at https://github.com/Aiden0526/Aristotle.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "136",
        "title": "Semantic Hierarchical Prompt Tuning for Parameter-Efficient Fine-Tuning",
        "author": [
            "Haowei Zhu",
            "Fangyuan Zhang",
            "Rui Qin",
            "Tianxiang Pan",
            "Junhai Yong",
            "Bin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16956",
        "abstract": "As the scale of vision models continues to grow, Visual Prompt Tuning (VPT) has emerged as a parameter-efficient transfer learning technique, noted for its superior performance compared to full fine-tuning. However, indiscriminately applying prompts to every layer without considering their inherent correlations, can cause significant disturbances, leading to suboptimal transferability. Additionally, VPT disrupts the original self-attention structure, affecting the aggregation of visual features, and lacks a mechanism for explicitly mining discriminative visual features, which are crucial for classification.\nTo address these issues, we propose a Semantic Hierarchical Prompt (SHIP) fine-tuning strategy. We adaptively construct semantic hierarchies and use semantic-independent and semantic-shared prompts to learn hierarchical representations. We also integrate attribute prompts and a prompt matching loss to enhance feature discrimination and employ decoupled attention for robustness and reduced inference costs. SHIP significantly improves performance, achieving a 4.8\\% gain in accuracy over VPT with a ViT-B/16 backbone on VTAB-1k tasks. Our code is available at https://github.com/haoweiz23/SHIP.",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "137",
        "title": "System-2 Mathematical Reasoning via Enriched Instruction Tuning",
        "author": [
            "Huanqia Cai",
            "Yijun Yang",
            "Zhifeng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16964",
        "abstract": "Solving complex mathematical problems via system-2 reasoning is a natural human skill, yet it remains a significant challenge for current large language models (LLMs). We identify the scarcity of deliberate multi-step reasoning data as a primary limiting factor. To this end, we introduce Enriched Instruction Tuning (EIT), a method that enriches existing human-annotated mathematical datasets by synergizing human and AI feedback to create fine-grained reasoning trajectories. These datasets are then used to fine-tune open-source LLMs, enhancing their mathematical reasoning abilities without reliance on any symbolic verification program. Concretely, EIT is composed of two critical steps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step (ERS). The former generates a high-level plan that breaks down complex instructions into a sequence of simpler objectives, while ERS fills in reasoning contexts often overlooked by human annotators, creating a smoother reasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods that generate reasoning chains only depending on LLM's internal knowledge, our method leverages human-annotated initial answers as ``meta-knowledge'' to help LLMs generate more detailed and precise reasoning processes, leading to a more trustworthy LLM expert for complex mathematical problems. In experiments, EIT achieves an accuracy of 84.1\\% on GSM8K and 32.5\\% on MATH, surpassing state-of-the-art fine-tuning and prompting methods, and even matching the performance of tool-augmented methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "138",
        "title": "Cannot or Should Not? Automatic Analysis of Refusal Composition in IFT/RLHF Datasets and Refusal Behavior of Black-Box LLMs",
        "author": [
            "Alexander von Recum",
            "Christoph Schnabl",
            "Gabor Hollbeck",
            "Silas Alberti",
            "Philip Blinde",
            "Marvin von Hagen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16974",
        "abstract": "Refusals - instances where large language models (LLMs) decline or fail to fully execute user instructions - are crucial for both AI safety and AI capabilities and the reduction of hallucinations in particular. These behaviors are learned during post-training, especially in instruction fine-tuning (IFT) and reinforcement learning from human feedback (RLHF). However, existing taxonomies and evaluation datasets for refusals are inadequate, often focusing solely on should-not-related (instead of cannot-related) categories, and lacking tools for auditing refusal content in black-box LLM outputs.\nWe present a comprehensive framework for classifying LLM refusals: (a) a taxonomy of 16 refusal categories, (b) a human-annotated dataset of over 8,600 instances from publicly available IFT and RLHF datasets, (c) a synthetic dataset with 8,000 examples for each refusal category, and (d) classifiers trained for refusal classification.\nOur work enables precise auditing of refusal behaviors in black-box LLMs and automatic analyses of refusal patterns in large IFT and RLHF datasets. This facilitates the strategic adjustment of LLM refusals, contributing to the development of more safe and reliable LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "139",
        "title": "PromptDresser: Improving the Quality and Controllability of Virtual Try-On via Generative Textual Prompt and Prompt-aware Mask",
        "author": [
            "Jeongho Kim",
            "Hoiyeong Jin",
            "Sunghyun Park",
            "Jaegul Choo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16978",
        "abstract": "Recent virtual try-on approaches have advanced by fine-tuning the pre-trained text-to-image diffusion models to leverage their powerful generative ability. However, the use of text prompts in virtual try-on is still underexplored. This paper tackles a text-editable virtual try-on task that changes the clothing item based on the provided clothing image while editing the wearing style (e.g., tucking style, fit) according to the text descriptions. In the text-editable virtual try-on, three key aspects exist: (i) designing rich text descriptions for paired person-clothing data to train the model, (ii) addressing the conflicts where textual information of the existing person's clothing interferes the generation of the new clothing, and (iii) adaptively adjust the inpainting mask aligned with the text descriptions, ensuring proper editing areas while preserving the original person's appearance irrelevant to the new clothing. To address these aspects, we propose PromptDresser, a text-editable virtual try-on model that leverages large multimodal model (LMM) assistance to enable high-quality and versatile manipulation based on generative text prompts. Our approach utilizes LMMs via in-context learning to generate detailed text descriptions for person and clothing images independently, including pose details and editing attributes using minimal human cost. Moreover, to ensure the editing areas, we adjust the inpainting mask depending on the text prompts adaptively. We found that our approach, utilizing detailed text prompts, not only enhances text editability but also effectively conveys clothing details that are difficult to capture through images alone, thereby enhancing image quality. Our code is available at https://github.com/rlawjdghek/PromptDresser.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "140",
        "title": "InterDance:Reactive 3D Dance Generation with Realistic Duet Interactions",
        "author": [
            "Ronghui Li",
            "Youliang Zhang",
            "Yachao Zhang",
            "Yuxiang Zhang",
            "Mingyang Su",
            "Jie Guo",
            "Ziwei Liu",
            "Yebin Liu",
            "Xiu Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16982",
        "abstract": "Humans perform a variety of interactive motions, among which duet dance is one of the most challenging interactions. However, in terms of human motion generative models, existing works are still unable to generate high-quality interactive motions, especially in the field of duet dance. On the one hand, it is due to the lack of large-scale high-quality datasets. On the other hand, it arises from the incomplete representation of interactive motion and the lack of fine-grained optimization of interactions. To address these challenges, we propose, InterDance, a large-scale duet dance dataset that significantly enhances motion quality, data scale, and the variety of dance genres. Built upon this dataset, we propose a new motion representation that can accurately and comprehensively describe interactive motion. We further introduce a diffusion-based framework with an interaction refinement guidance strategy to optimize the realism of interactions progressively. Extensive experiments demonstrate the effectiveness of our dataset and algorithm.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "141",
        "title": "GAS: Generative Auto-bidding with Post-training Search",
        "author": [
            "Yewen Li",
            "Shuai Mao",
            "Jingtong Gao",
            "Nan Jiang",
            "Yunjian Xu",
            "Qingpeng Cai",
            "Fei Pan",
            "Peng Jiang",
            "Bo An"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17018",
        "abstract": "Auto-bidding is essential in facilitating online advertising by automatically placing bids on behalf of advertisers. Generative auto-bidding, which generates bids based on an adjustable condition using models like transformers and diffusers, has recently emerged as a new trend due to its potential to learn optimal strategies directly from data and adjust flexibly to preferences. However, generative models suffer from low-quality data leading to a mismatch between condition, return to go, and true action value, especially in long sequential decision-making. Besides, the majority preference in the dataset may hinder models' generalization ability on minority advertisers' preferences. While it is possible to collect high-quality data and retrain multiple models for different preferences, the high cost makes it unaffordable, hindering the advancement of auto-bidding into the era of large foundation models. To address this, we propose a flexible and practical Generative Auto-bidding scheme using post-training Search, termed GAS, to refine a base policy model's output and adapt to various preferences. We use weak-to-strong search alignment by training small critics for different preferences and an MCTS-inspired search to refine the model's output. Specifically, a novel voting mechanism with transformer-based critics trained with policy indications could enhance search alignment performance. Additionally, utilizing the search, we provide a fine-tuning method for high-frequency preference scenarios considering computational efficiency. Extensive experiments conducted on the real-world dataset and online A/B test on the Kuaishou advertising platform demonstrate the effectiveness of GAS, achieving significant improvements, e.g., 1.554% increment of target cost.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "142",
        "title": "Reversed Attention: On The Gradient Descent Of Attention Layers In GPT",
        "author": [
            "Shahar Katz",
            "Lior Wolf"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17019",
        "abstract": "The success of Transformer-based Language Models (LMs) stems from their attention mechanism. While this mechanism has been extensively studied in explainability research, particularly through the attention values obtained during the forward pass of LMs, the backward pass of attention has been largely overlooked. In this work, we study the mathematics of the backward pass of attention, revealing that it implicitly calculates an attention matrix we refer to as \"Reversed Attention\". We examine the properties of Reversed Attention and demonstrate its ability to elucidate the models' behavior and edit dynamics. In an experimental setup, we showcase the ability of Reversed Attention to directly alter the forward pass of attention, without modifying the model's weights, using a novel method called \"attention patching\". In addition to enhancing the comprehension of how LM configure attention layers during backpropagation, Reversed Attention maps contribute to a more interpretable backward pass.",
        "tags": [
            "GPT",
            "Transformer"
        ]
    },
    {
        "id": "143",
        "title": "MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on New and Tail Knowledge",
        "author": [
            "Jie He",
            "Nan Hu",
            "Wanqiu Long",
            "Jiaoyan Chen",
            "Jeff Z. Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17032",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks but face significant challenges with complex, knowledge-intensive multi-hop queries, particularly those involving new or long-tail knowledge. Existing benchmarks often fail to fully address these challenges. To bridge this gap, we introduce MINTQA (Multi-hop Question Answering on New and Tail Knowledge), a comprehensive benchmark to evaluate LLMs' capabilities in multi-hop reasoning across four critical dimensions: question handling strategy, sub-question generation, retrieval-augmented generation, and iterative or dynamic decomposition and retrieval. MINTQA comprises 10,479 question-answer pairs for evaluating new knowledge and 17,887 pairs for assessing long-tail knowledge, with each question equipped with corresponding sub-questions and answers. Our systematic evaluation of 22 state-of-the-art LLMs on MINTQA reveals significant limitations in their ability to handle complex knowledge base queries, particularly in handling new or unpopular knowledge. Our findings highlight critical challenges and offer insights for advancing multi-hop reasoning capabilities. The MINTQA benchmark is available at https://github.com/probe2/multi-hop/.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "144",
        "title": "Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models",
        "author": [
            "Lang Gao",
            "Xiangliang Zhang",
            "Preslav Nakov",
            "Xiuying Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17034",
        "abstract": "Jailbreaking in Large Language Models (LLMs) is a major security concern as it can deceive LLMs to generate harmful text. Yet, there is still insufficient understanding of how jailbreaking works, which makes it hard to develop effective defense strategies. We aim to shed more light into this issue: we conduct a detailed large-scale analysis of seven different jailbreak methods and find that these disagreements stem from insufficient observation samples. In particular, we introduce \\textit{safety boundary}, and we find that jailbreaks shift harmful activations outside that safety boundary, where LLMs are less sensitive to harmful information. We also find that the low and the middle layers are critical in such shifts, while deeper layers have less impact. Leveraging on these insights, we propose a novel defense called \\textbf{Activation Boundary Defense} (ABD), which adaptively constrains the activations within the safety boundary. We further use Bayesian optimization to selectively apply the defense method to the low and the middle layers. Our experiments on several benchmarks show that ABD achieves an average DSR of over 98\\% against various forms of jailbreak attacks, with less than 2\\% impact on the model's general capabilities.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "145",
        "title": "Adapting Image-to-Video Diffusion Models for Large-Motion Frame Interpolation",
        "author": [
            "Luoxu Jin",
            "Hiroshi Watanabe"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17042",
        "abstract": "The development of video generation models has advanced significantly in recent years. For video frame interpolation, we adopt a pre-trained large-scale image-to-video diffusion model. To enable this adaptation, we propose a conditional encoder, which serves as a simple yet effective trainable module. By leveraging the first and last frames, we extract spatial and temporal features and input them into the conditional encoder. The computed features of the conditional encoder guide the video diffusion model in generating keyframe-guided video sequences. Our method demonstrates superior performance on the Fréchet Video Distance (FVD) metric compared to previous deterministic approaches in handling large-motion cases, highlighting advancements in generative-based methodologies.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "146",
        "title": "Modular Conversational Agents for Surveys and Interviews",
        "author": [
            "Jiangbo Yu",
            "Jinhua Zhao",
            "Luis Miranda-Moreno",
            "Matthew Korp"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17049",
        "abstract": "Surveys and interviews (structured, semi-structured, or unstructured) are widely used for collecting insights on emerging or hypothetical scenarios. Traditional human-led methods often face challenges related to cost, scalability, and consistency. Recently, various domains have begun to explore the use of conversational agents (chatbots) powered by large language models (LLMs). However, as public investments and policies on infrastructure and services often involve substantial public stakes and environmental risks, there is a need for a rigorous, transparent, privacy-preserving, and cost-efficient development framework tailored for such major decision-making processes. This paper addresses this gap by introducing a modular approach and its resultant parameterized process for designing conversational agents. We detail the system architecture, integrating engineered prompts, specialized knowledge bases, and customizable, goal-oriented conversational logic in the proposed approach. We demonstrate the adaptability, generalizability, and efficacy of our modular approach through three empirical studies: (1) travel preference surveys, highlighting multimodal (voice, text, and image generation) capabilities; (2) public opinion elicitation on a newly constructed, novel infrastructure project, showcasing question customization and multilingual (English and French) capabilities; and (3) transportation expert consultation about future transportation systems, highlighting real-time, clarification request capabilities for open-ended questions, resilience in handling erratic inputs, and efficient transcript post-processing. The results show the effectiveness of this modular approach and how it addresses key ethical, privacy, security, and token consumption concerns, setting the stage for the next-generation surveys and interviews.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "147",
        "title": "ViLBias: A Framework for Bias Detection using Linguistic and Visual Cues",
        "author": [
            "Shaina Raza",
            "Caesar Saleh",
            "Emrul Hasan",
            "Franklin Ogidi",
            "Maximus Powers",
            "Veronica Chatrath",
            "Marcelo Lotif",
            "Roya Javadi",
            "Anam Zahid",
            "Vahid Reza Khazaie"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17052",
        "abstract": "The integration of Large Language Models (LLMs) and Vision-Language Models (VLMs) opens new avenues for addressing complex challenges in multimodal content analysis, particularly in biased news detection. This study introduces ViLBias, a framework that leverages state of the art LLMs and VLMs to detect linguistic and visual biases in news content, addressing the limitations of traditional text-only approaches. Our contributions include a novel dataset pairing textual content with accompanying visuals from diverse news sources and a hybrid annotation framework, combining LLM-based annotations with human review to enhance quality while reducing costs and improving scalability. We evaluate the efficacy of LLMs and VLMs in identifying biases, revealing their strengths in detecting subtle framing and text-visual inconsistencies. Empirical analysis demonstrates that incorporating visual cues alongside text enhances bias detection accuracy by 3 to 5 %, showcasing the complementary strengths of LLMs in generative reasoning and Small Language Models (SLMs) in classification. This study offers a comprehensive exploration of LLMs and VLMs as tools for detecting multimodal biases in news content, highlighting both their potential and limitations. Our research paves the way for more robust, scalable, and nuanced approaches to media bias detection, contributing to the broader field of natural language processing and multimodal analysis. (The data and code will be made available for research purposes).",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "148",
        "title": "DR-Encoder: Encode Low-rank Gradients with Random Prior for Large Language Models Differentially Privately",
        "author": [
            "Huiwen Wu",
            "Deyi Zhang",
            "Xiaohan Li",
            "Xiaogang Xu",
            "Jiafei Wu",
            "Zhe Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17053",
        "abstract": "The emergence of the Large Language Model (LLM) has shown their superiority in a wide range of disciplines, including language understanding and translation, relational logic reasoning, and even partial differential equations solving. The transformer is the pervasive backbone architecture for the foundation model construction. It is vital to research how to adjust the Transformer architecture to achieve an end-to-end privacy guarantee in LLM fine-tuning. In this paper, we investigate three potential information leakage during a federated fine-tuning procedure for LLM (FedLLM). Based on the potential information leakage, we provide an end-to-end privacy guarantee solution for FedLLM by inserting two-stage randomness. The first stage is to train a gradient auto-encoder with a Gaussian random prior based on the statistical information of the gradients generated by local clients. The second stage is to fine-tune the overall LLM with a differential privacy guarantee by adopting appropriate Gaussian noises. We show the efficiency and accuracy gains of our proposed method with several foundation models and two popular evaluation benchmarks. Furthermore, we present a comprehensive privacy analysis with Gaussian Differential Privacy (GDP) and Renyi Differential Privacy (RDP).",
        "tags": [
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "149",
        "title": "The HalluRAG Dataset: Detecting Closed-Domain Hallucinations in RAG Applications Using an LLM's Internal States",
        "author": [
            "Fabian Ridder",
            "Malte Schilling"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17056",
        "abstract": "Detecting hallucinations in large language models (LLMs) is critical for enhancing their reliability and trustworthiness. Most research focuses on hallucinations as deviations from information seen during training. However, the opaque nature of an LLM's parametric knowledge complicates the understanding of why generated texts appear ungrounded: The LLM might not have picked up the necessary knowledge from large and often inaccessible datasets, or the information might have been changed or contradicted during further training. Our focus is on hallucinations involving information not used in training, which we determine by using recency to ensure the information emerged after a cut-off date. This study investigates these hallucinations by detecting them at sentence level using different internal states of various LLMs. We present HalluRAG, a dataset designed to train classifiers on these hallucinations. Depending on the model and quantization, MLPs trained on HalluRAG detect hallucinations with test accuracies ranging up to 75 %, with Mistral-7B-Instruct-v0.1 achieving the highest test accuracies. Our results show that IAVs detect hallucinations as effectively as CEVs and reveal that answerable and unanswerable prompts are encoded differently as separate classifiers for these categories improved accuracy. However, HalluRAG showed some limited generalizability, advocating for more diversity in datasets on hallucinations.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "150",
        "title": "SubstationAI: Multimodal Large Model-Based Approaches for Analyzing Substation Equipment Faults",
        "author": [
            "Jinzhi Wang",
            "Qinfeng Song",
            "Lidong Qian",
            "Haozhou Li",
            "Qinke Peng",
            "Jiangbo Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17077",
        "abstract": "The reliability of substation equipment is crucial to the stability of power systems, but traditional fault analysis methods heavily rely on manual expertise, limiting their effectiveness in handling complex and large-scale data. This paper proposes a substation equipment fault analysis method based on a multimodal large language model (MLLM). We developed a database containing 40,000 entries, including images, defect labels, and analysis reports, and used an image-to-video generation model for data augmentation. Detailed fault analysis reports were generated using GPT-4. Based on this database, we developed SubstationAI, the first model dedicated to substation fault analysis, and designed a fault diagnosis knowledge base along with knowledge enhancement methods. Experimental results show that SubstationAI significantly outperforms existing models, such as GPT-4, across various evaluation metrics, demonstrating higher accuracy and practicality in fault cause analysis, repair suggestions, and preventive measures, providing a more advanced solution for substation equipment fault analysis.",
        "tags": [
            "GPT",
            "Video Generation"
        ]
    },
    {
        "id": "151",
        "title": "SAIL: Sample-Centric In-Context Learning for Document Information Extraction",
        "author": [
            "Jinyu Zhang",
            "Zhiyuan You",
            "Jize Wang",
            "Xinyi Le"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17092",
        "abstract": "Document Information Extraction (DIE) aims to extract structured information from Visually Rich Documents (VRDs). Previous full-training approaches have demonstrated strong performance but may struggle with generalization to unseen data. In contrast, training-free methods leverage powerful pre-trained models like Large Language Models (LLMs) to address various downstream tasks with only a few examples. Nonetheless, training-free methods for DIE encounter two primary challenges: (1) understanding the complex relationship between layout and textual elements in VRDs, and (2) providing accurate guidance to pre-trained models. To address these challenges, we propose Sample-centric In-context Learning (SAIL) for DIE. SAIL introduces a fine-grained entity-level textual similarity to facilitate in-depth text analysis by LLMs and incorporates layout similarity to enhance the analysis of layouts in VRDs. Additionally, SAIL formulates a unified In-Context Learning (ICL) prompt template for various sample-centric examples, enabling tailored prompts that deliver precise guidance to pre-trained models for each sample. Extensive experiments on FUNSD, CORD, and SROIE benchmarks with various base models (e.g., LLMs) indicate that our method outperforms training-free baselines, even closer to the full-training methods. The results show the superiority and generalization of our method.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "152",
        "title": "Analysis on LLMs Performance for Code Summarization",
        "author": [
            "Md. Ahnaf Akib",
            "Md. Muktadir Mazumder",
            "Salman Ahsan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17094",
        "abstract": "Code summarization aims to generate concise natural language descriptions for source code. Deep learning has been used more and more recently in software engineering, particularly for tasks like code creation and summarization. Specifically, it appears that the most current Large Language Models with coding perform well on these tasks. Large Language Models (LLMs) have significantly advanced the field of code summarization, providing sophisticated methods for generating concise and accurate summaries of source code. This study aims to perform a comparative analysis of several open-source LLMs, namely LLaMA-3, Phi-3, Mistral, and Gemma. These models' performance is assessed using important metrics such as BLEU\\textsubscript{3.1} and ROUGE\\textsubscript{3.2}.\nThrough this analysis, we seek to identify the strengths and weaknesses of each model, offering insights into their applicability and effectiveness in code summarization tasks. Our findings contribute to the ongoing development and refinement of LLMs, supporting their integration into tools that enhance software development and maintenance processes.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "153",
        "title": "DreamOmni: Unified Image Generation and Editing",
        "author": [
            "Bin Xia",
            "Yuechen Zhang",
            "Jingyao Li",
            "Chengyao Wang",
            "Yitong Wang",
            "Xinglong Wu",
            "Bei Yu",
            "Jiaya Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17098",
        "abstract": "Currently, the success of large language models (LLMs) illustrates that a unified multitasking approach can significantly enhance model usability, streamline deployment, and foster synergistic benefits across different tasks. However, in computer vision, while text-to-image (T2I) models have significantly improved generation quality through scaling up, their framework design did not initially consider how to unify with downstream tasks, such as various types of editing. To address this, we introduce DreamOmni, a unified model for image generation and editing. We begin by analyzing existing frameworks and the requirements of downstream tasks, proposing a unified framework that integrates both T2I models and various editing tasks. Furthermore, another key challenge is the efficient creation of high-quality editing data, particularly for instruction-based and drag-based editing. To this end, we develop a synthetic data pipeline using sticker-like elements to synthesize accurate, high-quality datasets efficiently, which enables editing data scaling up for unified model training. For training, DreamOmni jointly trains T2I generation and downstream tasks. T2I training enhances the model's understanding of specific concepts and improves generation quality, while editing training helps the model grasp the nuances of the editing task. This collaboration significantly boosts editing performance. Extensive experiments confirm the effectiveness of DreamOmni. The code and model will be released.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Text-to-Image"
        ]
    },
    {
        "id": "154",
        "title": "Similarity Trajectories: Linking Sampling Process to Artifacts in Diffusion-Generated Images",
        "author": [
            "Dennis Menn",
            "Feng Liang",
            "Hung-Yueh Chiang",
            "Diana Marculescu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17109",
        "abstract": "Artifact detection algorithms are crucial to correcting the output generated by diffusion models. However, because of the variety of artifact forms, existing methods require substantial annotated data for training. This requirement limits their scalability and efficiency, which restricts their wide application. This paper shows that the similarity of denoised images between consecutive time steps during the sampling process is related to the severity of artifacts in images generated by diffusion models. Building on this observation, we introduce the concept of Similarity Trajectory to characterize the sampling process and its correlation with the image artifacts presented. Using an annotated data set of 680 images, which is only 0.1% of the amount of data used in the prior work, we trained a classifier on these trajectories to predict the presence of artifacts in images. By performing 10-fold validation testing on the balanced annotated data set, the classifier can achieve an accuracy of 72.35%, highlighting the connection between the Similarity Trajectory and the occurrence of artifacts. This approach enables differentiation between artifact-exhibiting and natural-looking images using limited training data.",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "155",
        "title": "Transformer-Based Model Predictive Path Integral Control",
        "author": [
            "Shrenik Zinage",
            "Vrushabh Zinage",
            "Efstathios Bakolas"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17118",
        "abstract": "This paper presents a novel approach to improve the Model Predictive Path Integral (MPPI) control by using a transformer to initialize the mean control sequence. Traditional MPPI methods often struggle with sample efficiency and computational costs due to suboptimal initial rollouts. We propose TransformerMPPI, which uses a transformer trained on historical control data to generate informed initial mean control sequences. TransformerMPPI combines the strengths of the attention mechanism in transformers and sampling-based control, leading to improved computational performance and sample efficiency. The ability of the transformer to capture long-horizon patterns in optimal control sequences allows TransformerMPPI to start from a more informed control sequence, reducing the number of samples required, and accelerating convergence to optimal control sequence. We evaluate our method on various control tasks, including avoidance of collisions in a 2D environment and autonomous racing in the presence of static and dynamic obstacles. Numerical simulations demonstrate that TransformerMPPI consistently outperforms traditional MPPI algorithms in terms of overall average cost, sample efficiency, and computational speed in the presence of static and dynamic obstacles.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "156",
        "title": "Lies, Damned Lies, and Distributional Language Statistics: Persuasion and Deception with Large Language Models",
        "author": [
            "Cameron R. Jones",
            "Benjamin K. Bergen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17128",
        "abstract": "Large Language Models (LLMs) can generate content that is as persuasive as human-written text and appear capable of selectively producing deceptive outputs. These capabilities raise concerns about potential misuse and unintended consequences as these systems become more widely deployed. This review synthesizes recent empirical work examining LLMs' capacity and proclivity for persuasion and deception, analyzes theoretical risks that could arise from these capabilities, and evaluates proposed mitigations. While current persuasive effects are relatively small, various mechanisms could increase their impact, including fine-tuning, multimodality, and social factors. We outline key open questions for future research, including how persuasive AI systems might become, whether truth enjoys an inherent advantage over falsehoods, and how effective different mitigation strategies may be in practice.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "157",
        "title": "Hate Speech Detection and Target Identification in Devanagari Languages via Parameter Efficient Fine-Tuning of LLMs",
        "author": [
            "Rushendra Sidibomma",
            "Pransh Patwa",
            "Parth Patwa",
            "Aman Chadha",
            "Vinija Jain",
            "Amitava Das"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17131",
        "abstract": "The detection of hate speech has become increasingly important in combating online hostility and its real-world consequences. Despite recent advancements, there is limited research addressing hate speech detection in Devanagari-scripted languages, where resources and tools are scarce. While large language models (LLMs) have shown promise in language-related tasks, traditional fine-tuning approaches are often infeasible given the size of the models. In this paper, we propose a Parameter Efficient Fine tuning (PEFT) based solution for hate speech detection and target identification. We evaluate multiple LLMs on the Devanagari dataset provided by (Thapa et al., 2025), which contains annotated instances in 2 languages - Hindi and Nepali. The results demonstrate the efficacy of our approach in handling Devanagari-scripted content.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "158",
        "title": "Empirical evaluation of normalizing flows in Markov Chain Monte Carlo",
        "author": [
            "David Nabergoj",
            "Erik Štrumbelj"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17136",
        "abstract": "Recent advances in MCMC use normalizing flows to precondition target distributions and enable jumps to distant regions. However, there is currently no systematic comparison of different normalizing flow architectures for MCMC. As such, many works choose simple flow architectures that are readily available and do not consider other models. Guidelines for choosing an appropriate architecture would reduce analysis time for practitioners and motivate researchers to take the recommended models as foundations to be improved. We provide the first such guideline by extensively evaluating many normalizing flow architectures on various flow-based MCMC methods and target distributions. When the target density gradient is available, we show that flow-based MCMC outperforms classic MCMC for suitable NF architecture choices with minor hyperparameter tuning. When the gradient is unavailable, flow-based MCMC wins with off-the-shelf architectures. We find contractive residual flows to be the best general-purpose models with relatively low sensitivity to hyperparameter choice. We also provide various insights into normalizing flow behavior within MCMC when varying their hyperparameters, properties of target distributions, and the overall computational budget.",
        "tags": [
            "Normalizing Flows"
        ]
    },
    {
        "id": "159",
        "title": "Style Transfer Dataset: What Makes A Good Stylization?",
        "author": [
            "Victor Kitov",
            "Valentin Abramov",
            "Mikhail Akhtyrchenko"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17139",
        "abstract": "We present a new dataset with the goal of advancing image style transfer - the task of rendering one image in the style of another image. The dataset covers various content and style images of different size and contains 10.000 stylizations manually rated by three annotators in 1-10 scale. Based on obtained ratings, we find which factors are mostly responsible for favourable and poor user evaluations and show quantitative measures having statistically significant impact on user grades. A methodology for creating style transfer datasets is discussed. Presented dataset can be used in automating multiple tasks, related to style transfer configuration and evaluation.",
        "tags": [
            "Style Transfer"
        ]
    },
    {
        "id": "160",
        "title": "LLM Agent for Fire Dynamics Simulations",
        "author": [
            "Leidong Xu",
            "Danyal Mohaddes",
            "Yi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17146",
        "abstract": "Significant advances have been achieved in leveraging foundation models, such as large language models (LLMs), to accelerate complex scientific workflows. In this work we introduce FoamPilot, a proof-of-concept LLM agent designed to enhance the usability of FireFOAM, a specialized solver for fire dynamics and fire suppression simulations built using OpenFOAM, a popular open-source toolbox for computational fluid dynamics (CFD). FoamPilot provides three core functionalities: code insight, case configuration and simulation evaluation. Code insight is an alternative to traditional keyword searching leveraging retrieval-augmented generation (RAG) and aims to enable efficient navigation and summarization of the FireFOAM source code for developers and experienced users. For case configuration, the agent interprets user requests in natural language and aims to modify existing simulation setups accordingly to support intermediate users. FoamPilot's job execution functionality seeks to manage the submission and execution of simulations in high-performance computing (HPC) environments and provide preliminary analysis of simulation results to support less experienced users. Promising results were achieved for each functionality, particularly for simple tasks, and opportunities were identified for significant further improvement for more complex tasks. The integration of these functionalities into a single LLM agent is a step aimed at accelerating the simulation workflow for engineers and scientists employing FireFOAM for complex simulations critical for improving fire safety.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "161",
        "title": "A Multi-AI Agent System for Autonomous Optimization of Agentic AI Solutions via Iterative Refinement and LLM-Driven Feedback Loops",
        "author": [
            "Kamer Ali Yuksel",
            "Hassan Sawaf"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17149",
        "abstract": "Agentic AI systems use specialized agents to handle tasks within complex workflows, enabling automation and efficiency. However, optimizing these systems often requires labor-intensive, manual adjustments to refine roles, tasks, and interactions. This paper introduces a framework for autonomously optimizing Agentic AI solutions across industries, such as NLP-driven enterprise applications. The system employs agents for Refinement, Execution, Evaluation, Modification, and Documentation, leveraging iterative feedback loops powered by an LLM (Llama 3.2-3B). The framework achieves optimal performance without human input by autonomously generating and testing hypotheses to improve system configurations. This approach enhances scalability and adaptability, offering a robust solution for real-world applications in dynamic environments. Case studies across diverse domains illustrate the transformative impact of this framework, showcasing significant improvements in output quality, relevance, and actionability. All data for these case studies, including original and evolved agent codes, along with their outputs, are here: https://anonymous.4open.science/r/evolver-1D11/",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "162",
        "title": "Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching",
        "author": [
            "Enshu Liu",
            "Xuefei Ning",
            "Yu Wang",
            "Zinan Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17153",
        "abstract": "Autoregressive (AR) models have achieved state-of-the-art performance in text and image generation but suffer from slow generation due to the token-by-token process. We ask an ambitious question: can a pre-trained AR model be adapted to generate outputs in just one or two steps? If successful, this would significantly advance the development and deployment of AR models. We notice that existing works that try to speed up AR generation by generating multiple tokens at once fundamentally cannot capture the output distribution due to the conditional dependencies between tokens, limiting their effectiveness for few-step generation. To address this, we propose Distilled Decoding (DD), which uses flow matching to create a deterministic mapping from Gaussian distribution to the output distribution of the pre-trained AR model. We then train a network to distill this mapping, enabling few-step generation. DD doesn't need the training data of the original AR model, making it more http://practical.We evaluate DD on state-of-the-art image AR models and present promising results on ImageNet-256. For VAR, which requires 10-step generation, DD enables one-step generation (6.3$\\times$ speed-up), with an acceptable increase in FID from 4.19 to 9.96. For LlamaGen, DD reduces generation from 256 steps to 1, achieving an 217.8$\\times$ speed-up with a comparable FID increase from 4.11 to 11.35. In both cases, baseline methods completely fail with FID>100. DD also excels on text-to-image generation, reducing the generation from 256 steps to 2 for LlamaGen with minimal FID increase from 25.70 to 28.95. As the first work to demonstrate the possibility of one-step generation for image AR models, DD challenges the prevailing notion that AR models are inherently slow, and opens up new opportunities for efficient AR generation. The project website is at https://imagination-research.github.io/distilled-decoding.",
        "tags": [
            "Flow Matching",
            "Text-to-Image"
        ]
    },
    {
        "id": "163",
        "title": "LLM-based relevance assessment still can't replace human relevance assessment",
        "author": [
            "Charles L. A. Clarke",
            "Laura Dietz"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17156",
        "abstract": "The use of large language models (LLMs) for relevance assessment in information retrieval has gained significant attention, with recent studies suggesting that LLM-based judgments provide comparable evaluations to human judgments. Notably, based on TREC 2024 data, Upadhyay et al. make a bold claim that LLM-based relevance assessments, such as those generated by the UMBRELA system, can fully replace traditional human relevance assessments in TREC-style evaluations. This paper critically examines this claim, highlighting practical and theoretical limitations that undermine the validity of this conclusion. First, we question whether the evidence provided by Upadhyay et al. really supports their claim, particularly if a test collection is used asa benchmark for future improvements. Second, through a submission deliberately intended to do so, we demonstrate the ease with which automatic evaluation metrics can be subverted, showing that systems designed to exploit these evaluations can achieve artificially high scores. Theoretical challenges -- such as the inherent narcissism of LLMs, the risk of overfitting to LLM-based metrics, and the potential degradation of future LLM performance -- must be addressed before LLM-based relevance assessments can be considered a viable replacement for human judgments.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "164",
        "title": "Generative Diffusion Modeling: A Practical Handbook",
        "author": [
            "Zihan Ding",
            "Chi Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17162",
        "abstract": "This handbook offers a unified perspective on diffusion models, encompassing diffusion probabilistic models, score-based generative models, consistency models, rectified flow, and related methods. By standardizing notations and aligning them with code implementations, it aims to bridge the \"paper-to-code\" gap and facilitate robust implementations and fair comparisons. The content encompasses the fundamentals of diffusion models, the pre-training process, and various post-training methods. Post-training techniques include model distillation and reward-based fine-tuning. Designed as a practical guide, it emphasizes clarity and usability over theoretical depth, focusing on widely adopted approaches in generative modeling with diffusion models.",
        "tags": [
            "Consistency Models",
            "Diffusion",
            "Rectified Flow",
            "Score-Based Generative"
        ]
    },
    {
        "id": "165",
        "title": "Survey on Abstractive Text Summarization: Dataset, Models, and Metrics",
        "author": [
            "Gospel Ozioma Nnadi",
            "Flavio Bertini"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17165",
        "abstract": "The advancements in deep learning, particularly the introduction of transformers, have been pivotal in enhancing various natural language processing (NLP) tasks. These include text-to-text applications such as machine translation, text classification, and text summarization, as well as data-to-text tasks like response generation and image-to-text tasks such as captioning. Transformer models are distinguished by their attention mechanisms, pretraining on general knowledge, and fine-tuning for downstream tasks. This has led to significant improvements, particularly in abstractive summarization, where sections of a source document are paraphrased to produce summaries that closely resemble human expression.\nThe effectiveness of these models is assessed using diverse metrics, encompassing techniques like semantic overlap and factual correctness. This survey examines the state of the art in text summarization models, with a specific focus on the abstractive summarization approach. It reviews various datasets and evaluation metrics used to measure model performance. Additionally, it includes the results of test cases using abstractive summarization models to underscore the advantages and limitations of contemporary transformer-based models. The source codes and the data are available at https://github.com/gospelnnadi/Text-Summarization-SOTA-Experiment.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "166",
        "title": "WPMixer: Efficient Multi-Resolution Mixing for Long-Term Time Series Forecasting",
        "author": [
            "Md Mahmuddun Nabi Murad",
            "Mehmet Aktukmak",
            "Yasin Yilmaz"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17176",
        "abstract": "Time series forecasting is crucial for various applications, such as weather forecasting, power load forecasting, and financial analysis. In recent studies, MLP-mixer models for time series forecasting have been shown as a promising alternative to transformer-based models. However, the performance of these models is still yet to reach its potential. In this paper, we propose Wavelet Patch Mixer (WPMixer), a novel MLP-based model, for long-term time series forecasting, which leverages the benefits of patching, multi-resolution wavelet decomposition, and mixing. Our model is based on three key components: (i) multi-resolution wavelet decomposition, (ii) patching and embedding, and (iii) MLP mixing. Multi-resolution wavelet decomposition efficiently extracts information in both the frequency and time domains. Patching allows the model to capture an extended history with a look-back window and enhances capturing local information while MLP mixing incorporates global information. Our model significantly outperforms state-of-the-art MLP-based and transformer-based models for long-term time series forecasting in a computationally efficient way, demonstrating its efficacy and potential for practical applications.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "167",
        "title": "Foundation Model for Lossy Compression of Spatiotemporal Scientific Data",
        "author": [
            "Xiao Li",
            "Jaemoon Lee",
            "Anand Rangarajan",
            "Sanjay Ranka"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17184",
        "abstract": "We present a foundation model (FM) for lossy scientific data compression, combining a variational autoencoder (VAE) with a hyper-prior structure and a super-resolution (SR) module. The VAE framework uses hyper-priors to model latent space dependencies, enhancing compression efficiency. The SR module refines low-resolution representations into high-resolution outputs, improving reconstruction quality. By alternating between 2D and 3D convolutions, the model efficiently captures spatiotemporal correlations in scientific data while maintaining low computational cost. Experimental results demonstrate that the FM generalizes well to unseen domains and varying data shapes, achieving up to 4 times higher compression ratios than state-of-the-art methods after domain-specific fine-tuning. The SR module improves compression ratio by 30 percent compared to simple upsampling techniques. This approach significantly reduces storage and transmission costs for large-scale scientific simulations while preserving data integrity and fidelity.",
        "tags": [
            "3D",
            "Super Resolution",
            "VAE"
        ]
    },
    {
        "id": "168",
        "title": "Better Think with Tables: Leveraging Tables to Enhance Large Language Model Comprehension",
        "author": [
            "Jio Oh",
            "Geon Heo",
            "Seungjun Oh",
            "Jindong Wang",
            "Xing Xie",
            "Steven Euijong Whang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17189",
        "abstract": "Despite the recent advancement of Large Langauge Models (LLMs), they struggle with complex queries often involving multiple conditions, common in real-world scenarios. We propose Thinking with Tables, a technique that assists LLMs to leverage tables for intermediate thinking aligning with human cognitive behavior. By introducing a pre-instruction that triggers an LLM to organize information in tables, our approach achieves a 40.29\\% average relative performance increase, higher robustness, and show generalizability to different requests, conditions, or scenarios. We additionally show the influence of data structuredness for the model by comparing results from four distinct structuring levels that we introduce.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "169",
        "title": "Assessing UML Models by ChatGPT: Implications for Education",
        "author": [
            "Chong Wang",
            "Beian Wang",
            "Peng Liang",
            "Jie Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17200",
        "abstract": "In software engineering (SE) research and practice, UML is well known as an essential modeling methodology for requirements analysis and software modeling in both academia and industry. In particular, fundamental knowledge of UML modeling and practice in creating high-quality UML models are included in SE-relevant courses in the undergraduate programs of many universities. This leads to a time-consuming and labor-intensive task for educators to review and grade a large number of UML models created by the students. Recent advancements in generative AI techniques, such as ChatGPT, have paved new ways to automate many SE tasks. However, current research or tools seldom explore the capabilities of ChatGPT in evaluating the quality of UML models. This paper aims to investigate the feasibility and effectiveness of ChatGPT in assessing the quality of UML use case diagrams, class diagrams, and sequence diagrams. First, 11 evaluation criteria with grading details were proposed for these UML models. Next, a series of experiments were designed and conducted on 40 students' UML modeling reports to explore the performance of ChatGPT in evaluating and grading these UML diagrams. The research findings reveal that ChatGPT performed well in this assessing task because the scores that ChatGPT gives to the UML models are similar to the ones by human experts, and there are three evaluation discrepancies between ChatGPT and human experts, but varying in different evaluation criteria used in different types of UML models.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "170",
        "title": "Discriminative Image Generation with Diffusion Models for Zero-Shot Learning",
        "author": [
            "Dingjie Fu",
            "Wenjin Hou",
            "Shiming Chen",
            "Shuhuang Chen",
            "Xinge You",
            "Salman Khan",
            "Fahad Shahbaz Khan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17219",
        "abstract": "Generative Zero-Shot Learning (ZSL) methods synthesize class-related features based on predefined class semantic prototypes, showcasing superior performance. However, this feature generation paradigm falls short of providing interpretable insights. In addition, existing approaches rely on semantic prototypes annotated by human experts, which exhibit a significant limitation in their scalability to generalized scenes. To overcome these deficiencies, a natural solution is to generate images for unseen classes using text prompts. To this end, We present DIG-ZSL, a novel Discriminative Image Generation framework for Zero-Shot Learning. Specifically, to ensure the generation of discriminative images for training an effective ZSL classifier, we learn a discriminative class token (DCT) for each unseen class under the guidance of a pre-trained category discrimination model (CDM). Harnessing DCTs, we can generate diverse and high-quality images, which serve as informative unseen samples for ZSL tasks. In this paper, the extensive experiments and visualizations on four datasets show that our DIG-ZSL: (1) generates diverse and high-quality images, (2) outperforms previous state-of-the-art nonhuman-annotated semantic prototype-based methods by a large margin, and (3) achieves comparable or better performance than baselines that leverage human-annotated semantic prototypes. The codes will be made available upon acceptance of the paper.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "171",
        "title": "CharGen: High Accurate Character-Level Visual Text Generation Model with MultiModal Encoder",
        "author": [
            "Lichen Ma",
            "Tiezhu Yue",
            "Pei Fu",
            "Yujie Zhong",
            "Kai Zhou",
            "Xiaoming Wei",
            "Jie Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17225",
        "abstract": "Recently, significant advancements have been made in diffusion-based visual text generation models. Although the effectiveness of these methods in visual text rendering is rapidly improving, they still encounter challenges such as inaccurate characters and strokes when rendering complex visual text. In this paper, we propose CharGen, a highly accurate character-level visual text generation and editing model. Specifically, CharGen employs a character-level multimodal encoder that not only extracts character-level text embeddings but also encodes glyph images character by character. This enables it to capture fine-grained cross-modality features more effectively. Additionally, we introduce a new perceptual loss in CharGen to enhance character shape supervision and address the issue of inaccurate strokes in generated text. It is worth mentioning that CharGen can be integrated into existing diffusion models to generate visual text with high accuracy. CharGen significantly improves text rendering accuracy, outperforming recent methods in public benchmarks such as AnyText-benchmark and MARIO-Eval, with improvements of more than 8% and 6%, respectively. Notably, CharGen achieved a 5.5% increase in accuracy on Chinese test sets.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "172",
        "title": "OLiDM: Object-aware LiDAR Diffusion Models for Autonomous Driving",
        "author": [
            "Tianyi Yan",
            "Junbo Yin",
            "Xianpeng Lang",
            "Ruigang Yang",
            "Cheng-Zhong Xu",
            "Jianbing Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17226",
        "abstract": "To enhance autonomous driving safety in complex scenarios, various methods have been proposed to simulate LiDAR point cloud data. Nevertheless, these methods often face challenges in producing high-quality, diverse, and controllable foreground objects. To address the needs of object-aware tasks in 3D perception, we introduce OLiDM, a novel framework capable of generating high-fidelity LiDAR data at both the object and the scene levels. OLiDM consists of two pivotal components: the Object-Scene Progressive Generation (OPG) module and the Object Semantic Alignment (OSA) module. OPG adapts to user-specific prompts to generate desired foreground objects, which are subsequently employed as conditions in scene generation, ensuring controllable outputs at both the object and scene levels. This also facilitates the association of user-defined object-level annotations with the generated LiDAR scenes. Moreover, OSA aims to rectify the misalignment between foreground objects and background scenes, enhancing the overall quality of the generated objects. The broad effectiveness of OLiDM is demonstrated across various LiDAR generation tasks, as well as in 3D perception tasks. Specifically, on the KITTI-360 dataset, OLiDM surpasses prior state-of-the-art methods such as UltraLiDAR by 17.5 in FPD. Additionally, in sparse-to-dense LiDAR completion, OLiDM achieves a significant improvement over LiDARGen, with a 57.47\\% increase in semantic IoU. Moreover, OLiDM enhances the performance of mainstream 3D detectors by 2.4\\% in mAP and 1.9\\% in NDS, underscoring its potential in advancing object-aware 3D tasks. Code is available at: https://yanty123.github.io/OLiDM.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "173",
        "title": "Brain-to-Text Benchmark '24: Lessons Learned",
        "author": [
            "Francis R. Willett",
            "Jingyuan Li",
            "Trung Le",
            "Chaofei Fan",
            "Mingfei Chen",
            "Eli Shlizerman",
            "Yue Chen",
            "Xin Zheng",
            "Tatsuo S. Okubo",
            "Tyler Benster",
            "Hyun Dong Lee",
            "Maxwell Kounga",
            "E. Kelly Buchanan",
            "David Zoltowski",
            "Scott W. Linderman",
            "Jaimie M. Henderson"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17227",
        "abstract": "Speech brain-computer interfaces aim to decipher what a person is trying to say from neural activity alone, restoring communication to people with paralysis who have lost the ability to speak intelligibly. The Brain-to-Text Benchmark '24 and associated competition was created to foster the advancement of decoding algorithms that convert neural activity to text. Here, we summarize the lessons learned from the competition ending on June 1, 2024 (the top 4 entrants also presented their experiences in a recorded webinar). The largest improvements in accuracy were achieved using an ensembling approach, where the output of multiple independent decoders was merged using a fine-tuned large language model (an approach used by all 3 top entrants). Performance gains were also found by improving how the baseline recurrent neural network (RNN) model was trained, including by optimizing learning rate scheduling and by using a diphone training objective. Improving upon the model architecture itself proved more difficult, however, with attempts to use deep state space models or transformers not yet appearing to offer a benefit over the RNN baseline. The benchmark will remain open indefinitely to support further work towards increasing the accuracy of brain-to-text algorithms.",
        "tags": [
            "RNN",
            "State Space Models"
        ]
    },
    {
        "id": "174",
        "title": "Selective Kalman Filter: When and How to Fuse Multi-Sensor Information to Overcome Degeneracy in SLAM",
        "author": [
            "Jie Xu",
            "Guanyu Huang",
            "Wenlu Yu",
            "Xuanxuan Zhang",
            "Lijun Zhao",
            "Ruifeng Li",
            "Shenghai Yuan",
            "Lihua Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17235",
        "abstract": "Research trends in SLAM systems are now focusing more on multi-sensor fusion to handle challenging and degenerative environments. However, most existing multi-sensor fusion SLAM methods mainly use all of the data from a range of sensors, a strategy we refer to as the all-in method. This method, while merging the benefits of different sensors, also brings in their weaknesses, lowering the robustness and accuracy and leading to high computational demands. To address this, we propose a new fusion approach -- Selective Kalman Filter -- to carefully choose and fuse information from multiple sensors (using LiDAR and visual observations as examples in this paper). For deciding when to fuse data, we implement degeneracy detection in LiDAR SLAM, incorporating visual measurements only when LiDAR SLAM exhibits degeneracy. Regarding degeneracy detection, we propose an elegant yet straightforward approach to determine the degeneracy of LiDAR SLAM and to identify the specific degenerative direction. This method fully considers the coupled relationship between rotational and translational constraints. In terms of how to fuse data, we use visual measurements only to update the specific degenerative states. As a result, our proposed method improves upon the all-in method by greatly enhancing real-time performance due to less processing visual data, and it introduces fewer errors from visual measurements. Experiments demonstrate that our method for degeneracy detection and fusion, in addressing degeneracy issues, exhibits higher precision and robustness compared to other state-of-the-art methods, and offers enhanced real-time performance relative to the all-in method. The code is openly available.",
        "tags": [
            "Detection",
            "SLAM"
        ]
    },
    {
        "id": "175",
        "title": "On the Generalization Ability of Machine-Generated Text Detectors",
        "author": [
            "Yule Liu",
            "Zhiyuan Zhong",
            "Yifan Liao",
            "Zhen Sun",
            "Jingyi Zheng",
            "Jiaheng Wei",
            "Qingyuan Gong",
            "Fenghua Tong",
            "Yang Chen",
            "Yang Zhang",
            "Xinlei He"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17242",
        "abstract": "The rise of large language models (LLMs) has raised concerns about machine-generated text (MGT), including ethical and practical issues like plagiarism and misinformation. Building a robust and highly generalizable MGT detection system has become increasingly important. This work investigates the generalization capabilities of MGT detectors in three aspects: First, we construct MGTAcademic, a large-scale dataset focused on academic writing, featuring human-written texts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with an extensible code framework for efficient benchmarking. Second, we investigate the transferability of detectors across domains and LLMs, leveraging fine-grained datasets to reveal insights into domain transferring and implementing few-shot techniques to improve the performance by roughly 13.2%. Third, we introduce a novel attribution task where models must adapt to new classes over time without (or with very limited) access to prior training data and benchmark detectors. We implement several adapting techniques to improve the performance by roughly 10% and highlight the inherent complexity of the task. Our findings provide insights into the generalization ability of MGT detectors across diverse scenarios and lay the foundation for building robust, adaptive detection systems.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "176",
        "title": "EM-MIAs: Enhancing Membership Inference Attacks in Large Language Models through Ensemble Modeling",
        "author": [
            "Zichen Song",
            "Sitan Huang",
            "Zhongfeng Kang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17249",
        "abstract": "With the widespread application of large language models (LLM), concerns about the privacy leakage of model training data have increasingly become a focus. Membership Inference Attacks (MIAs) have emerged as a critical tool for evaluating the privacy risks associated with these models. Although existing attack methods, such as LOSS, Reference-based, min-k, and zlib, perform well in certain scenarios, their effectiveness on large pre-trained language models often approaches random guessing, particularly in the context of large-scale datasets and single-epoch training. To address this issue, this paper proposes a novel ensemble attack method that integrates several existing MIAs techniques (LOSS, Reference-based, min-k, zlib) into an XGBoost-based model to enhance overall attack performance (EM-MIAs). Experimental results demonstrate that the ensemble model significantly improves both AUC-ROC and accuracy compared to individual attack methods across various large language models and datasets. This indicates that by combining the strengths of different methods, we can more effectively identify members of the model's training data, thereby providing a more robust tool for evaluating the privacy risks of LLM. This study offers new directions for further research in the field of LLM privacy protection and underscores the necessity of developing more powerful privacy auditing methods.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "177",
        "title": "SyNeg: LLM-Driven Synthetic Hard-Negatives for Dense Retrieval",
        "author": [
            "Xiaopeng Li",
            "Xiangyang Li",
            "Hao Zhang",
            "Zhaocheng Du",
            "Pengyue Jia",
            "Yichao Wang",
            "Xiangyu Zhao",
            "Huifeng Guo",
            "Ruiming Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17250",
        "abstract": "The performance of Dense retrieval (DR) is significantly influenced by the quality of negative sampling. Traditional DR methods primarily depend on naive negative sampling techniques or on mining hard negatives through external retriever and meticulously crafted strategies. However, naive negative sampling often fails to adequately capture the accurate boundaries between positive and negative samples, whereas existing hard negative sampling methods are prone to false negatives, resulting in performance degradation and training instability. Recent advancements in large language models (LLMs) offer an innovative solution to these challenges by generating contextually rich and diverse negative samples. In this work, we present a framework that harnesses LLMs to synthesize high-quality hard negative samples. We first devise a \\textit{multi-attribute self-reflection prompting strategy} to direct LLMs in hard negative sample generation. Then, we implement a \\textit{hybrid sampling strategy} that integrates these synthetic negatives with traditionally retrieved negatives, thereby stabilizing the training process and improving retrieval performance. Extensive experiments on five benchmark datasets demonstrate the efficacy of our approach, and code is also publicly available.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "178",
        "title": "A Coalition Game for On-demand Multi-modal 3D Automated Delivery System",
        "author": [
            "Farzan Moosavi",
            "Bilal Farooq"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17252",
        "abstract": "We introduce a multi-modal autonomous delivery optimization framework as a coalition game for a fleet of UAVs and ADRs operating in two overlaying networks to address last-mile delivery in urban environments, including high-density areas, road-based routing, and real-world operational challenges. The problem is defined as multiple depot pickup and delivery with time windows constrained over operational restrictions, such as vehicle battery limitation, precedence time window, and building obstruction. Subsequently, the coalition game theory is applied to investigate cooperation structures among the modes to capture how strategic collaboration among vehicles can improve overall routing efficiency. To do so, a generalized reinforcement learning model is designed to evaluate the cost-sharing and allocation to different coalitions for which sub-additive property and non-empty core exist. Our methodology leverages an end-to-end deep multi-agent policy gradient method augmented by a novel spatio-temporal adjacency neighbourhood graph attention network and transformer architecture using a heterogeneous edge-enhanced attention model. Conducting several numerical experiments on last-mile delivery applications, the result from the case study in the city of Mississauga shows that despite the incorporation of an extensive network in the graph for two modes and a complex training structure, the model addresses realistic operational constraints and achieves high-quality solutions compared with the existing transformer-based and heuristics methods and can perform well on non-homogeneous data distribution, generalizes well on the different scale and configuration, and demonstrate a robust performance under stochastic scenarios subject to wind speed and direction.",
        "tags": [
            "3D",
            "Transformer"
        ]
    },
    {
        "id": "179",
        "title": "Enhancing Multi-Text Long Video Generation Consistency without Tuning: Time-Frequency Analysis, Prompt Alignment, and Theory",
        "author": [
            "Xingyao Li",
            "Fengzhuo Zhang",
            "Jiachun Pan",
            "Yunlong Hou",
            "Vincent Y. F. Tan",
            "Zhuoran Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17254",
        "abstract": "Despite the considerable progress achieved in the long video generation problem, there is still significant room to improve the consistency of the videos, particularly in terms of smoothness and transitions between scenes. We address these issues to enhance the consistency and coherence of videos generated with either single or multiple prompts. We propose the Time-frequency based temporal Attention Reweighting Algorithm (TiARA), which meticulously edits the attention score matrix based on the Discrete Short-Time Fourier Transform. Our method is supported by a theoretical guarantee, the first-of-its-kind for frequency-based methods in diffusion models. For videos generated by multiple prompts, we further investigate key factors affecting prompt interpolation quality and propose PromptBlend, an advanced prompt interpolation pipeline. The efficacy of our proposed method is validated via extensive experimental results, exhibiting consistent and impressive improvements over baseline methods. The code will be released upon acceptance.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "180",
        "title": "Unlocking Cross-Lingual Sentiment Analysis through Emoji Interpretation: A Multimodal Generative AI Approach",
        "author": [
            "Rafid Ishrak Jahan",
            "Heng Fan",
            "Haihua Chen",
            "Yunhe Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17255",
        "abstract": "Emojis have become ubiquitous in online communication, serving as a universal medium to convey emotions and decorative elements. Their widespread use transcends language and cultural barriers, enhancing understanding and fostering more inclusive interactions. While existing work gained valuable insight into emojis understanding, exploring emojis' capability to serve as a universal sentiment indicator leveraging large language models (LLMs) has not been thoroughly examined. Our study aims to investigate the capacity of emojis to serve as reliable sentiment markers through LLMs across languages and cultures. We leveraged the multimodal capabilities of ChatGPT to explore the sentiments of various representations of emojis and evaluated how well emoji-conveyed sentiment aligned with text sentiment on a multi-lingual dataset collected from 32 countries. Our analysis reveals that the accuracy of LLM-based emoji-conveyed sentiment is 81.43%, underscoring emojis' significant potential to serve as a universal sentiment marker. We also found a consistent trend that the accuracy of sentiment conveyed by emojis increased as the number of emojis grew in text. The results reinforce the potential of emojis to serve as global sentiment indicators, offering insight into fields such as cross-lingual and cross-cultural sentiment analysis on social media platforms. Code: https://github.com/ResponsibleAILab/emoji-universal-sentiment.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "181",
        "title": "LegalAgentBench: Evaluating LLM Agents in Legal Domain",
        "author": [
            "Haitao Li",
            "Junjie Chen",
            "Jingli Yang",
            "Qingyao Ai",
            "Wei Jia",
            "Youfeng Liu",
            "Kai Lin",
            "Yueyue Wu",
            "Guozhi Yuan",
            "Yiran Hu",
            "Wuyue Wang",
            "Yiqun Liu",
            "Minlie Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17259",
        "abstract": "With the increasing intelligence and autonomy of LLM agents, their potential applications in the legal domain are becoming increasingly apparent. However, existing general-domain benchmarks cannot fully capture the complexity and subtle nuances of real-world judicial cognition and decision-making. Therefore, we propose LegalAgentBench, a comprehensive benchmark specifically designed to evaluate LLM Agents in the Chinese legal domain. LegalAgentBench includes 17 corpora from real-world legal scenarios and provides 37 tools for interacting with external knowledge. We designed a scalable task construction framework and carefully annotated 300 tasks. These tasks span various types, including multi-hop reasoning and writing, and range across different difficulty levels, effectively reflecting the complexity of real-world legal scenarios. Moreover, beyond evaluating final success, LegalAgentBench incorporates keyword analysis during intermediate processes to calculate progress rates, enabling more fine-grained evaluation. We evaluated eight popular LLMs, highlighting the strengths, limitations, and potential areas for improvement of existing models and methods. LegalAgentBench sets a new benchmark for the practical application of LLMs in the legal domain, with its code and data available at \\url{https://github.com/CSHaitao/LegalAgentBench}.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "182",
        "title": "LMD-PGN: Cross-Modal Knowledge Distillation from First-Person-View Images to Third-Person-View BEV Maps for Universal Point Goal Navigation",
        "author": [
            "Riku Uemura",
            "Kanji Tanaka",
            "Kenta Tsukahara",
            "Daiki Iwata"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17282",
        "abstract": "Point goal navigation (PGN) is a mapless navigation approach that trains robots to visually navigate to goal points without relying on pre-built maps. Despite significant progress in handling complex environments using deep reinforcement learning, current PGN methods are designed for single-robot systems, limiting their generalizability to multi-robot scenarios with diverse platforms. This paper addresses this limitation by proposing a knowledge transfer framework for PGN, allowing a teacher robot to transfer its learned navigation model to student robots, including those with unknown or black-box platforms. We introduce a novel knowledge distillation (KD) framework that transfers first-person-view (FPV) representations (view images, turning/forward actions) to universally applicable third-person-view (TPV) representations (local maps, subgoals). The state is redefined as reconstructed local maps using SLAM, while actions are mapped to subgoals on a predefined grid. To enhance training efficiency, we propose a sampling-efficient KD approach that aligns training episodes via a noise-robust local map descriptor (LMD). Although validated on 2D wheeled robots, this method can be extended to 3D action spaces, such as drones. Experiments conducted in Habitat-Sim demonstrate the feasibility of the proposed framework, requiring minimal implementation effort. This study highlights the potential for scalable and cross-platform PGN solutions, expanding the applicability of embodied AI systems in multi-robot scenarios.",
        "tags": [
            "3D",
            "Robot",
            "SLAM"
        ]
    },
    {
        "id": "183",
        "title": "LLM4AD: A Platform for Algorithm Design with Large Language Model",
        "author": [
            "Fei Liu",
            "Rui Zhang",
            "Zhuoliang Xie",
            "Rui Sun",
            "Kai Li",
            "Xi Lin",
            "Zhenkun Wang",
            "Zhichao Lu",
            "Qingfu Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17287",
        "abstract": "We introduce LLM4AD, a unified Python platform for algorithm design (AD) with large language models (LLMs). LLM4AD is a generic framework with modularized blocks for search methods, algorithm design tasks, and LLM interface. The platform integrates numerous key methods and supports a wide range of algorithm design tasks across various domains including optimization, machine learning, and scientific discovery. We have also designed a unified evaluation sandbox to ensure a secure and robust assessment of algorithms. Additionally, we have compiled a comprehensive suite of support resources, including tutorials, examples, a user manual, online resources, and a dedicated graphical user interface (GUI) to enhance the usage of LLM4AD. We believe this platform will serve as a valuable tool for fostering future development in the merging research direction of LLM-assisted algorithm design.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "184",
        "title": "Multi-Modal Grounded Planning and Efficient Replanning For Learning Embodied Agents with A Few Examples",
        "author": [
            "Taewoong Kim",
            "Byeonghwi Kim",
            "Jonghyun Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17288",
        "abstract": "Learning a perception and reasoning module for robotic assistants to plan steps to perform complex tasks based on natural language instructions often requires large free-form language annotations, especially for short high-level instructions. To reduce the cost of annotation, large language models (LLMs) are used as a planner with few data. However, when elaborating the steps, even the state-of-the-art planner that uses LLMs mostly relies on linguistic common sense, often neglecting the status of the environment at command reception, resulting in inappropriate plans. To generate plans grounded in the environment, we propose FLARE (Few-shot Language with environmental Adaptive Replanning Embodied agent), which improves task planning using both language command and environmental perception. As language instructions often contain ambiguities or incorrect expressions, we additionally propose to correct the mistakes using visual cues from the agent. The proposed scheme allows us to use a few language pairs thanks to the visual cues and outperforms state-of-the-art approaches. Our code is available at https://github.com/snumprlab/flare.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "185",
        "title": "Free-viewpoint Human Animation with Pose-correlated Reference Selection",
        "author": [
            "Fa-Ting Hong",
            "Zhan Xu",
            "Haiyang Liu",
            "Qinjie Lin",
            "Luchuan Song",
            "Zhixin Shu",
            "Yang Zhou",
            "Duygu Ceylan",
            "Dan Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17290",
        "abstract": "Diffusion-based human animation aims to animate a human character based on a source human image as well as driving signals such as a sequence of poses. Leveraging the generative capacity of diffusion model, existing approaches are able to generate high-fidelity poses, but struggle with significant viewpoint changes, especially in zoom-in/zoom-out scenarios where camera-character distance varies. This limits the applications such as cinematic shot type plan or camera control. We propose a pose-correlated reference selection diffusion network, supporting substantial viewpoint variations in human animation. Our key idea is to enable the network to utilize multiple reference images as input, since significant viewpoint changes often lead to missing appearance details on the human body. To eliminate the computational cost, we first introduce a novel pose correlation module to compute similarities between non-aligned target and source poses, and then propose an adaptive reference selection strategy, utilizing the attention map to identify key regions for animation generation. To train our model, we curated a large dataset from public TED talks featuring varied shots of the same character, helping the model learn synthesis for different perspectives. Our experimental results show that with the same number of reference images, our model performs favorably compared to the current SOTA methods under large viewpoint change. We further show that the adaptive reference selection is able to choose the most relevant reference regions to generate humans under free viewpoints.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "186",
        "title": "AV-EmoDialog: Chat with Audio-Visual Users Leveraging Emotional Cues",
        "author": [
            "Se Jin Park",
            "Yeonju Kim",
            "Hyeongseop Rha",
            "Bella Godiva",
            "Yong Man Ro"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17292",
        "abstract": "In human communication, both verbal and non-verbal cues play a crucial role in conveying emotions, intentions, and meaning beyond words alone. These non-linguistic information, such as facial expressions, eye contact, voice tone, and pitch, are fundamental elements of effective interactions, enriching conversations by adding emotional and contextual depth. Recognizing the importance of non-linguistic content in communication, we present AV-EmoDialog, a dialogue system designed to exploit verbal and non-verbal information from users' audio-visual inputs to generate more responsive and empathetic interactions. AV-EmoDialog systematically exploits the emotional cues in audio-visual dialogues; extracting speech content and emotional tones from speech, analyzing fine-grained facial expressions from visuals, and integrating these cues to generate emotionally aware responses in an end-to-end manner. Through extensive experiments, we validate that the proposed AV-EmoDialog outperforms existing multimodal LLMs in generating not only emotionally appropriate but also contextually appropriate responses.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "187",
        "title": "Prompting in the Wild: An Empirical Study of Prompt Evolution in Software Repositories",
        "author": [
            "Mahan Tafreshipour",
            "Aaron Imani",
            "Eric Huang",
            "Eduardo Almeida",
            "Thomas Zimmermann",
            "Iftekhar Ahmed"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17298",
        "abstract": "The adoption of Large Language Models (LLMs) is reshaping software development as developers integrate these LLMs into their applications. In such applications, prompts serve as the primary means of interacting with LLMs. Despite the widespread use of LLM-integrated applications, there is limited understanding of how developers manage and evolve prompts. This study presents the first empirical analysis of prompt evolution in LLM-integrated software development. We analyzed 1,262 prompt changes across 243 GitHub repositories to investigate the patterns and frequencies of prompt changes, their relationship with code changes, documentation practices, and their impact on system behavior. Our findings show that developers primarily evolve prompts through additions and modifications, with most changes occurring during feature development. We identified key challenges in prompt engineering: only 21.9\\% of prompt changes are documented in commit messages, changes can introduce logical inconsistencies, and misalignment often occurs between prompt changes and LLM responses. These insights emphasize the need for specialized testing frameworks, automated validation tools, and improved documentation practices to enhance the reliability of LLM-integrated applications.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "188",
        "title": "On the Feasibility of Vision-Language Models for Time-Series Classification",
        "author": [
            "Vinay Prithyani",
            "Mohsin Mohammed",
            "Richa Gadgil",
            "Ricardo Buitrago",
            "Vinija Jain",
            "Aman Chadha"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17304",
        "abstract": "We build upon time-series classification by leveraging the capabilities of Vision Language Models (VLMs). We find that VLMs produce competitive results after two or less epochs of fine-tuning. We develop a novel approach that incorporates graphical data representations as images in conjunction with numerical data. This approach is rooted in the hypothesis that graphical representations can provide additional contextual information that numerical data alone may not capture. Additionally, providing a graphical representation can circumvent issues such as limited context length faced by LLMs. To further advance this work, we implemented a scalable end-to-end pipeline for training on different scenarios, allowing us to isolate the most effective strategies for transferring learning capabilities from LLMs to Time Series Classification (TSC) tasks. Our approach works with univariate and multivariate time-series data. In addition, we conduct extensive and practical experiments to show how this approach works for time-series classification and generative labels.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "189",
        "title": "CodeV: Issue Resolving with Visual Data",
        "author": [
            "Linhao Zhang",
            "Daoguang Zan",
            "Quanshun Yang",
            "Zhirong Huang",
            "Dong Chen",
            "Bo Shen",
            "Tianyu Liu",
            "Yongshun Gong",
            "Pengjie Huang",
            "Xudong Lu",
            "Guangtai Liang",
            "Lizhen Cui",
            "Qianxiang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17315",
        "abstract": "Large Language Models (LLMs) have advanced rapidly in recent years, with their applications in software engineering expanding to more complex repository-level tasks. GitHub issue resolving is a key challenge among these tasks. While recent approaches have made progress on this task, they focus on textual data within issues, neglecting visual data. However, this visual data is crucial for resolving issues as it conveys additional knowledge that text alone cannot. We propose CodeV, the first approach to leveraging visual data to enhance the issue-resolving capabilities of LLMs. CodeV resolves each issue by following a two-phase process: data processing and patch generation. To evaluate CodeV, we construct a benchmark for visual issue resolving, namely Visual SWE-bench. Through extensive experiments, we demonstrate the effectiveness of CodeV, as well as provide valuable insights into leveraging visual data to resolve GitHub issues.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "190",
        "title": "Fast Gradient Computation for RoPE Attention in Almost Linear Time",
        "author": [
            "Yifang Chen",
            "Jiayan Huo",
            "Xiaoyu Li",
            "Yingyu Liang",
            "Zhenmei Shi",
            "Zhao Song"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17316",
        "abstract": "The Rotary Position Embedding (RoPE) mechanism has become a powerful enhancement to the Transformer architecture, which enables models to capture token relationships when encoding positional information. However, the RoPE mechanisms make the computations of attention mechanisms more complicated, which makes efficient algorithms challenging. Earlier research introduced almost linear time, i.e., $n^{1+o(1)}$ where $n$ is the number of input tokens, algorithms for the forward computation under specific parameter settings. However, achieving a subquadratic time algorithm for other parameter regimes remains impossible unless the widely accepted Strong Exponential Time Hypothesis (SETH) is disproven. In this work, we develop the first almost linear time algorithm for backward computations in the RoPE-based attention under bounded entries. Our approach builds on recent advancements in fast RoPE attention computations, utilizing a novel combination of the polynomial method and the Fast Fourier Transform. Furthermore, we show that with lower bounds derived from the SETH, the bounded entry condition is necessary for subquadratic performance.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "191",
        "title": "Assessing Human Editing Effort on LLM-Generated Texts via Compression-Based Edit Distance",
        "author": [
            "Nicolas Devatine",
            "Louis Abraham"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17321",
        "abstract": "Assessing the extent of human edits on texts generated by Large Language Models (LLMs) is crucial to understanding the human-AI interactions and improving the quality of automated text generation systems. Existing edit distance metrics, such as Levenshtein, BLEU, ROUGE, and TER, often fail to accurately measure the effort required for post-editing, especially when edits involve substantial modifications, such as block operations. In this paper, we introduce a novel compression-based edit distance metric grounded in the Lempel-Ziv-77 algorithm, designed to quantify the amount of post-editing applied to LLM-generated texts. Our method leverages the properties of text compression to measure the informational difference between the original and edited texts. Through experiments on real-world human edits datasets, we demonstrate that our proposed metric is highly correlated with actual edit time and effort. We also show that LLMs exhibit an implicit understanding of editing speed, that aligns well with our metric. Furthermore, we compare our metric with existing ones, highlighting its advantages in capturing complex edits with linear computational efficiency. Our code and data are available at: https://github.com/NDV-tiime/CompressionDistance",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "192",
        "title": "xPatch: Dual-Stream Time Series Forecasting with Exponential Seasonal-Trend Decomposition",
        "author": [
            "Artyom Stitsyuk",
            "Jaesik Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17323",
        "abstract": "In recent years, the application of transformer-based models in time-series forecasting has received significant attention. While often demonstrating promising results, the transformer architecture encounters challenges in fully exploiting the temporal relations within time series data due to its attention mechanism. In this work, we design eXponential Patch (xPatch for short), a novel dual-stream architecture that utilizes exponential decomposition. Inspired by the classical exponential smoothing approaches, xPatch introduces the innovative seasonal-trend exponential decomposition module. Additionally, we propose a dual-flow architecture that consists of an MLP-based linear stream and a CNN-based non-linear stream. This model investigates the benefits of employing patching and channel-independence techniques within a non-transformer model. Finally, we develop a robust arctangent loss function and a sigmoid learning rate adjustment scheme, which prevent overfitting and boost forecasting performance. The code is available at the following repository: https://github.com/stitsyuk/xPatch.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "193",
        "title": "A Dual-Perspective Metaphor Detection Framework Using Large Language Models",
        "author": [
            "Yujie Lin",
            "Jingyao Liu",
            "Yan Gao",
            "Ante Wang",
            "Jinsong Su"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17332",
        "abstract": "Metaphor detection, a critical task in natural language processing, involves identifying whether a particular word in a sentence is used metaphorically. Traditional approaches often rely on supervised learning models that implicitly encode semantic relationships based on metaphor theories. However, these methods often suffer from a lack of transparency in their decision-making processes, which undermines the reliability of their predictions. Recent research indicates that LLMs (large language models) exhibit significant potential in metaphor detection. Nevertheless, their reasoning capabilities are constrained by predefined knowledge graphs. To overcome these limitations, we propose DMD, a novel dual-perspective framework that harnesses both implicit and explicit applications of metaphor theories to guide LLMs in metaphor detection and adopts a self-judgment mechanism to validate the responses from the aforementioned forms of guidance. In comparison to previous methods, our framework offers more transparent reasoning processes and delivers more reliable predictions. Experimental results prove the effectiveness of DMD, demonstrating state-of-the-art performance across widely-used datasets.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "194",
        "title": "Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition",
        "author": [
            "Jaeheun Jung",
            "Jaehyuk Lee",
            "Chang-Hae Jung",
            "Hanyoung Kim",
            "Bosung Jung",
            "Donghun Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17333",
        "abstract": "Earthquakes are rare. Hence there is a fundamental call for reliable methods to generate realistic ground motion data for data-driven approaches in seismology. Recent GAN-based methods fall short of the call, as the methods either require special information such as geological traits or generate subpar waveforms that fail to satisfy seismological constraints such as phase arrival times. We propose a specialized Latent Diffusion Model (LDM) that reliably generates realistic waveforms after learning from real earthquake data with minimal conditions: location and magnitude. We also design a domain-specific training method that exploits the traits of earthquake dataset: multiple observed waveforms time-aligned and paired to each earthquake source that are tagged with seismological metadata comprised of earthquake magnitude, depth of focus, and the locations of epicenter and seismometers. We construct the time-aligned earthquake dataset using Southern California Earthquake Data Center (SCEDC) API, and train our model with the dataset and our proposed training method for performance evaluation. Our model surpasses all comparable data-driven methods in various test criteria not only from waveform generation domain but also from seismology such as phase arrival time, GMPE analysis, and spectrum analysis. Our result opens new future research directions for deep learning applications in seismology.",
        "tags": [
            "Diffusion",
            "GAN"
        ]
    },
    {
        "id": "195",
        "title": "End-to-end Generative Spatial-Temporal Ultrasonic Odometry and Mapping Framework",
        "author": [
            "Fuhua Jia",
            "Xiaoying Yang",
            "Mengshen Yang",
            "Yang Li",
            "Hang Xu",
            "Adam Rushworth",
            "Salman Ijaz",
            "Heng Yu",
            "Tianxiang Cui"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17343",
        "abstract": "Performing simultaneous localization and mapping (SLAM) in low-visibility conditions, such as environments filled with smoke, dust and transparent objets, has long been a challenging task. Sensors like cameras and Light Detection and Ranging (LiDAR) are significantly limited under these conditions, whereas ultrasonic sensors offer a more robust alternative. However, the low angular resolution, slow update frequency, and limited detection accuracy of ultrasonic sensors present barriers for SLAM. In this work, we propose a novel end-to-end generative ultrasonic SLAM framework. This framework employs a sensor array with overlapping fields of view, leveraging the inherently low angular resolution of ultrasonic sensors to implicitly encode spatial features in conjunction with the robot's motion. Consecutive time frame data is processed through a sliding window mechanism to capture temporal features. The spatiotemporally encoded sensor data is passed through multiple modules to generate dense scan point clouds and robot pose transformations for map construction and odometry. The main contributions of this work include a novel ultrasonic sensor array that spatiotemporally encodes the surrounding environment, and an end-to-end generative SLAM framework that overcomes the inherent defects of ultrasonic sensors. Several real-world experiments demonstrate the feasibility and robustness of the proposed framework.",
        "tags": [
            "Detection",
            "Robot",
            "SLAM"
        ]
    },
    {
        "id": "196",
        "title": "A Room to Roam: Reset Prediction Based on Physical Object Placement for Redirected Walking",
        "author": [
            "Sulim Chun",
            "Ho Jung Lee",
            "In-Kwon Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17375",
        "abstract": "In Redirected Walking (RDW), resets are an overt method that explicitly interrupts users, and they should be avoided to provide a quality user experience. The number of resets depends on the configuration of the physical environment; thus, inappropriate object placement can lead to frequent resets, causing motion sickness and degrading presence. However, estimating the number of resets based on the physical layout is challenging. It is difficult to measure reset frequency with real users repeatedly testing different layouts, and virtual simulations offer limited real-time verification. As a result, while rearranging objects can reduce resets, users have not been able to fully take advantage of this opportunity, highlighting the need for rapid assessment of object placement. To address this, in Study 1, we collected simulation data and analyzed the average number of resets for various object placements. In study 2, we developed a system that allows users to evaluate reset frequency using a real-time placement interface powered by the first learning-based reset prediction model. Our model predicts resets from top-down views of the physical space, leveraging a Vision Transformer architecture. The model achieved a root mean square error (RMSE) of $23.88$. We visualized the model's attention scores using heatmaps to analyze the regions of focus during prediction. Through the interface, users can reorganize furniture while instantly observing the change in the predicted number of resets, thus improving their interior for a better RDW experience with fewer resets.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "197",
        "title": "Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling",
        "author": [
            "Hao Gui",
            "Lin Hu",
            "Rui Chen",
            "Mingxiao Huang",
            "Yuxin Yin",
            "Jin Yang",
            "Yong Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17378",
        "abstract": "3D Gaussian Splatting (3DGS) is increasingly attracting attention in both academia and industry owing to its superior visual quality and rendering speed. However, training a 3DGS model remains a time-intensive task, especially in load imbalance scenarios where workload diversity among pixels and Gaussian spheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS, a Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS training process, perfectly solving load-imbalance issues. First, we innovatively introduce the inter-block dynamic workload distribution technique to map workloads to Streaming Multiprocessor(SM) resources within a single GPU dynamically, which constitutes the foundation of load balancing. Second, we are the first to propose the Gaussian-wise parallel rendering technique to significantly reduce workload divergence inside a warp, which serves as a critical component in addressing load imbalance. Based on the above two methods, we further creatively put forward the fine-grained combined load balancing technique to uniformly distribute workload across all SMs, which boosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we present a self-adaptive render kernel selection strategy during the 3DGS training process based on different load-balance situations, which effectively improves training efficiency.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "198",
        "title": "Interweaving Memories of a Siamese Large Language Model",
        "author": [
            "Xin Song",
            "Zhikai Xue",
            "Guoxiu He",
            "Jiawei Liu",
            "Wei Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17383",
        "abstract": "Parameter-efficient fine-tuning (PEFT) methods optimize large language models (LLMs) by modifying or introducing a small number of parameters to enhance alignment with downstream tasks. However, they can result in catastrophic forgetting, where LLMs prioritize new knowledge at the expense of comprehensive world knowledge. A promising approach to mitigate this issue is to recall prior memories based on the original knowledge. To this end, we propose a model-agnostic PEFT framework, IMSM, which Interweaves Memories of a Siamese Large Language Model. Specifically, our siamese LLM is equipped with an existing PEFT method. Given an incoming query, it generates two distinct memories based on the pre-trained and fine-tuned parameters. IMSM then incorporates an interweaving mechanism that regulates the contributions of both original and enhanced memories when generating the next token. This framework is theoretically applicable to all open-source LLMs and existing PEFT methods. We conduct extensive experiments across various benchmark datasets, evaluating the performance of popular open-source LLMs using the proposed IMSM, in comparison to both classical and leading PEFT methods. Our findings indicate that IMSM maintains comparable time and space efficiency to backbone PEFT methods while significantly improving performance and effectively mitigating catastrophic forgetting.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "199",
        "title": "Singular Value Scaling: Efficient Generative Model Compression via Pruned Weights Refinement",
        "author": [
            "Hyeonjin Kim",
            "Jaejun Yoo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17387",
        "abstract": "While pruning methods effectively maintain model performance without extra training costs, they often focus solely on preserving crucial connections, overlooking the impact of pruned weights on subsequent fine-tuning or distillation, leading to inefficiencies. Moreover, most compression techniques for generative models have been developed primarily for GANs, tailored to specific architectures like StyleGAN, and research into compressing Diffusion models has just begun. Even more, these methods are often applicable only to GANs or Diffusion models, highlighting the need for approaches that work across both model types. In this paper, we introduce Singular Value Scaling (SVS), a versatile technique for refining pruned weights, applicable to both model types. Our analysis reveals that pruned weights often exhibit dominant singular vectors, hindering fine-tuning efficiency and leading to suboptimal performance compared to random initialization. Our method enhances weight initialization by minimizing the disparities between singular values of pruned weights, thereby improving the fine-tuning process. This approach not only guides the compressed model toward superior solutions but also significantly speeds up fine-tuning. Extensive experiments on StyleGAN2, StyleGAN3 and DDPM demonstrate that SVS improves compression performance across model types without additional training costs. Our code is available at: https://github.com/LAIT-CVLab/Singular_Value_Scaling.",
        "tags": [
            "DDPM",
            "Diffusion",
            "StyleGAN"
        ]
    },
    {
        "id": "200",
        "title": "WarriorCoder: Learning from Expert Battles to Augment Code Large Language Models",
        "author": [
            "Huawen Feng",
            "Pu Zhao",
            "Qingfeng Sun",
            "Can Xu",
            "Fangkai Yang",
            "Lu Wang",
            "Qianli Ma",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17395",
        "abstract": "Despite recent progress achieved by code large language models (LLMs), their remarkable abilities are largely dependent on fine-tuning on the high-quality data, posing challenges for data collection and annotation. To address this, current methods often design various data flywheels to gather complex code instructions, enabling models to handle more intricate tasks. However, these approaches typically rely on off-the-shelf datasets and data augmentation from the limited pool of proprietary LLMs (e.g., Claude, GPT4, and so on), which limits the diversity of the constructed data and makes it prone to systemic biases. In this paper, we propose WarriorCoder which learns from expert battles to address these limitations. Specifically, we create an arena for current expert code LLMs, where each model challenges and responds to others' challenges, with evaluations conducted by uninvolved judge models. This competitive framework generates novel training data constructed from scratch, harnessing the strengths of all participants. Experimental results demonstrate that WarriorCoder achieves competitive performance compared to previous methods, even without relying on proprietary LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "201",
        "title": "Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted Reasoning via Iterative Preference Learning",
        "author": [
            "Huchen Jiang",
            "Yangyang Ma",
            "Chaofan Ding",
            "Kexin Luan",
            "Xinhan Di"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17397",
        "abstract": "With current state-of-the-art approaches aimed at enhancing the reasoning capabilities of Large Language Models(LLMs) through iterative preference learning inspired by AlphaZero, we propose to further enhance the step-wise reasoning capabilities through intrinsic self-correction to some extent. Our work leverages step-wise preference learning to enhance self-verification via reinforcement learning. We initially conduct our work through a two-stage training procedure. At the first stage, the self-correction reasoning ability of an LLM is enhanced through its own predictions, relying entirely on self-generated data within the intrinsic self-correction to some extent. At the second stage, the baseline step-wise preference learning is leveraged via the application of the enhanced self-correct policy achieved at the first stage. In the evaluation of arithmetic reasoning tasks, our approach outperforms OpenMath2-Llama3.1-8B, dart-math-mistral-7b-uniform on MATH with increases in accuracy to 71.34%(+4.18%) and 48.06%(+4.94%) and LLama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.1 on GSM8K with increases in accuracy to 86.76%(+2.00%) and 38.06%(+2.28%).",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "202",
        "title": "Multimodal Preference Data Synthetic Alignment with Reward Model",
        "author": [
            "Robert Wijaya",
            "Ngoc-Bao Nguyen",
            "Ngai-Man Cheung"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17417",
        "abstract": "Multimodal large language models (MLLMs) have significantly advanced tasks like caption generation and visual question answering by integrating visual and textual data. However, they sometimes produce misleading or hallucinate content due to discrepancies between their pre-training data and real user prompts. Existing approaches using Direct Preference Optimization (DPO) in vision-language tasks often rely on strong models like GPT-4 or CLIP to determine positive and negative responses. Here, we propose a new framework in generating synthetic data using a reward model as a proxy of human preference for effective multimodal alignment with DPO training. The resulting DPO dataset ranges from 2K to 9K image-text pairs, was evaluated on LLaVA-v1.5-7B, where our approach demonstrated substantial improvements in both the trustworthiness and reasoning capabilities of the base model across multiple hallucination and vision-language benchmark. The experiment results indicate that integrating selected synthetic data, such as from generative and rewards models can effectively reduce reliance on human-annotated data while enhancing MLLMs' alignment capability, offering a scalable solution for safer deployment.",
        "tags": [
            "CLIP",
            "GPT",
            "LLaVA",
            "Large Language Models"
        ]
    },
    {
        "id": "203",
        "title": "Condor: A Code Discriminator Integrating General Semantics with Code Details",
        "author": [
            "Qingyuan Liang",
            "Zhao Zhang",
            "Chen Liu",
            "Zeyu Sun",
            "Wenjie Zhang",
            "Yizhou Chen",
            "Zixiao Zhao",
            "Qi Luo",
            "Wentao Wang",
            "Yanjie Jiang",
            "Yingfei Xiong",
            "Lu Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17429",
        "abstract": "LLMs demonstrate significant potential across various software engineering tasks. However, they still face challenges in generating correct code on the first attempt when addressing complex requirements. Introducing a discriminator to select reliable outputs from multiple generated results is an effective way to enhance their reliability and stability. Currently, these discriminators fall into two categories: execution-based discriminators and non-execution-based discriminators. Execution-based discriminators face flexibility challenges due to difficulties in obtaining test cases and security concerns, while non-execution-based discriminators, although more flexible, struggle to capture subtle differences in code details. To maintain flexibility while improving the model's ability to capture fine-grained code details, this paper proposes Condor. We first design contrastive learning to optimize the code representations of the base model, enabling it to reflect differences in code details. Then, we leverage intermediate data from the code modification process to further enrich the discriminator's training data, enhancing its ability to discern code details. Experimental results indicate that on the subtle code difference dataset (i.e., CodeNanoFix), Condor significantly outperforms other discriminators in discriminative performance: Condor (1.3B) improves the discriminative F1 score of DeepSeek-Coder (1.3B) from 67% to 73%. In discriminating LLM-generated outputs, Condor (1.3B) and Condor (110M) raise the Pass@1 score of Meta-Llama-3.1-Instruct (70B) on the CodeNanoFix dataset from 52.64% to 62.63% and 59.64%, respectively. Moreover, Condor demonstrates strong generalization capabilities on the MBPP and APPS datasets. For example, Condor (1.3B) improves the Pass@1 of Meta-Llama-3.1-Instruct (70B) on the APPS dataset by 147.05%.",
        "tags": [
            "LLMs",
            "LLaMA"
        ]
    },
    {
        "id": "204",
        "title": "Applying LLM and Topic Modelling in Psychotherapeutic Contexts",
        "author": [
            "Alexander Vanin",
            "Vadim Bolshev",
            "Anastasia Panfilova"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17449",
        "abstract": "This study explores the use of Large language models to analyze therapist remarks in a psychotherapeutic setting. The paper focuses on the application of BERTopic, a machine learning-based topic modeling tool, to the dialogue of two different groups of therapists (classical and modern), which makes it possible to identify and describe a set of topics that consistently emerge across these groups. The paper describes in detail the chosen algorithm for BERTopic, which included creating a vector space from a corpus of therapist remarks, reducing its dimensionality, clustering the space, and creating and optimizing topic representation. Along with the automatic topical modeling by the BERTopic, the research involved an expert assessment of the findings and manual topic structure optimization. The topic modeling results highlighted the most common and stable topics in therapists speech, offering insights into how language patterns in therapy develop and remain stable across different therapeutic styles. This work contributes to the growing field of machine learning in psychotherapy by demonstrating the potential of automated methods to improve both the practice and training of therapists. The study highlights the value of topic modeling as a tool for gaining a deeper understanding of therapeutic dialogue and offers new opportunities for improving therapeutic effectiveness and clinical supervision.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "205",
        "title": "A Survey on Multi-Generative Agent System: Recent Advances and New Frontiers",
        "author": [
            "Shuaihang Chen",
            "Yuanxing Liu",
            "Wei Han",
            "Weinan Zhang",
            "Ting Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17481",
        "abstract": "Multi-generative agent systems (MGASs) have become a research hotspot since the rise of large language models (LLMs). However, with the continuous influx of new related works, the existing reviews struggle to capture them comprehensively. This paper presents a comprehensive survey of these studies. We first discuss the definition of MGAS, a framework encompassing much of previous work. We provide an overview of the various applications of MGAS in (i) solving complex tasks, (ii) simulating specific scenarios, and (iii) evaluating generative agents. Building on previous studies, we also highlight several challenges and propose future directions for research in this field.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "206",
        "title": "A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression",
        "author": [
            "Chenlong Deng",
            "Zhisong Zhang",
            "Kelong Mao",
            "Shuaiyi Li",
            "Xinting Huang",
            "Dong Yu",
            "Zhicheng Dou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17483",
        "abstract": "In this work, we provide a thorough investigation of gist-based context compression methods to improve long-context processing in large language models. We focus on two key questions: (1) How well can these methods replace full attention models? and (2) What potential failure patterns arise due to compression? Through extensive experiments, we show that while gist-based compression can achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, it faces challenges in tasks like synthetic recall. Furthermore, we identify three key failure patterns: lost by the boundary, lost if surprise, and lost along the way. To mitigate these issues, we propose two effective strategies: fine-grained autoencoding, which enhances the reconstruction of original token information, and segment-wise token importance estimation, which adjusts optimization based on token dependencies. Our work provides valuable insights into the understanding of gist token-based context compression and offers practical strategies for improving compression capabilities.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "207",
        "title": "Power- and Fragmentation-aware Online Scheduling for GPU Datacenters",
        "author": [
            "Francesco Lettich",
            "Emanuele Carlini",
            "Franco Maria Nardini",
            "Raffaele Perego",
            "Salvatore Trani"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17484",
        "abstract": "The rise of Artificial Intelligence and Large Language Models is driving increased GPU usage in data centers for complex training and inference tasks, impacting operational costs, energy demands, and the environmental footprint of large-scale computing infrastructures. This work addresses the online scheduling problem in GPU datacenters, which involves scheduling tasks without knowledge of their future arrivals. We focus on two objectives: minimizing GPU fragmentation and reducing power consumption. GPU fragmentation occurs when partial GPU allocations hinder the efficient use of remaining resources, especially as the datacenter nears full capacity. A recent scheduling policy, Fragmentation Gradient Descent (FGD), leverages a fragmentation metric to address this issue. Reducing power consumption is also crucial due to the significant power demands of GPUs. To this end, we propose PWR, a novel scheduling policy to minimize power usage by selecting power-efficient GPU and CPU combinations. This involves a simplified model for measuring power consumption integrated into a Kubernetes score plugin. Through an extensive experimental evaluation in a simulated cluster, we show how PWR, when combined with FGD, achieves a balanced trade-off between reducing power consumption and minimizing GPU fragmentation.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "208",
        "title": "Is ChatGPT Massively Used by Students Nowadays? A Survey on the Use of Large Language Models such as ChatGPT in Educational Settings",
        "author": [
            "Jérémie Sublime",
            "Ilaria Renna"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17486",
        "abstract": "The rapid adoption of Generative AI (GenAI) based on Large Language Models (LLMs) such as ChatGPT has recently and profoundly impacted education, offering transformative opportunities while raising significant concerns. In this study we present the results of a survey that investigates how 395 students aged 13 to 25 years old in France and Italy integrate LLMs into their educational routines.\nKey findings include the widespread use of these tools across all age groups and disciplines, with older students and male students demonstrating higher usage frequencies, particularly in scientific contexts. The results also show gender disparities, raising concerns about an emerging AI literacy and technological gender gap. Additionally, while most students utilise LLMs constructively, the lack of systematic proofreading and critical evaluation among younger users suggests potential risks to cognitive skills development, including critical thinking and foundational knowledge. The survey results underscore the need for educational institutions to adapt their curricula to integrate AI tools effectively, promoting ethical use, critical thinking, and awareness of AI limitations and environmental costs. This paper provides actionable recommendations for fostering equitable and effective cohabitation of LLMs and education while addressing emerging challenges.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "209",
        "title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought",
        "author": [
            "Jiaan Wang",
            "Fandong Meng",
            "Yunlong Liang",
            "Jie Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17498",
        "abstract": "Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT-o1, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating these texts to a target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs' long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought. In the multi-agent framework, a translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to judge whether the translation in the current round is better than the previous one or not. In this manner, we collect tens of thousands of long-thought MT data, which is used to train our DRT-o1. The experimental results on literature translation demonstrate the effectiveness of the DRT-o1. Using Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by DRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can outperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its effectiveness. The project is available at https://github.com/krystalan/DRT-o1",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "210",
        "title": "Improving the Noise Estimation of Latent Neural Stochastic Differential Equations",
        "author": [
            "Linus Heck",
            "Maximilian Gelbrecht",
            "Michael T. Schaub",
            "Niklas Boers"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17499",
        "abstract": "Latent neural stochastic differential equations (SDEs) have recently emerged as a promising approach for learning generative models from stochastic time series data. However, they systematically underestimate the noise level inherent in such data, limiting their ability to capture stochastic dynamics accurately. We investigate this underestimation in detail and propose a straightforward solution: by including an explicit additional noise regularization in the loss function, we are able to learn a model that accurately captures the diffusion component of the data. We demonstrate our results on a conceptual model system that highlights the improved latent neural SDE's capability to model stochastic bistable dynamics.",
        "tags": [
            "Diffusion",
            "SDE"
        ]
    },
    {
        "id": "211",
        "title": "DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak",
        "author": [
            "Hao Wang",
            "Hao Li",
            "Junda Zhu",
            "Xinyuan Wang",
            "Chengwei Pan",
            "MinLie Huang",
            "Lei Sha"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17522",
        "abstract": "Large Language Models (LLMs) are susceptible to generating harmful content when prompted with carefully crafted inputs, a vulnerability known as LLM jailbreaking. As LLMs become more powerful, studying jailbreak methods is critical to enhancing security and aligning models with human values. Traditionally, jailbreak techniques have relied on suffix addition or prompt templates, but these methods suffer from limited attack diversity. This paper introduces DiffusionAttacker, an end-to-end generative approach for jailbreak rewriting inspired by diffusion models. Our method employs a sequence-to-sequence (seq2seq) text diffusion model as a generator, conditioning on the original prompt and guiding the denoising process with a novel attack loss. Unlike previous approaches that use autoregressive LLMs to generate jailbreak prompts, which limit the modification of already generated tokens and restrict the rewriting space, DiffusionAttacker utilizes a seq2seq diffusion model, allowing more flexible token modifications. This approach preserves the semantic content of the original prompt while producing harmful content. Additionally, we leverage the Gumbel-Softmax technique to make the sampling process from the diffusion model's output distribution differentiable, eliminating the need for iterative token search. Extensive experiments on Advbench and Harmbench demonstrate that DiffusionAttacker outperforms previous methods across various evaluation metrics, including attack success rate (ASR), fluency, and diversity.",
        "tags": [
            "Diffusion",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "212",
        "title": "Exploring Dynamic Novel View Synthesis Technologies for Cinematography",
        "author": [
            "Adrian Azzarelli",
            "Nantheera Anantrasirichai",
            "David R Bull"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17532",
        "abstract": "Novel view synthesis (NVS) has shown significant promise for applications in cinematographic production, particularly through the exploitation of Neural Radiance Fields (NeRF) and Gaussian Splatting (GS). These methods model real 3D scenes, enabling the creation of new shots that are challenging to capture in the real world due to set topology or expensive equipment requirement. This innovation also offers cinematographic advantages such as smooth camera movements, virtual re-shoots, slow-motion effects, etc. This paper explores dynamic NVS with the aim of facilitating the model selection process. We showcase its potential through a short montage filmed using various NVS models.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "213",
        "title": "Behind Closed Words: Creating and Investigating the forePLay Annotated Dataset for Polish Erotic Discourse",
        "author": [
            "Anna Kołos",
            "Katarzyna Lorenc",
            "Emilia Wiśnios",
            "Agnieszka Karlińska"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17533",
        "abstract": "The surge in online content has created an urgent demand for robust detection systems, especially in non-English contexts where current tools demonstrate significant limitations. We present forePLay, a novel Polish language dataset for erotic content detection, featuring over 24k annotated sentences with a multidimensional taxonomy encompassing ambiguity, violence, and social unacceptability dimensions. Our comprehensive evaluation demonstrates that specialized Polish language models achieve superior performance compared to multilingual alternatives, with transformer-based architectures showing particular strength in handling imbalanced categories. The dataset and accompanying analysis establish essential frameworks for developing linguistically-aware content moderation systems, while highlighting critical considerations for extending such capabilities to morphologically complex languages.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "214",
        "title": "Resource-Aware Arabic LLM Creation: Model Adaptation, Integration, and Multi-Domain Testing",
        "author": [
            "Prakash Aryan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17548",
        "abstract": "This paper presents a novel approach to fine-tuning the Qwen2-1.5B model for Arabic language processing using Quantized Low-Rank Adaptation (QLoRA) on a system with only 4GB VRAM. We detail the process of adapting this large language model to the Arabic domain, using diverse datasets including Bactrian, OpenAssistant, and Wikipedia Arabic corpora. Our methodology involves custom data preprocessing, model configuration, and training optimization techniques such as gradient accumulation and mixed-precision training. We address specific challenges in Arabic NLP, including morphological complexity, dialectal variations, and diacritical mark handling. Experimental results over 10,000 training steps show significant performance improvements, with the final loss converging to 0.1083. We provide comprehensive analysis of GPU memory usage, training dynamics, and model evaluation across various Arabic language tasks, including text classification, question answering, and dialect identification. The fine-tuned model demonstrates robustness to input perturbations and improved handling of Arabic-specific linguistic phenomena. This research contributes to multilingual AI by demonstrating a resource-efficient approach for creating specialized language models, potentially democratizing access to advanced NLP technologies for diverse linguistic communities. Our work paves the way for future research in low-resource language adaptation and efficient fine-tuning of large language models.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "215",
        "title": "Comparative Analysis of Document-Level Embedding Methods for Similarity Scoring on Shakespeare Sonnets and Taylor Swift Lyrics",
        "author": [
            "Klara Kramer"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17552",
        "abstract": "This study evaluates the performance of TF-IDF weighting, averaged Word2Vec embeddings, and BERT embeddings for document similarity scoring across two contrasting textual domains. By analysing cosine similarity scores, the methods' strengths and limitations are highlighted. The findings underscore TF-IDF's reliance on lexical overlap and Word2Vec's superior semantic generalisation, particularly in cross-domain comparisons. BERT demonstrates lower performance in challenging domains, likely due to insufficient domainspecific fine-tuning.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "216",
        "title": "A Survey of Query Optimization in Large Language Models",
        "author": [
            "Mingyang Song",
            "Mao Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17558",
        "abstract": "\\textit{Query Optimization} (QO) refers to techniques aimed at enhancing the efficiency and quality of Large Language Models (LLMs) in understanding and answering queries, especially complex ones in scenarios like Retrieval-Augmented Generation (RAG). Specifically, RAG mitigates the limitations of LLMs by dynamically retrieving and leveraging up-to-date relevant information, which provides a cost-effective solution to the challenge of LLMs producing plausible but potentially inaccurate responses. Recently, as RAG evolves and incorporates multiple components that influence its performance, QO has emerged as a critical element, playing a pivotal role in determining the effectiveness of RAG's retrieval stage in accurately sourcing the necessary multiple pieces of evidence to answer queries correctly. In this paper, we trace the evolution of QO techniques by summarizing and analyzing significant studies. Through an organized framework and categorization, we aim to consolidate existing QO techniques in RAG, elucidate their technological foundations, and highlight their potential to enhance the versatility and applications of LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "217",
        "title": "GQSA: Group Quantization and Sparsity for Accelerating Large Language Model Inference",
        "author": [
            "Chao Zeng",
            "Songwei Liu",
            "Shu Yang",
            "Fangmin Chen",
            "Xing Mei",
            "Lean Fu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17560",
        "abstract": "With the rapid growth in the scale and complexity of large language models (LLMs), the costs of training and inference have risen substantially. Model compression has emerged as a mainstream solution to reduce memory usage and computational overhead. This paper presents Group Quantization and Sparse Acceleration (\\textbf{GQSA}), a novel compression technique tailored for LLMs. Traditional methods typically focus exclusively on either quantization or sparsification, but relying on a single strategy often results in significant performance loss at high compression rates. In contrast, GQSA integrates quantization and sparsification in a tightly coupled manner, leveraging GPU-friendly structured group sparsity and quantization for efficient acceleration. The proposed method consists of three key steps. First, GQSA applies group structured pruning to adhere to GPU-friendly sparse pattern constraints. Second, a two-stage sparsity-aware training process is employed to maximize performance retention after compression. Finally, the framework adopts the Block Sparse Row (BSR) format to enable practical deployment and efficient execution. Experimental results on the LLaMA model family show that GQSA achieves an excellent balance between model speed and accuracy. Furthermore, on the latest LLaMA-3 and LLaMA-3.1 models, GQSA outperforms existing LLM compression techniques significantly.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "218",
        "title": "The Dynamic Duo of Collaborative Masking and Target for Advanced Masked Autoencoder Learning",
        "author": [
            "Shentong Mo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17566",
        "abstract": "Masked autoencoders (MAE) have recently succeeded in self-supervised vision representation learning. Previous work mainly applied custom-designed (e.g., random, block-wise) masking or teacher (e.g., CLIP)-guided masking and targets. However, they ignore the potential role of the self-training (student) model in giving feedback to the teacher for masking and targets. In this work, we present to integrate Collaborative Masking and Targets for boosting Masked AutoEncoders, namely CMT-MAE. Specifically, CMT-MAE leverages a simple collaborative masking mechanism through linear aggregation across attentions from both teacher and student models. We further propose using the output features from those two models as the collaborative target of the decoder. Our simple and effective framework pre-trained on ImageNet-1K achieves state-of-the-art linear probing and fine-tuning performance. In particular, using ViT-base, we improve the fine-tuning results of the vanilla MAE from 83.6% to 85.7%.",
        "tags": [
            "CLIP",
            "ViT"
        ]
    },
    {
        "id": "219",
        "title": "HPCNeuroNet: A Neuromorphic Approach Merging SNN Temporal Dynamics with Transformer Attention for FPGA-based Particle Physics",
        "author": [
            "Murat Isik",
            "Hiruna Vishwamith",
            "Jonathan Naoukin",
            "I. Can Dikmen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17571",
        "abstract": "This paper presents the innovative HPCNeuroNet model, a pioneering fusion of Spiking Neural Networks (SNNs), Transformers, and high-performance computing tailored for particle physics, particularly in particle identification from detector responses. Our approach leverages SNNs' intrinsic temporal dynamics and Transformers' robust attention mechanisms to enhance performance when discerning intricate particle interactions. At the heart of HPCNeuroNet lies the integration of the sequential dynamism inherent in SNNs with the context-aware attention capabilities of Transformers, enabling the model to precisely decode and interpret complex detector data. HPCNeuroNet is realized through the HLS4ML framework and optimized for deployment in FPGA environments. The model accuracy and scalability are also enhanced by this architectural choice. Benchmarked against machine learning models, HPCNeuroNet showcases better performance metrics, underlining its transformative potential in high-energy physics. We demonstrate that the combination of SNNs, Transformers, and FPGA-based high-performance computing in particle physics signifies a significant step forward and provides a strong foundation for future research.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "220",
        "title": "HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data",
        "author": [
            "Ting Zhou",
            "Daoyuan Chen",
            "Qirui Jiao",
            "Bolin Ding",
            "Yaliang Li",
            "Ying Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17574",
        "abstract": "In the domain of Multimodal Large Language Models (MLLMs), achieving human-centric video understanding remains a formidable challenge. Existing benchmarks primarily emphasize object and action recognition, often neglecting the intricate nuances of human emotions, behaviors, and speech visual alignment within video content. We present HumanVBench, an innovative benchmark meticulously crafted to bridge these gaps in the evaluation of video MLLMs. HumanVBench comprises 17 carefully designed tasks that explore two primary dimensions: inner emotion and outer manifestations, spanning static and dynamic, basic and complex, as well as single-modal and cross-modal aspects. With two advanced automated pipelines for video annotation and distractor-included QA generation, HumanVBench utilizes diverse state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and quality assessment, minimizing human annotation dependency tailored to human-centric multimodal attributes. A comprehensive evaluation across 16 SOTA video MLLMs reveals notable limitations in current performance, especially in cross-modal and temporal alignment, underscoring the necessity for further refinement toward achieving more human-like understanding. HumanVBench is open-sourced to facilitate future advancements and real-world applications in video MLLMs.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "221",
        "title": "Investigating Length Issues in Document-level Machine Translation",
        "author": [
            "Ziqian Peng",
            "Rachel Bawden",
            "François Yvon"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17592",
        "abstract": "Transformer architectures are increasingly effective at processing and generating very long chunks of texts, opening new perspectives for document-level machine translation (MT). In this work, we challenge the ability of MT systems to handle texts comprising up to several thousands of tokens. We design and implement a new approach designed to precisely measure the effect of length increments on MT outputs. Our experiments with two representative architectures unambiguously show that (a)~translation performance decreases with the length of the input text; (b)~the position of sentences within the document matters and translation quality is higher for sentences occurring earlier in a document. We further show that manipulating the distribution of document lengths and of positional embeddings only marginally mitigates such problems. Our results suggest that even though document-level MT is computationally feasible, it does not yet match the performance of sentence-based MT.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "222",
        "title": "LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea Generation with Minimal Context",
        "author": [
            "Kai Ruan",
            "Xuan Wang",
            "Jixiang Hong",
            "Hao Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17596",
        "abstract": "While Large Language Models (LLMs) have demonstrated remarkable capabilities in scientific tasks, existing evaluation frameworks primarily assess their performance using rich contextual inputs, overlooking their ability to generate novel ideas from minimal information. We introduce LiveIdeaBench, a comprehensive benchmark that evaluates LLMs' scientific creativity and divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our framework employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across four key dimensions: originality, feasibility, fluency, and flexibility. Through extensive experimentation with 20 leading models across 1,180 keywords spanning 18 scientific domains, we reveal that scientific creative ability shows distinct patterns from general intelligence metrics. Notably, our results demonstrate that models like QwQ-32B-preview achieve comparable creative performance to top-tier models like o1-preview, despite significant gaps in their general intelligence scores. These findings highlight the importance of specialized evaluation frameworks for scientific creativity and suggest that the development of creative capabilities in LLMs may follow different trajectories than traditional problem-solving abilities.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "223",
        "title": "AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot Semantic Segmentation",
        "author": [
            "Jiaqi Ma",
            "Guo-Sen Xie",
            "Fang Zhao",
            "Zechao Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17601",
        "abstract": "Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5\\textsuperscript{i} and COCO-20\\textsuperscript{i} datasets demonstrate that AFANet has achieved state-of-the-art performance. The code is available at https://github.com/jarch-ma/AFANet.",
        "tags": [
            "CLIP",
            "Segmentation"
        ]
    },
    {
        "id": "224",
        "title": "EasyTime: Time Series Forecasting Made Easy",
        "author": [
            "Xiangfei Qiu",
            "Xiuwen Li",
            "Ruiyang Pang",
            "Zhicheng Pan",
            "Xingjian Wu",
            "Liu Yang",
            "Jilin Hu",
            "Yang Shu",
            "Xuesong Lu",
            "Chengcheng Yang",
            "Chenjuan Guo",
            "Aoying Zhou",
            "Christian S. Jensen",
            "Bin Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17603",
        "abstract": "Time series forecasting has important applications across diverse domains. EasyTime, the system we demonstrate, facilitates easy use of time-series forecasting methods by researchers and practitioners alike. First, EasyTime enables one-click evaluation, enabling researchers to evaluate new forecasting methods using the suite of diverse time series datasets collected in the preexisting time series forecasting benchmark (TFB). This is achieved by leveraging TFB's flexible and consistent evaluation pipeline. Second, when practitioners must perform forecasting on a new dataset, a nontrivial first step is often to find an appropriate forecasting method. EasyTime provides an Automated Ensemble module that combines the promising forecasting methods to yield superior forecasting accuracy compared to individual methods. Third, EasyTime offers a natural language Q&A module leveraging large language models. Given a question like \"Which method is best for long term forecasting on time series with strong seasonality?\", EasyTime converts the question into SQL queries on the database of results obtained by TFB and then returns an answer in natural language and charts. By demonstrating EasyTime, we intend to show how it is possible to simplify the use of time series forecasting and to offer better support for the development of new generations of time series forecasting methods.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "225",
        "title": "SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images",
        "author": [
            "Risa Shinoda",
            "Kuniaki Saito",
            "Shohei Tanaka",
            "Tosho Hirasawa",
            "Yoshitaka Ushiku"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17606",
        "abstract": "Building a large-scale figure QA dataset requires a considerable amount of work, from gathering and selecting figures to extracting attributes like text, numbers, and colors, and generating QAs. Although recent developments in LLMs have led to efforts to synthesize figures, most of these focus primarily on QA generation. Additionally, creating figures directly using LLMs often encounters issues such as code errors, similar-looking figures, and repetitive content in figures. To address this issue, we present SBSFigures (Stage-by-Stage Synthetic Figures), a dataset for pre-training figure QA. Our proposed pipeline enables the creation of chart figures with complete annotations of the visualized data and dense QA annotations without any manual annotation process. Our stage-by-stage pipeline makes it possible to create diverse topic and appearance figures efficiently while minimizing code errors. Our SBSFigures demonstrate a strong pre-training effect, making it possible to achieve efficient training with a limited amount of real-world chart data starting from our pre-trained weights.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "226",
        "title": "CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed Learning for Large Scene Reconstruction",
        "author": [
            "Yuanyuan Gao",
            "Yalun Dai",
            "Hao Li",
            "Weicai Ye",
            "Junyi Chen",
            "Danpeng Chen",
            "Dingwen Zhang",
            "Tong He",
            "Guofeng Zhang",
            "Junwei Han"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17612",
        "abstract": "3D Gaussian Splatting (3DGS) has demonstrated impressive performance in scene reconstruction. However, most existing GS-based surface reconstruction methods focus on 3D objects or limited scenes. Directly applying these methods to large-scale scene reconstruction will pose challenges such as high memory costs, excessive time consumption, and lack of geometric detail, which makes it difficult to implement in practical applications. To address these issues, we propose a multi-agent collaborative fast 3DGS surface reconstruction framework based on distributed learning for large-scale surface reconstruction. Specifically, we develop local model compression (LMC) and model aggregation schemes (MAS) to achieve high-quality surface representation of large scenes while reducing GPU memory consumption. Extensive experiments on Urban3d, MegaNeRF, and BlendedMVS demonstrate that our proposed method can achieve fast and scalable high-fidelity surface reconstruction and photorealistic rendering. Our project page is available at \\url{https://gyy456.github.io/CoSurfGS}.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "227",
        "title": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study",
        "author": [
            "Yang Xu",
            "Yi Wang",
            "Hao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17626",
        "abstract": "Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we: (1) introduce SAE-Track, a method to efficiently obtain a continual series of SAEs; (2) formulate the process of feature formation and conduct a mechanistic analysis; and (3) analyze and visualize feature drift during training. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "228",
        "title": "Editing Implicit and Explicit Representations of Radiance Fields: A Survey",
        "author": [
            "Arthur Hubert",
            "Gamal Elghazaly",
            "Raphael Frank"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17628",
        "abstract": "Neural Radiance Fields (NeRF) revolutionized novel view synthesis in recent years by offering a new volumetric representation, which is compact and provides high-quality image rendering. However, the methods to edit those radiance fields developed slower than the many improvements to other aspects of NeRF. With the recent development of alternative radiance field-based representations inspired by NeRF as well as the worldwide rise in popularity of text-to-image models, many new opportunities and strategies have emerged to provide radiance field editing. In this paper, we deliver a comprehensive survey of the different editing methods present in the literature for NeRF and other similar radiance field representations. We propose a new taxonomy for classifying existing works based on their editing methodologies, review pioneering models, reflect on current and potential new applications of radiance field editing, and compare state-of-the-art approaches in terms of editing options and performance.",
        "tags": [
            "NeRF",
            "Text-to-Image"
        ]
    },
    {
        "id": "229",
        "title": "Detail-Preserving Latent Diffusion for Stable Shadow Removal",
        "author": [
            "Jiamin Xu",
            "Yuxin Zheng",
            "Zelong Li",
            "Chi Wang",
            "Renshu Gu",
            "Weiwei Xu",
            "Gang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17630",
        "abstract": "Achieving high-quality shadow removal with strong generalizability is challenging in scenes with complex global illumination. Due to the limited diversity in shadow removal datasets, current methods are prone to overfitting training data, often leading to reduced performance on unseen cases. To address this, we leverage the rich visual priors of a pre-trained Stable Diffusion (SD) model and propose a two-stage fine-tuning pipeline to adapt the SD model for stable and efficient shadow removal. In the first stage, we fix the VAE and fine-tune the denoiser in latent space, which yields substantial shadow removal but may lose some high-frequency details. To resolve this, we introduce a second stage, called the detail injection stage. This stage selectively extracts features from the VAE encoder to modulate the decoder, injecting fine details into the final results. Experimental results show that our method outperforms state-of-the-art shadow removal techniques. The cross-dataset evaluation further demonstrates that our method generalizes effectively to unseen data, enhancing the applicability of shadow removal methods.",
        "tags": [
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": "230",
        "title": "ANID: How Far Are We? Evaluating the Discrepancies Between AI-synthesized Images and Natural Images through Multimodal Guidance",
        "author": [
            "Renyang Liu",
            "Ziyu Lyu",
            "Wei Zhou",
            "See-Kiong Ng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17632",
        "abstract": "In the rapidly evolving field of Artificial Intelligence Generated Content (AIGC), one of the key challenges is distinguishing AI-synthesized images from natural images. Despite the remarkable capabilities of advanced AI generative models in producing visually compelling images, significant discrepancies remain when these images are compared to natural ones. To systematically investigate and quantify these discrepancies, we introduce an AI-Natural Image Discrepancy Evaluation benchmark aimed at addressing the critical question: \\textit{how far are AI-generated images (AIGIs) from truly realistic images?} We have constructed a large-scale multimodal dataset, the Distinguishing Natural and AI-generated Images (DNAI) dataset, which includes over 440,000 AIGI samples generated by 8 representative models using both unimodal and multimodal prompts, such as Text-to-Image (T2I), Image-to-Image (I2I), and Text \\textit{vs.} Image-to-Image (TI2I). Our fine-grained assessment framework provides a comprehensive evaluation of the DNAI dataset across five key dimensions: naive visual feature quality, semantic alignment in multimodal generation, aesthetic appeal, downstream task applicability, and coordinated human validation. Extensive evaluation results highlight significant discrepancies across these dimensions, underscoring the necessity of aligning quantitative metrics with human judgment to achieve a holistic understanding of AI-generated image quality. Code is available at \\href{https://github.com/ryliu68/ANID}{https://github.com/ryliu68/ANID}.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "231",
        "title": "LangSurf: Language-Embedded Surface Gaussians for 3D Scene Understanding",
        "author": [
            "Hao Li",
            "Roy Qin",
            "Zhengyu Zou",
            "Diqi He",
            "Bohan Li",
            "Bingquan Dai",
            "Dingewn Zhang",
            "Junwei Han"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17635",
        "abstract": "Applying Gaussian Splatting to perception tasks for 3D scene understanding is becoming increasingly popular. Most existing works primarily focus on rendering 2D feature maps from novel viewpoints, which leads to an imprecise 3D language field with outlier languages, ultimately failing to align objects in 3D space. By utilizing masked images for feature extraction, these approaches also lack essential contextual information, leading to inaccurate feature representation. To this end, we propose a Language-Embedded Surface Field (LangSurf), which accurately aligns the 3D language fields with the surface of objects, facilitating precise 2D and 3D segmentation with text query, widely expanding the downstream tasks such as removal and editing. The core of LangSurf is a joint training strategy that flattens the language Gaussian on the object surfaces using geometry supervision and contrastive losses to assign accurate language features to the Gaussians of objects. In addition, we also introduce the Hierarchical-Context Awareness Module to extract features at the image level for contextual information then perform hierarchical mask pooling using masks segmented by SAM to obtain fine-grained language features in different hierarchies. Extensive experiments on open-vocabulary 2D and 3D semantic segmentation demonstrate that LangSurf outperforms the previous state-of-the-art method LangSplat by a large margin. As shown in Fig.~\\ref{fig:teaser}, our method is capable of segmenting objects in 3D space, thus boosting the effectiveness of our approach in instance recognition, removal, and editing, which is also supported by comprehensive experiments. \\url{https://langsurf.github.io}{Project Page}.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "232",
        "title": "SCBench: A Sports Commentary Benchmark for Video LLMs",
        "author": [
            "Kuangzhi Ge",
            "Lingjun Chen",
            "Kevin Zhang",
            "Yulin Luo",
            "Tianyu Shi",
            "Liaoyuan Fan",
            "Xiang Li",
            "Guanqun Wang",
            "Shanghang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17637",
        "abstract": "Recently, significant advances have been made in Video Large Language Models (Video LLMs) in both academia and industry. However, methods to evaluate and benchmark the performance of different Video LLMs, especially their fine-grained, temporal visual capabilities, remain very limited. On one hand, current benchmarks use relatively simple videos (e.g., subtitled movie clips) where the model can understand the entire video by processing just a few frames. On the other hand, their datasets lack diversity in task format, comprising only QA or multi-choice QA, which overlooks the models' capacity for generating in-depth and precise texts. Sports videos, which feature intricate visual information, sequential events, and emotionally charged commentary, present a critical challenge for Video LLMs, making sports commentary an ideal benchmarking task. Inspired by these challenges, we propose a novel task: sports video commentary generation, developed $\\textbf{SCBench}$ for Video LLMs. To construct such a benchmark, we introduce (1) $\\textbf{SCORES}$, a six-dimensional metric specifically designed for our task, upon which we propose a GPT-based evaluation method, and (2) $\\textbf{CommentarySet}$, a dataset consisting of 5,775 annotated video clips and ground-truth labels tailored to our metric. Based on SCBench, we conduct comprehensive evaluations on multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought baseline methods. Our results found that InternVL-Chat-2 achieves the best performance with 5.44, surpassing the second-best by 1.04. Our work provides a fresh perspective for future research, aiming to enhance models' overall capabilities in complex visual understanding tasks. Our dataset will be released soon.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaVA",
            "Large Language Models"
        ]
    },
    {
        "id": "233",
        "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
        "author": [
            "Federico Spurio",
            "Emad Bahrami",
            "Gianpiero Francesca",
            "Juergen Gall"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17640",
        "abstract": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (\\ours), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
        "tags": [
            "Segmentation",
            "Vector Quantization"
        ]
    },
    {
        "id": "234",
        "title": "DreamFit: Garment-Centric Human Generation via a Lightweight Anything-Dressing Encoder",
        "author": [
            "Ente Lin",
            "Xujie Zhang",
            "Fuwei Zhao",
            "Yuxuan Luo",
            "Xin Dong",
            "Long Zeng",
            "Xiaodan Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17644",
        "abstract": "Diffusion models for garment-centric human generation from text or image prompts have garnered emerging attention for their great application potential. However, existing methods often face a dilemma: lightweight approaches, such as adapters, are prone to generate inconsistent textures; while finetune-based methods involve high training costs and struggle to maintain the generalization capabilities of pretrained diffusion models, limiting their performance across diverse scenarios. To address these challenges, we propose DreamFit, which incorporates a lightweight Anything-Dressing Encoder specifically tailored for the garment-centric human generation. DreamFit has three key advantages: (1) \\textbf{Lightweight training}: with the proposed adaptive attention and LoRA modules, DreamFit significantly minimizes the model complexity to 83.4M trainable parameters. (2)\\textbf{Anything-Dressing}: Our model generalizes surprisingly well to a wide range of (non-)garments, creative styles, and prompt instructions, consistently delivering high-quality results across diverse scenarios. (3) \\textbf{Plug-and-play}: DreamFit is engineered for smooth integration with any community control plugins for diffusion models, ensuring easy compatibility and minimizing adoption barriers. To further enhance generation quality, DreamFit leverages pretrained large multi-modal models (LMMs) to enrich the prompt with fine-grained garment descriptions, thereby reducing the prompt gap between training and inference. We conduct comprehensive experiments on both $768 \\times 512$ high-resolution benchmarks and in-the-wild images. DreamFit surpasses all existing methods, highlighting its state-of-the-art capabilities of garment-centric human generation.",
        "tags": [
            "Diffusion",
            "LoRA"
        ]
    },
    {
        "id": "235",
        "title": "Benchmarking Generative AI Models for Deep Learning Test Input Generation",
        "author": [
            "Maryam",
            "Matteo Biagiola",
            "Andrea Stocco",
            "Vincenzo Riccio"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17652",
        "abstract": "Test Input Generators (TIGs) are crucial to assess the ability of Deep Learning (DL) image classifiers to provide correct predictions for inputs beyond their training and test sets. Recent advancements in Generative AI (GenAI) models have made them a powerful tool for creating and manipulating synthetic images, although these advancements also imply increased complexity and resource demands for training.\nIn this work, we benchmark and combine different GenAI models with TIGs, assessing their effectiveness, efficiency, and quality of the generated test images, in terms of domain validity and label preservation. We conduct an empirical study involving three different GenAI architectures (VAEs, GANs, Diffusion Models), five classification tasks of increasing complexity, and 364 human evaluations. Our results show that simpler architectures, such as VAEs, are sufficient for less complex datasets like MNIST. However, when dealing with feature-rich datasets, such as ImageNet, more sophisticated architectures like Diffusion Models achieve superior performance by generating a higher number of valid, misclassification-inducing inputs.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "236",
        "title": "Generating Completions for Fragmented Broca's Aphasic Sentences Using Large Language Models",
        "author": [
            "Sijbren van Vaals",
            "Yevgen Matusevych",
            "Frank Tsiwah"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17669",
        "abstract": "Broca's aphasia is a type of aphasia characterized by non-fluent, effortful and fragmented speech production with relatively good comprehension. Since traditional aphasia treatment methods are often time-consuming, labour-intensive, and do not reflect real-world conversations, applying natural language processing based approaches such as Large Language Models (LLMs) could potentially contribute to improving existing treatment approaches. To address this issue, we explore the use of sequence-to-sequence LLMs for completing fragmented Broca's aphasic sentences. We first generate synthetic Broca's aphasic data using a rule-based system designed to mirror the linguistic characteristics of Broca's aphasic speech. Using this synthetic data, we then fine-tune four pre-trained LLMs on the task of completing fragmented sentences. We evaluate our fine-tuned models on both synthetic and authentic Broca's aphasic data. We demonstrate LLMs' capability for reconstructing fragmented sentences, with the models showing improved performance with longer input utterances. Our result highlights the LLMs' potential in advancing communication aids for individuals with Broca's aphasia and possibly other clinical populations.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "237",
        "title": "A Bias-Free Training Paradigm for More General AI-generated Image Detection",
        "author": [
            "Fabrizio Guillaro",
            "Giada Zingarini",
            "Ben Usman",
            "Avneesh Sud",
            "Davide Cozzolino",
            "Luisa Verdoliva"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17671",
        "abstract": "Successful forensic detectors can produce excellent results in supervised learning benchmarks but struggle to transfer to real-world applications. We believe this limitation is largely due to inadequate training data quality. While most research focuses on developing new algorithms, less attention is given to training data selection, despite evidence that performance can be strongly impacted by spurious correlations such as content, format, or resolution. A well-designed forensic detector should detect generator specific artifacts rather than reflect data biases. To this end, we propose B-Free, a bias-free training paradigm, where fake images are generated from real ones using the conditioning procedure of stable diffusion models. This ensures semantic alignment between real and fake images, allowing any differences to stem solely from the subtle artifacts introduced by AI generation. Through content-based augmentation, we show significant improvements in both generalization and robustness over state-of-the-art detectors and more calibrated results across 27 different generative models, including recent releases, like FLUX and Stable Diffusion 3.5. Our findings emphasize the importance of a careful dataset curation, highlighting the need for further research in dataset design. Code and data will be publicly available at https://grip-unina.github.io/B-Free/",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "238",
        "title": "Understanding the Logic of Direct Preference Alignment through Logic",
        "author": [
            "Kyle Richardson",
            "Vivek Srikumar",
            "Ashish Sabharwal"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17696",
        "abstract": "Recent direct preference alignment algorithms (DPA), such as DPO, have shown great promise in aligning large language models to human preferences. While this has motivated the development of many new variants of the original DPO loss, understanding the differences between these recent proposals, as well as developing new DPA loss functions, remains difficult given the lack of a technical and conceptual framework for reasoning about the underlying semantics of these algorithms. In this paper, we attempt to remedy this by formalizing DPA losses in terms of discrete reasoning problems. Specifically, we ask: Given an existing DPA loss, can we systematically derive a symbolic expression that characterizes its semantics? How do the semantics of two losses relate to each other? We propose a novel formalism for characterizing preference losses for single model and reference model based approaches, and identify symbolic forms for a number of commonly used DPA variants. Further, we show how this formal view of preference learning sheds new light on both the size and structure of the DPA loss landscape, making it possible to not only rigorously characterize the relationships between recent loss proposals but also to systematically explore the landscape and derive new loss functions from first principles. We hope our framework and findings will help provide useful guidance to those working on human AI alignment.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "239",
        "title": "GaussianPainter: Painting Point Cloud into 3D Gaussians with Normal Guidance",
        "author": [
            "Jingqiu Zhou",
            "Lue Fan",
            "Xuesong Chen",
            "Linjiang Huang",
            "Si Liu",
            "Hongsheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17715",
        "abstract": "In this paper, we present GaussianPainter, the first method to paint a point cloud into 3D Gaussians given a reference image. GaussianPainter introduces an innovative feed-forward approach to overcome the limitations of time-consuming test-time optimization in 3D Gaussian splatting. Our method addresses a critical challenge in the field: the non-uniqueness problem inherent in the large parameter space of 3D Gaussian splatting. This space, encompassing rotation, anisotropic scales, and spherical harmonic coefficients, introduces the challenge of rendering similar images from substantially different Gaussian fields. As a result, feed-forward networks face instability when attempting to directly predict high-quality Gaussian fields, struggling to converge on consistent parameters for a given output. To address this issue, we propose to estimate a surface normal for each point to determine its Gaussian rotation. This strategy enables the network to effectively predict the remaining Gaussian parameters in the constrained space. We further enhance our approach with an appearance injection module, incorporating reference image appearance into Gaussian fields via a multiscale triplane representation. Our method successfully balances efficiency and fidelity in 3D Gaussian generation, achieving high-quality, diverse, and robust 3D content creation from point clouds in a single forward pass.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "240",
        "title": "VidTwin: Video VAE with Decoupled Structure and Dynamics",
        "author": [
            "Yuchi Wang",
            "Junliang Guo",
            "Xinyi Xie",
            "Tianyu He",
            "Xu Sun",
            "Jiang Bian"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17726",
        "abstract": "Recent advancements in video autoencoders (Video AEs) have significantly improved the quality and efficiency of video generation. In this paper, we propose a novel and compact video autoencoder, VidTwin, that decouples video into two distinct latent spaces: Structure latent vectors, which capture overall content and global movement, and Dynamics latent vectors, which represent fine-grained details and rapid movements. Specifically, our approach leverages an Encoder-Decoder backbone, augmented with two submodules for extracting these latent spaces, respectively. The first submodule employs a Q-Former to extract low-frequency motion trends, followed by downsampling blocks to remove redundant content details. The second averages the latent vectors along the spatial dimension to capture rapid motion. Extensive experiments show that VidTwin achieves a high compression rate of 0.20% with high reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), and performs efficiently and effectively in downstream generative tasks. Moreover, our model demonstrates explainability and scalability, paving the way for future research in video latent representation and generation. Our code has been released at https://github.com/microsoft/VidTok/tree/main/vidtwin.",
        "tags": [
            "VAE",
            "Video Generation"
        ]
    },
    {
        "id": "241",
        "title": "Knowledge Editing through Chain-of-Thought",
        "author": [
            "Changyue Wang",
            "Weihang Su",
            "Qingyao Ai",
            "Yiqun Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17727",
        "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across a wide range of natural language processing (NLP) tasks. However, keeping these models up-to-date with evolving world knowledge remains a significant challenge due to the high costs of frequent retraining. To address this challenge, knowledge editing techniques have emerged to update LLMs with new information without rebuilding the model from scratch. Among these, the in-context editing paradigm stands out for its effectiveness in integrating new knowledge while preserving the model's original capabilities. Despite its potential, existing in-context knowledge editing methods are often task-specific, focusing primarily on multi-hop QA tasks using structured knowledge triples. Moreover, their reliance on few-shot prompting for task decomposition makes them unstable and less effective in generalizing across diverse tasks.\nIn response to these limitations, we propose EditCoT, a novel knowledge editing framework that flexibly and efficiently updates LLMs across various tasks without retraining. EditCoT works by generating a chain-of-thought (CoT) for a given input and then iteratively refining this CoT process using a CoT editor based on updated knowledge. We evaluate EditCoT across a diverse range of benchmarks, covering multiple languages and tasks. The results demonstrate that our approach achieves state-of-the-art performance while offering superior generalization, effectiveness, and stability compared to existing methods, marking a significant advancement in the field of knowledge updating. Code and data are available at: https://github.com/bebr2/EditCoT.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "242",
        "title": "Chumor 2.0: Towards Benchmarking Chinese Humor Understanding",
        "author": [
            "Ruiqi He",
            "Yushu He",
            "Longju Bai",
            "Jiarui Liu",
            "Zhenjie Sun",
            "Zenghao Tang",
            "He Wang",
            "Hanchen Xia",
            "Rada Mihalcea",
            "Naihao Deng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17729",
        "abstract": "Existing humor datasets and evaluations predominantly focus on English, leaving limited resources for culturally nuanced humor in non-English languages like Chinese. To address this gap, we construct Chumor, the first Chinese humor explanation dataset that exceeds the size of existing humor datasets. Chumor is sourced from Ruo Zhi Ba, a Chinese Reddit-like platform known for sharing intellectually challenging and culturally specific jokes. We test ten LLMs through direct and chain-of-thought prompting, revealing that Chumor poses significant challenges to existing LLMs, with their accuracy slightly above random and far below human. In addition, our analysis highlights that human-annotated humor explanations are significantly better than those generated by GPT-4o and ERNIE-4-turbo. We release Chumor at https://huggingface.co/datasets/dnaihao/Chumor, our project page is at https://dnaihao.github.io/Chumor-dataset/, our leaderboard is at https://huggingface.co/spaces/dnaihao/Chumor, and our codebase is at https://github.com/dnaihao/Chumor-dataset.",
        "tags": [
            "GPT",
            "LLMs"
        ]
    },
    {
        "id": "243",
        "title": "Reasoning to Attend: Try to Understand How <SEG> Token Works",
        "author": [
            "Rui Qian",
            "Xin Yin",
            "Dejing Dou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17741",
        "abstract": "Current Large Multimodal Models (LMMs) empowered visual grounding typically rely on $\\texttt{<SEG>}$ token as a text prompt to jointly optimize the vision-language model (e.g., LLaVA) and the downstream task-specified model (\\eg, SAM). However, we observe that little research has looked into how it http://works.In this work, we first visualize the similarity maps, which are obtained by computing the semantic similarity between the $\\texttt{<SEG>}$ token and the image token embeddings derived from the last hidden layer in both the LLaVA encoder and SAM decoder. Intriguingly, we have found that a striking consistency holds in terms of activation responses in the similarity map,which reveals that what $\\texttt{<SEG>}$ token contributes to is the semantic similarity within image-text pairs. Specifically, $\\texttt{<SEG>}$ token, a placeholder expanded in text vocabulary, extensively queries among individual tokenized image patches to match the semantics of an object from text to the paired image while the Large Language Models (LLMs) are being fine-tuned. Upon the above findings, we present READ, which facilitates LMMs' resilient $\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the guidance of highly activated points borrowed from similarity maps. Remarkably, READ features an intuitive design, Similarity as Points module (SasP), which can be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play http://fashion.Also, extensive experiments have been conducted on the ReasonSeg and RefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic forgetting of previous skills after fine-tuning, we further assess its generation ability on an augmented FP-RefCOCO(+/g) dataset. All codes and models are publicly available at https://github.com/rui-qian/READ.",
        "tags": [
            "LLMs",
            "LLaVA",
            "Large Language Models",
            "SAM"
        ]
    },
    {
        "id": "244",
        "title": "YuLan-Mini: An Open Data-efficient Language Model",
        "author": [
            "Yiwen Hu",
            "Huatong Song",
            "Jia Deng",
            "Jiapeng Wang",
            "Jie Chen",
            "Kun Zhou",
            "Yutao Zhu",
            "Jinhao Jiang",
            "Zican Dong",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17743",
        "abstract": "Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "245",
        "title": "RepoTransBench: A Real-World Benchmark for Repository-Level Code Translation",
        "author": [
            "Yanli Wang",
            "Yanlin Wang",
            "Suiquan Wang",
            "Daya Guo",
            "Jiachi Chen",
            "John Grundy",
            "Xilin Liu",
            "Yuchi Ma",
            "Mingzhi Mao",
            "Hongyu Zhang",
            "Zibin Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17744",
        "abstract": "Repository-level code translation refers to translating an entire code repository from one programming language to another while preserving the functionality of the source repository. Many benchmarks have been proposed to evaluate the performance of such code translators. However, previous benchmarks mostly provide fine-grained samples, focusing at either code snippet, function, or file-level code translation. Such benchmarks do not accurately reflect real-world demands, where entire repositories often need to be translated, involving longer code length and more complex functionalities. To address this gap, we propose a new benchmark, named RepoTransBench, which is a real-world repository-level code translation benchmark with an automatically executable test suite. We conduct experiments on RepoTransBench to evaluate the translation performance of 11 advanced LLMs. We find that the Success@1 score (test success in one attempt) of the best-performing LLM is only 7.33%. To further explore the potential of LLMs for repository-level code translation, we provide LLMs with error-related feedback to perform iterative debugging and observe an average 7.09% improvement on Success@1. However, even with this improvement, the Success@1 score of the best-performing LLM is only 21%, which may not meet the need for reliable automatic repository-level code translation. Finally, we conduct a detailed error analysis and highlight current LLMs' deficiencies in repository-level code translation, which could provide a reference for further improvements.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "246",
        "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
        "author": [
            "Luyang Liu",
            "Jonas Pfeiffer",
            "Jiaxing Wu",
            "Jun Xie",
            "Arthur Szlam"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17747",
        "abstract": "Techniques enabling large language models (LLMs) to \"think more\" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "247",
        "title": "ADC: Enhancing Function Calling Via Adversarial Datasets and Code Line-Level Feedback",
        "author": [
            "Wei Zhang",
            "Yi Zhang",
            "Li Zhu",
            "Qianghuai Jia",
            "Feijun Jiang",
            "Hongcheng Guo",
            "Zhoujun Li",
            "Mengping Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17754",
        "abstract": "Large Language Models (LLMs) have made significant strides in Natural Language Processing and coding, yet they struggle with robustness and accuracy in complex function calls. To tackle these challenges, this paper introduces ADC, an innovative approach that enhances LLMs' ability to follow function formats and match complex parameters. ADC utilizes a high-quality code fine-tuning dataset with line-level execution feedback, providing granular process supervision that fosters strong logical reasoning and adherence to function formats. It also employs an adversarial dataset generation process to improve parameter matching. The staged training methodology capitalizes on both enriched code datasets and refined adversarial datasets, leading to marked improvements in function calling capabilities on the Berkeley Function-Calling Leaderboard (BFCL) Benchmark. The innovation of ADC lies in its strategic combination of process supervision, adversarial refinement, and incremental learning, setting a new standard for LLM proficiency in complex function calling.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "248",
        "title": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging",
        "author": [
            "Łukasz Borchmann"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17758",
        "abstract": "ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "249",
        "title": "Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy",
        "author": [
            "Priyaranjan Pattnayak",
            "Hitesh Laxmichand Patel",
            "Bhargava Kumar",
            "Amit Agarwal",
            "Ishan Banerjee",
            "Srikant Panda",
            "Tejaswini Kumar"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17759",
        "abstract": "Multimodal learning, a rapidly evolving field in artificial intelligence, seeks to construct more versatile and robust systems by integrating and analyzing diverse types of data, including text, images, audio, and video. Inspired by the human ability to assimilate information through many senses, this method enables applications such as text-to-video conversion, visual question answering, and image captioning. Recent developments in datasets that support multimodal language models (MLLMs) are highlighted in this overview. Large-scale multimodal datasets are essential because they allow for thorough testing and training of these models. With an emphasis on their contributions to the discipline, the study examines a variety of datasets, including those for training, domain-specific tasks, and real-world applications. It also emphasizes how crucial benchmark datasets are for assessing models' performance in a range of scenarios, scalability, and applicability. Since multimodal learning is always changing, overcoming these obstacles will help AI research and applications reach new heights.",
        "tags": [
            "Text-to-Video"
        ]
    },
    {
        "id": "250",
        "title": "The Superposition of Diffusion Models Using the It\\^o Density Estimator",
        "author": [
            "Marta Skreta",
            "Lazar Atanackovic",
            "Avishek Joey Bose",
            "Alexander Tong",
            "Kirill Neklyudov"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17762",
        "abstract": "The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. SuperDiff leverages a new scalable Itô density estimator for the log likelihood of the diffusion SDE which incurs no additional overhead compared to the well-known Hutchinson's estimator needed for divergence calculations. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed solely through composition during inference, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical OR and the logical AND. We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, and improved unconditional de novo structure design of proteins. https://github.com/necludov/super-diffusion",
        "tags": [
            "Diffusion",
            "Image Editing",
            "SDE"
        ]
    },
    {
        "id": "251",
        "title": "ResearchTown: Simulator of Human Research Community",
        "author": [
            "Haofei Yu",
            "Zhaochen Hong",
            "Zirui Cheng",
            "Kunlun Zhu",
            "Keyang Xuan",
            "Jinwei Yao",
            "Tao Feng",
            "Jiaxuan You"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17767",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in scientific domains, yet a fundamental question remains unanswered: Can we simulate human research communities with LLMs? Addressing this question can deepen our understanding of the processes behind idea brainstorming and inspire the automatic discovery of novel scientific insights. In this work, we propose ResearchTown, a multi-agent framework for research community simulation. Within this framework, the human research community is simplified and modeled as an agent-data graph, where researchers and papers are represented as agent-type and data-type nodes, respectively, and connected based on their collaboration relationships. We also introduce TextGNN, a text-based inference framework that models various research activities (e.g., paper reading, paper writing, and review writing) as special forms of a unified message-passing process on the agent-data graph. To evaluate the quality of the research simulation, we present ResearchBench, a benchmark that uses a node-masking prediction task for scalable and objective assessment based on similarity. Our experiments reveal three key findings: (1) ResearchTown can provide a realistic simulation of collaborative research activities, including paper writing and review writing; (2) ResearchTown can maintain robust simulation with multiple researchers and diverse papers; (3) ResearchTown can generate interdisciplinary research ideas that potentially inspire novel research directions.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "252",
        "title": "ActiveGS: Active Scene Reconstruction using Gaussian Splatting",
        "author": [
            "Liren Jin",
            "Xingguang Zhong",
            "Yue Pan",
            "Jens Behley",
            "Cyrill Stachniss",
            "Marija Popović"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17769",
        "abstract": "Robotics applications often rely on scene reconstructions to enable downstream tasks. In this work, we tackle the challenge of actively building an accurate map of an unknown scene using an on-board RGB-D camera. We propose a hybrid map representation that combines a Gaussian splatting map with a coarse voxel map, leveraging the strengths of both representations: the high-fidelity scene reconstruction capabilities of Gaussian splatting and the spatial modelling strengths of the voxel map. The core of our framework is an effective confidence modelling technique for the Gaussian splatting map to identify under-reconstructed areas, while utilising spatial information from the voxel map to target unexplored areas and assist in collision-free path planning. By actively collecting scene information in under-reconstructed and unexplored areas for map updates, our approach achieves superior Gaussian splatting reconstruction results compared to state-of-the-art approaches. Additionally, we demonstrate the applicability of our active scene reconstruction framework in the real world using an unmanned aerial vehicle.",
        "tags": [
            "Gaussian Splatting",
            "Robotics"
        ]
    },
    {
        "id": "253",
        "title": "Large Motion Video Autoencoding with Cross-modal Video VAE",
        "author": [
            "Yazhou Xing",
            "Yang Fei",
            "Yingqing He",
            "Jingye Chen",
            "Jiaxin Xie",
            "Xiaowei Chi",
            "Qifeng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17805",
        "abstract": "Learning a robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation can result in temporal inconsistencies and suboptimal compression rates due to a lack of temporal compression. Existing Video VAEs have begun to address temporal compression; however, they often suffer from inadequate reconstruction performance. In this paper, we present a novel and powerful video autoencoder capable of high-fidelity video encoding. First, we observe that entangling spatial and temporal compression by merely extending the image VAE to a 3D VAE can introduce motion blur and detail distortion artifacts. Thus, we propose temporal-aware spatial compression to better encode and decode the spatial information. Additionally, we integrate a lightweight motion compression model for further temporal compression. Second, we propose to leverage the textual information inherent in text-to-video datasets and incorporate text guidance into our model. This significantly enhances reconstruction quality, particularly in terms of detail preservation and temporal stability. Third, we further improve the versatility of our model through joint training on both images and videos, which not only enhances reconstruction quality but also enables the model to perform both image and video autoencoding. Extensive evaluations against strong recent baselines demonstrate the superior performance of our method. The project website can be found at~\\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.",
        "tags": [
            "3D",
            "Text-to-Video",
            "VAE",
            "Video Generation"
        ]
    },
    {
        "id": "254",
        "title": "Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders",
        "author": [
            "Rui Chen",
            "Jianfeng Zhang",
            "Yixun Liang",
            "Guan Luo",
            "Weiyu Li",
            "Jiarui Liu",
            "Xiu Li",
            "Xiaoxiao Long",
            "Jiashi Feng",
            "Ping Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17808",
        "abstract": "Recent 3D content generation pipelines commonly employ Variational Autoencoders (VAEs) to encode shapes into compact latent representations for diffusion-based generation. However, the widely adopted uniform point sampling strategy in Shape VAE training often leads to a significant loss of geometric details, limiting the quality of shape reconstruction and downstream generation tasks. We present Dora-VAE, a novel approach that enhances VAE reconstruction through our proposed sharp edge sampling strategy and a dual cross-attention mechanism. By identifying and prioritizing regions with high geometric complexity during training, our method significantly improves the preservation of fine-grained shape features. Such sampling strategy and the dual attention mechanism enable the VAE to focus on crucial geometric details that are typically missed by uniform sampling approaches. To systematically evaluate VAE reconstruction quality, we additionally propose Dora-bench, a benchmark that quantifies shape complexity through the density of sharp edges, introducing a new metric focused on reconstruction accuracy at these salient geometric features. Extensive experiments on the Dora-bench demonstrate that Dora-VAE achieves comparable reconstruction quality to the state-of-the-art dense XCube-VAE while requiring a latent space at least 8$\\times$ smaller (1,280 vs. > 10,000 codes). We will release our code and benchmark dataset to facilitate future research in 3D shape modeling.",
        "tags": [
            "3D",
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": "255",
        "title": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction",
        "author": [
            "Ziyang Wu",
            "Tianjiao Ding",
            "Yifu Lu",
            "Druv Pai",
            "Jingyuan Zhang",
            "Weida Wang",
            "Yaodong Yu",
            "Yi Ma",
            "Benjamin D. Haeffele"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17810",
        "abstract": "The attention operator is arguably the key distinguishing factor of transformer architectures, which have demonstrated state-of-the-art performance on a variety of tasks. However, transformer attention operators often impose a significant computational burden, with the computational complexity scaling quadratically with the number of tokens. In this work, we propose a novel transformer attention operator whose computational complexity scales linearly with the number of tokens. We derive our network architecture by extending prior work which has shown that a transformer style architecture naturally arises by \"white-box\" architecture design, where each layer of the network is designed to implement an incremental optimization step of a maximal coding rate reduction objective (MCR$^2$). Specifically, we derive a novel variational form of the MCR$^2$ objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention (TSSA). TSSA has linear computational and memory complexity and radically departs from the typical attention architecture that computes pairwise similarities between tokens. Experiments on vision, language, and long sequence tasks show that simply swapping TSSA for standard self-attention, which we refer to as the Token Statistics Transformer (ToST), achieves competitive performance with conventional transformers while being significantly more computationally efficient and interpretable. Our results also somewhat call into question the conventional wisdom that pairwise similarity style attention mechanisms are critical to the success of transformer architectures. Code will be available at https://github.com/RobinWu218/ToST.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "256",
        "title": "ChatGarment: Garment Estimation, Generation and Editing via Large Language Models",
        "author": [
            "Siyuan Bian",
            "Chenghao Xu",
            "Yuliang Xiu",
            "Artur Grigorev",
            "Zhen Liu",
            "Cewu Lu",
            "Michael J. Black",
            "Yao Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17811",
        "abstract": "We introduce ChatGarment, a novel approach that leverages large vision-language models (VLMs) to automate the estimation, generation, and editing of 3D garments from images or text descriptions. Unlike previous methods that struggle in real-world scenarios or lack interactive editing capabilities, ChatGarment can estimate sewing patterns from in-the-wild images or sketches, generate them from text descriptions, and edit garments based on user instructions, all within an interactive dialogue. These sewing patterns can then be draped into 3D garments, which are easily animatable and simulatable. This is achieved by finetuning a VLM to directly generate a JSON file that includes both textual descriptions of garment types and styles, as well as continuous numerical attributes. This JSON file is then used to create sewing patterns through a programming parametric model. To support this, we refine the existing programming model, GarmentCode, by expanding its garment type coverage and simplifying its structure for efficient VLM fine-tuning. Additionally, we construct a large-scale dataset of image-to-sewing-pattern and text-to-sewing-pattern pairs through an automated data pipeline. Extensive evaluations demonstrate ChatGarment's ability to accurately reconstruct, generate, and edit garments from multimodal inputs, highlighting its potential to revolutionize workflows in fashion and gaming applications. Code and data will be available at https://chatgarment.github.io/.",
        "tags": [
            "3D",
            "Large Language Models"
        ]
    },
    {
        "id": "257",
        "title": "FaceLift: Single Image to 3D Head with View Generation and GS-LRM",
        "author": [
            "Weijie Lyu",
            "Yi Zhou",
            "Ming-Hsuan Yang",
            "Zhixin Shu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17812",
        "abstract": "We present FaceLift, a feed-forward approach for rapid, high-quality, 360-degree head reconstruction from a single image. Our pipeline begins by employing a multi-view latent diffusion model that generates consistent side and back views of the head from a single facial input. These generated views then serve as input to a GS-LRM reconstructor, which produces a comprehensive 3D representation using Gaussian splats. To train our system, we develop a dataset of multi-view renderings using synthetic 3D human head as-sets. The diffusion-based multi-view generator is trained exclusively on synthetic head images, while the GS-LRM reconstructor undergoes initial training on Objaverse followed by fine-tuning on synthetic head data. FaceLift excels at preserving identity and maintaining view consistency across views. Despite being trained solely on synthetic data, FaceLift demonstrates remarkable generalization to real-world images. Through extensive qualitative and quantitative evaluations, we show that FaceLift outperforms state-of-the-art methods in 3D head reconstruction, highlighting its practical applicability and robust performance on real-world images. In addition to single image reconstruction, FaceLift supports video inputs for 4D novel view synthesis and seamlessly integrates with 2D reanimation techniques to enable 3D facial animation. Project page: https://weijielyu.github.io/FaceLift.",
        "tags": [
            "3D",
            "Diffusion",
            "Image-to-3D"
        ]
    },
    {
        "id": "258",
        "title": "Mean--Variance Portfolio Selection by Continuous-Time Reinforcement Learning: Algorithms, Regret Analysis, and Empirical Study",
        "author": [
            "Yilie Huang",
            "Yanwei Jia",
            "Xun Yu Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16175",
        "abstract": "We study continuous-time mean--variance portfolio selection in markets where stock prices are diffusion processes driven by observable factors that are also diffusion processes yet the coefficients of these processes are unknown. Based on the recently developed reinforcement learning (RL) theory for diffusion processes, we present a general data-driven RL algorithm that learns the pre-committed investment strategy directly without attempting to learn or estimate the market coefficients. For multi-stock Black--Scholes markets without factors, we further devise a baseline algorithm and prove its performance guarantee by deriving a sublinear regret bound in terms of Sharpe ratio. For performance enhancement and practical implementation, we modify the baseline algorithm into four variants, and carry out an extensive empirical study to compare their performance, in terms of a host of common metrics, with a large number of widely used portfolio allocation strategies on S\\&P 500 constituents. The results demonstrate that the continuous-time RL strategies are consistently among the best especially in a volatile bear market, and decisively outperform the model-based continuous-time counterparts by significant margins.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "259",
        "title": "VirusT5: Harnessing Large Language Models to Predicting SARS-CoV-2 Evolution",
        "author": [
            "Vishwajeet Marathe",
            "Deewan Bajracharya",
            "Changhui Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16262",
        "abstract": "During a virus's evolution,various regions of the genome are subjected to distinct levels of functional http://constraints.Combined with factors like codon bias and DNA repair efficiency,these constraints contribute to unique mutation patterns within the genome or a specific gene. In this project, we harnessed the power of Large Language Models(LLMs) to predict the evolution of SARS-CoV-2. By treating the mutation process from one generation to the next as a translation task, we trained a transformer model, called VirusT5, to capture the mutation patterns underlying SARS-CoV-2 evolution. We evaluated the VirusT5's ability to detect these mutation patterns including its ability to identify mutation hotspots and explored the potential of using VirusT5 to predict future virus variants. Our findings demonstrate the feasibility of using a large language model to model viral evolution as a translation process. This study establishes the groundbreaking concept of \"mutation-as-translation,\" paving the way for new methodologies and tools for combating virus threats",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "260",
        "title": "An explainable operator approximation framework under the guideline of Green's function",
        "author": [
            "Jianghang Gu",
            "Ling Wen",
            "Yuntian Chen",
            "Shiyi Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16644",
        "abstract": "Traditional numerical methods, such as the finite element method and finite volume method, adress partial differential equations (PDEs) by discretizing them into algebraic equations and solving these iteratively. However, this process is often computationally expensive and time-consuming. An alternative approach involves transforming PDEs into integral equations and solving them using Green's functions, which provide analytical solutions. Nevertheless, deriving Green's functions analytically is a challenging and non-trivial task, particularly for complex systems. In this study, we introduce a novel framework, termed GreensONet, which is constructed based on the strucutre of deep operator networks (DeepONet) to learn embedded Green's functions and solve PDEs via Green's integral formulation. Specifically, the Trunk Net within GreensONet is designed to approximate the unknown Green's functions of the system, while the Branch Net are utilized to approximate the auxiliary gradients of the Green's function. These outputs are subsequently employed to perform surface integrals and volume integrals, incorporating user-defined boundary conditions and source terms, respectively. The effectiveness of the proposed framework is demonstrated on three types of PDEs in bounded domains: 3D heat conduction equations, reaction-diffusion equations, and Stokes equations. Comparative results in these cases demonstrate that GreenONet's accuracy and generalization ability surpass those of existing methods, including Physics-Informed Neural Networks (PINN), DeepONet, Physics-Informed DeepONet (PI-DeepONet), and Fourier Neural Operators (FNO).",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "261",
        "title": "Autoregressive Speech Synthesis with Next-Distribution Prediction",
        "author": [
            "Xinfa Zhu",
            "Wenjie Tian",
            "Lei Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2412.16846",
        "abstract": "We introduce KALL-E, a novel autoregressive (AR) language modeling approach with next-distribution prediction for text-to-speech (TTS) synthesis. Unlike existing methods, KALL-E directly models and predicts the continuous speech distribution conditioned on text without relying on VAE- or diffusion-based components. Specifically, we use WaveVAE to extract continuous speech distributions from waveforms instead of using discrete speech tokens. A single AR language model predicts these continuous speech distributions from text, with a Kullback-Leibler divergence loss as the constraint. Experimental results show that KALL-E outperforms open-source implementations of YourTTS, VALL-E, NaturalSpeech 2, and CosyVoice in terms of naturalness and speaker similarity in zero-shot TTS scenarios. Moreover, KALL-E demonstrates exceptional zero-shot capabilities in emotion and accent cloning. Importantly, KALL-E presents a more straightforward and effective paradigm for using continuous speech representations in TTS. Audio samples are available at: \\url{https://zxf-icpc.github.io/kalle/}.",
        "tags": [
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": "262",
        "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
        "author": [
            "Hankun Wang",
            "Haoran Wang",
            "Yiwei Guo",
            "Zhihan Li",
            "Chenpeng Du",
            "Xie Chen",
            "Kai Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17048",
        "abstract": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "263",
        "title": "An Investigation on the Potential of KAN in Speech Enhancement",
        "author": [
            "Haoyang Li",
            "Yuchen Hu",
            "Chen Chen",
            "Eng Siong Chng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.17778",
        "abstract": "High-fidelity speech enhancement often requires sophisticated modeling to capture intricate, multiscale patterns. Standard activation functions, while introducing nonlinearity, lack the flexibility to fully address this complexity. Kolmogorov-Arnold Networks (KAN), an emerging methodology that employs learnable activation functions on graph edges, present a promising alternative. This work investigates two novel KAN variants based on rational and radial basis functions for speech enhancement. We integrate the rational variant into the 1D CNN blocks of Demucs and the GRU-Transformer blocks of MP-SENet, while the radial variant is adapted to the 2D CNN-based decoders of MP-SENet. Experiments on the VoiceBank-DEMAND dataset show that replacing standard activations with KAN-based activations improves speech quality across both the time-domain and time-frequency domain methods with minimal impact on model size and FLOP, underscoring KAN's potential to improve speech enhancement models.",
        "tags": [
            "KAN",
            "Kolmogorov-Arnold Networks",
            "Transformer"
        ]
    }
]