[
    {
        "id": "1",
        "title": "SNR-EQ-JSCC: Joint Source-Channel Coding with SNR-Based Embedding and Query",
        "author": [
            "Hongwei Zhang",
            "Meixia Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04732",
        "abstract": "Coping with the impact of dynamic channels is a critical issue in joint source-channel coding (JSCC)-based semantic communication systems. In this paper, we propose a lightweight channel-adaptive semantic coding architecture called SNR-EQ-JSCC. It is built upon the generic Transformer model and achieves channel adaptation (CA) by Embedding the signal-to-noise ratio (SNR) into the attention blocks and dynamically adjusting attention scores through channel-adaptive Queries. Meanwhile, penalty terms are introduced in the loss function to stabilize the training process. Considering that instantaneous SNR feedback may be imperfect, we propose an alternative method that uses only the average SNR, which requires no retraining of SNR-EQ-JSCC. Simulation results conducted on image transmission demonstrate that the proposed SNR-EQJSCC outperforms the state-of-the-art SwinJSCC in peak signal-to-noise ratio (PSNR) and perception metrics while only requiring 0.05% of the storage overhead and 6.38% of the computational complexity for CA. Moreover, the channel-adaptive query method demonstrates significant improvements in perception metrics. When instantaneous SNR feedback is imperfect, SNR-EQ-JSCC using only the average SNR still surpasses baseline schemes.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "2",
        "title": "Improving Human-Robot Teaching by Quantifying and Reducing Mental Model Mismatch",
        "author": [
            "Phillip Richter",
            "Heiko Wersing",
            "Anna-Lisa Vollmer"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04755",
        "abstract": "The rapid development of artificial intelligence and robotics has had a significant impact on our lives, with intelligent systems increasingly performing tasks traditionally performed by humans. Efficient knowledge transfer requires matching the mental model of the human teacher with the capabilities of the robot learner. This paper introduces the Mental Model Mismatch (MMM) Score, a feedback mechanism designed to quantify and reduce mismatches by aligning human teaching behavior with robot learning behavior. Using Large Language Models (LLMs), we analyze teacher intentions in natural language to generate adaptive feedback. A study with 150 participants teaching a virtual robot to solve a puzzle game shows that intention-based feedback significantly outperforms traditional performance-based feedback or no feedback. The results suggest that intention-based feedback improves instructional outcomes, improves understanding of the robot's learning process and reduces misconceptions. This research addresses a critical gap in human-robot interaction (HRI) by providing a method to quantify and mitigate discrepancies between human mental models and robot capabilities, with the goal of improving robot learning and human teaching effectiveness.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Robot",
            "Robotics"
        ]
    },
    {
        "id": "3",
        "title": "DAREK -- Distance Aware Error for Kolmogorov Networks",
        "author": [
            "Masoud Ataei",
            "Mohammad Javad Khojasteh",
            "Vikas Dhiman"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04757",
        "abstract": "In this paper, we provide distance-aware error bounds for Kolmogorov Arnold Networks (KANs). We call our new error bounds estimator DAREK -- Distance Aware Error for Kolmogorov networks. Z. Liu et al. provide error bounds, which may be loose, lack distance-awareness, and are defined only up to an unknown constant of proportionality. We review the error bounds for Newton's polynomial, which is then generalized to an arbitrary spline, under Lipschitz continuity assumptions. We then extend these bounds to nested compositions of splines, arriving at error bounds for KANs. We evaluate our method by estimating an object's shape from sparse laser scan points. We use KAN to fit a smooth function to the scans and provide error bounds for the fit. We find that our method is faster than Monte Carlo approaches, and that our error bounds enclose the true obstacle shape reliably.",
        "tags": [
            "KAN",
            "Kolmogorov-Arnold Networks"
        ]
    },
    {
        "id": "4",
        "title": "TREAD: Token Routing for Efficient Architecture-agnostic Diffusion Training",
        "author": [
            "Felix Krause",
            "Timy Phan",
            "Vincent Tao Hu",
            "Bj√∂rn Ommer"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04765",
        "abstract": "Diffusion models have emerged as the mainstream approach for visual generation. However, these models usually suffer from sample inefficiency and high training costs. This issue is particularly pronounced in the standard diffusion transformer architecture due to its quadratic complexity relative to input length. Recent works have addressed this by reducing the number of tokens processed in the model, often through masking. In contrast, this work aims to improve the training efficiency of the diffusion backbone by using predefined routes that store this information until it is reintroduced to deeper layers of the model, rather than discarding these tokens entirely. Further, we combine multiple routes and introduce an adapted auxiliary loss that accounts for all applied routes. Our method is not limited to the common transformer-based model - it can also be applied to state-space models. Unlike most current approaches, TREAD achieves this without architectural modifications. Finally, we show that our method reduces the computational cost and simultaneously boosts model performance on the standard benchmark ImageNet-1K 256 x 256 in class-conditional synthesis. Both of these benefits multiply to a convergence speedup of 9.55x at 400K training iterations compared to DiT and 25.39x compared to the best benchmark performance of DiT at 7M training iterations.",
        "tags": [
            "DiT",
            "Diffusion",
            "Diffusion Transformer",
            "State Space Models",
            "Transformer"
        ]
    },
    {
        "id": "5",
        "title": "GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting",
        "author": [
            "Andrew Bond",
            "Jui-Hsien Wang",
            "Long Mai",
            "Erkut Erdem",
            "Aykut Erdem"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04782",
        "abstract": "Efficient neural representations for dynamic video scenes are critical for applications ranging from video compression to interactive simulations. Yet, existing methods often face challenges related to high memory usage, lengthy training times, and temporal consistency. To address these issues, we introduce a novel neural video representation that combines 3D Gaussian splatting with continuous camera motion modeling. By leveraging Neural ODEs, our approach learns smooth camera trajectories while maintaining an explicit 3D scene representation through Gaussians. Additionally, we introduce a spatiotemporal hierarchical learning strategy, progressively refining spatial and temporal features to enhance reconstruction quality and accelerate convergence. This memory-efficient approach achieves high-quality rendering at impressive speeds. Experimental results show that our hierarchical learning, combined with robust camera motion modeling, captures complex dynamic scenes with strong temporal consistency, achieving state-of-the-art performance across diverse video datasets in both high- and low-motion scenarios.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "6",
        "title": "On the Impact of Requirements Smells in Prompts: The Case of Automated Traceability",
        "author": [
            "Andreas Vogelsang",
            "Alexander Korn",
            "Giovanna Broccia",
            "Alessio Ferrari",
            "Jannik Fischbach",
            "Chetan Arora"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04810",
        "abstract": "Large language models (LLMs) are increasingly used to generate software artifacts, such as source code, tests, and trace links. Requirements play a central role in shaping the input prompts that guide LLMs, as they are often used as part of the prompts to synthesize the artifacts. However, the impact of requirements formulation on LLM performance remains unclear. In this paper, we investigate the role of requirements smells-indicators of potential issues like ambiguity and inconsistency-when used in prompts for LLMs. We conducted experiments using two LLMs focusing on automated trace link generation between requirements and code. Our results show mixed outcomes: while requirements smells had a small but significant effect when predicting whether a requirement was implemented in a piece of code (i.e., a trace link exists), no significant effect was observed when tracing the requirements with the associated lines of code. These findings suggest that requirements smells can affect LLM performance in certain SE tasks but may not uniformly impact all tasks. We highlight the need for further research to understand these nuances and propose future work toward developing guidelines for mitigating the negative effects of requirements smells in AI-driven SE processes.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "7",
        "title": "Building Foundations for Natural Language Processing of Historical Turkish: Resources and Models",
        "author": [
            "≈ûaziye Bet√ºl √ñzate≈ü",
            "Tarƒ±k Emre Tƒ±ra≈ü",
            "Ece Elif Adak",
            "Berat Doƒüan",
            "Fatih Burak Karag√∂z",
            "Efe Eren Gen√ß",
            "Esma F. Bilgin Ta≈üdemir"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04828",
        "abstract": "This paper introduces foundational resources and models for natural language processing (NLP) of historical Turkish, a domain that has remained underexplored in computational linguistics. We present the first named entity recognition (NER) dataset, HisTR and the first Universal Dependencies treebank, OTA-BOUN for a historical form of the Turkish language along with transformer-based models trained using these datasets for named entity recognition, dependency parsing, and part-of-speech tagging tasks. Additionally, we introduce Ottoman Text Corpus (OTC), a clean corpus of transliterated historical Turkish texts that spans a wide range of historical periods. Our experimental results show significant improvements in the computational analysis of historical Turkish, achieving promising results in tasks that require understanding of historical linguistic structures. They also highlight existing challenges, such as domain adaptation and language variations across time periods. All of the presented resources and models are made available at https://huggingface.co/bucolin to serve as a benchmark for future progress in historical Turkish NLP.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "8",
        "title": "ActPC-Geom: Towards Scalable Online Neural-Symbolic Learning via Accelerating Active Predictive Coding with Information Geometry & Diverse Cognitive Mechanisms",
        "author": [
            "Ben Goertzel"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04832",
        "abstract": "This paper introduces ActPC-Geom, an approach to accelerate Active Predictive Coding (ActPC) in neural networks by integrating information geometry, specifically using Wasserstein-metric-based methods for measure-dependent gradient flows. We propose replacing KL-divergence in ActPC's predictive error assessment with the Wasserstein metric, suggesting this may enhance network robustness.\nTo make this computationally feasible, we present strategies including: (1) neural approximators for inverse measure-dependent Laplacians, (2) approximate kernel PCA embeddings for low-rank approximations feeding into these approximators, and (3) compositional hypervector embeddings derived from kPCA outputs, with algebra optimized for fuzzy FCA lattices learned through neural architectures analyzing network states.\nThis results in an ActPC architecture capable of real-time online learning and integrating continuous (e.g., transformer-like or Hopfield-net-like) and discrete symbolic ActPC networks, including frameworks like OpenCog Hyperon or ActPC-Chem for algorithmic chemistry evolution. Shared probabilistic, concept-lattice, and hypervector models enable symbolic-subsymbolic integration.\nKey features include (1) compositional reasoning via hypervector embeddings in transformer-like architectures for tasks like commonsense reasoning, and (2) Hopfield-net dynamics enabling associative long-term memory and attractor-driven cognitive features.\nWe outline how ActPC-Geom combines few-shot learning with online weight updates, enabling deliberative thinking and seamless symbolic-subsymbolic reasoning. Ideas from Galois connections are explored for efficient hybrid ActPC/ActPC-Chem processing. Finally, we propose a specialized HPC design optimized for real-time focused attention and deliberative reasoning tailored to ActPC-Geom's demands.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "9",
        "title": "Do Code LLMs Understand Design Patterns?",
        "author": [
            "Zhenyu Pan",
            "Xuefeng Song",
            "Yunkun Wang",
            "Rongyu Cao",
            "Binhua Li",
            "Yongbin Li",
            "Han Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04835",
        "abstract": "Code Large Language Models (LLMs) demonstrate great versatility in adapting to various downstream tasks, including code generation and completion, as well as bug detection and fixing. However, Code LLMs often fail to capture existing coding standards, leading to the generation of code that conflicts with the required design patterns for a given project. As a result, developers must post-process to adapt the generated code to the project's design norms. In this work, we empirically investigate the biases of Code LLMs in software development. Through carefully designed experiments, we assess the models' understanding of design patterns across recognition, comprehension, and generation. Our findings reveal that biases in Code LLMs significantly affect the reliability of downstream tasks.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "10",
        "title": "EDMB: Edge Detector with Mamba",
        "author": [
            "Yachuan Li",
            "Xavier Soria Poma",
            "Yun Bai",
            "Qian Xiao",
            "Chaozhi Yang",
            "Guanlin Li",
            "Zongmin Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04846",
        "abstract": "Transformer-based models have made significant progress in edge detection, but their high computational cost is prohibitive. Recently, vision Mamba have shown excellent ability in efficiently capturing long-range dependencies. Drawing inspiration from this, we propose a novel edge detector with Mamba, termed EDMB, to efficiently generate high-quality multi-granularity edges. In EDMB, Mamba is combined with a global-local architecture, therefore it can focus on both global information and fine-grained cues. The fine-grained cues play a crucial role in edge detection, but are usually ignored by ordinary Mamba. We design a novel decoder to construct learnable Gaussian distributions by fusing global features and fine-grained features. And the multi-grained edges are generated by sampling from the distributions. In order to make multi-granularity edges applicable to single-label data, we introduce Evidence Lower Bound loss to supervise the learning of the distributions. On the multi-label dataset BSDS500, our proposed EDMB achieves competitive single-granularity ODS 0.837 and multi-granularity ODS 0.851 without multi-scale test or extra PASCAL-VOC data. Remarkably, EDMB can be extended to single-label datasets such as NYUDv2 and BIPED. The source code is available at https://github.com/Li-yachuan/EDMB.",
        "tags": [
            "Detection",
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "11",
        "title": "Exploring Large Language Models for Semantic Analysis and Categorization of Android Malware",
        "author": [
            "Brandon J Walton",
            "Mst Eshita Khatun",
            "James M Ghawaly",
            "Aisha Ali-Gombe"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04848",
        "abstract": "Malware analysis is a complex process of examining and evaluating malicious software's functionality, origin, and potential impact. This arduous process typically involves dissecting the software to understand its components, infection vector, propagation mechanism, and payload. Over the years, deep reverse engineering of malware has become increasingly tedious, mainly due to modern malicious codebases' fast evolution and sophistication. Essentially, analysts are tasked with identifying the elusive needle in the haystack within the complexities of zero-day malware, all while under tight time constraints. Thus, in this paper, we explore leveraging Large Language Models (LLMs) for semantic malware analysis to expedite the analysis of known and novel samples. Built on GPT-4o-mini model, \\msp is designed to augment malware analysis for Android through a hierarchical-tiered summarization chain and strategic prompt engineering. Additionally, \\msp performs malware categorization, distinguishing potential malware from benign applications, thereby saving time during the malware reverse engineering process. Despite not being fine-tuned for Android malware analysis, we demonstrate that through optimized and advanced prompt engineering \\msp can achieve up to 77% classification accuracy while providing highly robust summaries at functional, class, and package levels. In addition, leveraging the backward tracing of the summaries from package to function levels allowed us to pinpoint the precise code snippets responsible for malicious behavior.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "12",
        "title": "Advancing Retrieval-Augmented Generation for Persian: Development of Language Models, Comprehensive Benchmarks, and Best Practices for Optimization",
        "author": [
            "Sara Bourbour Hosseinbeigi",
            "Sina Asghari",
            "Mohammad Ali Seif Kashani",
            "Mohammad Hossein Shalchian",
            "Mohammad Amin Abbasi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04858",
        "abstract": "This paper examines the specific obstacles of constructing Retrieval-Augmented Generation(RAG) systems in low-resource languages, with a focus on Persian's complicated morphology and versatile syntax. The research aims to improve retrieval and generation accuracy by introducing Persian-specific models, namely MatinaRoberta(a masked language model) and MatinaSRoberta(a fine-tuned Sentence-BERT), along with a comprehensive benchmarking framework. Three datasets-general knowledge(PQuad), scientifically specialized texts, and organizational reports, were used to assess these models after they were trained on a varied corpus of 73.11 billion Persian tokens. The methodology involved extensive pretraining, fine-tuning with tailored loss functions, and systematic evaluations using both traditional metrics and the Retrieval-Augmented Generation Assessment framework. The results show that MatinaSRoberta outperformed previous embeddings, achieving superior contextual relevance and retrieval accuracy across datasets. Temperature tweaking, chunk size modifications, and document summary indexing were explored to enhance RAG setups. Larger models like Llama-3.1 (70B) consistently demonstrated the highest generation accuracy, while smaller models faced challenges with domain-specific and formal contexts. The findings underscore the potential for developing RAG systems in Persian through customized embeddings and retrieval-generation settings and highlight the enhancement of NLP applications such as search engines and legal document analysis in low-resource languages.",
        "tags": [
            "BERT",
            "LLaMA",
            "RAG"
        ]
    },
    {
        "id": "13",
        "title": "Real-Time Textless Dialogue Generation",
        "author": [
            "Long Mai",
            "Julie Carson-Berndsen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04877",
        "abstract": "Recent advancements in large language models (LLMs) have led to significant progress in text-based dialogue systems. These systems can now generate high-quality responses that are accurate and coherent across a wide range of topics and tasks. However, spoken dialogue systems still lag behind in terms of naturalness. They tend to produce robotic interactions, with issues such as slow response times, overly generic or cautious replies, and a lack of natural rhythm and fluid turn-taking. This shortcoming is largely due to the over-reliance on the traditional cascaded design, which involve separate, sequential components, as well as the use of text as an intermediate representation. This paper propose a real-time, textless spoken dialogue generation model (RTTL-DG) that aims to overcome these challenges. Our system enables fluid turn-taking and generates responses with minimal delay by processing streaming spoken conversation directly. Additionally, our model incorporates backchannels, filters, laughter, and other paralinguistic signals, which are often absent in cascaded dialogue systems, to create more natural and human-like interactions. The implementations and generated samples are available in our repository: https://github.com/mailong25/rts2s-dg",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "14",
        "title": "Leveraging Log Probabilities in Language Models to Forecast Future Events",
        "author": [
            "Tommaso Soru",
            "Jim Marshall"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04880",
        "abstract": "In the constantly changing field of data-driven decision making, accurately predicting future events is crucial for strategic planning in various sectors. The emergence of Large Language Models (LLMs) marks a significant advancement in this area, offering advanced tools that utilise extensive text data for prediction. In this industry paper, we introduce a novel method for AI-driven foresight using LLMs. Building on top of previous research, we employ data on current trends and their trajectories for generating forecasts on 15 different topics. Subsequently, we estimate their probabilities via a multi-step approach based on log probabilities. We show we achieve a Brier score of 0.186, meaning a +26% improvement over random chance and a +19% improvement over widely-available AI systems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "15",
        "title": "SUGAR: Leveraging Contextual Confidence for Smarter Retrieval",
        "author": [
            "Hanna Zubkova",
            "Ji-Hoon Park",
            "Seong-Whan Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04899",
        "abstract": "Bearing in mind the limited parametric knowledge of Large Language Models (LLMs), retrieval-augmented generation (RAG) which supplies them with the relevant external knowledge has served as an approach to mitigate the issue of hallucinations to a certain extent. However, uniformly retrieving supporting context makes response generation source-inefficient, as triggering the retriever is not always necessary, or even inaccurate, when a model gets distracted by noisy retrieved content and produces an unhelpful answer. Motivated by these issues, we introduce Semantic Uncertainty Guided Adaptive Retrieval (SUGAR), where we leverage context-based entropy to actively decide whether to retrieve and to further determine between single-step and multi-step retrieval. Our empirical results show that selective retrieval guided by semantic uncertainty estimation improves the performance across diverse question answering tasks, as well as achieves a more efficient inference.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "16",
        "title": "ThriftLLM: On Cost-Effective Selection of Large Language Models for Classification Queries",
        "author": [
            "Keke Huang",
            "Yimin Shi",
            "Dujian Ding",
            "Yifei Li",
            "Yang Fei",
            "Laks Lakshmanan",
            "Xiaokui Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04901",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in comprehending and generating natural language content, attracting widespread popularity in both industry and academia in recent years. An increasing number of services have sprung up which offer LLMs for various tasks via APIs. Different LLMs demonstrate expertise in different domains of queries (e.g., text classification queries). Meanwhile, LLMs of different scales, complexity, and performance are priced diversely. Driven by this observation, a growing number of researchers are investigating the LLM ensemble strategy with a focus on cost-effectiveness, aiming to decrease overall usage costs while enhancing performance. However, to the best of our knowledge, none of the existing works addresses the problem, i.e., how to find an LLM ensemble subject to a cost budget, which maximizes the ensemble performance.\nIn this paper, we formalize the performance of an ensemble of models (LLMs) using the notion of prediction accuracy which we formally define. We develop an approach for aggregating responses from multiple LLMs to enhance ensemble performance. Building on this, we formulate the ensemble selection problem as that of selecting a set of LLMs subject to a cost budget such that the overall prediction accuracy is maximized. We theoretically establish the non-decreasing and non-submodular properties of the prediction accuracy function and provide evidence that the Optimal Ensemble Selection problem is likely to be NP-hard. Subsequently, we apply dynamic programming and propose an algorithm called ThriftLLM. We prove that ThriftLLM achieves a near-optimal approximation guarantee. In addition, it achieves state-of-the-art query performance on multiple real-world datasets against 3 competitors in our extensive experimental evaluation, strongly supporting the effectiveness and superiority of our method.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "17",
        "title": "JELLY: Joint Emotion Recognition and Context Reasoning with LLMs for Conversational Speech Synthesis",
        "author": [
            "Jun-Hyeok Cha",
            "Seung-Bin Kim",
            "Hyung-Seok Oh",
            "Seong-Whan Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04904",
        "abstract": "Recently, there has been a growing demand for conversational speech synthesis (CSS) that generates more natural speech by considering the conversational context. To address this, we introduce JELLY, a novel CSS framework that integrates emotion recognition and context reasoning for generating appropriate speech in conversation by fine-tuning a large language model (LLM) with multiple partial LoRA modules. We propose an Emotion-aware Q-former encoder, which enables the LLM to perceive emotions in speech. The encoder is trained to align speech emotions with text, utilizing datasets of emotional speech. The entire model is then fine-tuned with conversational speech data to infer emotional context for generating emotionally appropriate speech in conversation. Our experimental results demonstrate that JELLY excels in emotional context modeling, synthesizing speech that naturally aligns with conversation, while mitigating the scarcity of emotional conversational speech datasets.",
        "tags": [
            "LLMs",
            "LoRA"
        ]
    },
    {
        "id": "18",
        "title": "HaVen: Hallucination-Mitigated LLM for Verilog Code Generation Aligned with HDL Engineers",
        "author": [
            "Yiyao Yang",
            "Fu Teng",
            "Pengju Liu",
            "Mengnan Qi",
            "Chenyang Lv",
            "Ji Li",
            "Xuhong Zhang",
            "Zhezhi He"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04908",
        "abstract": "Recently, the use of large language models (LLMs) for Verilog code generation has attracted great research interest to enable hardware design automation. However, previous works have shown a gap between the ability of LLMs and the practical demands of hardware description language (HDL) engineering. This gap includes differences in how engineers phrase questions and hallucinations in the code generated. To address these challenges, we introduce HaVen, a novel LLM framework designed to mitigate hallucinations and align Verilog code generation with the practices of HDL engineers. HaVen tackles hallucination issues by proposing a comprehensive taxonomy and employing a chain-of-thought (CoT) mechanism to translate symbolic modalities (e.g. truth tables, state diagrams, etc.) into accurate natural language descriptions. Furthermore, HaVen bridges this gap by using a data augmentation strategy. It synthesizes high-quality instruction-code pairs that match real HDL engineering practices. Our experiments demonstrate that HaVen significantly improves the correctness of Verilog code generation, outperforming state-of-the-art LLM-based Verilog generation methods on VerilogEval and RTLLM benchmark. HaVen is publicly available at https://github.com/Intelligent-Computing-Research-Group/HaVen.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "19",
        "title": "From Mesh Completion to AI Designed Crown",
        "author": [
            "Golriz Hosseinimanesh",
            "Farnoosh Ghadiri",
            "Francois Guibault",
            "Farida Cheriet",
            "Julia Keren"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04914",
        "abstract": "Designing a dental crown is a time-consuming and labor intensive process. Our goal is to simplify crown design and minimize the tediousness of making manual adjustments while still ensuring the highest level of accuracy and consistency. To this end, we present a new end- to-end deep learning approach, coined Dental Mesh Completion (DMC), to generate a crown mesh conditioned on a point cloud context. The dental context includes the tooth prepared to receive a crown and its surroundings, namely the two adjacent teeth and the three closest teeth in the opposing jaw. We formulate crown generation in terms of completing this point cloud context. A feature extractor first converts the input point cloud into a set of feature vectors that represent local regions in the point cloud. The set of feature vectors is then fed into a transformer to predict a new set of feature vectors for the missing region (crown). Subsequently, a point reconstruction head, followed by a multi-layer perceptron, is used to predict a dense set of points with normals. Finally, a differentiable point-to-mesh layer serves to reconstruct the crown surface mesh. We compare our DMC method to a graph-based convolutional neural network which learns to deform a crown mesh from a generic crown shape to the target geometry. Extensive experiments on our dataset demonstrate the effectiveness of our method, which attains an average of 0.062 Chamfer http://Distance.The code is available at:https://github.com/Golriz-code/DMC.gi",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "20",
        "title": "Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency",
        "author": [
            "Shiji Zhao",
            "Ranjie Duan",
            "Fengxiang Wang",
            "Chi Chen",
            "Caixin Kang",
            "Jialing Tao",
            "YueFeng Chen",
            "Hui Xue",
            "Xingxing Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04931",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive performance and have been put into practical use in commercial applications, but they still have potential safety mechanism vulnerabilities. Jailbreak attacks are red teaming methods that aim to bypass safety mechanisms and discover MLLMs' potential risks. Existing MLLMs' jailbreak methods often bypass the model's safety mechanism through complex optimization methods or carefully designed image and text prompts. Despite achieving some progress, they have a low attack success rate on commercial closed-source MLLMs. Unlike previous research, we empirically find that there exists a Shuffle Inconsistency between MLLMs' comprehension ability and safety ability for the shuffled harmful instruction. That is, from the perspective of comprehension ability, MLLMs can understand the shuffled harmful text-image instructions well. However, they can be easily bypassed by the shuffled harmful instructions from the perspective of safety ability, leading to harmful responses. Then we innovatively propose a text-image jailbreak attack named SI-Attack. Specifically, to fully utilize the Shuffle Inconsistency and overcome the shuffle randomness, we apply a query-based black-box optimization method to select the most harmful shuffled inputs based on the feedback of the toxic judge model. A series of experiments show that SI-Attack can improve the attack's performance on three benchmarks. In particular, SI-Attack can obviously improve the attack success rate for commercial MLLMs such as GPT-4o or Claude-3.5-Sonnet.",
        "tags": [
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "21",
        "title": "Multi-Context Temporal Consistent Modeling for Referring Video Object Segmentation",
        "author": [
            "Sun-Hyuk Choi",
            "Hayoung Jo",
            "Seong-Whan Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04939",
        "abstract": "Referring video object segmentation aims to segment objects within a video corresponding to a given text description. Existing transformer-based temporal modeling approaches face challenges related to query inconsistency and the limited consideration of context. Query inconsistency produces unstable masks of different objects in the middle of the video. The limited consideration of context leads to the segmentation of incorrect objects by failing to adequately account for the relationship between the given text and instances. To address these issues, we propose the Multi-context Temporal Consistency Module (MTCM), which consists of an Aligner and a Multi-Context Enhancer (MCE). The Aligner removes noise from queries and aligns them to achieve query consistency. The MCE predicts text-relevant queries by considering multi-context. We applied MTCM to four different models, increasing performance across all of them, particularly achieving 47.6 J&F on the MeViS. Code is available at https://github.com/Choi58/MTCM.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "22",
        "title": "MambaHSI: Spatial-Spectral Mamba for Hyperspectral Image Classification",
        "author": [
            "Yapeng Li",
            "Yong Luo",
            "Lefei Zhang",
            "Zengmao Wang",
            "Bo Du"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04944",
        "abstract": "Transformer has been extensively explored for hyperspectral image (HSI) classification. However, transformer poses challenges in terms of speed and memory usage because of its quadratic computational complexity. Recently, the Mamba model has emerged as a promising approach, which has strong long-distance modeling capabilities while maintaining a linear computational complexity. However, representing the HSI is challenging for the Mamba due to the requirement for an integrated spatial and spectral understanding. To remedy these drawbacks, we propose a novel HSI classification model based on a Mamba model, named MambaHSI, which can simultaneously model long-range interaction of the whole image and integrate spatial and spectral information in an adaptive manner. Specifically, we design a spatial Mamba block (SpaMB) to model the long-range interaction of the whole image at the pixel-level. Then, we propose a spectral Mamba block (SpeMB) to split the spectral vector into multiple groups, mine the relations across different spectral groups, and extract spectral features. Finally, we propose a spatial-spectral fusion module (SSFM) to adaptively integrate spatial and spectral features of a HSI. To our best knowledge, this is the first image-level HSI classification model based on the Mamba. We conduct extensive experiments on four diverse HSI datasets. The results demonstrate the effectiveness and superiority of the proposed model for HSI classification. This reveals the great potential of Mamba to be the next-generation backbone for HSI models. Codes are available at https://github.com/li-yapeng/MambaHSI .",
        "tags": [
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "23",
        "title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models",
        "author": [
            "Qingyu Ren",
            "Jie Zeng",
            "Qianyu He",
            "Jiaqing Liang",
            "Yanghua Xiao",
            "Weikang Zhou",
            "Zeye Sun",
            "Fei Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04945",
        "abstract": "It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, soft constraints are semantically related and difficult to verify through automated methods. These constraints remain a significant challenge for LLMs. To enhance the ability of LLMs to follow soft constraints, we initially design a pipeline to obtain high-quality outputs automatically. Additionally, to fully utilize the acquired data, we introduce a training paradigm based on curriculum learning. We experimentally evaluate the effectiveness of our methods in improving LLMs' soft constraint following ability and analyze the factors driving the improvements. The datasets and code are publicly available at https://github.com/Rainier-rq/FollowSoftConstraints.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "24",
        "title": "Seeing with Partial Certainty: Conformal Prediction for Robotic Scene Recognition in Built Environments",
        "author": [
            "Yifan Xu",
            "Vineet Kamat",
            "Carol Menassa"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04947",
        "abstract": "In assistive robotics serving people with disabilities (PWD), accurate place recognition in built environments is crucial to ensure that robots navigate and interact safely within diverse indoor spaces. Language interfaces, particularly those powered by Large Language Models (LLM) and Vision Language Models (VLM), hold significant promise in this context, as they can interpret visual scenes and correlate them with semantic information. However, such interfaces are also known for their hallucinated predictions. In addition, language instructions provided by humans can also be ambiguous and lack precise details about specific locations, objects, or actions, exacerbating the hallucination issue. In this work, we introduce Seeing with Partial Certainty (SwPC) - a framework designed to measure and align uncertainty in VLM-based place recognition, enabling the model to recognize when it lacks confidence and seek assistance when necessary. This framework is built on the theory of conformal prediction to provide statistical guarantees on place recognition while minimizing requests for human help in complex indoor environment settings. Through experiments on the widely used richly-annotated scene dataset Matterport3D, we show that SwPC significantly increases the success rate and decreases the amount of human intervention required relative to the prior art. SwPC can be utilized with any VLMs directly without requiring model fine-tuning, offering a promising, lightweight approach to uncertainty modeling that complements and scales alongside the expanding capabilities of foundational models.",
        "tags": [
            "Large Language Models",
            "Robotics"
        ]
    },
    {
        "id": "25",
        "title": "Demystifying Domain-adaptive Post-training for Financial LLMs",
        "author": [
            "Zixuan Ke",
            "Yifei Ming",
            "Xuan-Phi Nguyen",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04961",
        "abstract": "Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain. Our approach begins by identifying the core capabilities required for the target domain and designing a comprehensive evaluation suite aligned with these needs. We then analyze the effectiveness of key post-training stages, including continual pretraining, instruction tuning, and preference alignment. Building on these insights, we propose an effective training recipe centered on a novel preference data distillation method, which leverages process signals from a generative reward model. The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs. Project page: https://github.com/SalesforceAIResearch/FinDap",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "26",
        "title": "V2C-CBM: Building Concept Bottlenecks with Vision-to-Concept Tokenizer",
        "author": [
            "Hangzhou He",
            "Lei Zhu",
            "Xinliang Zhang",
            "Shuang Zeng",
            "Qian Chen",
            "Yanye Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04975",
        "abstract": "Concept Bottleneck Models (CBMs) offer inherent interpretability by initially translating images into human-comprehensible concepts, followed by a linear combination of these concepts for classification. However, the annotation of concepts for visual recognition tasks requires extensive expert knowledge and labor, constraining the broad adoption of CBMs. Recent approaches have leveraged the knowledge of large language models to construct concept bottlenecks, with multimodal models like CLIP subsequently mapping image features into the concept feature space for classification. Despite this, the concepts produced by language models can be verbose and may introduce non-visual attributes, which hurts accuracy and interpretability. In this study, we investigate to avoid these issues by constructing CBMs directly from multimodal models. To this end, we adopt common words as base concept vocabulary and leverage auxiliary unlabeled images to construct a Vision-to-Concept (V2C) tokenizer that can explicitly quantize images into their most relevant visual concepts, thus creating a vision-oriented concept bottleneck tightly coupled with the multimodal model. This leads to our V2C-CBM which is training efficient and interpretable with high accuracy. Our V2C-CBM has matched or outperformed LLM-supervised CBMs on various visual classification benchmarks, validating the efficacy of our approach.",
        "tags": [
            "CLIP",
            "Large Language Models"
        ]
    },
    {
        "id": "27",
        "title": "CuRLA: Curriculum Learning Based Deep Reinforcement Learning for Autonomous Driving",
        "author": [
            "Bhargava Uppuluri",
            "Anjel Patel",
            "Neil Mehta",
            "Sridhar Kamath",
            "Pratyush Chakraborty"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04982",
        "abstract": "In autonomous driving, traditional Computer Vision (CV) agents often struggle in unfamiliar situations due to biases in the training data. Deep Reinforcement Learning (DRL) agents address this by learning from experience and maximizing rewards, which helps them adapt to dynamic environments. However, ensuring their generalization remains challenging, especially with static training environments. Additionally, DRL models lack transparency, making it difficult to guarantee safety in all scenarios, particularly those not seen during training. To tackle these issues, we propose a method that combines DRL with Curriculum Learning for autonomous driving. Our approach uses a Proximal Policy Optimization (PPO) agent and a Variational Autoencoder (VAE) to learn safe driving in the CARLA simulator. The agent is trained using two-fold curriculum learning, progressively increasing environment difficulty and incorporating a collision penalty in the reward function to promote safety. This method improves the agent's adaptability and reliability in complex environments, and understand the nuances of balancing multiple reward components from different feedback signals in a single scalar reward function. Keywords: Computer Vision, Deep Reinforcement Learning, Variational Autoencoder, Proximal Policy Optimization, Curriculum Learning, Autonomous Driving.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "28",
        "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
        "author": [
            "Ziwei He",
            "Jian Yuan",
            "Haoli Bai",
            "Jingwen Leng",
            "Bo Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04987",
        "abstract": "Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. It consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\\% of the budget at optimal efficiency.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "29",
        "title": "UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation",
        "author": [
            "Oleg Sautenkov",
            "Yasheerah Yaqoot",
            "Artem Lykov",
            "Muhammad Ahsan Mustafa",
            "Grik Tadevosyan",
            "Aibek Akhmetkazy",
            "Miguel Altamirano Cabrera",
            "Mikhail Martynov",
            "Sausar Karaf",
            "Dzmitry Tsetserukou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05014",
        "abstract": "The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate communication with aerial robots. By integrating satellite imagery processing with the Visual Language Model (VLM) and the powerful capabilities of GPT, UAV-VLA enables users to generate general flight paths-and-action plans through simple text requests. This system leverages the rich contextual information provided by satellite images, allowing for enhanced decision-making and mission planning. The combination of visual analysis by VLM and natural language processing by GPT can provide the user with the path-and-action set, making aerial operations more efficient and accessible. The newly developed method showed the difference in the length of the created trajectory in 22% and the mean error in finding the objects of interest on a map in 34.22 m by Euclidean distance in the K-Nearest Neighbors (KNN) approach.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "30",
        "title": "A General Retrieval-Augmented Generation Framework for Multimodal Case-Based Reasoning Applications",
        "author": [
            "Ofir Marom"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05030",
        "abstract": "Case-based reasoning (CBR) is an experience-based approach to problem solving, where a repository of solved cases is adapted to solve new cases. Recent research shows that Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) can support the Retrieve and Reuse stages of the CBR pipeline by retrieving similar cases and using them as additional context to an LLM query. Most studies have focused on text-only applications, however, in many real-world problems the components of a case are multimodal. In this paper we present MCBR-RAG, a general RAG framework for multimodal CBR applications. The MCBR-RAG framework converts non-text case components into text-based representations, allowing it to: 1) learn application-specific latent representations that can be indexed for retrieval, and 2) enrich the query provided to the LLM by incorporating all case components for better context. We demonstrate MCBR-RAG's effectiveness through experiments conducted on a simplified Math-24 application and a more complex Backgammon application. Our empirical results show that MCBR-RAG improves generation quality compared to a baseline LLM with no contextual information provided.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "31",
        "title": "Enhancing Human-Like Responses in Large Language Models",
        "author": [
            "Ethem Yaƒüƒ±z √áalƒ±k",
            "Talha R√ºzgar Akku≈ü"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05032",
        "abstract": "This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns. Our findings demonstrate that these enhancements not only improve user interactions but also open new possibilities for AI applications across different domains. Future work will address the ethical implications and potential biases introduced by these human-like attributes.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "32",
        "title": "LongViTU: Instruction Tuning for Long-Form Video Understanding",
        "author": [
            "Rujie Wu",
            "Xiaojian Ma",
            "Hai Ci",
            "Yue Fan",
            "Yuxuan Wang",
            "Haozhe Zhao",
            "Qing Li",
            "Yizhou Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05037",
        "abstract": "This paper introduce LongViTU, a large-scale (~121k QA pairs, ~900h videos), automatically generated dataset for long-form video understanding. We developed a systematic approach that organizes videos into a hierarchical tree structure and incorporates self-revision mechanisms to ensure high-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context (average certificate length of 4.6 minutes); 2) rich knowledge and condensed reasoning (commonsense, causality, planning, etc.); and 3) explicit timestamp labels for relevant events. LongViTU also serves as a benchmark for instruction following in long-form and streaming video understanding. We evaluate the open-source state-of-the-art long video understanding model, LongVU, and the commercial model, Gemini-1.5-Pro, on our benchmark. They achieve GPT-4 scores of 49.9 and 52.3, respectively, underscoring the substantial challenge posed by our benchmark. Further supervised fine-tuning (SFT) on LongVU led to performance improvements of 12.0% on our benchmark, 2.2% on the in-distribution (ID) benchmark EgoSchema, 1.0%, 2.2% and 1.2% on the out-of-distribution (OOD) benchmarks VideoMME (Long), WorldQA and OpenEQA, respectively. These outcomes demonstrate LongViTU's high data quality and robust OOD generalizability.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "33",
        "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution",
        "author": [
            "Chengxing Xie",
            "Bowen Li",
            "Chang Gao",
            "He Du",
            "Wai Lam",
            "Difan Zou",
            "Kai Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05040",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source LLM designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other LLM model to generate patches for the identified files. Then, to mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches, and train the two modules of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving state-of-the-art performance among open-source models with scores of 23.3% and 30.2%, respectively. These outcomes highlight the efficacy of our approach. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "34",
        "title": "On the Generalizability of Transformer Models to Code Completions of Different Lengths",
        "author": [
            "Nathan Cooper",
            "Rosalia Tufano",
            "Gabriele Bavota",
            "Denys Poshyvanyk"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05051",
        "abstract": "The programming landscape is nowadays being reshaped by the advent of Large Language Models (LLMs) able to automate code-related tasks related to code implementation (e.g., code completion) and comprehension (e.g., code summarization). Such a paradigm shift comes with a number of implications related to how software will be written, maintained, and evolved. Also, these LLMs are extremely expensive to train, posing questions on their sustainability over time. Given their training cost, their ability to generalize, namely their ability to work on task instances different from those on which they have been trained, is an aspect worth being investigated. Previous work already showed that transformer models can successfully support code completion in a cross-project setting. However, it is unclear whether LLM are able to generalize to inputs having lengths not seen during training. For example, it is known that training a model on short instances allows to substantially reduce the training cost. However, the extent to which such a model would provide good performance on sequences having lengths not seen during training is not known. Many recent works in Natural Language Processing (NLP) tackled this problem in the context of decoder-only LLMs, i.e., xPOS and ALiBi. To assess if these solutions extend to encoder-decoder LLMs usually adopted in the code-related tasks, we present a large empirical study evaluating this generalization property of these and other encoding schemes proposed in the literature, namely Sinusoidal, xPOS, ALiBi, and T5. We found that none of these solutions successfully generalize to unseen lengths and that the only safe solution is to ensure the representativeness in the training set of all lengths likely to be encountered at inference time.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "35",
        "title": "LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models",
        "author": [
            "Zengqi Peng",
            "Yubin Wang",
            "Xu Han",
            "Lei Zheng",
            "Jun Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05057",
        "abstract": "Recent advancements in reinforcement learning (RL) demonstrate the significant potential in autonomous driving. Despite this promise, challenges such as the manual design of reward functions and low sample efficiency in complex environments continue to impede the development of safe and effective driving policies. To tackle these issues, we introduce LearningFlow, an innovative automated policy learning workflow tailored to urban driving. This framework leverages the collaboration of multiple large language model (LLM) agents throughout the RL training process. LearningFlow includes a curriculum sequence generation process and a reward generation process, which work in tandem to guide the RL policy by generating tailored training curricula and reward functions. Particularly, each process is supported by an analysis agent that evaluates training progress and provides critical insights to the generation agent. Through the collaborative efforts of these LLM agents, LearningFlow automates policy learning across a series of complex driving tasks, and it significantly reduces the reliance on manual reward function design while enhancing sample efficiency. Comprehensive experiments are conducted in the high-fidelity CARLA simulator, along with comparisons with other existing methods, to demonstrate the efficacy of our proposed approach. The results demonstrate that LearningFlow excels in generating rewards and curricula. It also achieves superior performance and robust generalization across various driving tasks, as well as commendable adaptation to different RL algorithms.",
        "tags": [
            "Large Language Models",
            "RL"
        ]
    },
    {
        "id": "36",
        "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
        "author": [
            "Jiaxing Zhao",
            "Boyuan Sun",
            "Xiang Chen",
            "Xihan Wei",
            "Qibin Hou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05067",
        "abstract": "In this paper, we introduce LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model's performance in multimodal tasks. Experimental results demonstrate that LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as multimodal understanding, visual question answering, and video understanding, highlighting its broad application potential.",
        "tags": [
            "LLaVA"
        ]
    },
    {
        "id": "37",
        "title": "D3RM: A Discrete Denoising Diffusion Refinement Model for Piano Transcription",
        "author": [
            "Hounsu Kim",
            "Taegyun Kwon",
            "Juhan Nam"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05068",
        "abstract": "Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions. Moreover, they have shown competitive results on discriminative tasks, such as image segmentation. While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level. In this paper, we focus on discrete diffusion model's refinement capabilities and present a novel architecture for piano transcription. Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model. To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models. Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score. Our code is available in https://github.com/hanshounsu/d3rm.",
        "tags": [
            "Diffusion",
            "Segmentation"
        ]
    },
    {
        "id": "38",
        "title": "Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning",
        "author": [
            "Huabin Liu",
            "Filip Ilievski",
            "Cees G. M. Snoek"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05069",
        "abstract": "This paper proposes the first video-grounded entailment tree reasoning method for commonsense video question answering (VQA). Despite the remarkable progress of large visual-language models (VLMs), there are growing concerns that they learn spurious correlations between videos and likely answers, reinforced by their black-box nature and remaining benchmarking biases. Our method explicitly grounds VQA tasks to video fragments in four steps: entailment tree construction, video-language entailment verification, tree reasoning, and dynamic tree expansion. A vital benefit of the method is its generalizability to current video and image-based VLMs across reasoning types. To support fair evaluation, we devise a de-biasing procedure based on large-language models that rewrites VQA benchmark answer sets to enforce model reasoning. Systematic experiments on existing and de-biased benchmarks highlight the impact of our method components across benchmarks, VLMs, and reasoning types.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "39",
        "title": "Analyzing Memorization in Large Language Models through the Lens of Model Attribution",
        "author": [
            "Tarun Ram Menta",
            "Susmit Agrawal",
            "Chirag Agarwal"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05078",
        "abstract": "Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues. Existing research has mainly focused on posthoc analyses, such as extracting memorized content or developing memorization metrics, without exploring the underlying architectural factors that contribute to memorization. In this work, we investigate memorization from an architectural lens by analyzing how attention modules at different layers impact its memorization and generalization performance. Using attribution techniques, we systematically intervene in the LLM architecture by bypassing attention modules at specific blocks while keeping other components like layer normalization and MLP transformations intact. We provide theorems analyzing our intervention mechanism from a mathematical view, bounding the difference in layer outputs with and without our attributions. Our theoretical and empirical analyses reveal that attention modules in deeper transformer blocks are primarily responsible for memorization, whereas earlier blocks are crucial for the models generalization and reasoning capabilities. We validate our findings through comprehensive experiments on different LLM families (Pythia and GPTNeo) and five benchmark datasets. Our insights offer a practical approach to mitigate memorization in LLMs while preserving their performance, contributing to safer and more ethical deployment in real world applications.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "40",
        "title": "DriVLM: Domain Adaptation of Vision-Language Models in Autonomous Driving",
        "author": [
            "Xuran Zheng",
            "Chang D. Yoo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05081",
        "abstract": "In recent years, large language models have had a very impressive performance, which largely contributed to the development and application of artificial intelligence, and the parameters and performance of the models are still growing rapidly. In particular, multimodal large language models (MLLM) can combine multiple modalities such as pictures, videos, sounds, texts, etc., and have great potential in various tasks. However, most MLLMs require very high computational resources, which is a major challenge for most researchers and developers. In this paper, we explored the utility of small-scale MLLMs and applied small-scale MLLMs to the field of autonomous driving. We hope that this will advance the application of MLLMs in real-world scenarios.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "41",
        "title": "ResPanDiff: Diffusion Model with Disentangled Modulations for Image Fusion",
        "author": [
            "Shiqi Cao",
            "Liangjian Deng",
            "Shangqi Deng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05091",
        "abstract": "The implementation of diffusion-based pansharpening task is predominantly constrained by its slow inference speed, which results from numerous sampling steps. Despite the existing techniques aiming to accelerate sampling, they often compromise performance when fusing multi-source images. To ease this limitation, we introduce a novel and efficient diffusion model named Diffusion Model for Pansharpening by Inferring Residual Inference (ResPanDiff), which significantly reduces the number of diffusion steps without sacrificing the performance to tackle pansharpening task. In ResPanDiff, we innovatively propose a Markov chain that transits from noisy residuals to the residuals between the LRMS and HRMS images, thereby reducing the number of sampling steps and enhancing performance. Additionally, we design the latent space to help model extract more features at the encoding stage, Shallow Cond-Injection~(SC-I) to help model fetch cond-injected hidden features with higher dimensions, and loss functions to give a better guidance for the residual generation task. enabling the model to achieve superior performance in residual generation. Furthermore, experimental evaluations on pansharpening datasets demonstrate that the proposed method achieves superior outcomes compared to recent state-of-the-art~(SOTA) techniques, requiring only 15 sampling steps, which reduces over $90\\%$ step compared with the benchmark diffusion models. Our experiments also include thorough discussions and ablation studies to underscore the effectiveness of our approach.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "42",
        "title": "Optimizing Multitask Industrial Processes with Predictive Action Guidance",
        "author": [
            "Naval Kishore Mehta",
            "Arvind",
            "Shyam Sunder Prasad",
            "Sumeet Saurav",
            "Sanjay Singh"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05108",
        "abstract": "Monitoring complex assembly processes is critical for maintaining productivity and ensuring compliance with assembly standards. However, variability in human actions and subjective task preferences complicate accurate task anticipation and guidance. To address these challenges, we introduce the Multi-Modal Transformer Fusion and Recurrent Units (MMTFRU) Network for egocentric activity anticipation, utilizing multimodal fusion to improve prediction accuracy. Integrated with the Operator Action Monitoring Unit (OAMU), the system provides proactive operator guidance, preventing deviations in the assembly process. OAMU employs two strategies: (1) Top-5 MMTF-RU predictions, combined with a reference graph and an action dictionary, for next-step recommendations; and (2) Top-1 MMTF-RU predictions, integrated with a reference graph, for detecting sequence deviations and predicting anomaly scores via an entropy-informed confidence mechanism. We also introduce Time-Weighted Sequence Accuracy (TWSA) to evaluate operator efficiency and ensure timely task completion. Our approach is validated on the industrial Meccano dataset and the largescale EPIC-Kitchens-55 dataset, demonstrating its effectiveness in dynamic environments.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "43",
        "title": "3DIS-FLUX: simple and efficient multi-instance generation with DiT rendering",
        "author": [
            "Dewei Zhou",
            "Ji Xie",
            "Zongxin Yang",
            "Yi Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05131",
        "abstract": "The growing demand for controllable outputs in text-to-image generation has driven significant advancements in multi-instance generation (MIG), enabling users to define both instance layouts and attributes. Currently, the state-of-the-art methods in MIG are primarily adapter-based. However, these methods necessitate retraining a new adapter each time a more advanced model is released, resulting in significant resource consumption. A methodology named Depth-Driven Decoupled Instance Synthesis (3DIS) has been introduced, which decouples MIG into two distinct phases: 1) depth-based scene construction and 2) detail rendering with widely pre-trained depth control models. The 3DIS method requires adapter training solely during the scene construction phase, while enabling various models to perform training-free detail rendering. Initially, 3DIS focused on rendering techniques utilizing U-Net architectures such as SD1.5, SD2, and SDXL, without exploring the potential of recent DiT-based models like FLUX. In this paper, we present 3DIS-FLUX, an extension of the 3DIS framework that integrates the FLUX model for enhanced rendering capabilities. Specifically, we employ the FLUX.1-Depth-dev model for depth map controlled image generation and introduce a detail renderer that manipulates the Attention Mask in FLUX's Joint Attention mechanism based on layout information. This approach allows for the precise rendering of fine-grained attributes of each instance. Our experimental results indicate that 3DIS-FLUX, leveraging the FLUX model, outperforms the original 3DIS method, which utilized SD2 and SDXL, and surpasses current state-of-the-art adapter-based methods in terms of both performance and image quality. Project Page: https://limuloo.github.io/3DIS/.",
        "tags": [
            "DiT",
            "SDXL",
            "Text-to-Image"
        ]
    },
    {
        "id": "44",
        "title": "Biomedical Relation Extraction via Adaptive Document-Relation Cross-Mapping and Concept Unique Identifier",
        "author": [
            "Yufei Shang",
            "Yanrong Guo",
            "Shijie Hao",
            "Richang Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05155",
        "abstract": "Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify relations between biomedical entities within extensive texts, serving as a crucial subfield of biomedical text mining. Existing Bio-RE methods struggle with cross-sentence inference, which is essential for capturing relations spanning multiple sentences. Moreover, previous methods often overlook the incompleteness of documents and lack the integration of external knowledge, limiting contextual richness. Besides, the scarcity of annotated data further hampers model training. Recent advancements in large language models (LLMs) have inspired us to explore all the above issues for document-level Bio-RE. Specifically, we propose a document-level Bio-RE framework via LLM Adaptive Document-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique Identifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the Iteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In this way, Bio-RE task-specific synthetic data can be generated by guiding ChatGPT to focus on entity relations and iteratively refining synthetic data. Next, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes mappings across different documents and relations, enhancing the model's contextual understanding and cross-sentence inference capabilities. Finally, during the inference, a biomedical-specific RAG approach, named CUI RAG, is designed to leverage CUIs as indexes for entities, narrowing the retrieval scope and enriching the relevant document contexts. Experiments conducted on three Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art performance of our proposed method by comparing it with other related works.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "45",
        "title": "Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in Secure Software Engineering",
        "author": [
            "Matteo Esposito"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05165",
        "abstract": "Context. Developing secure and reliable software remains a key challenge in software engineering (SE). The ever-evolving technological landscape offers both opportunities and threats, creating a dynamic space where chaos and order compete. Secure software engineering (SSE) must continuously address vulnerabilities that endanger software systems and carry broader socio-economic risks, such as compromising critical national infrastructure and causing significant financial losses. Researchers and practitioners have explored methodologies like Static Application Security Testing Tools (SASTTs) and artificial intelligence (AI) approaches, including machine learning (ML) and large language models (LLMs), to detect and mitigate these vulnerabilities. Each method has unique strengths and limitations.\nAim. This thesis seeks to bring order to the chaos in SSE by addressing domain-specific differences that impact AI accuracy.\nMethodology. The research employs a mix of empirical strategies, such as evaluating effort-aware metrics, analyzing SASTTs, conducting method-level analysis, and leveraging evidence-based techniques like systematic dataset reviews. These approaches help characterize vulnerability prediction datasets.\nResults. Key findings include limitations in static analysis tools for identifying vulnerabilities, gaps in SASTT coverage of vulnerability types, weak relationships among vulnerability severity scores, improved defect prediction accuracy using just-in-time modeling, and threats posed by untouched methods.\nConclusions. This thesis highlights the complexity of SSE and the importance of contextual knowledge in improving AI-driven vulnerability and defect prediction. The comprehensive analysis advances effective prediction models, benefiting both researchers and practitioners.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "46",
        "title": "Emergence of human-like polarization among large language model agents",
        "author": [
            "Jinghua Piao",
            "Zhihong Lu",
            "Chen Gao",
            "Fengli Xu",
            "Fernando P. Santos",
            "Yong Li",
            "James Evans"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05171",
        "abstract": "Rapid advances in large language models (LLMs) have empowered autonomous agents to establish social relationships, communicate, and form shared and diverging opinions on political issues. Our understanding of their collective behaviours and underlying mechanisms remains incomplete, however, posing unexpected risks to human society. In this paper, we simulate a networked system involving thousands of large language model agents, discovering their social interactions, guided through LLM conversation, result in human-like polarization. We discover that these agents spontaneously develop their own social network with human-like properties, including homophilic clustering, but also shape their collective opinions through mechanisms observed in the real world, including the echo chamber effect. Similarities between humans and LLM agents -- encompassing behaviours, mechanisms, and emergent phenomena -- raise concerns about their capacity to amplify societal polarization, but also hold the potential to serve as a valuable testbed for identifying plausible strategies to mitigate polarization and its consequences.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "47",
        "title": "Deep Assessment of Code Review Generation Approaches: Beyond Lexical Similarity",
        "author": [
            "Yanjie Jiang",
            "Hui Liu",
            "Tianyi Chen",
            "Fu Fan",
            "Chunhao Dong",
            "Kui Liu",
            "Lu Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05176",
        "abstract": "Code review is a standard practice for ensuring the quality of software projects, and recent research has focused extensively on automated code review. While significant advancements have been made in generating code reviews, the automated assessment of these reviews remains less explored, with existing approaches and metrics often proving inaccurate. Current metrics, such as BLEU, primarily rely on lexical similarity between generated and reference reviews. However, such metrics tend to underestimate reviews that articulate the expected issues in ways different from the references. In this paper, we explore how semantic similarity between generated and reference reviews can enhance the automated assessment of code reviews. We first present a benchmark called \\textit{GradedReviews}, which is constructed by collecting real-world code reviews from open-source projects, generating reviews using state-of-the-art approaches, and manually assessing their quality. We then evaluate existing metrics for code review assessment using this benchmark, revealing their limitations. To address these limitations, we propose two novel semantic-based approaches for assessing code reviews. The first approach involves converting both the generated review and its reference into digital vectors using a deep learning model and then measuring their semantic similarity through Cosine similarity. The second approach generates a prompt based on the generated review and its reference, submits this prompt to ChatGPT, and requests ChatGPT to rate the generated review according to explicitly defined criteria. Our evaluation on the \\textit{GradedReviews} benchmark indicates that the proposed semantic-based approaches significantly outperform existing state-of-the-art metrics in assessing generated code review, improving the correlation coefficient between the resulting scores and human scores from 0.22 to 0.47.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "48",
        "title": "FaceMe: Robust Blind Face Restoration with Personal Identification",
        "author": [
            "Siyu Liu",
            "Zheng-Peng Duan",
            "Jia OuYang",
            "Jiayi Fu",
            "Hyunhee Park",
            "Zikun Liu",
            "Chun-Le Guo",
            "Chongyi Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05177",
        "abstract": "Blind face restoration is a highly ill-posed problem due to the lack of necessary context. Although existing methods produce high-quality outputs, they often fail to faithfully preserve the individual's identity. In this paper, we propose a personalized face restoration method, FaceMe, based on a diffusion model. Given a single or a few reference images, we use an identity encoder to extract identity-related features, which serve as prompts to guide the diffusion model in restoring high-quality and identity-consistent facial images. By simply combining identity-related features, we effectively minimize the impact of identity-irrelevant features during training and support any number of reference image inputs during inference. Additionally, thanks to the robustness of the identity encoder, synthesized images can be used as reference images during training, and identity changing during inference does not require fine-tuning the model. We also propose a pipeline for constructing a reference image training pool that simulates the poses and expressions that may appear in real-world scenarios. Experimental results demonstrate that our FaceMe can restore high-quality facial images while maintaining identity consistency, achieving excellent performance and robustness.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "49",
        "title": "Compression with Global Guidance: Towards Training-free High-Resolution MLLMs Acceleration",
        "author": [
            "Xuyang Liu",
            "Ziming Wang",
            "Yuhang Han",
            "Yingyao Wang",
            "Jiale Yuan",
            "Jun Song",
            "Bo Zheng",
            "Linfeng Zhang",
            "Siteng Huang",
            "Honggang Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05179",
        "abstract": "Multimodal large language models (MLLMs) have attracted considerable attention due to their exceptional performance in visual content understanding and reasoning. However, their inference efficiency has been a notable concern, as the increasing length of multimodal contexts leads to quadratic complexity. Token compression techniques, which reduce the number of visual tokens, have demonstrated their effectiveness in reducing computational costs. Yet, these approaches have struggled to keep pace with the rapid advancements in MLLMs, especially the AnyRes strategy in the context of high-resolution image understanding. In this paper, we propose a novel token compression method, GlobalCom$^2$, tailored for high-resolution MLLMs that receive both the thumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the thumbnail as the ``commander'' of the entire token compression process, directing the allocation of retention ratios and the specific compression for each crop. In this way, redundant tokens are eliminated while important local details are adaptively preserved to the highest extent feasible. Empirical results across 10 benchmarks reveal that GlobalCom$^2$ achieves an optimal balance between performance and efficiency, and consistently outperforms state-of-the-art token compression methods with LLaVA-NeXT-7B/13B models. Our code is released at \\url{https://github.com/xuyang-liu16/GlobalCom2}.",
        "tags": [
            "LLaVA",
            "Large Language Models"
        ]
    },
    {
        "id": "50",
        "title": "Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant Learning",
        "author": [
            "Xueyi Ke",
            "Satoshi Tsutsui",
            "Yayun Zhang",
            "Bihan Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05205",
        "abstract": "Infants develop complex visual understanding rapidly, even preceding of the acquisition of linguistic inputs. As computer vision seeks to replicate the human vision system, understanding infant visual development may offer valuable insights. In this paper, we present an interdisciplinary study exploring this question: can a computational model that imitates the infant learning process develop broader visual concepts that extend beyond the vocabulary it has heard, similar to how infants naturally learn? To investigate this, we analyze a recently published model in Science by Vong et al.,which is trained on longitudinal, egocentric images of a single child paired with transcribed parental speech. We introduce a training-free framework that can discover visual concept neurons hidden in the model's internal representations. Our findings show that these neurons can classify objects outside its original vocabulary. Furthermore, we compare the visual representations in infant-like models with those in moder computer vision models, such as CLIP or ImageNet pre-trained model, highlighting key similarities and differences. Ultimately, our work bridges cognitive science and computer vision by analyzing the internal representations of a computational model trained on an infant's visual and linguistic inputs.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "51",
        "title": "MHAFF: Multi-Head Attention Feature Fusion of CNN and Transformer for Cattle Identification",
        "author": [
            "Rabin Dulal",
            "Lihong Zheng",
            "Muhammad Ashad Kabir"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05209",
        "abstract": "Convolutional Neural Networks (CNNs) have drawn researchers' attention to identifying cattle using muzzle images. However, CNNs often fail to capture long-range dependencies within the complex patterns of the muzzle. The transformers handle these challenges. This inspired us to fuse the strengths of CNNs and transformers in muzzle-based cattle identification. Addition and concatenation have been the most commonly used techniques for feature fusion. However, addition fails to preserve discriminative information, while concatenation results in an increase in dimensionality. Both methods are simple operations and cannot discover the relationships or interactions between fusing features. This research aims to overcome the issues faced by addition and concatenation. This research introduces a novel approach called Multi-Head Attention Feature Fusion (MHAFF) for the first time in cattle identification. MHAFF captures relations between the different types of fusing features while preserving their originality. The experiments show that MHAFF outperformed addition and concatenation techniques and the existing cattle identification methods in accuracy on two publicly available cattle datasets. MHAFF demonstrates excellent performance and quickly converges to achieve optimum accuracy of 99.88% and 99.52% in two cattle datasets simultaneously.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "52",
        "title": "A Novel Approach to Scalable and Automatic Topic-Controlled Question Generation in Education",
        "author": [
            "Ziqing Li",
            "Mutlu Cukurova",
            "Sahan Bulathwela"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05220",
        "abstract": "The development of Automatic Question Generation (QG) models has the potential to significantly improve educational practices by reducing the teacher workload associated with creating educational content. This paper introduces a novel approach to educational question generation that controls the topical focus of questions. The proposed Topic-Controlled Question Generation (T-CQG) method enhances the relevance and effectiveness of the generated content for educational purposes. Our approach uses fine-tuning on a pre-trained T5-small model, employing specially created datasets tailored to educational needs. The research further explores the impacts of pre-training strategies, quantisation, and data augmentation on the model's performance. We specifically address the challenge of generating semantically aligned questions with paragraph-level contexts, thereby improving the topic specificity of the generated questions. In addition, we introduce and explore novel evaluation methods to assess the topical relatedness of the generated questions. Our results, validated through rigorous offline and human-backed evaluations, demonstrate that the proposed models effectively generate high-quality, topic-focused questions. These models have the potential to reduce teacher workload and support personalised tutoring systems by serving as bespoke question generators. With its relatively small number of parameters, the proposals not only advance the capabilities of question generation models for handling specific educational topics but also offer a scalable solution that reduces infrastructure costs. This scalability makes them feasible for widespread use in education without reliance on proprietary large language models like ChatGPT.",
        "tags": [
            "ChatGPT",
            "Large Language Models"
        ]
    },
    {
        "id": "53",
        "title": "Leveraging Large Language Models for Zero-shot Lay Summarisation in Biomedicine and Beyond",
        "author": [
            "Tomas Goldsack",
            "Carolina Scarton",
            "Chenghua Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05224",
        "abstract": "In this work, we explore the application of Large Language Models to zero-shot Lay Summarisation. We propose a novel two-stage framework for Lay Summarisation based on real-life processes, and find that summaries generated with this method are increasingly preferred by human judges for larger models. To help establish best practices for employing LLMs in zero-shot settings, we also assess the ability of LLMs as judges, finding that they are able to replicate the preferences of human judges. Finally, we take the initial steps towards Lay Summarisation for Natural Language Processing (NLP) articles, finding that LLMs are able to generalise to this new domain, and further highlighting the greater utility of summaries generated by our proposed approach via an in-depth human evaluation.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "54",
        "title": "Light Transport-aware Diffusion Posterior Sampling for Single-View Reconstruction of 3D Volumes",
        "author": [
            "Ludwic Leonard",
            "Nils Thuerey",
            "Ruediger Westermann"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05226",
        "abstract": "We introduce a single-view reconstruction technique of volumetric fields in which multiple light scattering effects are omnipresent, such as in clouds. We model the unknown distribution of volumetric fields using an unconditional diffusion model trained on a novel benchmark dataset comprising 1,000 synthetically simulated volumetric density fields. The neural diffusion model is trained on the latent codes of a novel, diffusion-friendly, monoplanar representation. The generative model is used to incorporate a tailored parametric diffusion posterior sampling technique into different reconstruction tasks. A physically-based differentiable volume renderer is employed to provide gradients with respect to light transport in the latent space. This stands in contrast to classic NeRF approaches and makes the reconstructions better aligned with observed data. Through various experiments, we demonstrate single-view reconstruction of volumetric clouds at a previously unattainable quality.",
        "tags": [
            "3D",
            "Diffusion",
            "NeRF"
        ]
    },
    {
        "id": "55",
        "title": "Harnessing Large Language and Vision-Language Models for Robust Out-of-Distribution Detection",
        "author": [
            "Pei-Kang Lee",
            "Jun-Cheng Chen",
            "Ja-Ling Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05228",
        "abstract": "Out-of-distribution (OOD) detection has seen significant advancements with zero-shot approaches by leveraging the powerful Vision-Language Models (VLMs) such as CLIP. However, prior research works have predominantly focused on enhancing Far-OOD performance, while potentially compromising Near-OOD efficacy, as observed from our pilot study. To address this issue, we propose a novel strategy to enhance zero-shot OOD detection performances for both Far-OOD and Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs) and VLMs. Our approach first exploit an LLM to generate superclasses of the ID labels and their corresponding background descriptions followed by feature extraction using CLIP. We then isolate the core semantic features for ID data by subtracting background features from the superclass features. The refined representation facilitates the selection of more appropriate negative labels for OOD data from a comprehensive candidate label set of WordNet, thereby enhancing the performance of zero-shot OOD detection in both scenarios. Furthermore, we introduce novel few-shot prompt tuning and visual prompt tuning to adapt the proposed framework to better align with the target distribution. Experimental results demonstrate that the proposed approach consistently outperforms current state-of-the-art methods across multiple benchmarks, with an improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95. Additionally, our method exhibits superior robustness against covariate shift across different domains, further highlighting its effectiveness in real-world scenarios.",
        "tags": [
            "CLIP",
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "56",
        "title": "Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs",
        "author": [
            "Artem Fedorchenko",
            "Tanel Alum√§e"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05234",
        "abstract": "This paper presents an approach for generating high-quality, same-language subtitles for Estonian TV content. We fine-tune the Whisper model on human-generated Estonian subtitles and enhance it with iterative pseudo-labeling and large language model (LLM) based post-editing. Our experiments demonstrate notable subtitle quality improvement through pseudo-labeling with an unlabeled dataset. We find that applying LLM-based editing at test time enhances subtitle accuracy, while its use during training does not yield further gains. This approach holds promise for creating subtitle quality close to human standard and could be extended to real-time applications.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "57",
        "title": "Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping",
        "author": [
            "Wen Tianci",
            "Liu Zhiang",
            "Lu Biao",
            "Fang Yongchun"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05242",
        "abstract": "3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM methods utilizing 3DGS have failed to provide high-quality novel view rendering for monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods perform well for RGB-D cameras but suffer significant degradation in rendering quality for monocular cameras. In this paper, we present Scaffold-SLAM, which delivers simultaneous localization and high-quality photorealistic mapping across monocular, stereo, and RGB-D cameras. We introduce two key innovations to achieve this state-of-the-art visual quality. First, we propose Appearance-from-Motion embedding, enabling 3D Gaussians to better model image appearance variations across different camera poses. Second, we introduce a frequency regularization pyramid to guide the distribution of Gaussians, allowing the model to effectively capture finer details in the scene. Extensive experiments on monocular, stereo, and RGB-D datasets demonstrate that Scaffold-SLAM significantly outperforms state-of-the-art methods in photorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D datasets for monocular cameras.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "SLAM"
        ]
    },
    {
        "id": "58",
        "title": "Online Prompt and Solver Selection for Program Synthesis",
        "author": [
            "Yixuan Li",
            "Lewis Frampton",
            "Federico Mora",
            "Elizabeth Polgreen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05247",
        "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities in the domain of program synthesis. This level of performance is not, however, universal across all tasks, all LLMs and all prompting styles. There are many areas where one LLM dominates, one prompting style dominates, or where calling a symbolic solver is a better choice than an LLM. A key challenge for the user then, is to identify not only when an LLM is the right choice of solver, and the appropriate LLM to call for a given synthesis task, but also the right way to call it. A non-expert user who makes the wrong choice, incurs a cost both in terms of results (number of tasks solved, and the time it takes to solve them) and financial cost, if using a closed-source language model via a commercial API. We frame this choice as an online learning problem. We use a multi-armed bandit algorithm to select which symbolic solver, or LLM and prompt combination to deploy in order to maximize a given reward function (which may prioritize solving time, number of synthesis tasks solved, or financial cost of solving). We implement an instance of this approach, called CYANEA, and evaluate it on synthesis queries from the literature in ranking function synthesis, from the syntax-guided synthesis competition, and fresh, unseen queries generated from SMT problems. CYANEA solves 37.2\\% more queries than the best single solver and achieves results within 4\\% of the virtual best solver.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "59",
        "title": "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient Pruning",
        "author": [
            "Laura Puccioni",
            "Alireza Farshin",
            "Mariano Scazzariello",
            "Changjie Wang",
            "Marco Chiesa",
            "Dejan Kostic"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05248",
        "abstract": "Large Language Models (LLMs) have demonstrated their exceptional performance in various complex code generation tasks. However, their broader adoption is limited by significant computational demands and high resource requirements, particularly memory and processing power. To mitigate such requirements, model pruning techniques are used to create more compact models with significantly fewer parameters. However, current approaches do not focus on the efficient extraction of programming-language-specific sub-models. In this work, we explore the idea of efficiently deriving coding-specific sub-models through unstructured pruning (i.e., Wanda). We investigate the impact of different domain-specific calibration datasets on pruning outcomes across three distinct domains and extend our analysis to extracting four language-specific sub-models: Python, Java, C++, and JavaScript. We are the first to efficiently extract programming-language-specific sub-models using appropriate calibration datasets while maintaining acceptable accuracy w.r.t. full models. We are also the first to provide analytical evidence that domain-specific tasks activate distinct regions within LLMs, supporting the creation of specialized sub-models through unstructured pruning. We believe that this work has significant potential to enhance LLM accessibility for coding by reducing computational requirements to enable local execution on consumer-grade hardware, and supporting faster inference times critical for real-time development feedback.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "60",
        "title": "From Scientific Texts to Verifiable Code: Automating the Process with Transformers",
        "author": [
            "Changjie Wang",
            "Mariano Scazzariello",
            "Marco Chiesa"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05252",
        "abstract": "Despite the vast body of research literature proposing algorithms with formal guarantees, the amount of verifiable code in today's systems remains minimal. This discrepancy stems from the inherent difficulty of verifying code, particularly due to the time-consuming nature and strict formalism of proof details that formal verification tools require. However, the emergence of transformers in Large Language Models presents a promising solution to this challenge. In this position paper, we believe that transformers have the potential to read research papers that propose algorithms with formal proofs and translate these proofs into verifiable code. We leverage transformers to first build a formal structure of the proof using the original text from the paper, and then to handle the tedious, low-level aspects of proofs that are often omitted by humans. We argue that this approach can significantly reduce the barrier to formal verification. The above idea of reading papers to write verifiable code opens new avenues for automating the verification of complex systems, enabling a future where formally verified algorithms from academic research can more seamlessly transition into real-world software systems, thereby improving code reliability and security.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "61",
        "title": "CallNavi: A Study and Challenge on Function Calling Routing and Invocation in Large Language Models",
        "author": [
            "Yewei Song",
            "Cedric Lothritz",
            "Xunzhu Tang",
            "Saad Ezzini",
            "Jacques Klein",
            "Tegawend√© F. Bissyand√©",
            "Andrey Boytsov",
            "Ulrick Ble",
            "Anne Goujon"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05255",
        "abstract": "Interacting with a software system via a chatbot can be challenging, especially when the chatbot needs to generate API calls, in the right order and with the right parameters, to communicate with the system. API calling in chatbot systems poses significant challenges, particularly in complex, multi-step tasks requiring accurate API selection and execution. We contribute to this domain in three ways: first, by introducing a novel dataset designed to assess models on API function selection, parameter generation, and nested API calls; second, by benchmarking state-of-the-art language models across varying levels of complexity to evaluate their performance in API function generation and parameter accuracy; and third, by proposing an enhanced API routing method that combines general-purpose large language models for API selection with fine-tuned models for parameter generation and some prompt engineering approach. These approaches lead to substantial improvements in handling complex API tasks, offering practical advancements for real-world API-driven chatbot systems.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "62",
        "title": "Automating the Detection of Code Vulnerabilities by Analyzing GitHub Issues",
        "author": [
            "Daniele Cipollone",
            "Changjie Wang",
            "Mariano Scazzariello",
            "Simone Ferlin",
            "Maliheh Izadi",
            "Dejan Kostic",
            "Marco Chiesa"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05258",
        "abstract": "In today's digital landscape, the importance of timely and accurate vulnerability detection has significantly increased. This paper presents a novel approach that leverages transformer-based models and machine learning techniques to automate the identification of software vulnerabilities by analyzing GitHub issues. We introduce a new dataset specifically designed for classifying GitHub issues relevant to vulnerability detection. We then examine various classification techniques to determine their effectiveness. The results demonstrate the potential of this approach for real-world application in early vulnerability detection, which could substantially reduce the window of exploitation for software vulnerabilities. This research makes a key contribution to the field by providing a scalable and computationally efficient framework for automated detection, enabling the prevention of compromised software usage before official notifications. This work has the potential to enhance the security of open-source software ecosystems.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "63",
        "title": "Enhancing Plagiarism Detection in Marathi with a Weighted Ensemble of TF-IDF and BERT Embeddings for Low-Resource Language Processing",
        "author": [
            "Atharva Mutsaddi",
            "Aditya Choudhary"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05260",
        "abstract": "Plagiarism involves using another person's work or concepts without proper attribution, presenting them as original creations. With the growing amount of data communicated in regional languages such as Marathi -- one of India's regional languages -- it is crucial to design robust plagiarism detection systems tailored for low-resource languages. Language models like Bidirectional Encoder Representations from Transformers (BERT) have demonstrated exceptional capability in text representation and feature extraction, making them essential tools for semantic analysis and plagiarism detection. However, the application of BERT for low-resource languages remains under-explored, particularly in the context of plagiarism detection. This paper presents a method to enhance the accuracy of plagiarism detection for Marathi texts using BERT sentence embeddings in conjunction with Term Frequency-Inverse Document Frequency (TF-IDF) feature representation. This approach effectively captures statistical, semantic, and syntactic aspects of text features through a weighted voting ensemble of machine learning models.",
        "tags": [
            "BERT",
            "Detection"
        ]
    },
    {
        "id": "64",
        "title": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction",
        "author": [
            "Hantao Lou",
            "Jiaming Ji",
            "Kaile Wang",
            "Yaodong Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05336",
        "abstract": "The rapid advancement of large language models (LLMs) has led to significant improvements in their capabilities, but also to increased concerns about their alignment with human values and intentions. Current alignment strategies, including adaptive training and inference-time methods, have demonstrated potential in this area. However, these approaches still struggle to balance deployment complexity and capability across various tasks and difficulties. In this work, we introduce the Streaming Distribution Induce Aligner (Stream Aligner), a novel alignment paradigm that combines efficiency with enhanced performance in various tasks throughout the generation process. Stream Aligner achieves dynamic sentence-level correction by using a small model to learn the preferences of the suffix sentence, iteratively correcting the suffix sentence output by the upstream model, and then using the corrected sentence to replace the suffix sentence in subsequent generations. Compared to Aligner, our experiments demonstrate that Stream Aligner reduces reliance on the capabilities of additional models, enhances the reasoning abilities of LLMs, and decreases latency during user interaction. Specifically, Stream Aligner-2B model has achieved an improvement of 76.1% in helpfulness, 36.0% in harmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has achieved an improvement of 3.5% on the math ability of the tested Llama3-70B-Instruct model.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "65",
        "title": "Accelerated Diffusion Models via Speculative Sampling",
        "author": [
            "Valentin De Bortoli",
            "Alexandre Galashov",
            "Arthur Gretton",
            "Arnaud Doucet"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05370",
        "abstract": "Speculative sampling is a popular technique for accelerating inference in Large Language Models by generating candidate tokens using a fast draft model and accepting or rejecting them based on the target model's distribution. While speculative sampling was previously limited to discrete sequences, we extend it to diffusion models, which generate samples via continuous, vector-valued Markov chains. In this context, the target model is a high-quality but computationally expensive diffusion model. We propose various drafting strategies, including a simple and effective approach that does not require training a draft model and is applicable out of the box to any diffusion model. Our experiments demonstrate significant generation speedup on various diffusion models, halving the number of function evaluations, while generating exact samples from the target model.",
        "tags": [
            "Diffusion",
            "Large Language Models"
        ]
    },
    {
        "id": "66",
        "title": "Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance",
        "author": [
            "Dimitrios Gerogiannis",
            "Foivos Paraperas Papantoniou",
            "Rolandos Alexandros Potamias",
            "Alexandros Lattas",
            "Stefanos Zafeiriou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05379",
        "abstract": "Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in reconstructing detailed 3D scenes within multi-view setups and the emergence of large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based method utilizing a human face foundation model as guidance with just a single image as input. To achieve that, we extend such a model for diverse-view human head generation by fine-tuning on synthetic data and modifying its conditioning. Our avatars maintain a dense correspondence with a human face mesh template, allowing blendshape-based expression generation. This is achieved through a modified 3DGS approach, connectivity regularizers, and a strategic initialization tailored for our task. Additionally, we propose an optional efficient SDS-based correction step to refine the blendshape expressions, enhancing realism and diversity. Experiments demonstrate that Arc2Avatar achieves state-of-the-art realism and identity preservation, effectively addressing color issues by allowing the use of very low guidance, enabled by our strong identity prior and initialization strategy, without compromising detail.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "67",
        "title": "FairCode: Evaluating Social Bias of LLMs in Code Generation",
        "author": [
            "Yongkang Du",
            "Jen-tse Huang",
            "Jieyu Zhao",
            "Lu Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05396",
        "abstract": "Large language models (LLMs) have demonstrated significant capability in code generation, drawing increasing attention to the evaluation of the quality and safety of their outputs. However, research on bias in code generation remains limited. Existing studies typically assess bias by applying malicious prompts or reapply tasks and dataset for discriminative models. Given that LLMs are often aligned with human values and that prior datasets are not fully optimized for code-related tasks, there is a pressing need for benchmarks specifically designed for evaluating code models. In this study, we introduce FairCode, a novel benchmark for evaluating bias in code generation. FairCode comprises two tasks: function implementation and test case generation, each evaluating social bias through diverse scenarios. Additionally, we propose a new metric, FairScore, to assess model performance on this benchmark. We conduct experiments on widely used LLMs and provide a comprehensive analysis of the results. The findings reveal that all tested LLMs exhibit bias. The code is available at https://github.com/YongkDu/FairCode.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "68",
        "title": "Mechanistic understanding and validation of large AI models with SemanticLens",
        "author": [
            "Maximilian Dreyer",
            "Jim Berend",
            "Tobias Labarta",
            "Johanna Vielhaben",
            "Thomas Wiegand",
            "Sebastian Lapuschkin",
            "Wojciech Samek"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05398",
        "abstract": "Unlike human-engineered systems such as aeroplanes, where each component's role and dependencies are well understood, the inner workings of AI models remain largely opaque, hindering verifiability and undermining trust. This paper introduces SemanticLens, a universal explanation method for neural networks that maps hidden knowledge encoded by components (e.g., individual neurons) into the semantically structured, multimodal space of a foundation model such as CLIP. In this space, unique operations become possible, including (i) textual search to identify neurons encoding specific concepts, (ii) systematic analysis and comparison of model representations, (iii) automated labelling of neurons and explanation of their functional roles, and (iv) audits to validate decision-making against requirements. Fully scalable and operating without human input, SemanticLens is shown to be effective for debugging and validation, summarizing model knowledge, aligning reasoning with expectations (e.g., adherence to the ABCDE-rule in melanoma classification), and detecting components tied to spurious correlations and their associated training data. By enabling component-level understanding and validation, the proposed approach helps bridge the \"trust gap\" between AI models and traditional engineered systems. We provide code for SemanticLens on https://github.com/jim-berend/semanticlens and a demo on https://semanticlens.hhi-research-insights.eu.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "69",
        "title": "TimeDP: Learning to Generate Multi-Domain Time Series with Domain Prompts",
        "author": [
            "Yu-Hao Huang",
            "Chang Xu",
            "Yueying Wu",
            "Wu-Jun Li",
            "Jiang Bian"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05403",
        "abstract": "Time series generation models are crucial for applications like data augmentation and privacy preservation. Most existing time series generation models are typically designed to generate data from one specified domain. While leveraging data from other domain for better generalization is proved to work in other application areas, this approach remains challenging for time series modeling due to the large divergence in patterns among different real world time series categories. In this paper, we propose a multi-domain time series diffusion model with domain prompts, named TimeDP. In TimeDP, we utilize a time series semantic prototype module which defines time series prototypes to represent time series basis, each prototype vector serving as \"word\" representing some elementary time series feature. A prototype assignment module is applied to extract the extract domain specific prototype weights, for learning domain prompts as generation condition. During sampling, we extract \"domain prompt\" with few-shot samples from the target domain and use the domain prompts as condition to generate time series samples. Experiments demonstrate that our method outperforms baselines to provide the state-of-the-art in-domain generation quality and strong unseen domain generation capability.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "70",
        "title": "LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation",
        "author": [
            "Xi Ye",
            "Fangcong Yin",
            "Yinghui He",
            "Joie Zhang",
            "Howard Yen",
            "Tianyu Gao",
            "Greg Durrett",
            "Danqi Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05414",
        "abstract": "Existing benchmarks for evaluating long-context language models (LCLMs) primarily focus on long-context recall, requiring models to produce short responses based on a few critical snippets while processing thousands of irrelevant tokens. We introduce LongProc (Long Procedural Generation), a new benchmark that requires both the integration of highly dispersed information and long-form generation. LongProc consists of six diverse procedural generation tasks, such as extracting structured information from HTML pages into a TSV format and executing complex search procedures to create travel plans. These tasks challenge LCLMs by testing their ability to follow detailed procedural instructions, synthesize and reason over dispersed information, and generate structured, long-form outputs (up to 8K tokens). Furthermore, as these tasks adhere to deterministic procedures and yield structured outputs, they enable reliable rule-based evaluation. We evaluate 17 LCLMs on LongProc across three difficulty levels, with maximum numbers of output tokens set at 500, 2K, and 8K. Notably, while all tested models claim a context window size above 32K tokens, open-weight models typically falter on 2K-token tasks, and closed-source models like GPT-4o show significant degradation on 8K-token tasks. Further analysis reveals that LCLMs struggle to maintain long-range coherence in long-form generations. These findings highlight critical limitations in current LCLMs and suggest substantial room for improvement. Data and code available at: https://princeton-pli.github.io/LongProc",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "71",
        "title": "Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D Generation",
        "author": [
            "Xuyi Meng",
            "Chen Wang",
            "Jiahui Lei",
            "Kostas Daniilidis",
            "Jiatao Gu",
            "Lingjie Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05427",
        "abstract": "Recent advances in 2D image generation have achieved remarkable quality,largely driven by the capacity of diffusion models and the availability of large-scale datasets. However, direct 3D generation is still constrained by the scarcity and lower fidelity of 3D datasets. In this paper, we introduce Zero-1-to-G, a novel approach that addresses this problem by enabling direct single-view generation on Gaussian splats using pretrained 2D diffusion models. Our key insight is that Gaussian splats, a 3D representation, can be decomposed into multi-view images encoding different attributes. This reframes the challenging task of direct 3D generation within a 2D diffusion framework, allowing us to leverage the rich priors of pretrained 2D diffusion models. To incorporate 3D awareness, we introduce cross-view and cross-attribute attention layers, which capture complex correlations and enforce 3D consistency across generated splats. This makes Zero-1-to-G the first direct image-to-3D generative model to effectively utilize pretrained 2D diffusion priors, enabling efficient training and improved generalization to unseen objects. Extensive experiments on both synthetic and in-the-wild datasets demonstrate superior performance in 3D object generation, offering a new approach to high-quality 3D generation.",
        "tags": [
            "3D",
            "Diffusion",
            "Image-to-3D"
        ]
    },
    {
        "id": "72",
        "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
        "author": [
            "Yiwen Huang",
            "Aaron Gokaslan",
            "Volodymyr Kuleshov",
            "James Tompkin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05441",
        "abstract": "There is a widely-spread claim that GANs are difficult to train, and GAN architectures in the literature are littered with empirical tricks. We provide evidence against this claim and build a modern GAN baseline in a more principled manner. First, we derive a well-behaved regularized relativistic GAN loss that addresses issues of mode dropping and non-convergence that were previously tackled via a bag of ad-hoc tricks. We analyze our loss mathematically and prove that it admits local convergence guarantees, unlike most existing relativistic losses. Second, our new loss allows us to discard all ad-hoc tricks and replace outdated backbones used in common GANs with modern architectures. Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline -- R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.",
        "tags": [
            "Diffusion",
            "GAN"
        ]
    },
    {
        "id": "73",
        "title": "Progressive Growing of Video Tokenizers for Highly Compressed Latent Spaces",
        "author": [
            "Aniruddha Mahapatra",
            "Long Mai",
            "Yitian Zhang",
            "David Bourgin",
            "Feng Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05442",
        "abstract": "Video tokenizers are essential for latent video diffusion models, converting raw video data into spatiotemporally compressed latent spaces for efficient training. However, extending state-of-the-art video tokenizers to achieve a temporal compression ratio beyond 4x without increasing channel capacity poses significant challenges. In this work, we propose an alternative approach to enhance temporal compression. We find that the reconstruction quality of temporally subsampled videos from a low-compression encoder surpasses that of high-compression encoders applied to original videos. This indicates that high-compression models can leverage representations from lower-compression models. Building on this insight, we develop a bootstrapped high-temporal-compression model that progressively trains high-compression blocks atop well-trained lower-compression models. Our method includes a cross-level feature-mixing module to retain information from the pretrained low-compression model and guide higher-compression blocks to capture the remaining details from the full video sequence. Evaluation of video benchmarks shows that our method significantly improves reconstruction quality while increasing temporal compression compared to direct extensions of existing video tokenizers. Furthermore, the resulting compact latent space effectively trains a video diffusion model for high-quality video generation with a reduced token budget.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "74",
        "title": "A survey of textual cyber abuse detection using cutting-edge language models and large language models",
        "author": [
            "Jose A. Diaz-Garcia",
            "Joao Paulo Carvalho"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05443",
        "abstract": "The success of social media platforms has facilitated the emergence of various forms of online abuse within digital communities. This abuse manifests in multiple ways, including hate speech, cyberbullying, emotional abuse, grooming, and sexting. In this paper, we present a comprehensive analysis of the different forms of abuse prevalent in social media, with a particular focus on how emerging technologies, such as Language Models (LMs) and Large Language Models (LLMs), are reshaping both the detection and generation of abusive content within these networks. We delve into the mechanisms through which social media abuse is perpetuated, exploring the psychological and social impact. Additionally, we examine the dual role of advanced language models-highlighting their potential to enhance automated detection systems for abusive behavior while also acknowledging their capacity to generate harmful content. This paper aims to contribute to the ongoing discourse on online safety and ethics, offering insights into the evolving landscape of cyberabuse and the technological innovations that both mitigate and exacerbate it.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "75",
        "title": "Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark",
        "author": [
            "Yunzhuo Hao",
            "Jiawei Gu",
            "Huichen Will Wang",
            "Linjie Li",
            "Zhengyuan Yang",
            "Lijuan Wang",
            "Yu Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05444",
        "abstract": "The ability to organically reason over and with both text and images is a pillar of human intelligence, yet the ability of Multimodal Large Language Models (MLLMs) to perform such multimodal reasoning remains under-explored. Existing benchmarks often emphasize text-dominant reasoning or rely on shallow visual cues, failing to adequately assess integrated visual and textual reasoning. We introduce EMMA (Enhanced MultiModal reAsoning), a benchmark targeting organic multimodal reasoning across mathematics, physics, chemistry, and coding. EMMA tasks demand advanced cross-modal reasoning that cannot be addressed by reasoning independently in each modality, offering an enhanced test suite for MLLMs' reasoning capabilities. Our evaluation of state-of-the-art MLLMs on EMMA reveals significant limitations in handling complex multimodal and multi-step reasoning tasks, even with advanced techniques like Chain-of-Thought prompting and test-time compute scaling underperforming. These findings underscore the need for improved multimodal architectures and training paradigms to close the gap between human and model reasoning in multimodality.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "76",
        "title": "Consistent Flow Distillation for Text-to-3D Generation",
        "author": [
            "Runjie Yan",
            "Yinbo Chen",
            "Xiaolong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05445",
        "abstract": "Score Distillation Sampling (SDS) has made significant strides in distilling image-generative models for 3D generation. However, its maximum-likelihood-seeking behavior often leads to degraded visual quality and diversity, limiting its effectiveness in 3D applications. In this work, we propose Consistent Flow Distillation (CFD), which addresses these limitations. We begin by leveraging the gradient of the diffusion ODE or SDE sampling process to guide the 3D generation. From the gradient-based sampling perspective, we find that the consistency of 2D image flows across different viewpoints is important for high-quality 3D generation. To achieve this, we introduce multi-view consistent Gaussian noise on the 3D object, which can be rendered from various viewpoints to compute the flow gradient. Our experiments demonstrate that CFD, through consistent flows, significantly outperforms previous methods in text-to-3D generation.",
        "tags": [
            "3D",
            "Diffusion",
            "ODE",
            "SDE",
            "Text-to-3D"
        ]
    },
    {
        "id": "77",
        "title": "Decentralized Diffusion Models",
        "author": [
            "David McAllister",
            "Matthew Tancik",
            "Jiaming Song",
            "Angjoo Kanazawa"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05450",
        "abstract": "Large-scale AI model training divides work across thousands of GPUs, then synchronizes gradients across them at each step. This incurs a significant network burden that only centralized, monolithic clusters can support, driving up infrastructure costs and straining power systems. We propose Decentralized Diffusion Models, a scalable framework for distributing diffusion model training across independent clusters or datacenters by eliminating the dependence on a centralized, high-bandwidth networking fabric. Our method trains a set of expert diffusion models over partitions of the dataset, each in full isolation from one another. At inference time, the experts ensemble through a lightweight router. We show that the ensemble collectively optimizes the same objective as a single model trained over the whole dataset. This means we can divide the training burden among a number of \"compute islands,\" lowering infrastructure costs and improving resilience to localized GPU failures. Decentralized diffusion models empower researchers to take advantage of smaller, more cost-effective and more readily available compute like on-demand GPU nodes rather than central integrated systems. We conduct extensive experiments on ImageNet and LAION Aesthetics, showing that decentralized diffusion models FLOP-for-FLOP outperform standard diffusion models. We finally scale our approach to 24 billion parameters, demonstrating that high-quality diffusion models can now be trained with just eight individual GPU nodes in less than a week.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "78",
        "title": "ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding",
        "author": [
            "Xingyu Fu",
            "Minqian Liu",
            "Zhengyuan Yang",
            "John Corring",
            "Yijuan Lu",
            "Jianwei Yang",
            "Dan Roth",
            "Dinei Florencio",
            "Cha Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05452",
        "abstract": "Structured image understanding, such as interpreting tables and charts, requires strategically refocusing across various structures and texts within an image, forming a reasoning sequence to arrive at the final answer. However, current multimodal large language models (LLMs) lack this multihop selective attention capability. In this work, we introduce ReFocus, a simple yet effective framework that equips multimodal LLMs with the ability to generate \"visual thoughts\" by performing visual editing on the input image through code, shifting and refining their visual focuses. Specifically, ReFocus enables multimodal LLMs to generate Python codes to call tools and modify the input image, sequentially drawing boxes, highlighting sections, and masking out areas, thereby enhancing the visual reasoning process. We experiment upon a wide range of structured image understanding tasks involving tables and charts. ReFocus largely improves performance on all tasks over GPT-4o without visual editing, yielding an average gain of 11.0% on table tasks and 6.8% on chart tasks. We present an in-depth analysis of the effects of different visual edits, and reasons why ReFocus can improve the performance without introducing additional information. Further, we collect a 14k training set using ReFocus, and prove that such visual chain-of-thought with intermediate information offers a better supervision than standard VQA data, reaching a 8.0% average gain over the same model trained with QA pairs and 2.6% over CoT.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "79",
        "title": "An Empirical Study of Autoregressive Pre-training from Videos",
        "author": [
            "Jathushan Rajasegaran",
            "Ilija Radosavovic",
            "Rahul Ravishankar",
            "Yossi Gandelsman",
            "Christoph Feichtenhofer",
            "Jitendra Malik"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05453",
        "abstract": "We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate. More details at https://brjathu.github.io/toto/",
        "tags": [
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "80",
        "title": "Geophysical inverse problems with measurement-guided diffusion models",
        "author": [
            "Matteo Ravasi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04881",
        "abstract": "Solving inverse problems with the reverse process of a diffusion model represents an appealing avenue to produce highly realistic, yet diverse solutions from incomplete and possibly noisy measurements, ultimately enabling uncertainty quantification at scale. However, because of the intractable nature of the score function of the likelihood term (i.e., $\\nabla_{\\mathbf{x}_t} p(\\mathbf{y} | \\mathbf{x}_t)$), various samplers have been proposed in the literature that use different (more or less accurate) approximations of such a gradient to guide the diffusion process towards solutions that match the observations. In this work, I consider two sampling algorithms recently proposed under the name of Diffusion Posterior Sampling (DPS) and Pseudo-inverse Guided Diffusion Model (PGDM), respectively. In DSP, the guidance term used at each step of the reverse diffusion process is obtained by applying the adjoint of the modeling operator to the residual obtained from a one-step denoising estimate of the solution. On the other hand, PGDM utilizes a pseudo-inverse operator that originates from the fact that the one-step denoised solution is not assumed to be deterministic, rather modeled as a Gaussian distribution. Through an extensive set of numerical examples on two geophysical inverse problems (namely, seismic interpolation and seismic inversion), I show that two key aspects for the success of any measurement-guided diffusion process are: i) our ability to re-parametrize the inverse problem such that the sought after model is bounded between -1 and 1 (a pre-requisite for any diffusion model); ii) the choice of the training dataset used to learn the implicit prior that guides the reverse diffusion process. Numerical examples on synthetic and field datasets reveal that PGDM outperforms DPS in both scenarios at limited additional cost.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "81",
        "title": "FLowHigh: Towards Efficient and High-Quality Audio Super-Resolution with Single-Step Flow Matching",
        "author": [
            "Jun-Hak Yun",
            "Seung-Bin Kim",
            "Seong-Whan Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2501.04926",
        "abstract": "Audio super-resolution is challenging owing to its ill-posed nature. Recently, the application of diffusion models in audio super-resolution has shown promising results in alleviating this challenge. However, diffusion-based models have limitations, primarily the necessity for numerous sampling steps, which causes significantly increased latency when synthesizing high-quality audio samples. In this paper, we propose FLowHigh, a novel approach that integrates flow matching, a highly efficient generative model, into audio super-resolution. We also explore probability paths specially tailored for audio super-resolution, which effectively capture high-resolution audio distributions, thereby enhancing reconstruction quality. The proposed method generates high-fidelity, high-resolution audio through a single-step sampling process across various input sampling rates. The experimental results on the VCTK benchmark dataset demonstrate that FLowHigh achieves state-of-the-art performance in audio super-resolution, as evaluated by log-spectral distance and ViSQOL while maintaining computational efficiency with only a single-step sampling process.",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "Super Resolution"
        ]
    },
    {
        "id": "82",
        "title": "Robust Score Matching",
        "author": [
            "Richard Schwank",
            "Andrew McCormack",
            "Mathias Drton"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05105",
        "abstract": "Proposed in Hyv√§rinen (2005), score matching is a parameter estimation procedure that does not require computation of distributional normalizing constants. In this work we utilize the geometric median of means to develop a robust score matching procedure that yields consistent parameter estimates in settings where the observed data has been contaminated. A special appeal of the proposed method is that it retains convexity in exponential family models. The new method is therefore particularly attractive for non-Gaussian, exponential family graphical models where evaluation of normalizing constants is intractable. Support recovery guarantees for such models when contamination is present are provided. Additionally, support recovery is studied in numerical experiments and on a precipitation dataset. We demonstrate that the proposed robust score matching estimator performs comparably to the standard score matching estimator when no contamination is present but greatly outperforms this estimator in a setting with contamination.",
        "tags": [
            "Score Matching"
        ]
    },
    {
        "id": "83",
        "title": "RadioTransformer: Accurate Radio Map Construction and Coverage Prediction",
        "author": [
            "Yuxuan Li",
            "Cheng Zhang",
            "Wen Wang",
            "Yongming Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05190",
        "abstract": "Radio map, or pathloss map prediction, is a crucial method for wireless network modeling and management. By leveraging deep learning to construct pathloss patterns from geographical maps, an accurate digital replica of the transmission environment could be established with less computational overhead and lower prediction error compared to traditional model-driven techniques. While existing state-of-the-art (SOTA) methods predominantly rely on convolutional architectures, this paper introduces a hybrid transformer-convolution model, termed RadioTransformer, to enhance the accuracy of radio map prediction. The proposed model features a multi-scale transformer-based encoder for efficient feature extraction and a convolution-based decoder for precise pixel-level image reconstruction. Simulation results demonstrate that the proposed scheme significantly improves prediction accuracy, and over a 30% reduction in root mean square error (RMSE) is achieved compared to typical SOTA approaches.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "84",
        "title": "Large Physics Models: Towards a collaborative approach with Large Language Models and Foundation Models",
        "author": [
            "Kristian G. Barman",
            "Sascha Caron",
            "Emily Sullivan",
            "Henk W. de Regt",
            "Roberto Ruiz de Austri",
            "Mieke Boon",
            "Michael F√§rber",
            "Stefan Fr√∂se",
            "Faegheh Hasibi",
            "Andreas Ipp",
            "Rukshak Kapoor",
            "Gregor Kasieczka",
            "Daniel Kostiƒá",
            "Michael Kr√§mer",
            "Tobias Golling",
            "Luis G. Lopez",
            "Jesus Marco",
            "Sydney Otten",
            "Pawel Pawlowski",
            "Pietro Vischia",
            "Erik Weber",
            "Christoph Weniger"
        ],
        "pdf": "https://arxiv.org/pdf/2501.05382",
        "abstract": "This paper explores ideas and provides a potential roadmap for the development and evaluation of physics-specific large-scale AI models, which we call Large Physics Models (LPMs). These models, based on foundation models such as Large Language Models (LLMs) - trained on broad data - are tailored to address the demands of physics research. LPMs can function independently or as part of an integrated framework. This framework can incorporate specialized tools, including symbolic reasoning modules for mathematical manipulations, frameworks to analyse specific experimental and simulated data, and mechanisms for synthesizing theories and scientific literature. We begin by examining whether the physics community should actively develop and refine dedicated models, rather than relying solely on commercial LLMs. We then outline how LPMs can be realized through interdisciplinary collaboration among experts in physics, computer science, and philosophy of science. To integrate these models effectively, we identify three key pillars: Development, Evaluation, and Philosophical Reflection. Development focuses on constructing models capable of processing physics texts, mathematical formulations, and diverse physical data. Evaluation assesses accuracy and reliability by testing and benchmarking. Finally, Philosophical Reflection encompasses the analysis of broader implications of LLMs in physics, including their potential to generate new scientific understanding and what novel collaboration dynamics might arise in research. Inspired by the organizational structure of experimental collaborations in particle physics, we propose a similarly interdisciplinary and collaborative approach to building and refining Large Physics Models. This roadmap provides specific objectives, defines pathways to achieve them, and identifies challenges that must be addressed to realise physics-specific large scale AI models.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    }
]