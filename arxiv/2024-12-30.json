[
    {
        "id": "1",
        "title": "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey",
        "author": [
            "Liang Chen",
            "Zekun Wang",
            "Shuhuai Ren",
            "Lei Li",
            "Haozhe Zhao",
            "Yunshui Li",
            "Zefan Cai",
            "Hongcheng Guo",
            "Lei Zhang",
            "Yizhe Xiong",
            "Yichi Zhang",
            "Ruoyu Wu",
            "Qingxiu Dong",
            "Ge Zhang",
            "Jian Yang",
            "Lingwei Meng",
            "Shujie Hu",
            "Yulong Chen",
            "Junyang Lin",
            "Shuai Bai",
            "Andreas Vlachos",
            "Xu Tan",
            "Minjia Zhang",
            "Wen Xiao",
            "Aaron Yee",
            "Tianyu Liu",
            "Baobao Chang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18619",
        "abstract": "Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success. As Large Language Models (LLMs) have advanced to unify understanding and generation tasks within the textual modality, recent research has shown that tasks from different modalities can also be effectively encapsulated within the NTP framework, transforming the multimodal information into tokens and predict the next one given the context. This survey introduces a comprehensive taxonomy that unifies both understanding and generation within multimodal learning through the lens of NTP. The proposed taxonomy covers five key aspects: Multimodal tokenization, MMNTP model architectures, unified task representation, datasets \\& evaluation, and open challenges. This new taxonomy aims to aid researchers in their exploration of multimodal intelligence. An associated GitHub repository collecting the latest papers and repos is available at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "2",
        "title": "Investigating the Feasibility of Mitigating Potential Copyright Infringement via Large Language Model Unlearning",
        "author": [
            "Guangyao Dou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18621",
        "abstract": "Pre-trained Large Language Models (LLMs) have demonstrated remarkable capabilities but also pose risks by learning and generating copyrighted material, leading to significant legal and ethical concerns. In a potential real-world scenario, model owners may need to continuously address copyright infringement in order to address requests for content removal that emerge at different time points. One potential way of addressing this is via sequential unlearning, where copyrighted content is removed sequentially as new requests arise. Despite its practical relevance, sequential unlearning in the context of copyright infringement has not been rigorously explored in existing literature. To address this gap, we propose Stable Sequential Unlearning (SSU), a novel framework designed to unlearn copyrighted content from LLMs over multiple time steps. Our approach works by identifying and removing specific weight updates in the model's parameters that correspond to copyrighted content using task vectors. We improve unlearning efficacy by introducing random labeling loss and ensuring the model retains its general-purpose knowledge by adjusting targeted parameters with gradient-based weight saliency. Extensive experimental results show that SSU sometimes achieves an effective trade-off between unlearning efficacy and general-purpose language abilities, outperforming existing baselines, but it's not a cure-all for unlearning copyrighted material.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "3",
        "title": "Why Do Large Language Models (LLMs) Struggle to Count Letters?",
        "author": [
            "Tairan Fu",
            "Raquel Ferrando",
            "Javier Conde",
            "Carlos Arriaga",
            "Pedro Reviriego"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18626",
        "abstract": "Large Language Models (LLMs) have achieved unprecedented performance on many complex tasks, being able, for example, to answer questions on almost any topic. However, they struggle with other simple tasks, such as counting the occurrences of letters in a word, as illustrated by the inability of many LLMs to count the number of \"r\" letters in \"strawberry\". Several works have studied this problem and linked it to the tokenization used by LLMs, to the intrinsic limitations of the attention mechanism, or to the lack of character-level training data. In this paper, we conduct an experimental study to evaluate the relations between the LLM errors when counting letters with 1) the frequency of the word and its components in the training dataset and 2) the complexity of the counting operation. We present a comprehensive analysis of the errors of LLMs when counting letter occurrences by evaluating a representative group of models over a large number of words. The results show a number of consistent trends in the models evaluated: 1) models are capable of recognizing the letters but not counting them; 2) the frequency of the word and tokens in the word does not have a significant impact on the LLM errors; 3) there is a positive correlation of letter frequency with errors, more frequent letters tend to have more counting errors, 4) the errors show a strong correlation with the number of letters or tokens in a word and 5) the strongest correlation occurs with the number of letters with counts larger than one, with most models being unable to correctly count words in which letters appear more than twice.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "4",
        "title": "KRAIL: A Knowledge-Driven Framework for Base Human Reliability Analysis Integrating IDHEAS and Large Language Models",
        "author": [
            "Xingyu Xiao",
            "Peng Chen",
            "Ben Qi",
            "Hongru Zhao",
            "Jingang Liang",
            "Jiejuan Tong",
            "Haitao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18627",
        "abstract": "Human reliability analysis (HRA) is crucial for evaluating and improving the safety of complex systems. Recent efforts have focused on estimating human error probability (HEP), but existing methods often rely heavily on expert knowledge,which can be subjective and time-consuming. Inspired by the success of large language models (LLMs) in natural language processing, this paper introduces a novel two-stage framework for knowledge-driven reliability analysis, integrating IDHEAS and LLMs (KRAIL). This innovative framework enables the semi-automated computation of base HEP values. Additionally, knowledge graphs are utilized as a form of retrieval-augmented generation (RAG) for enhancing the framework' s capability to retrieve and process relevant data efficiently. Experiments are systematically conducted and evaluated on authoritative datasets of human reliability. The experimental results of the proposed methodology demonstrate its superior performance on base HEP estimation under partial information for reliability assessment.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "5",
        "title": "DynaGRAG: Improving Language Understanding and Generation through Dynamic Subgraph Representation in Graph Retrieval-Augmented Generation",
        "author": [
            "Karishma Thakrar"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18644",
        "abstract": "Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to enhance language understanding and generation by leveraging external knowledge. However, effectively capturing and integrating the rich semantic information present in textual and structured data remains a challenge. To address this, a novel GRAG framework is proposed to focus on enhancing subgraph representation and diversity within the knowledge graph. By improving graph density, capturing entity and relation information more effectively, and dynamically prioritizing relevant and diverse subgraphs, the proposed approach enables a more comprehensive understanding of the underlying semantic structure. This is achieved through a combination of de-duplication processes, two-step mean pooling of embeddings, query-aware retrieval considering unique nodes, and a Dynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph Convolutional Networks (GCNs) and Large Language Models (LLMs) through hard prompting further enhances the learning of rich node and edge representations while preserving the hierarchical subgraph structure. Experimental results on multiple benchmark datasets demonstrate the effectiveness of the proposed GRAG framework, showcasing the significance of enhanced subgraph representation and diversity for improved language understanding and generation.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "6",
        "title": "Dissecting CLIP: Decomposition with a Schur Complement-based Approach",
        "author": [
            "Azim Ospanov",
            "Mohammad Jalali",
            "Farzan Farnia"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18645",
        "abstract": "The use of CLIP embeddings to assess the alignment of samples produced by text-to-image generative models has been extensively explored in the literature. While the widely adopted CLIPScore, derived from the cosine similarity of text and image embeddings, effectively measures the relevance of a generated image, it does not quantify the diversity of images generated by a text-to-image model. In this work, we extend the application of CLIP embeddings to quantify and interpret the intrinsic diversity of text-to-image models, which is responsible for generating diverse images from similar text prompts. To achieve this, we propose a decomposition of the CLIP-based kernel covariance matrix of image data into text-based and non-text-based components. Using the Schur complement of the joint image-text kernel covariance matrix, we perform this decomposition and define the matrix-based entropy of the decomposed component as the \\textit{Schur Complement Entropy (SCE)} score, a measure of the intrinsic diversity of a text-to-image model based on data collected with varying text prompts. Additionally, we demonstrate the use of the Schur complement-based decomposition to nullify the influence of a given prompt in the CLIP embedding of an image, enabling focus or defocus of embeddings on specific objects or properties for downstream tasks. We present several numerical results that apply our Schur complement-based approach to evaluate text-to-image models and modify CLIP image embeddings. The codebase is available at https://github.com/aziksh-ospanov/CLIP-DISSECTION",
        "tags": [
            "CLIP",
            "Text-to-Image"
        ]
    },
    {
        "id": "7",
        "title": "1.58-bit FLUX",
        "author": [
            "Chenglin Yang",
            "Celong Liu",
            "Xueqing Deng",
            "Dongwon Kim",
            "Xing Mei",
            "Xiaohui Shen",
            "Liang-Chieh Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18653",
        "abstract": "We present 1.58-bit FLUX, the first successful approach to quantizing the state-of-the-art text-to-image generation model, FLUX.1-dev, using 1.58-bit weights (i.e., values in {-1, 0, +1}) while maintaining comparable performance for generating 1024 x 1024 images. Notably, our quantization method operates without access to image data, relying solely on self-supervision from the FLUX.1-dev model. Additionally, we develop a custom kernel optimized for 1.58-bit operations, achieving a 7.7x reduction in model storage, a 5.1x reduction in inference memory, and improved inference latency. Extensive evaluations on the GenEval and T2I Compbench benchmarks demonstrate the effectiveness of 1.58-bit FLUX in maintaining generation quality while significantly enhancing computational efficiency.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "8",
        "title": "Comparing analytic and data-driven approaches to parameter identifiability: A power systems case study",
        "author": [
            "Nikolaos Evangelou",
            "Alexander M. Stankovic",
            "Ioannis G. Kevrekidis",
            "Mark K. Transtrum"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18663",
        "abstract": "Parameter identifiability refers to the capability of accurately inferring the parameter values of a model from its observations (data). Traditional analysis methods exploit analytical properties of the closed form model, in particular sensitivity analysis, to quantify the response of the model predictions to variations in parameters. Techniques developed to analyze data, specifically manifold learning methods, have the potential to complement, and even extend the scope of the traditional analytical approaches. We report on a study comparing and contrasting analytical and data-driven approaches to quantify parameter identifiability and, importantly, perform parameter reduction tasks. We use the infinite bus synchronous generator model, a well-understood model from the power systems domain, as our benchmark problem. Our traditional analysis methods use the Fisher Information Matrix to quantify parameter identifiability analysis, and the Manifold Boundary Approximation Method to perform parameter reduction. We compare these results to those arrived at through data-driven manifold learning schemes: Output - Diffusion Maps and Geometric Harmonics. For our test case, we find that the two suites of tools (analytical when a model is explicitly available, as well as data-driven when the model is lacking and only measurement data are available) give (correct) comparable results; these results are also in agreement with traditional analysis based on singular perturbation theory. We then discuss the prospects of using data-driven methods for such model analysis.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "9",
        "title": "Map2Text: New Content Generation from Low-Dimensional Visualizations",
        "author": [
            "Xingjian Zhang",
            "Ziyang Xiong",
            "Shixuan Liu",
            "Yutong Xie",
            "Tolga Ergen",
            "Dongsub Shim",
            "Hua Xu",
            "Honglak Lee",
            "Qiaozhu Me"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18673",
        "abstract": "Low-dimensional visualizations, or \"projection maps\" of datasets, are widely used across scientific research and creative industries as effective tools for interpreting large-scale and complex information. These visualizations not only support understanding existing knowledge spaces but are often used implicitly to guide exploration into unknown areas. While powerful methods like TSNE or UMAP can create such visual maps, there is currently no systematic way to leverage them for generating new content. To bridge this gap, we introduce Map2Text, a novel task that translates spatial coordinates within low-dimensional visualizations into new, coherent, and accurately aligned textual content. This allows users to explore and navigate undiscovered information embedded in these spatial layouts interactively and intuitively. To evaluate the performance of Map2Text methods, we propose Atometric, an evaluation metric that provides a granular assessment of logical coherence and alignment of the atomic statements in the generated texts. Experiments conducted across various datasets demonstrate the versatility of Map2Text in generating scientific research hypotheses, crafting synthetic personas, and devising strategies for testing large language models. Our findings highlight the potential of Map2Text to unlock new pathways for interacting with and navigating large-scale textual datasets, offering a novel framework for spatially guided content generation and discovery.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "10",
        "title": "TAB: Transformer Attention Bottlenecks enable User Intervention and Debugging in Vision-Language Models",
        "author": [
            "Pooyan Rahmanzadehgrevi",
            "Hung Huy Nguyen",
            "Rosanne Liu",
            "Long Mai",
            "Anh Totti Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18675",
        "abstract": "Multi-head self-attention (MHSA) is a key component of Transformers, a widely popular architecture in both language and vision. Multiple heads intuitively enable different parallel processes over the same input. Yet, they also obscure the attribution of each input patch to the output of a model. We propose a novel 1-head Transformer Attention Bottleneck (TAB) layer, inserted after the traditional MHSA architecture, to serve as an attention bottleneck for interpretability and intervention. Unlike standard self-attention, TAB constrains the total attention over all patches to $\\in [0, 1]$. That is, when the total attention is 0, no visual information is propagated further into the network and the vision-language model (VLM) would default to a generic, image-independent response. To demonstrate the advantages of TAB, we train VLMs with TAB to perform image difference captioning. Over three datasets, our models perform similarly to baseline VLMs in captioning but the bottleneck is superior in localizing changes and in identifying when no changes occur. TAB is the first architecture to enable users to intervene by editing attention, which often produces expected outputs by VLMs.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "11",
        "title": "Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation",
        "author": [
            "Faraz Waseem",
            "Muhammad Shahzad"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18688",
        "abstract": "An image may convey a thousand words, but a video composed of hundreds or thousands of image frames tells a more intricate story. Despite significant progress in multimodal large language models (MLLMs), generating extended videos remains a formidable challenge. As of this writing, OpenAI's Sora, the current state-of-the-art system, is still limited to producing videos that are up to one minute in length. This limitation stems from the complexity of long video generation, which requires more than generative AI techniques for approximating density functions essential aspects such as planning, story development, and maintaining spatial and temporal consistency present additional hurdles. Integrating generative AI with a divide-and-conquer approach could improve scalability for longer videos while offering greater control. In this survey, we examine the current landscape of long video generation, covering foundational techniques like GANs and diffusion models, video generation strategies, large-scale training datasets, quality metrics for evaluating long videos, and future research areas to address the limitations of the existing video generation capabilities. We believe it would serve as a comprehensive foundation, offering extensive information to guide future advancements and research in the field of long video generation.",
        "tags": [
            "Diffusion",
            "Large Language Models",
            "Sora",
            "Video Generation"
        ]
    },
    {
        "id": "12",
        "title": "AgreeMate: Teaching LLMs to Haggle",
        "author": [
            "Ainesh Chatterjee",
            "Samuel Miller",
            "Nithin Parepally"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18690",
        "abstract": "We introduce AgreeMate, a framework for training Large Language Models (LLMs) to perform strategic price negotiations through natural language. We apply recent advances to a negotiation setting where two agents (i.e. buyer or seller) use natural language to bargain on goods using coarse actions. Specifically, we present the performance of Large Language Models when used as agents within a decoupled (modular) bargaining architecture. We demonstrate that using prompt engineering, fine-tuning, and chain-of-thought prompting enhances model performance, as defined by novel metrics. We use attention probing to show model attention to semantic relationships between tokens during negotiations.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "13",
        "title": "TimelyLLM: Segmented LLM Serving System for Time-sensitive Robotic Applications",
        "author": [
            "Neiwen Ling",
            "Guojun Chen",
            "Lin Zhong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18695",
        "abstract": "Large Language Models (LLMs) such as GPT-4 and Llama3 can already comprehend complex commands and process diverse tasks. This advancement facilitates their application in controlling drones and robots for various tasks. However, existing LLM serving systems typically employ a first-come, first-served (FCFS) batching mechanism, which fails to address the time-sensitive requirements of robotic applications. To address it, this paper proposes a new system named TimelyLLM serving multiple robotic agents with time-sensitive requests. TimelyLLM introduces novel mechanisms of segmented generation and scheduling that optimally leverage redundancy between robot plan generation and execution phases. We report an implementation of TimelyLLM on a widely-used LLM serving framework and evaluate it on a range of robotic applications. Our evaluation shows that TimelyLLM improves the time utility up to 1.97x, and reduces the overall waiting time by 84%.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models",
            "Robot"
        ]
    },
    {
        "id": "14",
        "title": "CypherBench: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the LLM Era",
        "author": [
            "Yanlin Feng",
            "Simone Papicchio",
            "Sajjadur Rahman"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18702",
        "abstract": "Retrieval from graph data is crucial for augmenting large language models (LLM) with both open-domain knowledge and private enterprise data, and it is also a key component in the recent GraphRAG system (edge et al., 2024). Despite decades of research on knowledge graphs and knowledge base question answering, leading LLM frameworks (e.g. Langchain and LlamaIndex) have only minimal support for retrieval from modern encyclopedic knowledge graphs like Wikidata. In this paper, we analyze the root cause and suggest that modern RDF knowledge graphs (e.g. Wikidata, Freebase) are less efficient for LLMs due to overly large schemas that far exceed the typical LLM context window, use of resource identifiers, overlapping relation types and lack of normalization. As a solution, we propose property graph views on top of the underlying RDF graph that can be efficiently queried by LLMs using Cypher. We instantiated this idea on Wikidata and introduced CypherBench, the first benchmark with 11 large-scale, multi-domain property graphs with 7.8 million entities and over 10,000 questions. To achieve this, we tackled several key challenges, including developing an RDF-to-property graph conversion engine, creating a systematic pipeline for text-to-Cypher task generation, and designing new evaluation metrics.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "15",
        "title": "Multiple References with Meaningful Variations Improve Literary Machine Translation",
        "author": [
            "Si Wu",
            "John Wieting",
            "David A. Smith"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18707",
        "abstract": "While a source sentence can be translated in many ways, most machine translation (MT) models are trained with only a single reference. Previous work has shown that using synthetic paraphrases can improve MT. This paper investigates best practices for employing multiple references by analyzing the semantic similarity among different English translations of world literature in the Par3 dataset. We classify the semantic similarity between paraphrases into three groups: low, medium, and high, and fine-tune two different LLMs (mT5-large and LLaMA-2-7B) for downstream MT tasks. Across different models, holding the total training instances constant, single-reference but more source texts only marginally outperforms multiple-reference with half of the source texts. Moreover, using paraphrases of medium and high semantic similarity outperforms an unfiltered dataset (+BLEU 0.3-0.5, +COMET 0.2-0.9, +chrF++ 0.25-0.32). Our code is publicly available on GitHub.",
        "tags": [
            "LLMs",
            "LLaMA"
        ]
    },
    {
        "id": "16",
        "title": "Simi-SFX: A similarity-based conditioning method for controllable sound effect synthesis",
        "author": [
            "Yunyi Liu",
            "Craig Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18710",
        "abstract": "Generating sound effects with controllable variations is a challenging task, traditionally addressed using sophisticated physical models that require in-depth knowledge of signal processing parameters and algorithms. In the era of generative and large language models, text has emerged as a common, human-interpretable interface for controlling sound synthesis. However, the discrete and qualitative nature of language tokens makes it difficult to capture subtle timbral variations across different sounds. In this research, we propose a novel similarity-based conditioning method for sound synthesis, leveraging differentiable digital signal processing (DDSP). This approach combines the use of latent space for learning and controlling audio timbre with an intuitive guiding vector, normalized within the range [0,1], to encode categorical acoustic information. By utilizing pre-trained audio representation models, our method achieves expressive and fine-grained timbre control. To benchmark our approach, we introduce two sound effect datasets--Footstep-set and Impact-set--designed to evaluate both controllability and sound quality. Regression analysis demonstrates that the proposed similarity score effectively controls timbre variations and enables creative applications such as timbre interpolation between discrete classes. Our work provides a robust and versatile framework for sound effect synthesis, bridging the gap between traditional signal processing and modern machine learning techniques.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "17",
        "title": "Using Large Language Models for Automated Grading of Student Writing about Science",
        "author": [
            "Chris Impey",
            "Matthew Wenger",
            "Nikhil Garuda",
            "Shahriar Golchin",
            "Sarah Stamer"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18719",
        "abstract": "Assessing writing in large classes for formal or informal learners presents a significant challenge. Consequently, most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience consisted of adult learners in three massive open online courses (MOOCs) offered through Coursera. One course was on astronomy, the second was on astrobiology, and the third was on the history and philosophy of astronomy. The results should also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "18",
        "title": "SAFLITE: Fuzzing Autonomous Systems via Large Language Models",
        "author": [
            "Taohong Zhu",
            "Adrians Skapars",
            "Fardeen Mackenzie",
            "Declan Kehoe",
            "William Newton",
            "Suzanne Embury",
            "Youcheng Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18727",
        "abstract": "Fuzz testing effectively uncovers software vulnerabilities; however, it faces challenges with Autonomous Systems (AS) due to their vast search spaces and complex state spaces, which reflect the unpredictability and complexity of real-world environments. This paper presents a universal framework aimed at improving the efficiency of fuzz testing for AS. At its core is SaFliTe, a predictive component that evaluates whether a test case meets predefined safety criteria. By leveraging the large language model (LLM) with information about the test objective and the AS state, SaFliTe assesses the relevance of each test case. We evaluated SaFliTe by instantiating it with various LLMs, including GPT-3.5, Mistral-7B, and Llama2-7B, and integrating it into four fuzz testing tools: PGFuzz, DeepHyperion-UAV, CAMBA, and TUMB. These tools are designed specifically for testing autonomous drone control systems, such as ArduPilot, PX4, and PX4-Avoidance. The experimental results demonstrate that, compared to PGFuzz, SaFliTe increased the likelihood of selecting operations that triggered bug occurrences in each fuzzing iteration by an average of 93.1\\%. Additionally, after integrating SaFliTe, the ability of DeepHyperion-UAV, CAMBA, and TUMB to generate test cases that caused system violations increased by 234.5\\%, 33.3\\%, and 17.8\\%, respectively. The benchmark for this evaluation was sourced from a UAV Testing Competition.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "19",
        "title": "Optimizing Large Language Models with an Enhanced LoRA Fine-Tuning Algorithm for Efficiency and Robustness in NLP Tasks",
        "author": [
            "Jiacheng Hu",
            "Xiaoxuan Liao",
            "Jia Gao",
            "Zhen Qi",
            "Hongye Zheng",
            "Chihang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18729",
        "abstract": "This study proposes a large language model optimization method based on the improved LoRA fine-tuning algorithm, aiming to improve the accuracy and computational efficiency of the model in natural language processing tasks. We fine-tune the large language model through a low-rank adaptation strategy, which significantly reduces the consumption of computing resources while maintaining the powerful capabilities of the pre-trained model. The experiment uses the QQP task as the evaluation scenario. The results show that the improved LoRA algorithm shows significant improvements in accuracy, F1 score, and MCC compared with traditional models such as BERT, Roberta, T5, and GPT-4. In particular, in terms of F1 score and MCC, our model shows stronger robustness and discrimination ability, which proves the potential of the improved LoRA algorithm in fine-tuning large-scale pre-trained models. In addition, this paper also discusses the application prospects of the improved LoRA algorithm in other natural language processing tasks, emphasizing its advantages in multi-task learning and scenarios with limited computing resources. Future research can further optimize the LoRA fine-tuning strategy and expand its application in larger-scale pre-trained models to improve the generalization ability and task adaptability of the model.",
        "tags": [
            "BERT",
            "GPT",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "20",
        "title": "Elucidating Flow Matching ODE Dynamics with respect to Data Geometries",
        "author": [
            "Gal Mishne",
            "Zhengchao Wan",
            "Qingsong Wang",
            "Yusu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18730",
        "abstract": "Diffusion-based generative models have become the standard for image generation. ODE-based samplers and flow matching models improve efficiency, in comparison to diffusion models, by reducing sampling steps through learned vector fields. However, the theoretical foundations of flow matching models remain limited, particularly regarding the convergence of individual sample trajectories at terminal time - a critical property that impacts sample quality and being critical assumption for models like the consistency model. In this paper, we advance the theory of flow matching models through a comprehensive analysis of sample trajectories, centered on the denoiser that drives ODE dynamics. We establish the existence, uniqueness and convergence of ODE trajectories at terminal time, ensuring stable sampling outcomes under minimal assumptions. Our analysis reveals how trajectories evolve from capturing global data features to local structures, providing the geometric characterization of per-sample behavior in flow matching models. We also explain the memorization phenomenon in diffusion-based training through our terminal time analysis. These findings bridge critical gaps in understanding flow matching models, with practical implications for sampling stability and model design.",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "ODE"
        ]
    },
    {
        "id": "21",
        "title": "The Impact of Input Order Bias on Large Language Models for Software Fault Localization",
        "author": [
            "Md Nakhla Rafi",
            "Dong Jae Kim",
            "Tse-Hsun Chen",
            "Shaowei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18750",
        "abstract": "Large Language Models (LLMs) show great promise in software engineering tasks like Fault Localization (FL) and Automatic Program Repair (APR). This study examines how input order and context size affect LLM performance in FL, a key step for many downstream software engineering tasks. We test different orders for methods using Kendall Tau distances, including \"perfect\" (where ground truths come first) and \"worst\" (where ground truths come last). Our results show a strong bias in order, with Top-1 accuracy falling from 57\\% to 20\\% when we reverse the code order. Breaking down inputs into smaller contexts helps reduce this bias, narrowing the performance gap between perfect and worst orders from 22\\% to just 1\\%. We also look at ordering methods based on traditional FL techniques and metrics. Ordering using DepGraph's ranking achieves 48\\% Top-1 accuracy, better than more straightforward ordering approaches like CallGraph. These findings underscore the importance of how we structure inputs, manage contexts, and choose ordering methods to improve LLM performance in FL and other software engineering tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "22",
        "title": "Attack-in-the-Chain: Bootstrapping Large Language Models for Attacks Against Black-box Neural Ranking Models",
        "author": [
            "Yu-An Liu",
            "Ruqing Zhang",
            "Jiafeng Guo",
            "Maarten de Rijke",
            "Yixing Fan",
            "Xueqi Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18770",
        "abstract": "Neural ranking models (NRMs) have been shown to be highly effective in terms of retrieval performance. Unfortunately, they have also displayed a higher degree of sensitivity to attacks than previous generation models. To help expose and address this lack of robustness, we introduce a novel ranking attack framework named Attack-in-the-Chain, which tracks interactions between large language models (LLMs) and NRMs based on chain-of-thought (CoT) prompting to generate adversarial examples under black-box settings. Our approach starts by identifying anchor documents with higher ranking positions than the target document as nodes in the reasoning chain. We then dynamically assign the number of perturbation words to each node and prompt LLMs to execute attacks. Finally, we verify the attack performance of all nodes at each reasoning step and proceed to generate the next reasoning step. Empirical results on two web search benchmarks show the effectiveness of our method.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "23",
        "title": "ObitoNet: Multimodal High-Resolution Point Cloud Reconstruction",
        "author": [
            "Apoorv Thapliyal",
            "Vinay Lanka",
            "Swathi Baskaran"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18775",
        "abstract": "ObitoNet employs a Cross Attention mechanism to integrate multimodal inputs, where Vision Transformers (ViT) extract semantic features from images and a point cloud tokenizer processes geometric information using Farthest Point Sampling (FPS) and K Nearest Neighbors (KNN) for spatial structure capture. The learned multimodal features are fed into a transformer-based decoder for high-resolution point cloud reconstruction. This approach leverages the complementary strengths of both modalities rich image features and precise geometric details ensuring robust point cloud generation even in challenging conditions such as sparse or noisy data.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "24",
        "title": "ArtNVG: Content-Style Separated Artistic Neighboring-View Gaussian Stylization",
        "author": [
            "Zixiao Gu",
            "Mengtian Li",
            "Ruhua Chen",
            "Zhongxia Ji",
            "Sichen Guo",
            "Zhenye Zhang",
            "Guangnan Ye",
            "Zuo Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18783",
        "abstract": "As demand from the film and gaming industries for 3D scenes with target styles grows, the importance of advanced 3D stylization techniques increases. However, recent methods often struggle to maintain local consistency in color and texture throughout stylized scenes, which is essential for maintaining aesthetic coherence. To solve this problem, this paper introduces ArtNVG, an innovative 3D stylization framework that efficiently generates stylized 3D scenes by leveraging reference style images. Built on 3D Gaussian Splatting (3DGS), ArtNVG achieves rapid optimization and rendering while upholding high reconstruction quality. Our framework realizes high-quality 3D stylization by incorporating two pivotal techniques: Content-Style Separated Control and Attention-based Neighboring-View Alignment. Content-Style Separated Control uses the CSGO model and the Tile ControlNet to decouple the content and style control, reducing risks of information leakage. Concurrently, Attention-based Neighboring-View Alignment ensures consistency of local colors and textures across neighboring views, significantly improving visual quality. Extensive experiments validate that ArtNVG surpasses existing methods, delivering superior results in content preservation, style alignment, and local consistency.",
        "tags": [
            "3D",
            "ControlNet",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "25",
        "title": "DRDM: A Disentangled Representations Diffusion Model for Synthesizing Realistic Person Images",
        "author": [
            "Enbo Huang",
            "Yuan Zhang",
            "Faliang Huang",
            "Guangyu Zhang",
            "Yang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18797",
        "abstract": "Person image synthesis with controllable body poses and appearances is an essential task owing to the practical needs in the context of virtual try-on, image editing and video production. However, existing methods face significant challenges with details missing, limbs distortion and the garment style deviation. To address these issues, we propose a Disentangled Representations Diffusion Model (DRDM) to generate photo-realistic images from source portraits in specific desired poses and appearances. First, a pose encoder is responsible for encoding pose features into a high-dimensional space to guide the generation of person images. Second, a body-part subspace decoupling block (BSDB) disentangles features from the different body parts of a source figure and feeds them to the various layers of the noise prediction block, thereby supplying the network with rich disentangled features for generating a realistic target image. Moreover, during inference, we develop a parsing map-based disentangled classifier-free guided sampling method, which amplifies the conditional signals of texture and pose. Extensive experimental results on the Deepfashion dataset demonstrate the effectiveness of our approach in achieving pose transfer and appearance control.",
        "tags": [
            "Diffusion",
            "Image Editing",
            "Virtual Try-On"
        ]
    },
    {
        "id": "26",
        "title": "Ister: Inverted Seasonal-Trend Decomposition Transformer for Explainable Multivariate Time Series Forecasting",
        "author": [
            "Fanpu Cao",
            "Shu Yang",
            "Zhengjian Chen",
            "Ye Liu",
            "Laizhong Cui"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18798",
        "abstract": "In long-term time series forecasting, Transformer-based models have achieved great success, due to its ability to capture long-range dependencies. However, existing transformer-based methods face challenges in accurately identifying which variables play a pivotal role in the prediction process and tend to overemphasize noisy channels, thereby limiting the interpretability and practical effectiveness of the models. Besides, it faces scalability issues due to quadratic computational complexity of self-attention. In this paper, we propose a new model named Inverted Seasonal-Trend Decomposition Transformer (Ister), which addresses these challenges in long-term multivariate time series forecasting by designing an improved Transformer-based structure. Ister firstly decomposes original time series into seasonal and trend components. Then we propose a new Dot-attention mechanism to process the seasonal component, which improves both accuracy, computation complexity and interpretability. Upon completion of the training phase, it allows users to intuitively visualize the significance of each feature in the overall prediction. We conduct comprehensive experiments, and the results show that Ister achieves state-of-the-art (SOTA) performance on multiple datasets, surpassing existing models in long-term prediction tasks.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "27",
        "title": "Improving Generated and Retrieved Knowledge Combination Through Zero-shot Generation",
        "author": [
            "Xinkai Du",
            "Quanjie Han",
            "Chao Lv",
            "Yan Liu",
            "Yalin Sun",
            "Hao Shu",
            "Hongbo Shan",
            "Maosong Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18800",
        "abstract": "Open-domain Question Answering (QA) has garnered substantial interest by combining the advantages of faithfully retrieved passages and relevant passages generated through Large Language Models (LLMs). However, there is a lack of definitive labels available to pair these sources of knowledge. In order to address this issue, we propose an unsupervised and simple framework called Bi-Reranking for Merging Generated and Retrieved Knowledge (BRMGR), which utilizes re-ranking methods for both retrieved passages and LLM-generated passages. We pair the two types of passages using two separate re-ranking methods and then combine them through greedy matching. We demonstrate that BRMGR is equivalent to employing a bipartite matching loss when assigning each retrieved passage with a corresponding LLM-generated passage. The application of our model yielded experimental results from three datasets, improving their performance by +1.7 and +1.6 on NQ and WebQ datasets, respectively, and obtaining comparable result on TriviaQA dataset when compared to competitive baselines.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "28",
        "title": "FOR: Finetuning for Object Level Open Vocabulary Image Retrieval",
        "author": [
            "Hila Levi",
            "Guy Heller",
            "Dan Levi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18806",
        "abstract": "As working with large datasets becomes standard, the task of accurately retrieving images containing objects of interest by an open set textual query gains practical importance. The current leading approach utilizes a pre-trained CLIP model without any adaptation to the target domain, balancing accuracy and efficiency through additional post-processing. In this work, we propose FOR: Finetuning for Object-centric Open-vocabulary Image Retrieval, which allows finetuning on a target dataset using closed-set labels while keeping the visual-language association crucial for open vocabulary retrieval. FOR is based on two design elements: a specialized decoder variant of the CLIP head customized for the intended task, and its coupling within a multi-objective training framework. Together, these design choices result in a significant increase in accuracy, showcasing improvements of up to 8 mAP@50 points over SoTA across three datasets. Additionally, we demonstrate that FOR is also effective in a semi-supervised setting, achieving impressive results even when only a small portion of the dataset is labeled.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "29",
        "title": "DebiasDiff: Debiasing Text-to-image Diffusion Models with Self-discovering Latent Attribute Directions",
        "author": [
            "Yilei Jiang",
            "Weihong Li",
            "Yiyuan Zhang",
            "Minghong Cai",
            "Xiangyu Yue"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18810",
        "abstract": "While Diffusion Models (DM) exhibit remarkable performance across various image generative tasks, they nonetheless reflect the inherent bias presented in the training set. As DMs are now widely used in real-world applications, these biases could perpetuate a distorted worldview and hinder opportunities for minority groups. Existing methods on debiasing DMs usually requires model re-training with a human-crafted reference dataset or additional classifiers, which suffer from two major limitations: (1) collecting reference datasets causes expensive annotation cost; (2) the debiasing performance is heavily constrained by the quality of the reference dataset or the additional classifier. To address the above limitations, we propose DebiasDiff, a plug-and-play method that learns attribute latent directions in a self-discovering manner, thus eliminating the reliance on such reference dataset. Specifically, DebiasDiff consists of two parts: a set of attribute adapters and a distribution indicator. Each adapter in the set aims to learn an attribute latent direction, and is optimized via noise composition through a self-discovering process. Then, the distribution indicator is multiplied by the set of adapters to guide the generation process towards the prescribed distribution. Our method enables debiasing multiple attributes in DMs simultaneously, while remaining lightweight and easily integrable with other DMs, eliminating the need for re-training. Extensive experiments on debiasing gender, racial, and their intersectional biases show that our method outperforms previous SOTA by a large margin.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "30",
        "title": "DCIS: Efficient Length Extrapolation of LLMs via Divide-and-Conquer Scaling Factor Search",
        "author": [
            "Lei Yang",
            "Shaoyang Xu",
            "Deyi Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18811",
        "abstract": "Large language models (LLMs) based on the Transformer architecture usually have their context length limited due to the high training cost. Recent advancements extend the context window by adjusting the scaling factors of RoPE and fine-tuning. However, suboptimal initialization of these factors results in increased fine-tuning costs and reduced performance at target length. To address these challenges, we propose an innovative RoPE-based fine-tuning framework that diverges from conventional scaling factors search. Specifically, we present a Divide-and-Conquer Incremental Search (DCIS) algorithm that strategically determines the better scaling factors. Further fine-tuning with the identified scaling factors effectively extends the context window of LLMs. Empirical results demonstrate that our methodology not only mitigates performance decay at extended target lengths but also allows the model to fine-tune on short contexts and generalize to long contexts, thereby reducing the cost of fine-tuning. The scaling factors obtained through DCIS can even perform effectively without fine-tuning. Further analysis of the search space reveals that DCIS achieves twice the search efficiency compared to other methods. We also examine the impact of the non-strictly increasing scaling factors utilized in DCIS and evaluate the general capabilities of LLMs across various context lengths.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "31",
        "title": "GSAVS: Gaussian Splatting-based Autonomous Vehicle Simulator",
        "author": [
            "Rami Wilson"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18816",
        "abstract": "Modern autonomous vehicle simulators feature an ever-growing library of assets, including vehicles, buildings, roads, pedestrians, and more. While this level of customization proves beneficial when creating virtual urban environments, this process becomes cumbersome when intending to train within a digital twin or a duplicate of a real scene. Gaussian splatting emerged as a powerful technique in scene reconstruction and novel view synthesis, boasting high fidelity and rendering speeds. In this paper, we introduce GSAVS, an autonomous vehicle simulator that supports the creation and development of autonomous vehicle models. Every asset within the simulator is a 3D Gaussian splat, including the vehicles and the environment. However, the simulator runs within a classical 3D engine, rendering 3D Gaussian splats in real-time. This allows the simulator to utilize the photorealism that 3D Gaussian splatting boasts while providing the customization and ease of use of a classical 3D engine.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "32",
        "title": "LLM-assisted vector similarity search",
        "author": [
            "Md Riyadh",
            "Muqi Li",
            "Felix Haryanto Lie",
            "Jia Long Loh",
            "Haotian Mi",
            "Sayam Bohra"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18819",
        "abstract": "As data retrieval demands become increasingly complex, traditional search methods often fall short in addressing nuanced and conceptual queries. Vector similarity search has emerged as a promising technique for finding semantically similar information efficiently. However, its effectiveness diminishes when handling intricate queries with contextual nuances. This paper explores a hybrid approach combining vector similarity search with Large Language Models (LLMs) to enhance search accuracy and relevance. The proposed two-step solution first employs vector similarity search to shortlist potential matches, followed by an LLM for context-aware ranking of the results. Experiments on structured datasets demonstrate that while vector similarity search alone performs well for straightforward queries, the LLM-assisted approach excels in processing complex queries involving constraints, negations, or conceptual requirements. By leveraging the natural language understanding capabilities of LLMs, this method improves the accuracy of search results for complex tasks without sacrificing efficiency. We also discuss real-world applications and propose directions for future research to refine and scale this technique for diverse datasets and use cases.\nOriginal article: https://engineering.grab.com/llm-assisted-vector-similarity-search",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "33",
        "title": "RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting",
        "author": [
            "Yilei Jiang",
            "Yingshui Tan",
            "Xiangyu Yue"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18826",
        "abstract": "While Multimodal Large Language Models (MLLMs) have made remarkable progress in vision-language reasoning, they are also more susceptible to producing harmful content compared to models that focus solely on text. Existing defensive prompting techniques rely on a static, unified safety guideline that fails to account for the specific risks inherent in different multimodal contexts. To address these limitations, we propose RapGuard, a novel framework that uses multimodal chain-of-thought reasoning to dynamically generate scenario-specific safety prompts. RapGuard enhances safety by adapting its prompts to the unique risks of each input, effectively mitigating harmful outputs while maintaining high performance on benign tasks. Our experimental results across multiple MLLM benchmarks demonstrate that RapGuard achieves state-of-the-art safety performance, significantly reducing harmful content without degrading the quality of responses.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "34",
        "title": "LoGFiLM: Fine-Tuning A Large Language Model for Automated Generation of Log Statements",
        "author": [
            "Hao Zhang",
            "Dongjun Yu",
            "Lei Zhang",
            "Guoping Rong",
            "Yongda Yu",
            "Haifeng Shen",
            "He Zhang",
            "Dong Shao",
            "Hongyu Kuang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18835",
        "abstract": "Log statements have become an integral part of modern software systems. Prior research efforts have focused on supporting the decisions of placing log statements, such as where/what to log, while automated generation or completion of log statements has received little attention. With the increasing use of Large Language Models (LLMs) for code-related tasks such as code completion or generation, automated methods for generating or completing log statements have gained much momentum. Fine-tuning open-source LLMs like the Llama series is often preferred by enterprises over using commercial ones like the GPT series due to considerations including privacy, security, openness, performance, etc. Fine-tuning LLMs requires task-specific training data and custom-designed processing algorithms, which, however, have not been thoroughly explored for the log statement generation task. This paper fills this gap by contributing such a fine-tuning method LoGFiLM and an exemplar model by using the proposed method to fine-tune Llama-3-8B. Experiments with our own curated dataset and a public dataset show that LoGFiLM consistently outperforms the original Llama-3-8B and the commercial LLMs of GPT-3.5 and GPT-4. The results further reveal that fine-tuning Llama-3-8B with data encompassing broader contextual ranges surrounding log statements yields a better model for the automated generation of log statements.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "35",
        "title": "DiFiC: Your Diffusion Model Holds the Secret to Fine-Grained Clustering",
        "author": [
            "Ruohong Yang",
            "Peng Hu",
            "Xi Peng",
            "Xiting Liu",
            "Yunfan Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18838",
        "abstract": "Fine-grained clustering is a practical yet challenging task, whose essence lies in capturing the subtle differences between instances of different classes. Such subtle differences can be easily disrupted by data augmentation or be overwhelmed by redundant information in data, leading to significant performance degradation for existing clustering methods. In this work, we introduce DiFiC a fine-grained clustering method building upon the conditional diffusion model. Distinct from existing works that focus on extracting discriminative features from images, DiFiC resorts to deducing the textual conditions used for image generation. To distill more precise and clustering-favorable object semantics, DiFiC further regularizes the diffusion target and guides the distillation process utilizing neighborhood similarity. Extensive experiments demonstrate that DiFiC outperforms both state-of-the-art discriminative and generative clustering methods on four fine-grained image clustering benchmarks. We hope the success of DiFiC will inspire future research to unlock the potential of diffusion models in tasks beyond generation. The code will be released.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "36",
        "title": "Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM Dataset",
        "author": [
            "Neil Shah",
            "Shirish Karande",
            "Vineet Gandhi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18839",
        "abstract": "Current Non-Audible Murmur (NAM)-to-speech techniques rely on voice cloning to simulate ground-truth speech from paired whispers. However, the simulated speech often lacks intelligibility and fails to generalize well across different speakers. To address this issue, we focus on learning phoneme-level alignments from paired whispers and text and employ a Text-to-Speech (TTS) system to simulate the ground-truth. To reduce dependence on whispers, we learn phoneme alignments directly from NAMs, though the quality is constrained by the available training data. To further mitigate reliance on NAM/whisper data for ground-truth simulation, we propose incorporating the lip modality to infer speech and introduce a novel diffusion-based method that leverages recent advancements in lip-to-speech technology. Additionally, we release the MultiNAM dataset with over $7.96$ hours of paired NAM, whisper, video, and text data from two speakers and benchmark all methods on this dataset. Speech samples and the dataset are available at \\url{https://diff-nam.github.io/DiffNAM/}",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "37",
        "title": "Improving the Readability of Automatically Generated Tests using Large Language Models",
        "author": [
            "Matteo Biagiola",
            "Gianluca Ghislotti",
            "Paolo Tonella"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18843",
        "abstract": "Search-based test generators are effective at producing unit tests with high coverage. However, such automatically generated tests have no meaningful test and variable names, making them hard to understand and interpret by developers. On the other hand, large language models (LLMs) can generate highly readable test cases, but they are not able to match the effectiveness of search-based generators, in terms of achieved code coverage.\nIn this paper, we propose to combine the effectiveness of search-based generators with the readability of LLM generated tests. Our approach focuses on improving test and variable names produced by search-based tools, while keeping their semantics (i.e., their coverage) unchanged.\nOur evaluation on nine industrial and open source LLMs show that our readability improvement transformations are overall semantically-preserving and stable across multiple repetitions. Moreover, a human study with ten professional developers, show that our LLM-improved tests are as readable as developer-written tests, regardless of the LLM employed.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "38",
        "title": "Improving Integrated Gradient-based Transferable Adversarial Examples by Refining the Integration Path",
        "author": [
            "Yuchen Ren",
            "Zhengyu Zhao",
            "Chenhao Lin",
            "Bo Yang",
            "Lu Zhou",
            "Zhe Liu",
            "Chao Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18844",
        "abstract": "Transferable adversarial examples are known to cause threats in practical, black-box attack scenarios. A notable approach to improving transferability is using integrated gradients (IG), originally developed for model interpretability. In this paper, we find that existing IG-based attacks have limited transferability due to their naive adoption of IG in model interpretability. To address this limitation, we focus on the IG integration path and refine it in three aspects: multiplicity, monotonicity, and diversity, supported by theoretical analyses. We propose the Multiple Monotonic Diversified Integrated Gradients (MuMoDIG) attack, which can generate highly transferable adversarial examples on different CNN and ViT models and defenses. Experiments validate that MuMoDIG outperforms the latest IG-based attack by up to 37.3\\% and other state-of-the-art attacks by 8.4\\%. In general, our study reveals that migrating established techniques to improve transferability may require non-trivial efforts. Code is available at \\url{https://github.com/RYC-98/MuMoDIG}.",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "39",
        "title": "Bootstrap Your Own Context Length",
        "author": [
            "Liang Wang",
            "Nan Yang",
            "Xingxing Zhang",
            "Xiaolong Huang",
            "Furu Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18860",
        "abstract": "We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "40",
        "title": "WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting",
        "author": [
            "Chenghao Qian",
            "Yuhu Guo",
            "Wenjing Li",
            "Gustav Markkula"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18862",
        "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for 3D scene reconstruction, but still suffers from complex outdoor environments, especially under adverse weather. This is because 3DGS treats the artifacts caused by adverse weather as part of the scene and will directly reconstruct them, largely reducing the clarity of the reconstructed scene. To address this challenge, we propose WeatherGS, a 3DGS-based framework for reconstructing clear scenes from multi-view images under different weather conditions. Specifically, we explicitly categorize the multi-weather artifacts into the dense particles and lens occlusions that have very different characters, in which the former are caused by snowflakes and raindrops in the air, and the latter are raised by the precipitation on the camera lens. In light of this, we propose a dense-to-sparse preprocess strategy, which sequentially removes the dense particles by an Atmospheric Effect Filter (AEF) and then extracts the relatively sparse occlusion masks with a Lens Effect Detector (LED). Finally, we train a set of 3D Gaussians by the processed images and generated masks for excluding occluded areas, and accurately recover the underlying clear scene by Gaussian splatting. We conduct a diverse and challenging benchmark to facilitate the evaluation of 3D reconstruction under complex weather scenarios. Extensive experiments on this benchmark demonstrate that our WeatherGS consistently produces high-quality, clean scenes across various weather scenarios, outperforming existing state-of-the-art methods. See project page:https://jumponthemoon.github.io/weather-gs.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "41",
        "title": "Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual Language Models",
        "author": [
            "Meltem Aksoy"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18863",
        "abstract": "Large language models (LLMs) have become integral tools in diverse domains, yet their moral reasoning capabilities across cultural and linguistic contexts remain underexplored. This study investigates whether multilingual LLMs, such as GPT-3.5-Turbo, GPT-4o-mini, Llama 3.1, and MistralNeMo, reflect culturally specific moral values or impose dominant moral norms, particularly those rooted in English. Using the updated Moral Foundations Questionnaire (MFQ-2) in eight languages, Arabic, Farsi, English, Spanish, Japanese, Chinese, French, and Russian, the study analyzes the models' adherence to six core moral foundations: care, equality, proportionality, loyalty, authority, and purity. The results reveal significant cultural and linguistic variability, challenging the assumption of universal moral consistency in LLMs. Although some models demonstrate adaptability to diverse contexts, others exhibit biases influenced by the composition of the training data. These findings underscore the need for culturally inclusive model development to improve fairness and trust in multilingual AI systems.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "42",
        "title": "IUST_PersonReId: A New Domain in Person Re-Identification Datasets",
        "author": [
            "Alireza Sedighi Moghaddam",
            "Fatemeh Anvari",
            "Mohammadjavad Mirshekari Haghighi",
            "Mohammadali Fakhari",
            "Mohammad Reza Mohammadi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18874",
        "abstract": "Person re-identification (ReID) models often struggle to generalize across diverse cultural contexts, particularly in Islamic regions like Iran, where modest clothing styles are prevalent. Existing datasets predominantly feature Western and East Asian fashion, limiting their applicability in these settings. To address this gap, we introduce IUST_PersonReId, a dataset designed to reflect the unique challenges of ReID in new cultural environments, emphasizing modest attire and diverse scenarios from Iran, including markets, campuses, and mosques. Experiments on IUST_PersonReId with state-of-the-art models, such as Solider and CLIP-ReID, reveal significant performance drops compared to benchmarks like Market1501 and MSMT17, highlighting the challenges posed by occlusion and limited distinctive features. Sequence-based evaluations show improvements by leveraging temporal context, emphasizing the dataset's potential for advancing culturally sensitive and robust ReID systems. IUST_PersonReId offers a critical resource for addressing fairness and bias in ReID research globally. The dataset is publicly available at https://computervisioniust.github.io/IUST_PersonReId/.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "43",
        "title": "Goal State Generation for Robotic Manipulation Based on Linguistically Guided Hybrid Gaussian Diffusion",
        "author": [
            "Yichen Xu",
            "Faliang Chang",
            "Chunsheng Liu",
            "Dexin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18877",
        "abstract": "In robotic manipulation tasks, achieving a designated target state for the manipulated object is often essential to facilitate motion planning for robotic arms. Specifically, in tasks such as hanging a mug, the mug must be positioned within a feasible region around the hook. Previous approaches have enabled the generation of multiple feasible target states for mugs; however, these target states are typically generated randomly, lacking control over the specific generation locations. This limitation makes such methods less effective in scenarios where constraints exist, such as hooks already occupied by other mugs or when specific operational objectives must be met. Moreover, due to the frequent physical interactions between the mug and the rack in real-world hanging scenarios, imprecisely generated target states from end-to-end models often result in overlapping point clouds. This overlap adversely impacts subsequent motion planning for the robotic arm. To address these challenges, we propose a Linguistically Guided Hybrid Gaussian Diffusion (LHGD) network for generating manipulation target states, combined with a gravity coverage coefficient-based method for target state refinement. To evaluate our approach under a language-specified distribution setting, we collected multiple feasible target states for 10 types of mugs across 5 different racks with 10 distinct hooks. Additionally, we prepared five unseen mug designs for validation purposes. Experimental results demonstrate that our method achieves the highest success rates across single-mode, multi-mode, and language-specified distribution manipulation tasks. Furthermore, it significantly reduces point cloud overlap, directly producing collision-free target states and eliminating the need for additional obstacle avoidance operations by the robotic arm.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "44",
        "title": "CoEvo: Continual Evolution of Symbolic Solutions Using Large Language Models",
        "author": [
            "Ping Guo",
            "Qingfu Zhang",
            "Xi Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18890",
        "abstract": "Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence, capable of processing and understanding extensive human knowledge to enhance problem-solving across various domains. This paper explores the potential of LLMs to drive the discovery of symbolic solutions within scientific and engineering disciplines, where such solutions are crucial for advancing theoretical and practical applications. We propose a novel framework that utilizes LLMs in an evolutionary search methodology, augmented by a dynamic knowledge library that integrates and refines insights in an \\textit{open-ended manner}. This approach aims to tackle the dual challenges of efficiently navigating complex symbolic representation spaces and leveraging both existing and newly generated knowledge to foster open-ended innovation. By enabling LLMs to interact with and expand upon a knowledge library, we facilitate the continuous generation of novel solutions in diverse forms such as language, code, and mathematical expressions. Our experimental results demonstrate that this method not only enhances the efficiency of searching for symbolic solutions but also supports the ongoing discovery process, akin to human scientific endeavors. This study represents a first effort in conceptualizing the search for symbolic solutions as a lifelong, iterative process, marking a significant step towards harnessing AI in the perpetual pursuit of scientific and engineering breakthroughs. We have open-sourced our code and data, please visit \\url{https://github.com/pgg3/CoEvo} for more information.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "45",
        "title": "EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior Generation",
        "author": [
            "Carl Qi",
            "Dan Haramati",
            "Tal Daniel",
            "Aviv Tamar",
            "Amy Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18907",
        "abstract": "Object manipulation is a common component of everyday tasks, but learning to manipulate objects from high-dimensional observations presents significant challenges. These challenges are heightened in multi-object environments due to the combinatorial complexity of the state space as well as of the desired behaviors. While recent approaches have utilized large-scale offline data to train models from pixel observations, achieving performance gains through scaling, these methods struggle with compositional generalization in unseen object configurations with constrained network and dataset sizes. To address these issues, we propose a novel behavioral cloning (BC) approach that leverages object-centric representations and an entity-centric Transformer with diffusion-based optimization, enabling efficient learning from offline image data. Our method first decomposes observations into an object-centric representation, which is then processed by our entity-centric Transformer that computes attention at the object level, simultaneously predicting object dynamics and the agent's actions. Combined with the ability of diffusion models to capture multi-modal behavior distributions, this results in substantial performance improvements in multi-object tasks and, more importantly, enables compositional generalization. We present BC agents capable of zero-shot generalization to tasks with novel compositions of objects and goals, including larger numbers of objects than seen during training. We provide video rollouts on our webpage: https://sites.google.com/view/ec-diffuser.",
        "tags": [
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "46",
        "title": "AdaEAGLE: Optimizing Speculative Decoding via Explicit Modeling of Adaptive Draft Structures",
        "author": [
            "Situo Zhang",
            "Hankun Wang",
            "Da Ma",
            "Zichen Zhu",
            "Lu Chen",
            "Kunyao Lan",
            "Kai Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18910",
        "abstract": "Speculative Decoding (SD) is a popular lossless technique for accelerating the inference of Large Language Models (LLMs). We show that the decoding speed of SD frameworks with static draft structures can be significantly improved by incorporating context-aware adaptive draft structures. However, current studies on adaptive draft structures are limited by their performance, modeling approaches, and applicability. In this paper, we introduce AdaEAGLE, the first SD framework that explicitly models adaptive draft structures. AdaEAGLE leverages the Lightweight Draft Length Predictor (LDLP) module to explicitly predict the optimal number of draft tokens during inference to guide the draft model. It achieves comparable speedup results without manual thresholds and allows for deeper, more specialized optimizations. Moreover, together with threshold-based strategies, AdaEAGLE achieves a $1.62\\times$ speedup over the vanilla AR decoding and outperforms fixed-length SotA baseline while maintaining output quality.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "47",
        "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
        "author": [
            "Chang Zou",
            "Evelyn Zhang",
            "Runlin Guo",
            "Haohang Xu",
            "Conghui He",
            "Xuming Hu",
            "Linfeng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18911",
        "abstract": "Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. However, on the one hand, aggressively reusing all the features cached in previous timesteps leads to a severe drop in generation quality. On the other hand, conservatively caching only the features in the redundant layers or tokens but still computing the important ones successfully preserves the generation quality but results in reductions in acceleration ratios. Observing such a tradeoff between generation quality and acceleration performance, this paper begins by quantitatively studying the accumulated error from cached features. Surprisingly, we find that aggressive caching does not introduce significantly more caching errors in the caching step, and the conservative feature caching can fix the error introduced by aggressive caching. Thereby, we propose a dual caching strategy that adopts aggressive and conservative caching iteratively, leading to significant acceleration and high generation quality at the same time. Besides, we further introduce a V-caching strategy for token-wise conservative caching, which is compatible with flash attention and requires no training and calibration data.\nOur codes have been released in Github: \\textbf{Code: \\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
        "tags": [
            "DiT",
            "Diffusion",
            "Flash Attention",
            "Video Generation"
        ]
    },
    {
        "id": "48",
        "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With Structured Memories",
        "author": [
            "Dulhan Jayalath",
            "James Bradley Wendt",
            "Nicholas Monath",
            "Sandeep Tata",
            "Beliz Gunel"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18914",
        "abstract": "Long-range tasks require reasoning over long inputs. Existing solutions either need large compute budgets, training data, access to model weights, or use complex, task-specific approaches. We present PRISM, which alleviates these concerns by processing information as a stream of chunks, maintaining a structured in-context memory specified by a typed hierarchy schema. This approach demonstrates superior performance to baselines on diverse tasks while using at least 4x smaller contexts than long-context models. Moreover, PRISM is token-efficient. By producing short outputs and efficiently leveraging key-value (KV) caches, it achieves up to 54% cost reduction when compared to alternative short-context approaches. The method also scales down to tiny information chunks (e.g., 500 tokens) without increasing the number of tokens encoded or sacrificing quality. Furthermore, we show that it is possible to generate schemas to generalize our approach to new tasks with minimal effort.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "49",
        "title": "Open-Vocabulary Panoptic Segmentation Using BERT Pre-Training of Vision-Language Multiway Transformer Model",
        "author": [
            "Yi-Chia Chen",
            "Wei-Hua Li",
            "Chu-Song Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18917",
        "abstract": "Open-vocabulary panoptic segmentation remains a challenging problem. One of the biggest difficulties lies in training models to generalize to an unlimited number of classes using limited categorized training data. Recent popular methods involve large-scale vision-language pre-trained foundation models, such as CLIP. In this paper, we propose OMTSeg for open-vocabulary segmentation using another large-scale vision-language pre-trained model called BEiT-3 and leveraging the cross-modal attention between visual and linguistic features in BEiT-3 to achieve better performance. Experiments result demonstrates that OMTSeg performs favorably against state-of-the-art models.",
        "tags": [
            "BERT",
            "CLIP",
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "50",
        "title": "UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal Transformer for Image Generation",
        "author": [
            "Lunhao Duan",
            "Shanshan Zhao",
            "Wenjun Yan",
            "Yinglun Li",
            "Qing-Guo Chen",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang",
            "Mingming Gong",
            "Gui-Song Xia"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18928",
        "abstract": "Recently, text-to-image generation models have achieved remarkable advancements, particularly with diffusion models facilitating high-quality image synthesis from textual descriptions. However, these models often struggle with achieving precise control over pixel-level layouts, object appearances, and global styles when using text prompts alone. To mitigate this issue, previous works introduce conditional images as auxiliary inputs for image generation, enhancing control but typically necessitating specialized models tailored to different types of reference inputs. In this paper, we explore a new approach to unify controllable generation within a single framework. Specifically, we propose the unified image-instruction adapter (UNIC-Adapter) built on the Multi-Modal-Diffusion Transformer architecture, to enable flexible and controllable generation across diverse conditions without the need for multiple specialized models. Our UNIC-Adapter effectively extracts multi-modal instruction information by incorporating both conditional images and task instructions, injecting this information into the image generation process through a cross-attention mechanism enhanced by Rotary Position Embedding. Experimental results across a variety of tasks, including pixel-level spatial control, subject-driven image generation, and style-image-based image synthesis, demonstrate the effectiveness of our UNIC-Adapter in unified controllable image generation.",
        "tags": [
            "Diffusion",
            "Diffusion Transformer",
            "Text-to-Image",
            "Transformer"
        ]
    },
    {
        "id": "51",
        "title": "Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference",
        "author": [
            "Libo Zhang",
            "Zhaoning Zhang",
            "Baizhou Xu",
            "Songzhu Mei",
            "Dongsheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18934",
        "abstract": "Due to the high resource demands of Large Language Models (LLMs), achieving widespread deployment on consumer-grade devices presents significant challenges. Typically, personal or consumer-grade devices, including servers configured prior to the era of large-scale models, generally have relatively weak GPUs and relatively strong CPUs. However, most current methods primarily depend on GPUs for computation. Therefore, we propose Dovetail, an approach that deploys the draft model on the GPU to generate draft tokens while allowing the target model to perform parallel verification on the CPU, thereby improving the utilization of all available hardware resources and occupying less inter-device communication bandwidth. Accordingly, we have redesigned the draft model to better align with heterogeneous hardware characteristics. To this end, we implemented several optimizations: reducing the number of draft tokens to mitigate latency in parallel verification, increasing the depth of the draft model to enhance its predictive capacity, and introducing DGF (Dynamic Gating Fusion) to improve the integration of features and token embeddings. In the HumanEval benchmark, Dovetail achieved an inference speed of 5.86 tokens per second for LLaMA2-Chat-7B using 3GB of VRAM, representing an approximately 2.77x improvement over CPU-only inference. Furthermore, the inference speed was increased to 8 tokens per second when utilizing 7GB of VRAM.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "52",
        "title": "Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations",
        "author": [
            "Yewon Kim",
            "Sung-Ju Lee",
            "Chris Donahue"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18940",
        "abstract": "Songwriting is often driven by multimodal inspirations, such as imagery, narratives, or existing music, yet songwriters remain unsupported by current music AI systems in incorporating these multimodal inputs into their creative processes. We introduce Amuse, a songwriting assistant that transforms multimodal (image, text, or audio) inputs into chord progressions that can be seamlessly incorporated into songwriters' creative processes. A key feature of Amuse is its novel method for generating coherent chords that are relevant to music keywords in the absence of datasets with paired examples of multimodal inputs and chords. Specifically, we propose a method that leverages multimodal large language models (LLMs) to convert multimodal inputs into noisy chord suggestions and uses a unimodal chord model to filter the suggestions. A user study with songwriters shows that Amuse effectively supports transforming multimodal ideas into coherent musical suggestions, enhancing users' agency and creativity throughout the songwriting process.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "53",
        "title": "Single Trajectory Distillation for Accelerating Image and Video Style Transfer",
        "author": [
            "Sijie Xu",
            "Runqi Wang",
            "Wei Zhu",
            "Dejia Song",
            "Nemo Chen",
            "Xu Tang",
            "Yao Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18945",
        "abstract": "Diffusion-based stylization methods typically denoise from a specific partial noise state for image-to-image and video-to-video tasks. This multi-step diffusion process is computationally expensive and hinders real-world application. A promising solution to speed up the process is to obtain few-step consistency models through trajectory distillation. However, current consistency models only force the initial-step alignment between the probability flow ODE (PF-ODE) trajectories of the student and the imperfect teacher models. This training strategy can not ensure the consistency of whole trajectories. To address this issue, we propose single trajectory distillation (STD) starting from a specific partial noise state. We introduce a trajectory bank to store the teacher model's trajectory states, mitigating the time cost during training. Besides, we use an asymmetric adversarial loss to enhance the style and quality of the generated images. Extensive experiments on image and video stylization demonstrate that our method surpasses existing acceleration models in terms of style similarity and aesthetic evaluations. Our code and results will be available on the project page: https://single-trajectory-distillation.github.io.",
        "tags": [
            "Consistency Models",
            "Diffusion",
            "ODE",
            "Style Transfer"
        ]
    },
    {
        "id": "54",
        "title": "TopoBDA: Towards Bezier Deformable Attention for Road Topology Understanding",
        "author": [
            "Muhammet Esat Kalfaoglu",
            "Halil Ibrahim Ozturk",
            "Ozsel Kilinc",
            "Alptekin Temizel"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18951",
        "abstract": "Understanding road topology is crucial for autonomous driving. This paper introduces TopoBDA (Topology with Bezier Deformable Attention), a novel approach that enhances road topology understanding by leveraging Bezier Deformable Attention (BDA). BDA utilizes Bezier control points to drive the deformable attention mechanism, significantly improving the detection and representation of elongated and thin polyline structures, such as lane centerlines. TopoBDA processes multi-camera 360-degree imagery to generate Bird's Eye View (BEV) features, which are refined through a transformer decoder employing BDA. This method enhances computational efficiency while maintaining high accuracy in centerline prediction. Additionally, TopoBDA incorporates an instance mask formulation and an auxiliary one-to-many set prediction loss strategy to further refine centerline detection and improve road topology understanding. Experimental evaluations on the OpenLane-V2 dataset demonstrate that TopoBDA outperforms existing methods, achieving state-of-the-art results in centerline detection and topology reasoning. The integration of multi-modal data, including lidar and radar, specifically for road topology understanding, further enhances the model's performance, underscoring its importance in autonomous driving applications.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "55",
        "title": "Musings About the Future of Search: A Return to the Past?",
        "author": [
            "Jimmy Lin",
            "Pankaj Gupta",
            "Will Horn",
            "Gilad Mishne"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18956",
        "abstract": "When you have a question, the most effective way to have the question answered is to directly connect with experts on the topic and have a conversation with them. Prior to the invention of writing, this was the only way. Although effective, this solution exhibits scalability challenges. Writing allowed knowledge to be materialized, preserved, and replicated, enabling the development of different technologies over the centuries to connect information seekers with relevant information. This progression ultimately culminated in the ten-blue-links web search paradigm we're familiar with, just before the recent emergence of generative AI. However, we often forget that consuming static content is an imperfect solution. With the advent of large language models, it has become possible to develop a superior experience by allowing users to directly engage with experts. These interactions can of course satisfy information needs, but expert models can do so much more. This coming future requires reimagining search.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "56",
        "title": "ModelGrow: Continual Text-to-Video Pre-training with Model Expansion and Language Understanding Enhancement",
        "author": [
            "Zhefan Rao",
            "Liya Ji",
            "Yazhou Xing",
            "Runtao Liu",
            "Zhaoyang Liu",
            "Jiaxin Xie",
            "Ziqiao Peng",
            "Yingqing He",
            "Qifeng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18966",
        "abstract": "Text-to-video (T2V) generation has gained significant attention recently. However, the costs of training a T2V model from scratch remain persistently high, and there is considerable room for improving the generation performance, especially under limited computation resources. This work explores the continual general pre-training of text-to-video models, enabling the model to \"grow\" its abilities based on a pre-trained foundation, analogous to how humans acquire new knowledge based on past experiences. There is a lack of extensive study of the continual pre-training techniques in T2V generation. In this work, we take the initial step toward exploring this task systematically and propose ModelGrow. Specifically, we break this task into two key aspects: increasing model capacity and improving semantic understanding. For model capacity, we introduce several novel techniques to expand the model size, enabling it to store new knowledge and improve generation performance. For semantic understanding, we propose a method that leverages large language models as advanced text encoders, integrating them into T2V models to enhance language comprehension and guide generation results according to detailed prompts. This approach enables the model to achieve better semantic alignment, particularly in response to complex user prompts. Extensive experiments demonstrate the effectiveness of our method across various metrics. The source code and the model of ModelGrow will be publicly available.",
        "tags": [
            "Large Language Models",
            "Text-to-Video"
        ]
    },
    {
        "id": "57",
        "title": "Adopting Trustworthy AI for Sleep Disorder Prediction: Deep Time Series Analysis with Temporal Attention Mechanism and Counterfactual Explanations",
        "author": [
            "Pegah Ahadian",
            "Wei Xu",
            "Sherry Wang",
            "Qiang Guan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18971",
        "abstract": "Sleep disorders have a major impact on both lifestyle and health. Effective sleep disorder prediction from lifestyle and physiological data can provide essential details for early intervention. This research utilizes three deep time series models and facilitates them with explainability approaches for sleep disorder prediction. Specifically, our approach adopts Temporal Convolutional Networks (TCN), Long Short-Term Memory (LSTM) for time series data analysis, and Temporal Fusion Transformer model (TFT). Meanwhile, the temporal attention mechanism and counterfactual explanation with SHapley Additive exPlanations (SHAP) approach are employed to ensure dependable, accurate, and interpretable predictions. Finally, using a large dataset of sleep health measures, our evaluation demonstrates the effect of our method in predicting sleep disorders.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "58",
        "title": "MTCAE-DFER: Multi-Task Cascaded Autoencoder for Dynamic Facial Expression Recognition",
        "author": [
            "Peihao Xiang",
            "Kaida Wu",
            "Chaohao Lin",
            "Ou Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18988",
        "abstract": "This paper expands the cascaded network branch of the autoencoder-based multi-task learning (MTL) framework for dynamic facial expression recognition, namely Multi-Task Cascaded Autoencoder for Dynamic Facial Expression Recognition (MTCAE-DFER). MTCAE-DFER builds a plug-and-play cascaded decoder module, which is based on the Vision Transformer (ViT) architecture and employs the decoder concept of Transformer to reconstruct the multi-head attention module. The decoder output from the previous task serves as the query (Q), representing local dynamic features, while the Video Masked Autoencoder (VideoMAE) shared encoder output acts as both the key (K) and value (V), representing global dynamic features. This setup facilitates interaction between global and local dynamic features across related tasks. Additionally, this proposal aims to alleviate overfitting of complex large model. We utilize autoencoder-based multi-task cascaded learning approach to explore the impact of dynamic face detection and dynamic face landmark on dynamic facial expression recognition, which enhances the model's generalization ability. After we conduct extensive ablation experiments and comparison with state-of-the-art (SOTA) methods on various public datasets for dynamic facial expression recognition, the robustness of the MTCAE-DFER model and the effectiveness of global-local dynamic feature interaction among related tasks have been proven.",
        "tags": [
            "Detection",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "59",
        "title": "How Propense Are Large Language Models at Producing Code Smells? A Benchmarking Study",
        "author": [
            "Alejandro Velasco",
            "Daniel Rodriguez-Cardenas",
            "David N. Palacio",
            "Luftar Rahman Alif",
            "Denys Poshyvanyk"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18989",
        "abstract": "Large Language Models (LLMs) have shown significant potential in automating software engineering tasks, particularly in code generation. However, current evaluation benchmarks, which primarily focus on accuracy, fall short in assessing the quality of the code generated by these models, specifically their tendency to produce code smells. To address this limitation, we introduce CodeSmellEval, a benchmark designed to evaluate the propensity of LLMs for generating code smells. Our benchmark includes a novel metric: Propensity Smelly Score (PSC), and a curated dataset of method-level code smells: CodeSmellData. To demonstrate the use of CodeSmellEval, we conducted a case study with two state-of-the-art LLMs, CodeLlama and Mistral. The results reveal that both models tend to generate code smells, such as simplifiable-condition and consider-merging-isinstance. These findings highlight the effectiveness of our benchmark in evaluating LLMs, providing valuable insights into their reliability and their propensity to introduce code smells in code generation tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "60",
        "title": "MGAN-CRCM: A Novel Multiple Generative Adversarial Network and Coarse-Refinement Based Cognizant Method for Image Inpainting",
        "author": [
            "Nafiz Al Asad",
            "Md. Appel Mahmud Pranto",
            "Shbiruzzaman Shiam",
            "Musaddeq Mahmud Akand",
            "Mohammad Abu Yousuf",
            "Khondokar Fida Hasan",
            "Mohammad Ali Moni"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19000",
        "abstract": "Image inpainting is a widely used technique in computer vision for reconstructing missing or damaged pixels in images. Recent advancements with Generative Adversarial Networks (GANs) have demonstrated superior performance over traditional methods due to their deep learning capabilities and adaptability across diverse image domains. Residual Networks (ResNet) have also gained prominence for their ability to enhance feature representation and compatibility with other architectures. This paper introduces a novel architecture combining GAN and ResNet models to improve image inpainting outcomes. Our framework integrates three components: Transpose Convolution-based GAN for guided and blind inpainting, Fast ResNet-Convolutional Neural Network (FR-CNN) for object removal, and Co-Modulation GAN (Co-Mod GAN) for refinement. The model's performance was evaluated on benchmark datasets, achieving accuracies of 96.59% on Image-Net, 96.70% on Places2, and 96.16% on CelebA. Comparative analyses demonstrate that the proposed architecture outperforms existing methods, highlighting its effectiveness in both qualitative and quantitative evaluations.",
        "tags": [
            "GAN",
            "Inpainting"
        ]
    },
    {
        "id": "61",
        "title": "Let the Rule Speak: Enhancing In-context Learning Debiasing with Interpretability",
        "author": [
            "Ruixi Lin",
            "Yang You"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19018",
        "abstract": "In-context learning, which allows large language models to perform diverse tasks with a few demonstrations, is found to have imbalanced per-class prediction accuracy on multi-class text classification. Although notable output correction methods have been developed to tackle the issue and simultaneously improve downstream prediction accuracy, they may fail to answer the core interpretability challenges: why and which certain classes need corrections, and more importantly, a tailored correction for per-sample, per-class's probability. To address such interpretability gaps, we first find that the imbalance arises from certain classes consistently receiving high ICL output probabilities, whereas others receiving lower or mixed ranges, so the former is more frequently chosen, resulting in higher accuracy; more crucially, we find that these ranges have significantly varying degrees of influence on the accuracy bias, highlighting the need for precise, interpretable probability corrections by range. Motivated by this, we propose FuRud, a Fuzzy Rule Optimization based Debiasing method, that (1) detects which classes need corrections, and (2) for each correction-needed class, detects its probability ranges and applies asymmetric amplifications or reductions to correct them interpretably. Notably, across seven benchmark datasets, FuRud reduces the pairwise class accuracy bias (COBias) by more than half (56%), while achieving a relative increase of 21% in accuracy, outperforming state-of-the-art debiasing methods. Moreover, FuRud can optimize downstream tasks with as few as 10 optimization examples. Furthermore, FuRud can work for prompt formats that lead to highly skewed predictions. For example, FuRud greatly improves ICL outputs which use letter options, with 44% relative accuracy increase and 54% relative COBias reduction.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "62",
        "title": "Repository Structure-Aware Training Makes SLMs Better Issue Resolver",
        "author": [
            "Zexiong Ma",
            "Shengnan An",
            "Zeqi Lin",
            "Yanzhen Zou",
            "Bing Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19031",
        "abstract": "Language models have been applied to various software development tasks, but the performance varies according to the scale of the models. Large Language Models (LLMs) outperform Small Language Models (SLMs) in complex tasks like repository-level issue resolving, but raise concerns about privacy and cost. In contrast, SLMs are more accessible but under-perform in complex tasks. In this paper, we introduce ReSAT (Repository Structure-Aware Training), construct training data based on a large number of issues and corresponding pull requests from open-source communities to enhance the model's understanding of repository structure and issue resolving ability. We construct two types of training data: (1) localization training data, a multi-level progressive localization data to improve code understanding and localization capability; (2) code edit training data, which improves context-based code editing capability. The evaluation results on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively enhances SLMs' issue-resolving and repository-level long-context understanding capabilities.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "63",
        "title": "Indonesian-English Code-Switching Speech Synthesizer Utilizing Multilingual STEN-TTS and Bert LID",
        "author": [
            "Ahmad Alfani Handoyo",
            "Chung Tran",
            "Dessi Puji Lestari",
            "Sakriani Sakti"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19043",
        "abstract": "Multilingual text-to-speech systems convert text into speech across multiple languages. In many cases, text sentences may contain segments in different languages, a phenomenon known as code-switching. This is particularly common in Indonesia, especially between Indonesian and English. Despite its significance, no research has yet developed a multilingual TTS system capable of handling code-switching between these two languages. This study addresses Indonesian-English code-switching in STEN-TTS. Key modifications include adding a language identification component to the text-to-phoneme conversion using finetuned BERT for per-word language identification, as well as removing language embedding from the base model. Experimental results demonstrate that the code-switching model achieves superior naturalness and improved speech intelligibility compared to the Indonesian and English baseline STEN-TTS models.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "64",
        "title": "SpectralKD: Understanding and Optimizing Vision Transformer Distillation through Spectral Analysis",
        "author": [
            "Huiyuan Tian",
            "Bonan Xu",
            "Shijian Li",
            "Gang Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19055",
        "abstract": "Knowledge distillation effectively reduces model complexity while improving performance, yet the underlying knowledge transfer mechanisms remain poorly understood. We propose novel spectral analysis methods and guidelines to optimize distillation, making the knowledge transfer process more interpretable. Our analysis reveals that CaiT models concentrate information in their first and last few layers, informing optimal layer selection for feature map distillation. Surprisingly, we discover that Swin Transformer and CaiT exhibit similar spectral encoding patterns despite their architectural differences, enhancing our understanding of transformer architectures and leading to improved feature map alignment strategies. Based on these insights, we introduce a simple yet effective spectral alignment method named SpectralKD. Experimental results demonstrate that following our guidelines enables SpectralKD to achieve state-of-the-art performance (DeiT-Tiny: $+5.2\\%$, Swin-Tiny: $+1.4\\%$ in ImageNet-1k Top-1 accuracy). Furthermore, through spectral analysis of student models trained with and without distillation, we show that distilled models mirror spectral patterns of their teachers, providing a new lens for interpreting knowledge distillation dynamics. Our code, pre-trained models, and experimental logs will be made publicly available.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "65",
        "title": "DAPoinTr: Domain Adaptive Point Transformer for Point Cloud Completion",
        "author": [
            "Yinghui Li",
            "Qianyu Zhou",
            "Jingyu Gong",
            "Ye Zhu",
            "Richard Dazeley",
            "Xinkui Zhao",
            "Xuequan Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19062",
        "abstract": "Point Transformers (PoinTr) have shown great potential in point cloud completion recently. Nevertheless, effective domain adaptation that improves transferability toward target domains remains unexplored. In this paper, we delve into this topic and empirically discover that direct feature alignment on point Transformer's CNN backbone only brings limited improvements since it cannot guarantee sequence-wise domain-invariant features in the Transformer. To this end, we propose a pioneering Domain Adaptive Point Transformer (DAPoinTr) framework for point cloud completion. DAPoinTr consists of three key components: Domain Query-based Feature Alignment (DQFA), Point Token-wise Feature alignment (PTFA), and Voted Prediction Consistency (VPC). In particular, DQFA is presented to narrow the global domain gaps from the sequence via the presented domain proxy and domain query at the Transformer encoder and decoder, respectively. PTFA is proposed to close the local domain shifts by aligning the tokens, \\emph{i.e.,} point proxy and dynamic query, at the Transformer encoder and decoder, respectively. VPC is designed to consider different Transformer decoders as multiple of experts (MoE) for ensembled prediction voting and pseudo-label generation. Extensive experiments with visualization on several domain adaptation benchmarks demonstrate the effectiveness and superiority of our DAPoinTr compared with state-of-the-art methods. Code will be publicly available at: https://github.com/Yinghui-Li-New/DAPoinTr",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "66",
        "title": "Hierarchical Multi-agent Meta-Reinforcement Learning for Cross-channel Bidding",
        "author": [
            "Shenghong He",
            "Chao Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19064",
        "abstract": "Real-time bidding (RTB) plays a pivotal role in online advertising ecosystems. Advertisers employ strategic bidding to optimize their advertising impact while adhering to various financial constraints, such as the return-on-investment (ROI) and cost-per-click (CPC). Primarily focusing on bidding with fixed budget constraints, traditional approaches cannot effectively manage the dynamic budget allocation problem where the goal is to achieve global optimization of bidding performance across multiple channels with a shared budget. In this paper, we propose a hierarchical multi-agent reinforcement learning framework for multi-channel bidding optimization. In this framework, the top-level strategy applies a CPC constrained diffusion model to dynamically allocate budgets among the channels according to their distinct features and complex interdependencies, while the bottom-level strategy adopts a state-action decoupled actor-critic method to address the problem of extrapolation errors in offline learning caused by out-of-distribution actions and a context-based meta-channel knowledge learning method to improve the state representation capability of the policy based on the shared knowledge among different channels. Comprehensive experiments conducted on a large scale real-world industrial dataset from the Meituan ad bidding platform demonstrate that our method achieves a state-of-the-art performance.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "67",
        "title": "Advancing LLM detection in the ALTA 2024 Shared Task: Techniques and Analysis",
        "author": [
            "Dima Galat"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19076",
        "abstract": "The recent proliferation of AI-generated content has prompted significant interest in developing reliable detection methods. This study explores techniques for identifying AI-generated text through sentence-level evaluation within hybrid articles. Our findings indicate that ChatGPT-3.5 Turbo exhibits distinct, repetitive probability patterns that enable consistent in-domain detection. Empirical tests show that minor textual modifications, such as rewording, have minimal impact on detection accuracy. These results provide valuable insights for advancing AI detection methodologies, offering a pathway toward robust solutions to address the complexities of synthetic text identification.",
        "tags": [
            "ChatGPT",
            "Detection"
        ]
    },
    {
        "id": "68",
        "title": "Mask Factory: Towards High-quality Synthetic Data Generation for Dichotomous Image Segmentation",
        "author": [
            "Haotian Qian",
            "YD Chen",
            "Shengtao Lou",
            "Fahad Shahbaz Khan",
            "Xiaogang Jin",
            "Deng-Ping Fan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19080",
        "abstract": "Dichotomous Image Segmentation (DIS) tasks require highly precise annotations, and traditional dataset creation methods are labor intensive, costly, and require extensive domain expertise. Although using synthetic data for DIS is a promising solution to these challenges, current generative models and techniques struggle with the issues of scene deviations, noise-induced errors, and limited training sample variability. To address these issues, we introduce a novel approach, \\textbf{\\ourmodel{}}, which provides a scalable solution for generating diverse and precise datasets, markedly reducing preparation time and costs. We first introduce a general mask editing method that combines rigid and non-rigid editing techniques to generate high-quality synthetic masks. Specially, rigid editing leverages geometric priors from diffusion models to achieve precise viewpoint transformations under zero-shot conditions, while non-rigid editing employs adversarial training and self-attention mechanisms for complex, topologically consistent modifications. Then, we generate pairs of high-resolution image and accurate segmentation mask using a multi-conditional control generation method. Finally, our experiments on the widely-used DIS5K dataset benchmark demonstrate superior performance in quality and efficiency compared to existing methods. The code is available at \\url{https://qian-hao-tian.github.io/MaskFactory/}.",
        "tags": [
            "Diffusion",
            "Segmentation"
        ]
    },
    {
        "id": "69",
        "title": "Integrating Artificial Open Generative Artificial Intelligence into Software Supply Chain Security",
        "author": [
            "Vasileios Alevizos",
            "George A Papakostas",
            "Akebu Simasiku",
            "Dimitra Malliarou",
            "Antonis Messinis",
            "Sabrina Edralin",
            "Clark Xu",
            "Zongliang Yue"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19088",
        "abstract": "While new technologies emerge, human errors always looming. Software supply chain is increasingly complex and intertwined, the security of a service has become paramount to ensuring the integrity of products, safeguarding data privacy, and maintaining operational continuity. In this work, we conducted experiments on the promising open Large Language Models (LLMs) into two main software security challenges: source code language errors and deprecated code, with a focus on their potential to replace conventional static and dynamic security scanners that rely on predefined rules and patterns. Our findings suggest that while LLMs present some unexpected results, they also encounter significant limitations, particularly in memory complexity and the management of new and unfamiliar data patterns. Despite these challenges, the proactive application of LLMs, coupled with extensive security databases and continuous updates, holds the potential to fortify Software Supply Chain (SSC) processes against emerging threats.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "70",
        "title": "Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from Unsynchronized and Uncalibrated Videos",
        "author": [
            "Changwoon Choi",
            "Jeongjun Kim",
            "Geonho Cha",
            "Minkwan Kim",
            "Dongyoon Wee",
            "Young Min Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19089",
        "abstract": "Recent works on dynamic neural field reconstruction assume input from synchronized multi-view videos with known poses. These input constraints are often unmet in real-world setups, making the approach impractical. We demonstrate that unsynchronized videos with unknown poses can generate dynamic neural fields if the videos capture human motion. Humans are one of the most common dynamic subjects whose poses can be estimated using state-of-the-art methods. While noisy, the estimated human shape and pose parameters provide a decent initialization for the highly non-convex and under-constrained problem of training a consistent dynamic neural representation. Given the sequences of pose and shape of humans, we estimate the time offsets between videos, followed by camera pose estimations by analyzing 3D joint locations. Then, we train dynamic NeRF employing multiresolution rids while simultaneously refining both time offsets and camera poses. The setup still involves optimizing many parameters, therefore, we introduce a robust progressive learning strategy to stabilize the process. Experiments show that our approach achieves accurate spatiotemporal calibration and high-quality scene reconstruction in challenging conditions.",
        "tags": [
            "3D",
            "NeRF"
        ]
    },
    {
        "id": "71",
        "title": "From Coin to Data: The Impact of Object Detection on Digital Numismatics",
        "author": [
            "Rafael Cabral",
            "Maria De Iorio",
            "Andrew Harris"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19091",
        "abstract": "In this work we investigate the application of advanced object detection techniques to digital numismatics, focussing on the analysis of historical coins. Leveraging models such as Contrastive Language-Image Pre-training (CLIP), we develop a flexible framework for identifying and classifying specific coin features using both image and textual descriptions. By examining two distinct datasets, modern Russian coins featuring intricate \"Saint George and the Dragon\" designs and degraded 1st millennium AD Southeast Asian coins bearing Hindu-Buddhist symbols, we evaluate the efficacy of different detection algorithms in search and classification tasks. Our results demonstrate the superior performance of larger CLIP models in detecting complex imagery, while traditional methods excel in identifying simple geometric patterns. Additionally, we propose a statistical calibration mechanism to enhance the reliability of similarity scores in low-quality datasets. This work highlights the transformative potential of integrating state-of-the-art object detection into digital numismatics, enabling more scalable, precise, and efficient analysis of historical artifacts. These advancements pave the way for new methodologies in cultural heritage research, artefact provenance studies, and the detection of forgeries.",
        "tags": [
            "CLIP",
            "Detection"
        ]
    },
    {
        "id": "72",
        "title": "BSDB-Net: Band-Split Dual-Branch Network with Selective State Spaces Mechanism for Monaural Speech Enhancement",
        "author": [
            "Cunhang Fan",
            "Enrui Liu",
            "Andong Li",
            "Jianhua Tao",
            "Jian Zhou",
            "Jiahao Li",
            "Chengshi Zheng",
            "Zhao Lv"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19099",
        "abstract": "Although the complex spectrum-based speech enhancement(SE) methods have achieved significant performance, coupling amplitude and phase can lead to a compensation effect, where amplitude information is sacrificed to compensate for the phase that is harmful to SE. In addition, to further improve the performance of SE, many modules are stacked onto SE, resulting in increased model complexity that limits the application of SE. To address these problems, we proposed a dual-path network based on compressed frequency using Mamba. First, we extract amplitude and phase information through parallel dual branches. This approach leverages structured complex spectra to implicitly capture phase information and solves the compensation effect by decoupling amplitude and phase, and the network incorporates an interaction module to suppress unnecessary parts and recover missing components from the other branch. Second, to reduce network complexity, the network introduces a band-split strategy to compress the frequency dimension. To further reduce complexity while maintaining good performance, we designed a Mamba-based module that models the time and frequency dimensions under linear complexity. Finally, compared to baselines, our model achieves an average 8.3 times reduction in computational complexity while maintaining superior performance. Furthermore, it achieves a 25 times reduction in complexity compared to transformer-based models.",
        "tags": [
            "Mamba",
            "Selective State Spaces",
            "Transformer"
        ]
    },
    {
        "id": "73",
        "title": "Improving Generative Pre-Training: An In-depth Study of Masked Image Modeling and Denoising Models",
        "author": [
            "Hyesong Choi",
            "Daeun Kim",
            "Sungmin Cha",
            "Kwang Moo Yi",
            "Dongbo Min"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19104",
        "abstract": "In this work, we dive deep into the impact of additive noise in pre-training deep networks. While various methods have attempted to use additive noise inspired by the success of latent denoising diffusion models, when used in combination with masked image modeling, their gains have been marginal when it comes to recognition tasks. We thus investigate why this would be the case, in an attempt to find effective ways to combine the two ideas. Specifically, we find three critical conditions: corruption and restoration must be applied within the encoder, noise must be introduced in the feature space, and an explicit disentanglement between noised and masked tokens is necessary. By implementing these findings, we demonstrate improved pre-training performance for a wide range of recognition tasks, including those that require fine-grained, high-frequency information to solve.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "74",
        "title": "Discrete vs. Continuous Trade-offs for Generative Models",
        "author": [
            "Jathin Korrapati",
            "Tanish Baranwal",
            "Rahul Shah"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19114",
        "abstract": "This work explores the theoretical and practical foundations of denoising diffusion probabilistic models (DDPMs) and score-based generative models, which leverage stochastic processes and Brownian motion to model complex data distributions. These models employ forward and reverse diffusion processes defined through stochastic differential equations (SDEs) to iteratively add and remove noise, enabling high-quality data generation. By analyzing the performance bounds of these models, we demonstrate how score estimation errors propagate through the reverse process and bound the total variation distance using discrete Girsanov transformations, Pinsker's inequality, and the data processing inequality (DPI) for an information theoretic lens.",
        "tags": [
            "Diffusion",
            "Score-Based Generative"
        ]
    },
    {
        "id": "75",
        "title": "Convergence rate of Euler-Maruyama scheme for McKean-Vlasov SDEs with density-dependent drift",
        "author": [
            "Anh-Dung Le"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19121",
        "abstract": "In this paper, we study weak well-posedness of a McKean-Vlasov stochastic differential equations (SDEs) whose drift is density-dependent and whose diffusion is constant. The existence part is due to Hlder stability estimates of the associated Euler-Maruyama scheme. The uniqueness part is due to that of the associated Fokker-Planck equation. We also obtain convergence rate in weighted $L^1$ norm for the Euler-Maruyama scheme.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "76",
        "title": "Semantic Residual for Multimodal Unified Discrete Representation",
        "author": [
            "Hai Huang",
            "Shulei Wang",
            "Yan Xia"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19128",
        "abstract": "Recent research in the domain of multimodal unified representations predominantly employs codebook as representation forms, utilizing Vector Quantization(VQ) for quantization, yet there has been insufficient exploration of other quantization representation forms. Our work explores more precise quantization methods and introduces a new framework, Semantic Residual Cross-modal Information Disentanglement (SRCID), inspired by the numerical residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs semantic residual-based information disentanglement for multimodal data to better handle the inherent discrepancies between different modalities. Our method enhances the capabilities of unified multimodal representations and demonstrates exceptional performance in cross-modal generalization and cross-modal zero-shot retrieval. Its average results significantly surpass existing state-of-the-art models, as well as previous attempts with RVQ and Finite Scalar Quantization (FSQ) based on these modals.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "77",
        "title": "MVS-GS: High-Quality 3D Gaussian Splatting Mapping via Online Multi-View Stereo",
        "author": [
            "Byeonggwon Lee",
            "Junkyu Park",
            "Khang Truong Giang",
            "Sungho Jo",
            "Soohwan Song"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19130",
        "abstract": "This study addresses the challenge of online 3D model generation for neural rendering using an RGB image stream. Previous research has tackled this issue by incorporating Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3DGS) as scene representations within dense SLAM methods. However, most studies focus primarily on estimating coarse 3D scenes rather than achieving detailed reconstructions. Moreover, depth estimation based solely on images is often ambiguous, resulting in low-quality 3D models that lead to inaccurate renderings. To overcome these limitations, we propose a novel framework for high-quality 3DGS modeling that leverages an online multi-view stereo (MVS) approach. Our method estimates MVS depth using sequential frames from a local time window and applies comprehensive depth refinement techniques to filter out outliers, enabling accurate initialization of Gaussians in 3DGS. Furthermore, we introduce a parallelized backend module that optimizes the 3DGS model efficiently, ensuring timely updates with each new keyframe. Experimental results demonstrate that our method outperforms state-of-the-art dense SLAM methods, particularly excelling in challenging outdoor environments.",
        "tags": [
            "3D",
            "Depth Estimation",
            "Gaussian Splatting",
            "NeRF",
            "SLAM"
        ]
    },
    {
        "id": "78",
        "title": "PlanLLM: Video Procedure Planning with Refinable Large Language Models",
        "author": [
            "Dejie Yang",
            "Zijing Zhao",
            "YangLiu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19139",
        "abstract": "Video procedure planning, i.e., planning a sequence of action steps given the video frames of start and goal states, is an essential ability for embodied AI. Recent works utilize Large Language Models (LLMs) to generate enriched action step description texts to guide action step decoding. Although LLMs are introduced, these methods decode the action steps into a closed-set of one-hot vectors, limiting the model's capability of generalizing to new steps or tasks. Additionally, fixed action step descriptions based on world-level commonsense may contain noise in specific instances of visual states. In this paper, we propose PlanLLM, a cross-modal joint learning framework with LLMs for video procedure planning. We propose an LLM-Enhanced Planning module which fully uses the generalization ability of LLMs to produce free-form planning output and to enhance action step decoding. We also propose Mutual Information Maximization module to connect world-level commonsense of step descriptions and sample-specific information of visual states, enabling LLMs to employ the reasoning ability to generate step sequences. With the assistance of LLMs, our method can both closed-set and open vocabulary procedure planning tasks. Our PlanLLM achieves superior performance on three benchmarks, demonstrating the effectiveness of our designs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "79",
        "title": "CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian Splatting",
        "author": [
            "Siyu Jiao",
            "Haoye Dong",
            "Yuyang Yin",
            "Zequn Jie",
            "Yinlong Qian",
            "Yao Zhao",
            "Humphrey Shi",
            "Yunchao Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19142",
        "abstract": "Recent works in 3D multimodal learning have made remarkable progress. However, typically 3D multimodal models are only capable of handling point clouds. Compared to the emerging 3D representation technique, 3D Gaussian Splatting (3DGS), the spatially sparse point cloud cannot depict the texture information of 3D objects, resulting in inferior reconstruction capabilities. This limitation constrains the potential of point cloud-based 3D multimodal representation learning. In this paper, we present CLIP-GS, a novel multimodal representation learning framework grounded in 3DGS. We introduce the GS Tokenizer to generate serialized gaussian tokens, which are then processed through transformer layers pre-initialized with weights from point cloud models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss between 3DGS and the visual-text embeddings of CLIP, and we introduce an image voting loss to guide the directionality and convergence of gradient optimization. Furthermore, we develop an efficient way to generate triplets of 3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal representations. Leveraging the well-aligned multimodal representations, CLIP-GS demonstrates versatility and outperforms point cloud-based models on various 3D tasks, including multimodal retrieval, zero-shot, and few-shot classification.",
        "tags": [
            "3D",
            "CLIP",
            "Gaussian Splatting",
            "Transformer"
        ]
    },
    {
        "id": "80",
        "title": "Generating Editable Head Avatars with 3D Gaussian GANs",
        "author": [
            "Guohao Li",
            "Hongyu Yang",
            "Yifang Men",
            "Di Huang",
            "Weixin Li",
            "Ruijie Yang",
            "Yunhong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19149",
        "abstract": "Generating animatable and editable 3D head avatars is essential for various applications in computer vision and graphics. Traditional 3D-aware generative adversarial networks (GANs), often using implicit fields like Neural Radiance Fields (NeRF), achieve photorealistic and view-consistent 3D head synthesis. However, these methods face limitations in deformation flexibility and editability, hindering the creation of lifelike and easily modifiable 3D heads. We propose a novel approach that enhances the editability and animation control of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit 3D representation. This method enables easier illumination control and improved editability. Central to our approach is the Editable Gaussian Head (EG-Head) model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing precise expression control and flexible texture editing for accurate animation while preserving identity. To capture complex non-facial geometries like hair, we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments demonstrate that our approach delivers high-quality 3D-aware synthesis with state-of-the-art controllability. Our code and models are available at https://github.com/liguohao96/EGG3D.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "81",
        "title": "Referencing Where to Focus: Improving VisualGrounding with Referential Query",
        "author": [
            "Yabing Wang",
            "Zhuotao Tian",
            "Qingpei Guo",
            "Zheng Qin",
            "Sanping Zhou",
            "Ming Yang",
            "Le Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19155",
        "abstract": "Visual Grounding aims to localize the referring object in an image given a natural language expression. Recent advancements in DETR-based visual grounding methods have attracted considerable attention, as they directly predict the coordinates of the target object without relying on additional efforts, such as pre-generated proposal candidates or pre-defined anchor boxes. However, existing research primarily focuses on designing stronger multi-modal decoder, which typically generates learnable queries by random initialization or by using linguistic embeddings. This vanilla query generation approach inevitably increases the learning difficulty for the model, as it does not involve any target-related information at the beginning of decoding. Furthermore, they only use the deepest image feature during the query learning process, overlooking the importance of features from other levels. To address these issues, we propose a novel approach, called RefFormer. It consists of the query adaption module that can be seamlessly integrated into CLIP and generate the referential query to provide the prior context for decoder, along with a task-specific decoder. By incorporating the referential query into the decoder, we can effectively mitigate the learning difficulty of the decoder, and accurately concentrate on the target object. Additionally, our proposed query adaption module can also act as an adapter, preserving the rich knowledge within CLIP without the need to tune the parameters of the backbone network. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method, outperforming state-of-the-art approaches on five visual grounding benchmarks.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "82",
        "title": "Dual Channel Multi-Attention in ViT for Biometric Authentication using Forehead Subcutaneous Vein Pattern and Periocular Pattern",
        "author": [
            "Arun K. Sharma",
            "Shubhobrata Bhattacharya",
            "Motahar Reza"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19160",
        "abstract": "Traditional biometric systems, like face and fingerprint recognition, have encountered significant setbacks due to wearing face masks and hygiene concerns. To meet the challenges of the partially covered face due to face masks and hygiene concerns of fingerprint recognition, this paper proposes a novel dual-channel multi-attention Vision Transformer (ViT) framework for biometric authentication using forehead subcutaneous vein patterns and periocular patterns, offering a promising alternative to traditional methods, capable of performing well even with face masks and without any physical touch. The proposed framework leverages a dual-channel ViT architecture, designed to handle two distinct biometric traits. It can capture long-range dependencies of independent features from the vein and periocular patterns. A custom classifier is then designed to integrate the independently extracted features, producing a final class prediction. The performance of the proposed algorithm was rigorously evaluated using the Forehead Subcutaneous Vein Pattern and Periocular Biometric Pattern (FSVP-PBP) database. The results demonstrated the superiority of the algorithm over state-of-the-art methods, achieving remarkable classification accuracy of $99.3 \\pm 0.02\\%$ with the combined vein and periocular patterns.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "83",
        "title": "Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal Video-Text Retrieval",
        "author": [
            "Yang Du",
            "Yuqi Liu",
            "Qin Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19178",
        "abstract": "Cross-modal (e.g. image-text, video-text) retrieval is an important task in information retrieval and multimodal vision-language understanding field. Temporal understanding makes video-text retrieval more challenging than image-text retrieval. However, we find that the widely used video-text benchmarks have shortcomings in comprehensively assessing abilities of models, especially in temporal understanding, causing large-scale image-text pre-trained models can already achieve comparable zero-shot performance with video-text pre-trained models. In this paper, we introduce RTime, a novel temporal-emphasized video-text retrieval dataset. We first obtain videos of actions or events with significant temporality, and then reverse these videos to create harder negative samples. We then recruit annotators to judge the significance and reversibility of candidate videos, and write captions for qualified videos. We further adopt GPT-4 to extend more captions based on human-written captions. Our RTime dataset currently consists of 21k videos with 10 captions per video, totalling about 122 hours. Based on RTime, we propose three retrieval benchmark tasks: RTime-Origin, RTime-Hard, and RTime-Binary. We further enhance the use of harder-negatives in model training, and benchmark a variety of video-text models on RTime. Extensive experiment analysis proves that RTime indeed poses new and higher challenges to video-text retrieval. We release our RTime dataset\\footnote{\\url{https://github.com/qyr0403/Reversed-in-Time}} to further advance video-text retrieval and multimodal understanding research.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "84",
        "title": "An End-to-End Depth-Based Pipeline for Selfie Image Rectification",
        "author": [
            "Ahmed Alhawwary",
            "Phong Nguyen-Ha",
            "Janne Mustaniemi",
            "Janne Heikkil"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19189",
        "abstract": "Portraits or selfie images taken from a close distance typically suffer from perspective distortion. In this paper, we propose an end-to-end deep learning-based rectification pipeline to mitigate the effects of perspective distortion. We learn to predict the facial depth by training a deep CNN. The estimated depth is utilized to adjust the camera-to-subject distance by moving the camera farther, increasing the camera focal length, and reprojecting the 3D image features to the new perspective. The reprojected features are then fed to an inpainting module to fill in the missing pixels. We leverage a differentiable renderer to enable end-to-end training of our depth estimation and feature extraction nets to improve the rectified outputs. To boost the results of the inpainting module, we incorporate an auxiliary module to predict the horizontal movement of the camera which decreases the area that requires hallucination of challenging face parts such as ears. Unlike previous works, we process the full-frame input image at once without cropping the subject's face and processing it separately from the rest of the body, eliminating the need for complex post-processing steps to attach the face back to the subject's body. To train our network, we utilize the popular game engine Unreal Engine to generate a large synthetic face dataset containing various subjects, head poses, expressions, eyewear, clothes, and lighting. Quantitative and qualitative results show that our rectification pipeline outperforms previous methods, and produces comparable results with a time-consuming 3D GAN-based method while being more than 260 times faster.",
        "tags": [
            "3D",
            "Depth Estimation",
            "GAN",
            "Inpainting"
        ]
    },
    {
        "id": "85",
        "title": "Personalized Dynamic Music Emotion Recognition with Dual-Scale Attention-Based Meta-Learning",
        "author": [
            "Dengming Zhang",
            "Weitao You",
            "Ziheng Liu",
            "Lingyun Sun",
            "Pei Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19200",
        "abstract": "Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of different moments in music, playing a crucial role in music information retrieval. The existing DMER methods struggle to capture long-term dependencies when dealing with sequence data, which limits their performance. Furthermore, these methods often overlook the influence of individual differences on emotion perception, even though everyone has their own personalized emotional perception in the real world. Motivated by these issues, we explore more effective sequence processing methods and introduce the Personalized DMER (PDMER) problem, which requires models to predict emotions that align with personalized perception. Specifically, we propose a Dual-Scale Attention-Based Meta-Learning (DSAML) method. This method fuses features from a dual-scale feature extractor and captures both short and long-term dependencies using a dual-scale attention transformer, improving the performance in traditional DMER. To achieve PDMER, we design a novel task construction strategy that divides tasks by annotators. Samples in a task are annotated by the same annotator, ensuring consistent perception. Leveraging this strategy alongside meta-learning, DSAML can predict personalized perception of emotions with just one personalized annotation sample. Our objective and subjective experiments demonstrate that our method can achieve state-of-the-art performance in both traditional DMER and PDMER.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "86",
        "title": "Context-Aware Deep Learning for Multi Modal Depression Detection",
        "author": [
            "Genevieve Lam",
            "Huang Dongyan",
            "Weisi Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19209",
        "abstract": "In this study, we focus on automated approaches to detect depression from clinical interviews using multi-modal machine learning (ML). Our approach differentiates from other successful ML methods such as context-aware analysis through feature engineering and end-to-end deep neural networks for depression detection utilizing the Distress Analysis Interview Corpus. We propose a novel method that incorporates: (1) pre-trained Transformer combined with data augmentation based on topic modelling for textual data; and (2) deep 1D convolutional neural network (CNN) for acoustic feature modeling. The simulation results demonstrate the effectiveness of the proposed method for training multi-modal deep learning models. Our deep 1D CNN and Transformer models achieved state-of-the-art performance for audio and text modalities respectively. Combining them in a multi-modal framework also outperforms state-of-the-art for the combined setting. Code available at https://github.com/genandlam/multi-modal-depression-detection",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "87",
        "title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
        "author": [
            "Yuxin You",
            "Zhen Liu",
            "Xiangchao Wen",
            "Yongtao Zhang",
            "Wei Ai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19211",
        "abstract": "Graph mining is an important area in data mining and machine learning that involves extracting valuable information from graph-structured data. In recent years, significant progress has been made in this field through the development of graph neural networks (GNNs). However, GNNs are still deficient in generalizing to diverse graph data. Aiming to this issue, Large Language Models (LLMs) could provide new solutions for graph mining tasks with their superior semantic understanding. In this review, we systematically review the combination and application techniques of LLMs and GNNs and present a novel taxonomy for research in this interdisciplinary field, which involves three main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving. Within this framework, we reveal the capabilities of LLMs in enhancing graph feature extraction as well as improving the effectiveness of downstream tasks such as node classification, link prediction, and community detection. Although LLMs have demonstrated their great potential in handling graph-structured data, their high computational requirements and complexity remain challenges. Future research needs to continue to explore how to efficiently fuse LLMs and GNNs to achieve more powerful graph learning and reasoning capabilities and provide new impetus for the development of graph mining techniques.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "88",
        "title": "Transformer-Based Wireless Capsule Endoscopy Bleeding Tissue Detection and Classification",
        "author": [
            "Basit Alawode",
            "Shibani Hamza",
            "Adarsh Ghimire",
            "Divya Velayudhan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19218",
        "abstract": "Informed by the success of the transformer model in various computer vision tasks, we design an end-to-end trainable model for the automatic detection and classification of bleeding and non-bleeding frames extracted from Wireless Capsule Endoscopy (WCE) videos. Based on the DETR model, our model uses the Resnet50 for feature extraction, the transformer encoder-decoder for bleeding and non-bleeding region detection, and a feedforward neural network for classification. Trained in an end-to-end approach on the Auto-WCEBleedGen Version 1 challenge training set, our model performs both detection and classification tasks as a single unit. Our model achieves an accuracy, recall, and F1-score classification percentage score of 98.28, 96.79, and 98.37 respectively, on the Auto-WCEBleedGen version 1 validation set. Further, we record an average precision (AP @ 0.5), mean-average precision (mAP) of 0.7447 and 0.7328 detection results. This earned us a 3rd place position in the challenge. Our code is publicly available via https://github.com/BasitAlawode/WCEBleedGen.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "89",
        "title": "Completion as Enhancement: A Degradation-Aware Selective Image Guided Network for Depth Completion",
        "author": [
            "Zhiqiang Yan",
            "Zhengxue Wang",
            "Kun Wang",
            "Jun Li",
            "Jian Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19225",
        "abstract": "In this paper, we introduce the Selective Image Guided Network (SigNet), a novel degradation-aware framework that transforms depth completion into depth enhancement for the first time. Moving beyond direct completion using convolutional neural networks (CNNs), SigNet initially densifies sparse depth data through non-CNN densification tools to obtain coarse yet dense depth. This approach eliminates the mismatch and ambiguity caused by direct convolution over irregularly sampled sparse data. Subsequently, SigNet redefines completion as enhancement, establishing a self-supervised degradation bridge between the coarse depth and the targeted dense depth for effective RGB-D fusion. To achieve this, SigNet leverages the implicit degradation to adaptively select high-frequency components (e.g., edges) of RGB data to compensate for the coarse depth. This degradation is further integrated into a multi-modal conditional Mamba, dynamically generating the state parameters to enable efficient global high-frequency information interaction. We conduct extensive experiments on the NYUv2, DIML, SUN RGBD, and TOFDC datasets, demonstrating the state-of-the-art (SOTA) performance of SigNet.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "90",
        "title": "6Diffusion: IPv6 Target Generation Using a Diffusion Model with Global-Local Attention Mechanisms for Internet-wide IPv6 Scanning",
        "author": [
            "Nabo He",
            "DanDan Li",
            "Xiaohong Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19243",
        "abstract": "Due to the vast address space of IPv6, the brute-force scanning methods originally applicable to IPv4 are no longer suitable for proactive scanning of IPv6. The recently proposed target generation algorithms have a low hit rate for existing IPv6 target generation algorithms, primarily because they do not accurately fit the distribution patterns of active IPv6 addresses. This paper introduces a diffusion model-based IPv6 target generation algorithm called 6Diffusion. 6Diffusion first maps addresses to vector space for language modeling, adds noise to active IPv6 addresses in the forward process, diffusing them throughout the entire IPv6 address space, and then performs a reverse process to gradually denoise and recover to active IPv6 addresses. We use the DDIM sampler to increase the speed of generating candidate sets. At the same time, we introduce the GLF-MSA (Global-Local Fusion Multi-Head Self-Attention) mechanism to adapt to the top-down global allocation pattern of IPv6 addresses and the local characteristics of IPv6 address segments, thus better learning the deep-level features of active IPv6 addresses. Experimental results show that compared to existing methods, 6Diffusion can generate higher quality candidate sets and outperforms state-of-the-art target generation algorithms across multiple metrics.",
        "tags": [
            "DDIM",
            "Diffusion"
        ]
    },
    {
        "id": "91",
        "title": "Reflective Gaussian Splatting",
        "author": [
            "Yuxuan Yao",
            "Zixuan Zeng",
            "Chun Gu",
            "Xiatian Zhu",
            "Li Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19282",
        "abstract": "Novel view synthesis has experienced significant advancements owing to increasingly capable NeRF- and 3DGS-based methods. However, reflective object reconstruction remains challenging, lacking a proper solution to achieve real-time, high-quality rendering while accommodating inter-reflection. To fill this gap, we introduce a Reflective Gaussian splatting (\\textbf{Ref-Gaussian}) framework characterized with two components: (I) {\\em Physically based deferred rendering} that empowers the rendering equation with pixel-level material properties via formulating split-sum approximation; (II) {\\em Gaussian-grounded inter-reflection} that realizes the desired inter-reflection function within a Gaussian splatting paradigm for the first time. To enhance geometry modeling, we further introduce material-aware normal propagation and an initial per-Gaussian shading stage, along with 2D Gaussian primitives. Extensive experiments on standard datasets demonstrate that Ref-Gaussian surpasses existing approaches in terms of quantitative metrics, visual quality, and compute efficiency. Further, we show that our method serves as a unified solution for both reflective and non-reflective scenes, going beyond the previous alternatives focusing on only reflective scenes. Also, we illustrate that Ref-Gaussian supports more applications such as relighting and editing.",
        "tags": [
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "92",
        "title": "ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image Captioning",
        "author": [
            "Taewhan Kim",
            "Soeun Lee",
            "Si-Woo Kim",
            "Dong-Jin Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19289",
        "abstract": "Recent lightweight image captioning models using retrieved data mainly focus on text prompts. However, previous works only utilize the retrieved text as text prompts, and the visual information relies only on the CLIP visual embedding. Because of this issue, there is a limitation that the image descriptions inherent in the prompt are not sufficiently reflected in the visual embedding space. To tackle this issue, we propose ViPCap, a novel retrieval text-based visual prompt for lightweight image captioning. ViPCap leverages the retrieved text with image information as visual prompts to enhance the ability of the model to capture relevant visual information. By mapping text prompts into the CLIP space and generating multiple randomized Gaussian distributions, our method leverages sampling to explore randomly augmented distributions and effectively retrieves the semantic features that contain image information. These retrieved features are integrated into the image and designated as the visual prompt, leading to performance improvements on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results demonstrate that ViPCap significantly outperforms prior lightweight captioning models in efficiency and effectiveness, demonstrating the potential for a plug-and-play solution.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "93",
        "title": "RAG with Differential Privacy",
        "author": [
            "Nicolas Grislain"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19291",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to provide *Large Language Models* (LLM) with fresh and relevant context, mitigating the risk of hallucinations and improving the overall quality of responses in environments with large and fast moving knowledge bases. However, the integration of external documents into the generation process raises significant privacy concerns. Indeed, when added to a prompt, it is not possible to guarantee a response will not inadvertently expose confidential data, leading to potential breaches of privacy and ethical dilemmas. This paper explores a practical solution to this problem suitable to general knowledge extraction from personal data. It shows *differentially private token generation* is a viable approach to private RAG.",
        "tags": [
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "94",
        "title": "When SAM2 Meets Video Shadow and Mirror Detection",
        "author": [
            "Leiping Jie"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19293",
        "abstract": "As the successor to the Segment Anything Model (SAM), the Segment Anything Model 2 (SAM2) not only improves performance in image segmentation but also extends its capabilities to video segmentation. However, its effectiveness in segmenting rare objects that seldom appear in videos remains underexplored. In this study, we evaluate SAM2 on three distinct video segmentation tasks: Video Shadow Detection (VSD) and Video Mirror Detection (VMD). Specifically, we use ground truth point or mask prompts to initialize the first frame and then predict corresponding masks for subsequent frames. Experimental results show that SAM2's performance on these tasks is suboptimal, especially when point prompts are used, both quantitatively and qualitatively. Code is available at \\url{https://github.com/LeipingJie/SAM2Video}",
        "tags": [
            "Detection",
            "SAM",
            "Segment Anything",
            "Segmentation"
        ]
    },
    {
        "id": "95",
        "title": "Manga Generation via Layout-controllable Diffusion",
        "author": [
            "Siyu Chen",
            "Dengjie Li",
            "Zenghao Bao",
            "Yao Zhou",
            "Lingfeng Tan",
            "Yujie Zhong",
            "Zheng Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19303",
        "abstract": "Generating comics through text is widely studied. However, there are few studies on generating multi-panel Manga (Japanese comics) solely based on plain text. Japanese manga contains multiple panels on a single page, with characteristics such as coherence in storytelling, reasonable and diverse page layouts, consistency in characters, and semantic correspondence between panel drawings and panel scripts. Therefore, generating manga poses a significant challenge. This paper presents the manga generation task and constructs the Manga109Story dataset for studying manga generation solely from plain text. Additionally, we propose MangaDiffusion to facilitate the intra-panel and inter-panel information interaction during the manga generation process. The results show that our method particularly ensures the number of panels, reasonable and diverse page layouts. Based on our approach, there is potential to converting a large amount of textual stories into more engaging manga readings, leading to significant application prospects.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "96",
        "title": "Perceive, Query & Reason: Enhancing Video QA with Question-Guided Temporal Queries",
        "author": [
            "Roberto Amoroso",
            "Gengyuan Zhang",
            "Rajat Koner",
            "Lorenzo Baraldi",
            "Rita Cucchiara",
            "Volker Tresp"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19304",
        "abstract": "Video Question Answering (Video QA) is a challenging video understanding task that requires models to comprehend entire videos, identify the most relevant information based on contextual cues from a given question, and reason accurately to provide answers. Recent advancements in Multimodal Large Language Models (MLLMs) have transformed video QA by leveraging their exceptional commonsense reasoning capabilities. This progress is largely driven by the effective alignment between visual data and the language space of MLLMs. However, for video QA, an additional space-time alignment poses a considerable challenge for extracting question-relevant information across frames. In this work, we investigate diverse temporal modeling techniques to integrate with MLLMs, aiming to achieve question-guided temporal modeling that leverages pre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel temporal modeling method that creates a question-guided temporal bridge between frame-wise visual perception and the reasoning capabilities of LLMs. Our evaluation across multiple video QA benchmarks demonstrates that T-Former competes favorably with existing temporal modeling approaches and aligns with recent advancements in video QA.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "97",
        "title": "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment",
        "author": [
            "Ziang Yan",
            "Zhilin Li",
            "Yinan He",
            "Chenting Wang",
            "Kunchang Li",
            "Xinhao Li",
            "Xiangyu Zeng",
            "Zilei Wang",
            "Yali Wang",
            "Yu Qiao",
            "Limin Wang",
            "Yi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19326",
        "abstract": "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals though they give comprehensive perception and reasoning in a spectrum of vision applications. Recent studies either develop tool-using or unify specific visual tasks into the autoregressive framework, often at the expense of overall multimodal performance. To address this issue and enhance MLLMs with visual tasks in a scalable fashion, we propose Task Preference Optimization (TPO), a novel method that utilizes differentiable task preferences derived from typical fine-grained visual tasks. TPO introduces learnable task tokens that establish connections between multiple task-specific heads and the MLLM. By leveraging rich visual labels during training, TPO significantly enhances the MLLM's multimodal capabilities and task-specific performance. Through multi-task co-training within TPO, we observe synergistic benefits that elevate individual task performance beyond what is achievable through single-task training methodologies. Our instantiation of this approach with VideoChat and LLaVA demonstrates an overall 14.6% improvement in multimodal performance compared to baseline models. Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across various tasks, performing comparably to state-of-the-art supervised models. The code will be released at https://github.com/OpenGVLab/TPO",
        "tags": [
            "LLaVA",
            "Large Language Models"
        ]
    },
    {
        "id": "98",
        "title": "On the Expressiveness and Length Generalization of Selective State-Space Models on Regular Languages",
        "author": [
            "Aleksandar Terzi",
            "Michael Hersche",
            "Giacomo Camposampiero",
            "Thomas Hofmann",
            "Abu Sebastian",
            "Abbas Rahimi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19350",
        "abstract": "Selective state-space models (SSMs) are an emerging alternative to the Transformer, offering the unique advantage of parallel training and sequential inference. Although these models have shown promising performance on a variety of tasks, their formal expressiveness and length generalization properties remain underexplored. In this work, we provide insight into the workings of selective SSMs by analyzing their expressiveness and length generalization performance on regular language tasks, i.e., finite-state automaton (FSA) emulation. We address certain limitations of modern SSM-based architectures by introducing the Selective Dense State-Space Model (SD-SSM), the first selective SSM that exhibits perfect length generalization on a set of various regular language tasks using a single layer. It utilizes a dictionary of dense transition matrices, a softmax selection mechanism that creates a convex combination of dictionary matrices at each time step, and a readout consisting of layer normalization followed by a linear map. We then proceed to evaluate variants of diagonal selective SSMs by considering their empirical performance on commutative and non-commutative automata. We explain the experimental results with theoretical considerations. Our code is available at https://github.com/IBM/selective-dense-state-space-model.",
        "tags": [
            "SSMs",
            "State Space Models",
            "Transformer"
        ]
    },
    {
        "id": "99",
        "title": "ETTA: Elucidating the Design Space of Text-to-Audio Models",
        "author": [
            "Sang-gil Lee",
            "Zhifeng Kong",
            "Arushi Goel",
            "Sungwon Kim",
            "Rafael Valle",
            "Bryan Catanzaro"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19351",
        "abstract": "Recent years have seen significant progress in Text-To-Audio (TTA) synthesis, enabling users to enrich their creative workflows with synthetic audio generated from natural language prompts. Despite this progress, the effects of data, model architecture, training objective functions, and sampling strategies on target benchmarks are not well understood. With the purpose of providing a holistic understanding of the design space of TTA models, we set up a large-scale empirical experiment focused on diffusion and flow matching models. Our contributions include: 1) AF-Synthetic, a large dataset of high quality synthetic captions obtained from an audio understanding model; 2) a systematic comparison of different architectural, training, and inference design choices for TTA models; 3) an analysis of sampling methods and their Pareto curves with respect to generation quality and inference speed. We leverage the knowledge obtained from this extensive analysis to propose our best model dubbed Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps, ETTA provides improvements over the baselines trained on publicly available data, while being competitive with models trained on proprietary data. Finally, we show ETTA's improved ability to generate creative audio following complex and imaginative captions -- a task that is more challenging than current benchmarks.",
        "tags": [
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "100",
        "title": "Dynamic Skill Adaptation for Large Language Models",
        "author": [
            "Jiaao Chen",
            "Diyi Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19361",
        "abstract": "We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework to adapt novel and complex skills to Large Language Models (LLMs). Compared with previous work which learns from human-curated and static data in random orders, we propose to first automatically generate and organize the training data by mimicking the learning pathways of human and then dynamically tailor the training data based on the training dynamics. Specifically, inspired by the learning structures and teaching strategies in the human education system, we first construct a skill graph by decomposing complex skills into sub-skills and arranging them based on their dependencies in human syllables. For every skill, we utilize LLMs to generate both textbook-like data which contains detailed descriptions of skills for pre-training and exercise-like data which targets at explicitly utilizing the skills to solve problems for instruction-tuning. Furthermore, during the instruction-tuning, we dynamically update the training data which down-weight easy-to-learn examples, generate more complex examples, and filter out data with errors. Experiments on large language models such as LLAMA and Mistral demonstrate the effectiveness of our proposed methods in adapting math reasoning skills and social study skills.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "101",
        "title": "BeSplat -- Gaussian Splatting from a Single Blurry Image and Event Stream",
        "author": [
            "Gopi Raju Matta",
            "Reddypalli Trisha",
            "Kaushik Mitra"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19370",
        "abstract": "Novel view synthesis has been greatly enhanced by the development of radiance field methods. The introduction of 3D Gaussian Splatting (3DGS) has effectively addressed key challenges, such as long training times and slow rendering speeds, typically associated with Neural Radiance Fields (NeRF), while maintaining high-quality reconstructions. In this work (BeSplat), we demonstrate the recovery of sharp radiance field (Gaussian splats) from a single motion-blurred image and its corresponding event stream. Our method jointly learns the scene representation via Gaussian Splatting and recovers the camera motion through Bezier SE(3) formulation effectively, minimizing discrepancies between synthesized and real-world measurements of both blurry image and corresponding event stream. We evaluate our approach on both synthetic and real datasets, showcasing its ability to render view-consistent, sharp images from the learned radiance field and the estimated camera trajectory. To the best of our knowledge, ours is the first work to address this highly challenging ill-posed problem in a Gaussian Splatting framework with the effective incorporation of temporal information captured using the event stream.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "102",
        "title": "An Engorgio Prompt Makes Large Language Model Babble on",
        "author": [
            "Jianshuo Dong",
            "Ziyuan Zhang",
            "Qingjie Zhang",
            "Han Qiu",
            "Tianwei Zhang",
            "Hao Wang",
            "Hewu Li",
            "Qi Li",
            "Chao Zhang",
            "Ke Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19394",
        "abstract": "Auto-regressive large language models (LLMs) have yielded impressive performance in many real-world tasks. However, the new paradigm of these LLMs also exposes novel threats. In this paper, we explore their vulnerability to inference cost attacks, where a malicious user crafts Engorgio prompts to intentionally increase the computation cost and latency of the inference process. We design Engorgio, a novel methodology, to efficiently generate adversarial Engorgio prompts to affect the target LLM's service availability. Engorgio has the following two technical contributions. (1) We employ a parameterized distribution to track LLMs' prediction trajectory. (2) Targeting the auto-regressive nature of LLMs' inference process, we propose novel loss functions to stably suppress the appearance of the <EOS> token, whose occurrence will interrupt the LLM's generation process. We conduct extensive experiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B. The results show that Engorgio prompts can successfully induce LLMs to generate abnormally long outputs (i.e., roughly 2-13$\\times$ longer to reach 90%+ of the output length limit) in a white-box scenario and our real-world experiment demonstrates Engergio's threat to LLM service with limited computing resources. The code is accessible at https://github.com/jianshuod/Engorgio-prompt.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "103",
        "title": "MLLM-SUL: Multimodal Large Language Model for Semantic Scene Understanding and Localization in Traffic Scenarios",
        "author": [
            "Jiaqi Fan",
            "Jianhua Wu",
            "Jincheng Gao",
            "Jianhao Yu",
            "Yafei Wang",
            "Hongqing Chu",
            "Bingzhao Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19406",
        "abstract": "Multimodal large language models (MLLMs) have shown satisfactory effects in many autonomous driving tasks. In this paper, MLLMs are utilized to solve joint semantic scene understanding and risk localization tasks, while only relying on front-view images. In the proposed MLLM-SUL framework, a dual-branch visual encoder is first designed to extract features from two resolutions, and rich visual information is conducive to the language model describing risk objects of different sizes accurately. Then for the language generation, LLaMA model is fine-tuned to predict scene descriptions, containing the type of driving scenario, actions of risk objects, and driving intentions and suggestions of ego-vehicle. Ultimately, a transformer-based network incorporating a regression token is trained to locate the risk objects. Extensive experiments on the existing DRAMA-ROLISP dataset and the extended DRAMA-SRIS dataset demonstrate that our method is efficient, surpassing many state-of-the-art image-based and video-based methods. Specifically, our method achieves 80.1% BLEU-1 score and 298.5% CIDEr score in the scene understanding task, and 59.6% accuracy in the localization task. Codes and datasets are available at https://github.com/fjq-tongji/MLLM-SUL.",
        "tags": [
            "LLaMA",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "104",
        "title": "Multi-scale Latent Point Consistency Models for 3D Shape Generation",
        "author": [
            "Bi'an Du",
            "Wei Hu",
            "Renjie Liao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19413",
        "abstract": "Consistency Models (CMs) have significantly accelerated the sampling process in diffusion models, yielding impressive results in synthesizing high-resolution images. To explore and extend these advancements to point-cloud-based 3D shape generation, we propose a novel Multi-scale Latent Point Consistency Model (MLPCM). Our MLPCM follows a latent diffusion framework and introduces hierarchical levels of latent representations, ranging from point-level to super-point levels, each corresponding to a different spatial resolution. We design a multi-scale latent integration module along with 3D spatial attention to effectively denoise the point-level latent representations conditioned on those from multiple super-point levels. Additionally, we propose a latent consistency model, learned through consistency distillation, that compresses the prior into a one-step generator. This significantly improves sampling efficiency while preserving the performance of the original teacher model. Extensive experiments on standard benchmarks ShapeNet and ShapeNet-Vol demonstrate that MLPCM achieves a 100x speedup in the generation process, while surpassing state-of-the-art diffusion models in terms of both shape quality and diversity.",
        "tags": [
            "3D",
            "Consistency Models",
            "Diffusion"
        ]
    },
    {
        "id": "105",
        "title": "KALAHash: Knowledge-Anchored Low-Resource Adaptation for Deep Hashing",
        "author": [
            "Shu Zhao",
            "Tan Yu",
            "Xiaoshuai Hao",
            "Wenchao Ma",
            "Vijaykrishnan Narayanan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19417",
        "abstract": "Deep hashing has been widely used for large-scale approximate nearest neighbor search due to its storage and search efficiency. However, existing deep hashing methods predominantly rely on abundant training data, leaving the more challenging scenario of low-resource adaptation for deep hashing relatively underexplored. This setting involves adapting pre-trained models to downstream tasks with only an extremely small number of training samples available. Our preliminary benchmarks reveal that current methods suffer significant performance degradation due to the distribution shift caused by limited training samples. To address these challenges, we introduce Class-Calibration LoRA (CLoRA), a novel plug-and-play approach that dynamically constructs low-rank adaptation matrices by leveraging class-level textual knowledge embeddings. CLoRA effectively incorporates prior class knowledge as anchors, enabling parameter-efficient fine-tuning while maintaining the original data distribution. Furthermore, we propose Knowledge-Guided Discrete Optimization (KIDDO), a framework to utilize class knowledge to compensate for the scarcity of visual information and enhance the discriminability of hash codes. Extensive experiments demonstrate that our proposed method, Knowledge- Anchored Low-Resource Adaptation Hashing (KALAHash), significantly boosts retrieval performance and achieves a 4x data efficiency in low-resource scenarios.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "106",
        "title": "Revisiting PCA for time series reduction in temporal dimension",
        "author": [
            "Jiaxin Gao",
            "Wenbo Hu",
            "Yuntian Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19423",
        "abstract": "Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao, Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series analysis (TSA), enabling the extraction of complex patterns for tasks like classification, forecasting, and regression. Although dimensionality reduction has traditionally focused on the variable space-achieving notable success in minimizing data redundancy and computational complexity-less attention has been paid to reducing the temporal dimension. In this study, we revisit Principal Component Analysis (PCA), a classical dimensionality reduction technique, to explore its utility in temporal dimension reduction for time series data. It is generally thought that applying PCA to the temporal dimension would disrupt temporal dependencies, leading to limited exploration in this area. However, our theoretical analysis and extensive experiments demonstrate that applying PCA to sliding series windows not only maintains model performance, but also enhances computational efficiency. In auto-regressive forecasting, the temporal structure is partially preserved through windowing, and PCA is applied within these windows to denoise the time series while retaining their statistical information. By preprocessing time-series data with PCA, we reduce the temporal dimensionality before feeding it into TSA models such as Linear, Transformer, CNN, and RNN architectures. This approach accelerates training and inference and reduces resource consumption. Notably, PCA improves Informer training and inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%, without sacrificing model accuracy. Comparative analysis against other reduction methods further highlights the effectiveness of PCA in improving the efficiency of TSA models.",
        "tags": [
            "RNN",
            "Transformer"
        ]
    },
    {
        "id": "107",
        "title": "Temporal Context Consistency Above All: Enhancing Long-Term Anticipation by Learning and Enforcing Temporal Constraints",
        "author": [
            "Alberto Mat",
            "Mariella Dimiccoli"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19424",
        "abstract": "This paper proposes a method for long-term action anticipation (LTA), the task of predicting action labels and their duration in a video given the observation of an initial untrimmed video interval. We build on an encoder-decoder architecture with parallel decoding and make two key contributions. First, we introduce a bi-directional action context regularizer module on the top of the decoder that ensures temporal context coherence in temporally adjacent segments. Second, we learn from classified segments a transition matrix that models the probability of transitioning from one action to another and the sequence is optimized globally over the full prediction interval. In addition, we use a specialized encoder for the task of action segmentation to increase the quality of the predictions in the observation interval at inference time, leading to a better understanding of the past. We validate our methods on four benchmark datasets for LTA, the EpicKitchen-55, EGTEA+, 50Salads and Breakfast demonstrating superior or comparable performance to state-of-the-art methods, including probabilistic models and also those based on Large Language Models, that assume trimmed video as input. The code will be released upon acceptance.",
        "tags": [
            "Large Language Models",
            "Segmentation"
        ]
    },
    {
        "id": "108",
        "title": "A Survey on Large Language Model Acceleration based on KV Cache Management",
        "author": [
            "Haoyang Li",
            "Yiming Li",
            "Anxin Tian",
            "Tianhao Tang",
            "Zhanchao Xu",
            "Xuejia Chen",
            "Nicole Hu",
            "Wei Dong",
            "Qing Li",
            "Lei Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19442",
        "abstract": "Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "109",
        "title": "Feature Alignment-Based Knowledge Distillation for Efficient Compression of Large Language Models",
        "author": [
            "Shuo Wang",
            "Chihang Wang",
            "Jia Gao",
            "Zhen Qi",
            "Hongye Zheng",
            "Xiaoxuan Liao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19449",
        "abstract": "This study proposes a knowledge distillation algorithm based on large language models and feature alignment, aiming to effectively transfer the knowledge of large pre-trained models into lightweight student models, thereby reducing computational costs while maintaining high model performance. Different from the traditional soft label distillation method, this method introduces a multi-layer feature alignment strategy to deeply align the intermediate features and attention mechanisms of the teacher model and the student model, maximally retaining the semantic expression ability and context modeling ability of the teacher model. In terms of method design, a multi-task loss function is constructed, including feature matching loss, attention alignment loss, and output distribution matching loss, to ensure multi-level information transfer through joint optimization. The experiments were comprehensively evaluated on the GLUE data set and various natural language processing tasks. The results show that the proposed model performs very close to the state-of-the-art GPT-4 model in terms of evaluation indicators such as perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline models such as DeBERTa, XLNet, and GPT-3, showing significant performance improvements and computing efficiency advantages. Research results show that the feature alignment distillation strategy is an effective model compression method that can significantly reduce computational overhead and storage requirements while maintaining model capabilities. Future research can be further expanded in the directions of self-supervised learning, cross-modal feature alignment, and multi-task transfer learning to provide more flexible and efficient solutions for the deployment and optimization of deep learning models.",
        "tags": [
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "110",
        "title": "Find the Intention of Instruction: Comprehensive Evaluation of Instruction Understanding for Large Language Models",
        "author": [
            "Hyeonseok Moon",
            "Jaehyung Seo",
            "Seungyoon Lee",
            "Chanjun Park",
            "Heuiseok Lim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19450",
        "abstract": "One of the key strengths of Large Language Models (LLMs) is their ability to interact with humans by generating appropriate responses to given instructions. This ability, known as instruction-following capability, has established a foundation for the use of LLMs across various fields and serves as a crucial metric for evaluating their performance. While numerous evaluation benchmarks have been developed, most focus solely on clear and coherent instructions. However, we have noted that LLMs can become easily distracted by instruction-formatted statements, which may lead to an oversight of their instruction comprehension skills. To address this issue, we introduce the Intention of Instruction (IoInst) benchmark. This benchmark evaluates LLMs' capacity to remain focused and understand instructions without being misled by extraneous instructions. The primary objective of this benchmark is to identify the appropriate instruction that accurately guides the generation of a given context. Our findings suggest that even recently introduced state-of-the-art models still lack instruction understanding capability. Along with the proposition of IoInst in this study, we also present broad analyses of the several strategies potentially applicable to IoInst.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "111",
        "title": "NijiGAN: Transform What You See into Anime with Contrastive Semi-Supervised Learning and Neural Ordinary Differential Equations",
        "author": [
            "Kevin Putra Santoso",
            "Anny Yuniarti",
            "Dwiyasa Nakula",
            "Dimas Prihady Setyawan",
            "Adam Haidar Azizi",
            "Jeany Aurellia P. Dewati",
            "Farah Dhia Fadhila",
            "Maria T. Elvara Bumbungan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19455",
        "abstract": "Generative AI has transformed the animation industry. Several models have been developed for image-to-image translation, particularly focusing on converting real-world images into anime through unpaired translation. Scenimefy, a notable approach utilizing contrastive learning, achieves high fidelity anime scene translation by addressing limited paired data through semi-supervised training. However, it faces limitations due to its reliance on paired data from a fine-tuned StyleGAN in the anime domain, often producing low-quality datasets. Additionally, Scenimefy's high parameter architecture presents opportunities for computational optimization. This research introduces NijiGAN, a novel model incorporating Neural Ordinary Differential Equations (NeuralODEs), which offer unique advantages in continuous transformation modeling compared to traditional residual networks. NijiGAN successfully transforms real-world scenes into high fidelity anime visuals using half of Scenimefy's parameters. It employs pseudo-paired data generated through Scenimefy for supervised training, eliminating dependence on low-quality paired data and improving the training process. Our comprehensive evaluation includes ablation studies, qualitative, and quantitative analysis comparing NijiGAN to similar models. The testing results demonstrate that NijiGAN produces higher-quality images compared to AnimeGAN, as evidenced by a Mean Opinion Score (MOS) of 2.192, it surpasses AnimeGAN's MOS of 2.160. Furthermore, our model achieved a Frechet Inception Distance (FID) score of 58.71, outperforming Scenimefy's FID score of 60.32. These results demonstrate that NijiGAN achieves competitive performance against existing state-of-the-arts, especially Scenimefy as the baseline model.",
        "tags": [
            "StyleGAN"
        ]
    },
    {
        "id": "112",
        "title": "DriveEditor: A Unified 3D Information-Guided Framework for Controllable Object Editing in Driving Scenes",
        "author": [
            "Yiyuan Liang",
            "Zhiying Yan",
            "Liqun Chen",
            "Jiahuan Zhou",
            "Luxin Yan",
            "Sheng Zhong",
            "Xu Zou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19458",
        "abstract": "Vision-centric autonomous driving systems require diverse data for robust training and evaluation, which can be augmented by manipulating object positions and appearances within existing scene captures. While recent advancements in diffusion models have shown promise in video editing, their application to object manipulation in driving scenarios remains challenging due to imprecise positional control and difficulties in preserving high-fidelity object appearances. To address these challenges in position and appearance control, we introduce DriveEditor, a diffusion-based framework for object editing in driving videos. DriveEditor offers a unified framework for comprehensive object editing operations, including repositioning, replacement, deletion, and insertion. These diverse manipulations are all achieved through a shared set of varying inputs, processed by identical position control and appearance maintenance modules. The position control module projects the given 3D bounding box while preserving depth information and hierarchically injects it into the diffusion process, enabling precise control over object position and orientation. The appearance maintenance module preserves consistent attributes with a single reference image by employing a three-tiered approach: low-level detail preservation, high-level semantic maintenance, and the integration of 3D priors from a novel view synthesis model. Extensive qualitative and quantitative evaluations on the nuScenes dataset demonstrate DriveEditor's exceptional fidelity and controllability in generating diverse driving scene edits, as well as its remarkable ability to facilitate downstream tasks.",
        "tags": [
            "3D",
            "Diffusion",
            "Video Editing"
        ]
    },
    {
        "id": "113",
        "title": "Generative Adversarial Network on Motion-Blur Image Restoration",
        "author": [
            "Zhengdong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19479",
        "abstract": "In everyday life, photographs taken with a camera often suffer from motion blur due to hand vibrations or sudden movements. This phenomenon can significantly detract from the quality of the images captured, making it an interesting challenge to develop a deep learning model that utilizes the principles of adversarial networks to restore clarity to these blurred pixels. In this project, we will focus on leveraging Generative Adversarial Networks (GANs) to effectively deblur images affected by motion blur. A GAN-based Tensorflow model is defined, training and evaluating by GoPro dataset which comprises paired street view images featuring both clear and blurred versions. This adversarial training process between Discriminator and Generator helps to produce increasingly realistic images over time. Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) are the two evaluation metrics used to provide quantitative measures of image quality, allowing us to evaluate the effectiveness of the deblurring process. Mean PSNR in 29.1644 and mean SSIM in 0.7459 with average 4.6921 seconds deblurring time are achieved in this project. The blurry pixels are sharper in the output of GAN model shows a good image restoration effect in real world applications.",
        "tags": [
            "Deblurring",
            "GAN"
        ]
    },
    {
        "id": "114",
        "title": "Learning Radiance Fields from a Single Snapshot Compressive Image",
        "author": [
            "Yunhao Li",
            "Xiang Liu",
            "Xiaodong Wang",
            "Xin Yuan",
            "Peidong Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19483",
        "abstract": "In this paper, we explore the potential of Snapshot Compressive Imaging (SCI) technique for recovering the underlying 3D scene structure from a single temporal compressed image. SCI is a cost-effective method that enables the recording of high-dimensional data, such as hyperspectral or temporal information, into a single image using low-cost 2D imaging sensors. To achieve this, a series of specially designed 2D masks are usually employed, reducing storage and transmission requirements and offering potential privacy protection. Inspired by this, we take one step further to recover the encoded 3D scene information leveraging powerful 3D scene representation capabilities of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we formulate the physical imaging process of SCI as part of the training of NeRF, allowing us to exploit its impressive performance in capturing complex scene structures. In addition, we further integrate the popular 3D Gaussian Splatting (3DGS) framework and propose SCISplat to improve 3D scene reconstruction quality and training/rendering speed by explicitly optimizing point clouds into 3D Gaussian representations. To assess the effectiveness of our method, we conduct extensive evaluations using both synthetic data and real data captured by our SCI system. Experimental results demonstrate that our proposed approach surpasses the state-of-the-art methods in terms of image reconstruction and novel view synthesis. Moreover, our method also exhibits the ability to render high frame-rate multi-view consistent images in real time by leveraging SCI and the rendering capabilities of 3DGS. Codes will be available at: https://github.com/WU- CVGL/SCISplat.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "115",
        "title": "RAIN: Real-time Animation of Infinite Video Stream",
        "author": [
            "Zhilei Shu",
            "Ruili Feng",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19489",
        "abstract": "Live animation has gained immense popularity for enhancing online engagement, yet achieving high-quality, real-time, and stable animation with diffusion models remains challenging, especially on consumer-grade GPUs. Existing methods struggle with generating long, consistent video streams efficiently, often being limited by latency issues and degraded visual quality over extended periods. In this paper, we introduce RAIN, a pipeline solution capable of animating infinite video streams in real-time with low latency using a single RTX 4090 GPU. The core idea of RAIN is to efficiently compute frame-token attention across different noise levels and long time-intervals while simultaneously denoising a significantly larger number of frame-tokens than previous stream-based methods. This design allows RAIN to generate video frames with much shorter latency and faster speed, while maintaining long-range attention over extended video streams, resulting in enhanced continuity and consistency. Consequently, a Stable Diffusion model fine-tuned with RAIN in just a few epochs can produce video streams in real-time and low latency without much compromise in quality or consistency, up to infinite long. Despite its advanced capabilities, the RAIN only introduces a few additional 1D attention blocks, imposing minimal additional burden. Experiments in benchmark datasets and generating super-long videos demonstrating that RAIN can animate characters in real-time with much better quality, accuracy, and consistency than competitors while costing less latency. All code and models will be made publicly available.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "116",
        "title": "Retrieval-augmented Generation for GenAI-enabled Semantic Communications",
        "author": [
            "Shunpu Tang",
            "Ruichen Zhang",
            "Yuxuan Yan",
            "Qianqian Yang",
            "Dusit Niyato",
            "Xianbin Wang",
            "Shiwen Mao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19494",
        "abstract": "Semantic communication (SemCom) is an emerging paradigm aiming at transmitting only task-relevant semantic information to the receiver, which can significantly improve communication efficiency. Recent advancements in generative artificial intelligence (GenAI) have empowered GenAI-enabled SemCom (GenSemCom) to further expand its potential in various applications. However, current GenSemCom systems still face challenges such as semantic inconsistency, limited adaptability to diverse tasks and dynamic environments, and the inability to leverage insights from past transmission. Motivated by the success of retrieval-augmented generation (RAG) in the domain of GenAI, this paper explores the integration of RAG in GenSemCom systems. Specifically, we first provide a comprehensive review of existing GenSemCom systems and the fundamentals of RAG techniques. We then discuss how RAG can be integrated into GenSemCom. Following this, we conduct a case study on semantic image transmission using an RAG-enabled diffusion-based SemCom system, demonstrating the effectiveness of the proposed integration. Finally, we outline future directions for advancing RAG-enabled GenSemCom systems.",
        "tags": [
            "Diffusion",
            "RAG"
        ]
    },
    {
        "id": "117",
        "title": "Casevo: A Cognitive Agents and Social Evolution Simulator",
        "author": [
            "Zexun Jiang",
            "Yafang Shi",
            "Maoxu Li",
            "Hongjiang Xiao",
            "Yunxiao Qin",
            "Qinglan Wei",
            "Ye Wang",
            "Yuan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19498",
        "abstract": "In this paper, we introduce a multi-agent simulation framework Casevo (Cognitive Agents and Social Evolution Simulator), that integrates large language models (LLMs) to simulate complex social phenomena and decision-making processes. Casevo is designed as a discrete-event simulator driven by agents with features such as Chain of Thoughts (CoT), Retrieval-Augmented Generation (RAG), and Customizable Memory Mechanism. Casevo enables dynamic social modeling, which can support various scenarios such as social network analysis, public opinion dynamics, and behavior prediction in complex social systems. To demonstrate the effectiveness of Casevo, we utilize one of the U.S. 2020 midterm election TV debates as a simulation example. Our results show that Casevo facilitates more realistic and flexible agent interactions, improving the quality of dynamic social phenomena simulation. This work contributes to the field by providing a robust system for studying large-scale, high-fidelity social behaviors with advanced LLM-driven agents, expanding the capabilities of traditional agent-based modeling (ABM). The open-source code repository address of casevo is https://github.com/rgCASS/casevo.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "118",
        "title": "RobotDiffuse: Motion Planning for Redundant Manipulator based on Diffusion Model",
        "author": [
            "Xiaohan Zhang",
            "Xudong Mou",
            "Rui Wang",
            "Tianyu Wo",
            "Ningbo Gu",
            "Tiejun Wang",
            "Cangbai Xu",
            "Xudong Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19500",
        "abstract": "Redundant manipulators, with their higher Degrees of Freedom (DOFs), offer enhanced kinematic performance and versatility, making them suitable for applications like manufacturing, surgical robotics, and human-robot collaboration. However, motion planning for these manipulators is challenging due to increased DOFs and complex, dynamic environments. While traditional motion planning algorithms struggle with high-dimensional spaces, deep learning-based methods often face instability and inefficiency in complex tasks. This paper introduces RobotDiffuse, a diffusion model-based approach for motion planning in redundant manipulators. By integrating physical constraints with a point cloud encoder and replacing the U-Net structure with an encoder-only transformer, RobotDiffuse improves the model's ability to capture temporal dependencies and generate smoother, more coherent motion plans. We validate the approach using a complex simulator, and release a new dataset with 35M robot poses and 0.14M obstacle avoidance scenarios. Experimental results demonstrate the effectiveness of RobotDiffuse and the promise of diffusion models for motion planning tasks. The code can be accessed at https://github.com/ACRoboT-buaa/RobotDiffuse.",
        "tags": [
            "Diffusion",
            "Robot",
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "119",
        "title": "DrivingWorld: ConstructingWorld Model for Autonomous Driving via Video GPT",
        "author": [
            "Xiaotao Hu",
            "Wei Yin",
            "Mingkai Jia",
            "Junyuan Deng",
            "Xiaoyang Guo",
            "Qian Zhang",
            "Xiaoxiao Long",
            "Ping Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19505",
        "abstract": "Recent successes in autoregressive (AR) generation models, such as the GPT series in natural language processing, have motivated efforts to replicate this success in visual tasks. Some works attempt to extend this approach to autonomous driving by building video-based world models capable of generating realistic future video sequences and predicting ego states. However, prior works tend to produce unsatisfactory results, as the classic GPT framework is designed to handle 1D contextual information, such as text, and lacks the inherent ability to model the spatial and temporal dynamics essential for video generation. In this paper, we present DrivingWorld, a GPT-style world model for autonomous driving, featuring several spatial-temporal fusion mechanisms. This design enables effective modeling of both spatial and temporal dynamics, facilitating high-fidelity, long-duration video generation. Specifically, we propose a next-state prediction strategy to model temporal coherence between consecutive frames and apply a next-token prediction strategy to capture spatial information within each frame. To further enhance generalization ability, we propose a novel masking strategy and reweighting strategy for token prediction to mitigate long-term drifting issues and enable precise control. Our work demonstrates the ability to produce high-fidelity and consistent video clips of over 40 seconds in duration, which is over 2 times longer than state-of-the-art driving world models. Experiments show that, in contrast to prior works, our method achieves superior visual quality and significantly more accurate controllable future video generation. Our code is available at https://github.com/YvanYin/DrivingWorld.",
        "tags": [
            "GPT",
            "Video Generation"
        ]
    },
    {
        "id": "120",
        "title": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models",
        "author": [
            "Shiyao Li",
            "Yingchun Hu",
            "Xuefei Ning",
            "Xihui Liu",
            "Ke Hong",
            "Xiaotao Jia",
            "Xiuhong Li",
            "Yaqi Yan",
            "Pei Ran",
            "Guohao Dai",
            "Shengen Yan",
            "Huazhong Yang",
            "Yu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19509",
        "abstract": "Vision-Language Models (VLMs) have enabled a variety of real-world applications. The large parameter size of VLMs brings large memory and computation overhead which poses significant challenges for deployment. Post-Training Quantization (PTQ) is an effective technique to reduce the memory and computation overhead. Existing PTQ methods mainly focus on large language models (LLMs), without considering the differences across other modalities. In this paper, we discover that there is a significant difference in sensitivity between language and vision tokens in large VLMs. Therefore, treating tokens from different modalities equally, as in existing PTQ methods, may over-emphasize the insensitive modalities, leading to significant accuracy loss. To deal with the above issue, we propose a simple yet effective method, Modality-Balanced Quantization (MBQ), for large VLMs. Specifically, MBQ incorporates the different sensitivities across modalities during the calibration process to minimize the reconstruction loss for better quantization parameters. Extensive experiments show that MBQ can significantly improve task accuracy by up to 4.4% and 11.6% under W3 and W4A8 quantization for 7B to 70B VLMs, compared to SOTA baselines. Additionally, we implement a W3 GPU kernel that fuses the dequantization and GEMV operators, achieving a 1.4x speedup on LLaVA-onevision-7B on the RTX 4090. The code is available at https://github.com/thu-nics/MBQ.",
        "tags": [
            "LLMs",
            "LLaVA",
            "Large Language Models"
        ]
    },
    {
        "id": "121",
        "title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging",
        "author": [
            "Hua Farn",
            "Hsuan Su",
            "Shachi H Kumar",
            "Saurav Sahay",
            "Shang-Tse Chen",
            "Hung-yi Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19512",
        "abstract": "Fine-tuning large language models (LLMs) for downstream tasks is a widely adopted approach, but it often leads to safety degradation in safety-aligned LLMs. Currently, many solutions address this issue by incorporating additional safety data, which can be impractical in many cases. In this paper, we address the question: How can we improve downstream task performance while preserving safety in LLMs without relying on additional safety data? We propose a simple and effective method that maintains the inherent safety of LLMs while enhancing their downstream task performance: merging the weights of pre- and post-fine-tuned safety-aligned models. Experimental results across various downstream tasks, models, and merging methods demonstrate that this approach effectively mitigates safety degradation while improving downstream task performance, offering a practical solution for adapting safety-aligned LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "122",
        "title": "Confidence v.s. Critique: A Decomposition of Self-Correction Capability for LLMs",
        "author": [
            "Zhe Yang",
            "Yichang Zhang",
            "Yudong Wang",
            "Ziyao Xu",
            "Junyang Lin",
            "Zhifang Sui"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19513",
        "abstract": "Large Language Models (LLMs) can correct their self-generated responses, but a decline in accuracy after self-correction is also witnessed. To have a deeper understanding of self-correction, we endeavor to decompose, evaluate, and analyze the self-correction behaviors of LLMs. By enumerating and analyzing answer correctness before and after self-correction, we decompose the self-correction capability into confidence (being confident to correct answers) and critique (turning wrong answers to correct) capabilities, and propose two metrics from a probabilistic perspective to measure these 2 capabilities, along with another metric for overall self-correction capability evaluation. Based on our decomposition and evaluation metrics, we conduct extensive experiments and draw some empirical conclusions. For example, we find different models can exhibit distinct behaviors: some models are confident while others are more critical. We also find the trade-off between the two capabilities (i.e. improving one can lead to a decline in the other) when manipulating model self-correction behavior by prompts or in-context learning. Further, we find a simple yet efficient strategy to improve self-correction capability by transforming Supervision Fine-Tuning (SFT) data format, and our strategy outperforms vanilla SFT in both capabilities and achieves much higher accuracy after self-correction. Our code will be publicly available on GitHub.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "123",
        "title": "Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from Sparse Uncalibrated Images",
        "author": [
            "Xudong Cai",
            "Yongcai Wang",
            "Zhaoxin Fan",
            "Deng Haoran",
            "Shuo Wang",
            "Wanting Li",
            "Deying Li",
            "Lun Luo",
            "Minhang Wang",
            "Jintao Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19518",
        "abstract": "Photo-realistic scene reconstruction from sparse-view, uncalibrated images is highly required in practice. Although some successes have been made, existing methods are either Sparse-View but require accurate camera parameters (i.e., intrinsic and extrinsic), or SfM-free but need densely captured images. To combine the advantages of both methods while addressing their respective weaknesses, we propose Dust to Tower (D2T), an accurate and efficient coarse-to-fine framework to optimize 3DGS and image poses simultaneously from sparse and uncalibrated images. Our key idea is to first construct a coarse model efficiently and subsequently refine it using warped and inpainted images at novel viewpoints. To do this, we first introduce a Coarse Construction Module (CCM) which exploits a fast Multi-View Stereo model to initialize a 3D Gaussian Splatting (3DGS) and recover initial camera poses. To refine the 3D model at novel viewpoints, we propose a Confidence Aware Depth Alignment (CADA) module to refine the coarse depth maps by aligning their confident parts with estimated depths by a Mono-depth model. Then, a Warped Image-Guided Inpainting (WIGI) module is proposed to warp the training images to novel viewpoints by the refined depth maps, and inpainting is applied to fulfill the ``holes\" in the warped images caused by view-direction changes, providing high-quality supervision to further optimize the 3D model and the camera poses. Extensive experiments and ablation studies demonstrate the validity of D2T and its design choices, achieving state-of-the-art performance in both tasks of novel view synthesis and pose estimation while keeping high efficiency. Codes will be publicly available.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Inpainting",
            "Pose Estimation"
        ]
    },
    {
        "id": "124",
        "title": "L\\'{e}vy Score Function and Score-Based Particle Algorithm for Nonlinear L\\'{e}vy--Fokker--Planck Equations",
        "author": [
            "Yuanfei Huang",
            "Chengyu Liu",
            "Xiang Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19520",
        "abstract": "The score function for the diffusion process, also known as the gradient of the log-density, is a basic concept to characterize the probability flow with important applications in the score-based diffusion generative modelling and the simulation of It stochastic differential equations. However, neither the probability flow nor the corresponding score function for the diffusion-jump process are known. This paper delivers mathematical derivation, numerical algorithm, and error analysis focusing on the corresponding score function in non-Gaussian systems with jumps and discontinuities represented by the nonlinear Lvy--Fokker--Planck equations. We propose the Lvy score function for such stochastic equations, which features a nonlocal double-integral term, and we develop its training algorithm by minimizing the proposed loss function from samples. Based on the equivalence of the probability flow with deterministic dynamics, we develop a self-consistent score-based transport particle algorithm to sample the interactive Lvy stochastic process at discrete time grid points. We provide error bound for the Kullback--Leibler divergence between the numerical and true probability density functions by overcoming the nonlocal challenges in the Lvy score. The full error analysis with the Monte Carlo error and the time discretization error is furthermore established. To show the usefulness and efficiency of our approach, numerical examples from applications in biology and finance are tested.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "125",
        "title": "Attribution for Enhanced Explanation with Transferable Adversarial eXploration",
        "author": [
            "Zhiyu Zhu",
            "Jiayu Zhang",
            "Zhibo Jin",
            "Huaming Chen",
            "Jianlong Zhou",
            "Fang Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19523",
        "abstract": "The interpretability of deep neural networks is crucial for understanding model decisions in various applications, including computer vision. AttEXplore++, an advanced framework built upon AttEXplore, enhances attribution by incorporating transferable adversarial attack methods such as MIG and GRA, significantly improving the accuracy and robustness of model explanations. We conduct extensive experiments on five models, including CNNs (Inception-v3, ResNet-50, VGG16) and vision transformers (MaxViT-T, ViT-B/16), using the ImageNet dataset. Our method achieves an average performance improvement of 7.57\\% over AttEXplore and 32.62\\% compared to other state-of-the-art interpretability algorithms. Using insertion and deletion scores as evaluation metrics, we show that adversarial transferability plays a vital role in enhancing attribution results. Furthermore, we explore the impact of randomness, perturbation rate, noise amplitude, and diversity probability on attribution performance, demonstrating that AttEXplore++ provides more stable and reliable explanations across various models. We release our code at: https://anonymous.4open.science/r/ATTEXPLOREP-8435/",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "126",
        "title": "Is Your Text-to-Image Model Robust to Caption Noise?",
        "author": [
            "Weichen Yu",
            "Ziyan Yang",
            "Shanchuan Lin",
            "Qi Zhao",
            "Jianyi Wang",
            "Liangke Gui",
            "Matt Fredrikson",
            "Lu Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19531",
        "abstract": "In text-to-image (T2I) generation, a prevalent training technique involves utilizing Vision Language Models (VLMs) for image re-captioning. Even though VLMs are known to exhibit hallucination, generating descriptive content that deviates from the visual reality, the ramifications of such caption hallucinations on T2I generation performance remain under-explored. Through our empirical investigation, we first establish a comprehensive dataset comprising VLM-generated captions, and then systematically analyze how caption hallucination influences generation outcomes. Our findings reveal that (1) the disparities in caption quality persistently impact model outputs during fine-tuning. (2) VLMs confidence scores serve as reliable indicators for detecting and characterizing noise-related patterns in the data distribution. (3) even subtle variations in caption fidelity have significant effects on the quality of learned representations. These findings collectively emphasize the profound impact of caption quality on model performance and highlight the need for more sophisticated robust training algorithm in T2I. In response to these observations, we propose a approach leveraging VLM confidence score to mitigate caption noise, thereby enhancing the robustness of T2I models against hallucination in caption.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "127",
        "title": "P3S-Diffusion:A Selective Subject-driven Generation Framework via Point Supervision",
        "author": [
            "Junjie Hu",
            "Shuyong Gao",
            "Lingyi Hong",
            "Qishan Wang",
            "Yuzhou Zhao",
            "Yan Wang",
            "Wenqiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19533",
        "abstract": "Recent research in subject-driven generation increasingly emphasizes the importance of selective subject features. Nevertheless, accurately selecting the content in a given reference image still poses challenges, especially when selecting the similar subjects in an image (e.g., two different dogs). Some methods attempt to use text prompts or pixel masks to isolate specific elements. However, text prompts often fall short in precisely describing specific content, and pixel masks are often expensive. To address this, we introduce P3S-Diffusion, a novel architecture designed for context-selected subject-driven generation via point supervision. P3S-Diffusion leverages minimal cost label (e.g., points) to generate subject-driven images. During fine-tuning, it can generate an expanded base mask from these points, obviating the need for additional segmentation models. The mask is employed for inpainting and aligning with subject representation. The P3S-Diffusion preserves fine features of the subjects through Multi-layers Condition Injection. Enhanced by the Attention Consistency Loss for improved training, extensive experiments demonstrate its excellent feature preservation and image generation capabilities.",
        "tags": [
            "Diffusion",
            "Inpainting",
            "Segmentation"
        ]
    },
    {
        "id": "128",
        "title": "StyleRWKV: High-Quality and High-Efficiency Style Transfer with RWKV-like Architecture",
        "author": [
            "Miaomiao Dai",
            "Qianyu Zhou",
            "Lizhuang Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19535",
        "abstract": "Style transfer aims to generate a new image preserving the content but with the artistic representation of the style source. Most of the existing methods are based on Transformers or diffusion models, however, they suffer from quadratic computational complexity and high inference time. RWKV, as an emerging deep sequence models, has shown immense potential for long-context sequence modeling in NLP tasks. In this work, we present a novel framework StyleRWKV, to achieve high-quality style transfer with limited memory usage and linear time complexity. Specifically, we propose a Recurrent WKV (Re-WKV) attention mechanism, which incorporates bidirectional attention to establish a global receptive field. Additionally, we develop a Deformable Shifting (Deform-Shifting) layer that introduces learnable offsets to the sampling grid of the convolution kernel, allowing tokens to shift flexibly and adaptively from the region of interest, thereby enhancing the model's ability to capture local dependencies. Finally, we propose a Skip Scanning (S-Scanning) method that effectively establishes global contextual dependencies. Extensive experiments with analysis including qualitative and quantitative evaluations demonstrate that our approach outperforms state-of-the-art methods in terms of stylization quality, model complexity, and inference efficiency.",
        "tags": [
            "Diffusion",
            "RWKV",
            "Style Transfer"
        ]
    },
    {
        "id": "129",
        "title": "Diverse Rare Sample Generation with Pretrained GANs",
        "author": [
            "Subeen Lee",
            "Jiyeon Han",
            "Soyeon Kim",
            "Jaesik Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19543",
        "abstract": "Deep generative models are proficient in generating realistic data but struggle with producing rare samples in low density regions due to their scarcity of training datasets and the mode collapse problem. While recent methods aim to improve the fidelity of generated samples, they often reduce diversity and coverage by ignoring rare and novel samples. This study proposes a novel approach for generating diverse rare samples from high-resolution image datasets with pretrained GANs. Our method employs gradient-based optimization of latent vectors within a multi-objective framework and utilizes normalizing flows for density estimation on the feature space. This enables the generation of diverse rare images, with controllable parameters for rarity, diversity, and similarity to a reference image. We demonstrate the effectiveness of our approach both qualitatively and quantitatively across various datasets and GANs without retraining or fine-tuning the pretrained GANs.",
        "tags": [
            "Normalizing Flows"
        ]
    },
    {
        "id": "130",
        "title": "Performance Evaluation of IoT LoRa Networks on Mars Through ns-3 Simulations",
        "author": [
            "Manuele Favero",
            "Alessandro Canova",
            "Marco Giordani",
            "Michele Zorzi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19549",
        "abstract": "In recent years, there has been a significant surge of interest in Mars exploration, driven by the planet's potential for human settlement and its proximity to Earth. In this paper, we explore the performance of the LoRaWAN technology on Mars, to study whether commercial off-the-shelf IoT products, designed and developed on Earth, can be deployed on the Martian surface. We use the ns-3 simulator to model various environmental conditions, primarily focusing on the Free Space Path Loss (FSPL) and the impact of Martian dust storms. Simulation results are given with respect to Earth, as a function of the distance, packet size, offered traffic, and the impact of Mars' atmospheric perturbations. We show that LoRaWAN can be a viable communication solution on Mars, although the performance is heavily affected by the extreme Martian environment over long distances.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "131",
        "title": "Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied Instruction Following",
        "author": [
            "Yuxiao Yang",
            "Shenao Zhang",
            "Zhihan Liu",
            "Huaxiu Yao",
            "Zhaoran Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19562",
        "abstract": "This work focuses on building a task planner for Embodied Instruction Following (EIF) using Large Language Models (LLMs). Previous works typically train a planner to imitate expert trajectories, treating this as a supervised task. While these methods achieve competitive performance, they often lack sufficient robustness. When a suboptimal action is taken, the planner may encounter an out-of-distribution state, which can lead to task failure. In contrast, we frame the task as a Partially Observable Markov Decision Process (POMDP) and aim to develop a robust planner under a few-shot assumption. Thus, we propose a closed-loop planner with an adaptation module and a novel hindsight method, aiming to use as much information as possible to assist the planner. Our experiments on the ALFRED dataset indicate that our planner achieves competitive performance under a few-shot assumption. For the first time, our few-shot agent's performance approaches and even surpasses that of the full-shot supervised agent.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "132",
        "title": "DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction",
        "author": [
            "Kai Xu",
            "Tze Ho Elden Tse",
            "Jizong Peng",
            "Angela Yao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19584",
        "abstract": "We propose a novel framework for scene decomposition and static background reconstruction from everyday videos. By integrating the trained motion masks and modeling the static scene as Gaussian splats with dynamics-aware optimization, our method achieves more accurate background reconstruction results than previous works. Our proposed method is termed DAS3R, an abbreviation for Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction. Compared to existing methods, DAS3R is more robust in complex motion scenarios, capable of handling videos where dynamic objects occupy a significant portion of the scene, and does not require camera pose inputs or point cloud data from SLAM-based methods. We compared DAS3R against recent distractor-free approaches on the DAVIS and Sintel datasets; DAS3R demonstrates enhanced performance and robustness with a margin of more than 2 dB in PSNR. The project's webpage can be accessed via \\url{https://kai422.github.io/DAS3R/}",
        "tags": [
            "Gaussian Splatting",
            "SLAM"
        ]
    },
    {
        "id": "133",
        "title": "SocRATES: Towards Automated Scenario-based Testing of Social Navigation Algorithms",
        "author": [
            "Shashank Rao Marpally",
            "Pranav Goyal",
            "Harold Soh"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19595",
        "abstract": "Current social navigation methods and benchmarks primarily focus on proxemics and task efficiency. While these factors are important, qualitative aspects such as perceptions of a robot's social competence are equally crucial for successful adoption and integration into human environments. We propose a more comprehensive evaluation of social navigation through scenario-based testing, where specific human-robot interaction scenarios can reveal key robot behaviors. However, creating such scenarios is often labor-intensive and complex. In this work, we address this challenge by introducing a pipeline that automates the generation of context-, and location-appropriate social navigation scenarios, ready for simulation. Our pipeline transforms simple scenario metadata into detailed textual scenarios, infers pedestrian and robot trajectories, and simulates pedestrian behaviors, which enables more controlled evaluation. We leverage the social reasoning and code-generation capabilities of Large Language Models (LLMs) to streamline scenario generation and translation. Our experiments show that our pipeline produces realistic scenarios and significantly improves scenario translation over naive LLM prompting. Additionally, we present initial feedback from a usability study with social navigation experts and a case-study demonstrating a scenario-based evaluation of three navigation algorithms.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Robot"
        ]
    },
    {
        "id": "134",
        "title": "Machine Generated Product Advertisements: Benchmarking LLMs Against Human Performance",
        "author": [
            "Sanjukta Ghosh"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19610",
        "abstract": "This study compares the performance of AI-generated and human-written product descriptions using a multifaceted evaluation model. We analyze descriptions for 100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4) with and without sample descriptions, against human-written descriptions. Our evaluation metrics include sentiment, readability, persuasiveness, Search Engine Optimization(SEO), clarity, emotional appeal, and call-to-action effectiveness. The results indicate that ChatGPT 4 performs the best. In contrast, other models demonstrate significant shortcomings, producing incoherent and illogical output that lacks logical structure and contextual relevance. These models struggle to maintain focus on the product being described, resulting in disjointed sentences that do not convey meaningful information. This research provides insights into the current capabilities and limitations of AI in the creation of content for e-Commerce.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "LLaMA"
        ]
    },
    {
        "id": "135",
        "title": "Gradient Weight-normalized Low-rank Projection for Efficient LLM Training",
        "author": [
            "Jia-Hong Huang",
            "Yixian Shen",
            "Hongyi Zhu",
            "Stevan Rudinac",
            "Evangelos Kanoulas"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19616",
        "abstract": "Large Language Models (LLMs) have shown remarkable performance across various tasks, but the escalating demands on computational resources pose significant challenges, particularly in the extensive utilization of full fine-tuning for downstream tasks. To address this, parameter-efficient fine-tuning (PEFT) methods have been developed, but they often underperform compared to full fine-tuning and struggle with memory efficiency. In this work, we introduce Gradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach that enhances both parameter and memory efficiency while maintaining comparable performance to full fine-tuning. GradNormLoRP normalizes the weight matrix to improve gradient conditioning, facilitating better convergence during optimization. Additionally, it applies low-rank approximations to the weight and gradient matrices, significantly reducing memory usage during training. Extensive experiments demonstrate that our 8-bit GradNormLoRP reduces optimizer memory usage by up to 89.5% and enables the pre-training of large LLMs, such as LLaMA 7B, on consumer-level GPUs like the NVIDIA RTX 4090, without additional inference costs. Moreover, GradNormLoRP outperforms existing low-rank methods in fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all GLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65, surpassing LoRA's score of 79.23. These results underscore GradNormLoRP as a promising alternative for efficient LLM pre-training and fine-tuning. Source code and Appendix: https://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "136",
        "title": "IMTP: Search-based Code Generation for In-memory Tensor Programs",
        "author": [
            "Yongwon Shin",
            "Dookyung Kang",
            "Hyojin Sung"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19630",
        "abstract": "Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for accelerating memory-intensive operations in modern applications, such as Large Language Models (LLMs). Despite its potential, current software stacks for DRAM-PIM face significant challenges, including reliance on hand-tuned libraries that hinder programmability, limited support for high-level abstractions, and the lack of systematic optimization frameworks. To address these limitations, we present IMTP, a search-based optimizing tensor compiler for UPMEM. Key features of IMTP include: (1) automated searches of the joint search space for host and kernel tensor programs, (2) PIM-aware optimizations for efficiently handling boundary conditions, and (3) improved search algorithms for the expanded search space of UPMEM systems. Our experimental results on UPMEM hardware demonstrate performance gains of up to 8.21x for various UPMEM benchmark kernels and 5.33x for GPT-J layers. To the best of our knowledge, IMTP is the first tensor compiler to provide fully automated, autotuning-integrated code generation support for a DRAM-PIM system. By bridging the gap between high-level tensor computation abstractions and low-level hardware-specific requirements, IMTP establishes a foundation for advancing DRAM-PIM programmability and enabling streamlined optimization.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "137",
        "title": "ReNeg: Learning Negative Embedding with Reward Guidance",
        "author": [
            "Xiaomin Li",
            "Yixuan Liu",
            "Takashi Isobe",
            "Xu Jia",
            "Qinpeng Cui",
            "Dong Zhou",
            "Dong Li",
            "You He",
            "Huchuan Lu",
            "Zhongdao Wang",
            "Emad Barsoum"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19637",
        "abstract": "In text-to-image (T2I) generation applications, negative embeddings have proven to be a simple yet effective approach for enhancing generation quality. Typically, these negative embeddings are derived from user-defined negative prompts, which, while being functional, are not necessarily optimal. In this paper, we introduce ReNeg, an end-to-end method designed to learn improved Negative embeddings guided by a Reward model. We employ a reward feedback learning framework and integrate classifier-free guidance (CFG) into the training process, which was previously utilized only during inference, thus enabling the effective learning of negative embeddings. We also propose two strategies for learning both global and per-sample negative embeddings. Extensive experiments show that the learned negative embedding significantly outperforms null-text and handcrafted counterparts, achieving substantial improvements in human preference alignment. Additionally, the negative embedding learned within the same text embedding space exhibits strong generalization capabilities. For example, using the same CLIP text encoder, the negative embedding learned on SD1.5 can be seamlessly transferred to text-to-image or even text-to-video models such as ControlNet, ZeroScope, and VideoCrafter2, resulting in consistent performance improvements across the board.",
        "tags": [
            "CLIP",
            "ControlNet",
            "Text-to-Image",
            "Text-to-Video"
        ]
    },
    {
        "id": "138",
        "title": "VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models",
        "author": [
            "Tao Wu",
            "Yong Zhang",
            "Xiaodong Cun",
            "Zhongang Qi",
            "Junfu Pu",
            "Huanzhang Dou",
            "Guangcong Zheng",
            "Ying Shan",
            "Xi Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19645",
        "abstract": "Zero-shot customized video generation has gained significant attention due to its substantial application potential. Existing methods rely on additional models to extract and inject reference subject features, assuming that the Video Diffusion Model (VDM) alone is insufficient for zero-shot customized video generation. However, these methods often struggle to maintain consistent subject appearance due to suboptimal feature extraction and injection techniques. In this paper, we reveal that VDM inherently possesses the force to extract and inject subject features. Departing from previous heuristic approaches, we introduce a novel framework that leverages VDM's inherent force to enable high-quality zero-shot customized video generation. Specifically, for feature extraction, we directly input reference images into VDM and use its intrinsic feature extraction process, which not only provides fine-grained features but also significantly aligns with VDM's pre-trained knowledge. For feature injection, we devise an innovative bidirectional interaction between subject features and generated content through spatial self-attention within VDM, ensuring that VDM has better subject fidelity while maintaining the diversity of the generated http://video.Experiments on both customized human and object video generation validate the effectiveness of our framework.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "139",
        "title": "Chimera: A Block-Based Neural Architecture Search Framework for Event-Based Object Detection",
        "author": [
            "Diego A. Silva",
            "Ahmed Elsheikh",
            "Kamilya Smagulova",
            "Mohammed E. Fouda",
            "Ahmed M. Eltawil"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19646",
        "abstract": "Event-based cameras are sensors that simulate the human eye, offering advantages such as high-speed robustness and low power consumption. Established Deep Learning techniques have shown effectiveness in processing event data. Chimera is a Block-Based Neural Architecture Search (NAS) framework specifically designed for Event-Based Object Detection, aiming to create a systematic approach for adapting RGB-domain processing methods to the event domain. The Chimera design space is constructed from various macroblocks, including Attention blocks, Convolutions, State Space Models, and MLP-mixer-based architectures, which provide a valuable trade-off between local and global processing capabilities, as well as varying levels of complexity. The results on the PErson Detection in Robotics (PEDRo) dataset demonstrated performance levels comparable to leading state-of-the-art models, alongside an average parameter reduction of 1.6 times.",
        "tags": [
            "Detection",
            "Robotics",
            "State Space Models"
        ]
    },
    {
        "id": "140",
        "title": "Toward Modality Gap: Vision Prototype Learning for Weakly-supervised Semantic Segmentation with CLIP",
        "author": [
            "Zhongxing Xu",
            "Feilong Tang",
            "Zhe Chen",
            "Yingxue Su",
            "Zhiyi Zhao",
            "Ge Zhang",
            "Jionglong Su",
            "Zongyuan Ge"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19650",
        "abstract": "The application of Contrastive Language-Image Pre-training (CLIP) in Weakly Supervised Semantic Segmentation (WSSS) research powerful cross-modal semantic understanding capabilities. Existing methods attempt to optimize input text prompts for improved alignment of images and text, by finely adjusting text prototypes to facilitate semantic matching. Nevertheless, given the modality gap between text and vision spaces, the text prototypes employed by these methods have not effectively established a close correspondence with pixel-level vision features. In this work, our theoretical analysis indicates that the inherent modality gap results in misalignment of text and region features, and that this gap cannot be sufficiently reduced by minimizing contrast loss in CLIP. To mitigate the impact of the modality gap, we propose a Vision Prototype Learning (VPL) framework, by introducing more representative vision prototypes. The core of this framework is to learn class-specific vision prototypes in vision space with the help of text prototypes, for capturing high-quality localization maps. Moreover, we propose a regional semantic contrast module that contrasts regions embedding with corresponding prototypes, leading to more comprehensive and robust feature learning. Experimental results show that our proposed framework achieves state-of-the-art performance on two benchmark datasets.",
        "tags": [
            "CLIP",
            "Segmentation"
        ]
    },
    {
        "id": "141",
        "title": "CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs",
        "author": [
            "Siyu Wang",
            "Cailian Chen",
            "Xinyi Le",
            "Qimin Xu",
            "Lei Xu",
            "Yanzhou Zhang",
            "Jie Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19663",
        "abstract": "Computer-aided design (CAD) significantly enhances the efficiency, accuracy, and innovation of design processes by enabling precise 2D and 3D modeling, extensive analysis, and optimization. Existing methods for creating CAD models rely on latent vectors or point clouds, which are difficult to obtain and costly to store. Recent advances in Multimodal Large Language Models (MLLMs) have inspired researchers to use natural language instructions and images for CAD model construction. However, these models still struggle with inferring accurate 3D spatial location and orientation, leading to inaccuracies in determining the spatial 3D starting points and extrusion directions for constructing geometries. This work introduces CAD-GPT, a CAD synthesis method with spatial reasoning-enhanced MLLM that takes either a single image or a textual description as input. To achieve precise spatial inference, our approach introduces a 3D Modeling Spatial Mechanism. This method maps 3D spatial positions and 3D sketch plane rotation angles into a 1D linguistic feature space using a specialized spatial unfolding mechanism, while discretizing 2D sketch coordinates into an appropriate planar space to enable precise determination of spatial starting position, sketch orientation, and 2D sketch coordinate translations. Extensive experiments demonstrate that CAD-GPT consistently outperforms existing state-of-the-art methods in CAD model synthesis, both quantitatively and qualitatively.",
        "tags": [
            "3D",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "142",
        "title": "Optimizing Local-Global Dependencies for Accurate 3D Human Pose Estimation",
        "author": [
            "Guangsheng Xu",
            "Guoyi Zhang",
            "Lejia Ye",
            "Shuwei Gan",
            "Xiaohu Zhang",
            "Xia Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19676",
        "abstract": "Transformer-based methods have recently achieved significant success in 3D human pose estimation, owing to their strong ability to model long-range dependencies. However, relying solely on the global attention mechanism is insufficient for capturing the fine-grained local details, which are crucial for accurate pose estimation. To address this, we propose SSR-STF, a dual-stream model that effectively integrates local features with global dependencies to enhance 3D human pose estimation. Specifically, we introduce SSRFormer, a simple yet effective module that employs the skeleton selective refine attention (SSRA) mechanism to capture fine-grained local dependencies in human pose sequences, complementing the global dependencies modeled by the Transformer. By adaptively fusing these two feature streams, SSR-STF can better learn the underlying structure of human poses, overcoming the limitations of traditional methods in local feature extraction. Extensive experiments on the Human3.6M and MPI-INF-3DHP datasets demonstrate that SSR-STF achieves state-of-the-art performance, with P1 errors of 37.4 mm and 13.2 mm respectively, outperforming existing methods in both accuracy and generalization. Furthermore, the motion representations learned by our model prove effective in downstream tasks such as human mesh recovery. Codes are available at https://github.com/poker-xu/SSR-STF.",
        "tags": [
            "3D",
            "Pose Estimation",
            "Transformer"
        ]
    },
    {
        "id": "143",
        "title": "Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free, Adaptive, Universal Prompt Optimization Framework",
        "author": [
            "Jiang Liu",
            "Bolin Li",
            "Haoyuan Li",
            "Tianwei Lin",
            "Wenqiao Zhang",
            "Tao Zhong",
            "Zhelun Yu",
            "Jinghao Wei",
            "Hao Cheng",
            "Hao Jiang",
            "Zheqi Lv",
            "Juncheng Li",
            "Siliang Tang",
            "Yueting Zhuang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19684",
        "abstract": "Efficient multimodal large language models (EMLLMs), in contrast to multimodal large language models (MLLMs), reduce model size and computational costs and are often deployed on resource-constrained devices. However, due to data privacy concerns, existing open-source EMLLMs rarely have access to private domain-specific data during the pre-training process, making them difficult to directly apply in device-specific domains, such as certain business scenarios. To address this weakness, this paper focuses on the efficient adaptation of EMLLMs to private domains, specifically in two areas: 1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning. Specifically, we propose a tun\\textbf{\\underline{I}}ng-free, a\\textbf{\\underline{D}}aptiv\\textbf{\\underline{E}}, univers\\textbf{\\underline{AL}} \\textbf{\\underline{Prompt}} Optimization Framework, abbreviated as \\textit{\\textbf{\\ourmethod{}}} which consists of two stages: 1) Predefined Prompt, based on the reinforcement searching strategy, generate a prompt optimization strategy tree to acquire optimization priors; 2) Prompt Reflection initializes the prompt based on optimization priors, followed by self-reflection to further search and refine the prompt. By doing so, \\ourmethod{} elegantly generates the ``ideal prompts'' for processing private domain-specific data. Note that our method requires no parameter fine-tuning and only a small amount of data to quickly adapt to the data distribution of private data. Extensive experiments across multiple tasks demonstrate that our proposed \\ourmethod{} significantly improves both efficiency and performance compared to baselines.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "144",
        "title": "An Integrated Optimization and Deep Learning Pipeline for Predicting Live Birth Success in IVF Using Feature Optimization and Transformer-Based Models",
        "author": [
            "Arezoo Borji",
            "Hossam Haick",
            "Birgit Pohn",
            "Antonia Graf",
            "Jana Zakall",
            "S M Ragib Shahriar Islam",
            "Gernot Kronreif",
            "Daniel Kovatchki",
            "Heinz Strohmer",
            "Sepideh Hatamikia"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19696",
        "abstract": "In vitro fertilization (IVF) is a widely utilized assisted reproductive technology, yet predicting its success remains challenging due to the multifaceted interplay of clinical, demographic, and procedural factors. This study develops a robust artificial intelligence (AI) pipeline aimed at predicting live birth outcomes in IVF treatments. The pipeline uses anonymized data from 2010 to 2018, obtained from the Human Fertilization and Embryology Authority (HFEA). We evaluated the prediction performance of live birth success as a binary outcome (success/failure) by integrating different feature selection methods, such as principal component analysis (PCA) and particle swarm optimization (PSO), with different traditional machine learning-based classifiers including random forest (RF) and decision tree, as well as deep learning-based classifiers including custom transformer-based model and a tab transformer model with an attention mechanism. Our research demonstrated that the best performance was achieved by combining PSO for feature selection with the TabTransformer-based deep learning model, yielding an accuracy of 99.50% and an AUC of 99.96%, highlighting its significant performance to predict live births. This study establishes a highly accurate AI pipeline for predicting live birth outcomes in IVF, demonstrating its potential to enhance personalized fertility treatments.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "145",
        "title": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback",
        "author": [
            "Sijia Chen",
            "Baochun Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19707",
        "abstract": "Large language models (LLMs) have been routinely used to solve various tasks using step-by-step reasoning. However, the structure of intermediate reasoning steps, or thoughts, is rigid and unidirectional, such as chains, trees, or acyclic-directed graphs. Consequently, the resulting inflexible and forward-only reasoning may not address challenging tasks and fail when the LLM frequently gives false responses, i.e., ``hallucinations''. This paper proposes a new reasoning framework, called Thought Rollback (TR), allowing LLMs to adaptively build thought structure while maintaining effective reasoning toward problem-solving under ``hallucinations''. The core mechanism of TR is rolling back thoughts, which allows LLMs to perform error analysis on thoughts, and thus roll back to any previously mistaken thought for revision. Subsequently, by including such trial-and-error in the prompt to guide the LLM, each rollback leads to one more reliable reasoning path. Therefore, starting with a simple prompt without human annotations, LLM with TR adaptively and gradually explores thoughts for a correct solution. Comprehensive experiments on mathematical problems and multi-task reasoning demonstrate the state-of-the-art performance of TR in terms of problem-solving rate and interaction cost. For instance, the solving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH dataset.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "146",
        "title": "Text2Insight: Transform natural language text into insights seamlessly using multi-model architecture",
        "author": [
            "Pradeep Sain"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19718",
        "abstract": "The growing demand for dynamic, user-centric data analysis and visualization is evident across domains like healthcare, finance, and research. Traditional visualization tools often fail to meet individual user needs due to their static and predefined nature. To address this gap, Text2Insight is introduced as an innovative solution that delivers customized data analysis and visualizations based on user-defined natural language requirements. Leveraging a multi-model architecture, Text2Insight transforms user inputs into actionable insights and dynamic visualizations.\nThe methodology begins with analyzing the input dataset to extract structural details such as columns and values. A pre-trained Llama3 model converts the user's natural language query into an SQL query, which is further refined using a Named Entity Recognition (NER) model for accuracy. A chart predictor determines the most suitable visualization type, while the Llama3 model generates insights based on the SQL query's results. The output is a user-friendly and visually informative chart. To enhance analysis capabilities, the system integrates a question-answering model and a predictive model using the BERT framework. These models provide insights into historical data and predict future trends.\nPerformance evaluation of Text2Insight demonstrates its effectiveness, achieving high accuracy (99%), precision (100%), recall (99%), and F1-score (99%), with a BLEU score of 0.5. The question-answering model attained an accuracy of 89% and the predictive model achieved 70% accuracy. These results validate Text2Insight as a robust and viable solution for transforming natural language text into dynamic, user-specific data analysis and visualizations.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "147",
        "title": "Can Large Language Models Adapt to Other Agents In-Context?",
        "author": [
            "Matthew Riemer",
            "Zahra Ashktorab",
            "Djallel Bouneffouf",
            "Payel Das",
            "Miao Liu",
            "Justin D. Weisz",
            "Murray Campbell"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19726",
        "abstract": "As the research community aims to build better AI assistants that are more dynamic and personalized to the diversity of humans that they interact with, there is increased interest in evaluating the theory of mind capabilities of large language models (LLMs). Indeed, several recent studies suggest that LLM theory of mind capabilities are quite impressive, approximating human-level performance. Our paper aims to rebuke this narrative and argues instead that past studies were not directly measuring agent performance, potentially leading to findings that are illusory in nature as a result. We draw a strong distinction between what we call literal theory of mind i.e. measuring the agent's ability to predict the behavior of others and functional theory of mind i.e. adapting to agents in-context based on a rational response to predictions of their behavior. We find that top performing open source LLMs may display strong capabilities in literal theory of mind, depending on how they are prompted, but seem to struggle with functional theory of mind -- even when partner policies are exceedingly simple. Our work serves to highlight the double sided nature of inductive bias in LLMs when adapting to new situations. While this bias can lead to strong performance over limited horizons, it often hinders convergence to optimal long-term behavior.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "148",
        "title": "Generative Pretrained Embedding and Hierarchical Irregular Time Series Representation for Daily Living Activity Recognition",
        "author": [
            "Damien Bouchabou",
            "Sao Mai Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19732",
        "abstract": "Within the evolving landscape of smart homes, the precise recognition of daily living activities using ambient sensor data stands paramount. This paper not only aims to bolster existing algorithms by evaluating two distinct pretrained embeddings suited for ambient sensor activations but also introduces a novel hierarchical architecture. We delve into an architecture anchored on Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT design, and contrast it with the previously established state-of-the-art (SOTA) ELMo embeddings for ambient sensors. Our proposed hierarchical structure leverages the strengths of each pre-trained embedding, enabling the discernment of activity dependencies and sequence order, thereby enhancing classification precision. To further refine recognition, we incorporate into our proposed architecture an hour-of-the-day embedding. Empirical evaluations underscore the preeminence of the Transformer Decoder embedding in classification endeavors. Additionally, our innovative hierarchical design significantly bolsters the efficacy of both pre-trained embeddings, notably in capturing inter-activity nuances. The integration of temporal aspects subtly but distinctively augments classification, especially for time-sensitive activities. In conclusion, our GPT-inspired hierarchical approach, infused with temporal insights, outshines the SOTA ELMo benchmark.",
        "tags": [
            "GPT",
            "Transformer"
        ]
    },
    {
        "id": "149",
        "title": "\"Did my figure do justice to the answer?\" : Towards Multimodal Short Answer Grading with Feedback (MMSAF)",
        "author": [
            "Pritam Sil",
            "Bhaskaran Raman",
            "Pushpak Bhattacharyya"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19755",
        "abstract": "Personalized feedback plays a vital role in a student's learning process. While existing systems are adept at providing feedback over MCQ-based evaluation, this work focuses more on subjective and open-ended questions, which is similar to the problem of Automatic Short Answer Grading (ASAG) with feedback. Additionally, we introduce the Multimodal Short Answer grading with Feedback (MMSAF) problem over the traditional ASAG feedback problem to address the scenario where the student answer and reference answer might contain images. Moreover, we introduce the MMSAF dataset with 2197 data points along with an automated framework for generating such data sets. Our evaluations on existing LLMs over this dataset achieved an overall accuracy of 55\\% on Level of Correctness labels, 75\\% on Image Relevance labels and a score of 4.27 out of 5 in correctness level of LLM generated feedback as rated by experts. As per experts, Pixtral achieved a rating of above 4 out of all metrics, indicating that it is more aligned to human judgement, and that it is the best solution for assisting students.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "150",
        "title": "Generative Video Propagation",
        "author": [
            "Shaoteng Liu",
            "Tianyu Wang",
            "Jui-Hsien Wang",
            "Qing Liu",
            "Zhifei Zhang",
            "Joon-Young Lee",
            "Yijun Li",
            "Bei Yu",
            "Zhe Lin",
            "Soo Ye Kim",
            "Jiaya Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19761",
        "abstract": "Large-scale video generation models have the inherent ability to realistically model natural scenes. In this paper, we demonstrate that through a careful design of a generative video propagation framework, various video tasks can be addressed in a unified way by leveraging the generative power of such models. Specifically, our framework, GenProp, encodes the original video with a selective content encoder and propagates the changes made to the first frame using an image-to-video generation model. We propose a data generation scheme to cover multiple video tasks based on instance-level video segmentation datasets. Our model is trained by incorporating a mask prediction decoder head and optimizing a region-aware loss to aid the encoder to preserve the original content while the generation model propagates the modified region. This novel design opens up new possibilities: In editing scenarios, GenProp allows substantial changes to an object's shape; for insertion, the inserted objects can exhibit independent motion; for removal, GenProp effectively removes effects like shadows and reflections from the whole video; for tracking, GenProp is capable of tracking objects and their associated effects together. Experiment results demonstrate the leading performance of our model in various video tasks, and we further provide in-depth analyses of the proposed framework.",
        "tags": [
            "Segmentation",
            "Video Generation"
        ]
    },
    {
        "id": "151",
        "title": "Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via Multi-Turn Dialogue and Dual-Agent Integration",
        "author": [
            "Le Chen",
            "Bin Lei",
            "Dunzhi Zhou",
            "Pei-Hung Lin",
            "Chunhua Liao",
            "Caiwen Ding",
            "Ali Jannesari"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19770",
        "abstract": "Migrating Fortran code to C++ is a common task for many scientific computing teams, driven by the need to leverage modern programming paradigms, enhance cross-platform compatibility, and improve maintainability. Automating this translation process using large language models (LLMs) has shown promise, but the lack of high-quality, specialized datasets has hindered their effectiveness. In this paper, we address this challenge by introducing a novel multi-turn dialogue dataset, Fortran2CPP, specifically designed for Fortran-to-C++ code migration. Our dataset, significantly larger than existing alternatives, is generated using a unique LLM-driven, dual-agent pipeline incorporating iterative compilation, execution, and code repair to ensure high quality and functional correctness. To demonstrate the effectiveness of our dataset, we fine-tuned several open-weight LLMs on Fortran2CPP and evaluated their performance on two independent benchmarks. Fine-tuning on our dataset led to remarkable gains, with models achieving up to a 3.31x increase in CodeBLEU score and a 92\\% improvement in compilation success rate. This highlights the dataset's ability to enhance both the syntactic accuracy and compilability of the translated C++ code. Our dataset and model have been open-sourced and are available on our public GitHub repository\\footnote{\\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "152",
        "title": "Can AI Help with Your Personal Finances?",
        "author": [
            "Oudom Hean",
            "Utsha Saha",
            "Binita Saha"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19784",
        "abstract": "In recent years, Large Language Models (LLMs) have emerged as a transformative development in artificial intelligence (AI), drawing significant attention from industry and academia. Trained on vast datasets, these sophisticated AI systems exhibit impressive natural language processing and content generation capabilities. This paper explores the potential of LLMs to address key challenges in personal finance, focusing on the United States. We evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini, Anthropic's Claude, and Meta's Llama, to assess their effectiveness in providing accurate financial advice on topics such as mortgages, taxes, loans, and investments. Our findings show that while these models achieve an average accuracy rate of approximately 70%, they also display notable limitations in certain areas. Specifically, LLMs struggle to provide accurate responses for complex financial queries, with performance varying significantly across different topics. Despite these limitations, the analysis reveals notable improvements in newer versions of these models, highlighting their growing utility for individuals and financial advisors. As these AI systems continue to evolve, their potential for advancing AI-driven applications in personal finance becomes increasingly promising.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "153",
        "title": "MVTamperBench: Evaluating Robustness of Vision-Language Models",
        "author": [
            "Amit Agarwal",
            "Srikant Panda",
            "Angeline Charles",
            "Bhargava Kumar",
            "Hitesh Patel",
            "Priyanranjan Pattnayak",
            "Taki Hasan Rafi",
            "Tejaswini Kumar",
            "Dong-Kyu Chae"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19794",
        "abstract": "Recent advancements in Vision-Language Models (VLMs) have enabled significant progress in complex video understanding tasks. However, their robustness to real-world manipulations remains underexplored, limiting their reliability in critical applications. To address this gap, we introduce MVTamperBench, a comprehensive benchmark designed to evaluate VLM's resilience to video tampering effects, including rotation, dropping, masking, substitution, and repetition. By systematically assessing state-of-the-art models, MVTamperBench reveals substantial variability in robustness, with models like InternVL2-8B achieving high performance, while others, such as Llama-VILA1.5-8B, exhibit severe vulnerabilities. To foster broader adoption and reproducibility, MVTamperBench is integrated into VLMEvalKit, a modular evaluation toolkit, enabling streamlined testing and facilitating advancements in model robustness. Our benchmark represents a critical step towards developing tamper-resilient VLMs, ensuring their dependability in real-world scenarios.\nProject Page: https://amitbcp.github.io/MVTamperBench/",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "154",
        "title": "Investigating Acoustic-Textual Emotional Inconsistency Information for Automatic Depression Detection",
        "author": [
            "Rongfeng Su",
            "Changqing Xu",
            "Xinyi Wu",
            "Feng Xu",
            "Xie Chen",
            "Lan Wangt",
            "Nan Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18614",
        "abstract": "Previous studies have demonstrated that emotional features from a single acoustic sentiment label can enhance depression diagnosis accuracy. Additionally, according to the Emotion Context-Insensitivity theory and our pilot study, individuals with depression might convey negative emotional content in an unexpectedly calm manner, showing a high degree of inconsistency in emotional expressions during natural conversations. So far, few studies have recognized and leveraged the emotional expression inconsistency for depression detection. In this paper, a multimodal cross-attention method is presented to capture the Acoustic-Textual Emotional Inconsistency (ATEI) information. This is achieved by analyzing the intricate local and long-term dependencies of emotional expressions across acoustic and textual domains, as well as the mismatch between the emotional content within both domains. A Transformer-based model is then proposed to integrate this ATEI information with various fusion strategies for detecting depression. Furthermore, a scaling technique is employed to adjust the ATEI feature degree during the fusion process, thereby enhancing the model's ability to discern patients with depression across varying levels of severity. To best of our knowledge, this work is the first to incorporate emotional expression inconsistency information into depression detection. Experimental results on a counseling conversational dataset illustrate the effectiveness of our method.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "155",
        "title": "Implicit factorized transformer approach to fast prediction of turbulent channel flows",
        "author": [
            "Huiyu Yang",
            "Yunpeng Wang",
            "Jianchun Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.18840",
        "abstract": "Transformer neural operators have recently become an effective approach for surrogate modeling of nonlinear systems governed by partial differential equations (PDEs). In this paper, we introduce a modified implicit factorized transformer (IFactFormer-m) model which replaces the original chained factorized attention with parallel factorized attention. The IFactFormer-m model successfully performs long-term predictions for turbulent channel flow, whereas the original IFactFormer (IFactFormer-o), Fourier neural operator (FNO), and implicit Fourier neural operator (IFNO) exhibit a poor performance. Turbulent channel flows are simulated by direct numerical simulation using fine grids at friction Reynolds numbers $\\text{Re}_{\\tau}\\approx 180,395,590$, and filtered to coarse grids for training neural operator. The neural operator takes the current flow field as input and predicts the flow field at the next time step, and long-term prediction is achieved in the posterior through an autoregressive approach. The prediction results show that IFactFormer-m, compared to other neural operators and the traditional large eddy simulation (LES) methods including dynamic Smagorinsky model (DSM) and the wall-adapted local eddy-viscosity (WALE) model, reduces prediction errors in the short term, and achieves stable and accurate long-term prediction of various statistical properties and flow structures, including the energy spectrum, mean streamwise velocity, root mean square (rms) values of fluctuating velocities, Reynolds shear stress, and spatial structures of instantaneous velocity. Moreover, the trained IFactFormer-m is much faster than traditional LES methods.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "156",
        "title": "Stochastic normalizing flows for Effective String Theory",
        "author": [
            "Michele Caselle",
            "Elia Cellini",
            "Alessandro Nada"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19109",
        "abstract": "Effective String Theory (EST) is a powerful tool used to study confinement in pure gauge theories by modeling the confining flux tube connecting a static quark-anti-quark pair as a thin vibrating string. Recently, flow-based samplers have been applied as an efficient numerical method to study EST regularized on the lattice, opening the route to study observables previously inaccessible to standard analytical methods. Flow-based samplers are a class of algorithms based on Normalizing Flows (NFs), deep generative models recently proposed as a promising alternative to traditional Markov Chain Monte Carlo methods in lattice field theory calculations. By combining NF layers with out-of-equilibrium stochastic updates, we obtain Stochastic Normalizing Flows (SNFs), a scalable class of machine learning algorithms that can be explained in terms of stochastic thermodynamics. In this contribution, we outline EST and SNFs, and report some numerical results for the shape of the flux tube.",
        "tags": [
            "Normalizing Flows"
        ]
    },
    {
        "id": "157",
        "title": "Sentiment trading with large language models",
        "author": [
            "Kemal Kirtac",
            "Guido Germano"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19245",
        "abstract": "We investigate the efficacy of large language models (LLMs) in sentiment analysis of U.S. financial news and their potential in predicting stock market returns. We analyze a dataset comprising 965,375 news articles that span from January 1, 2010, to June 30, 2023; we focus on the performance of various LLMs, including BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary model, which has been a dominant methodology in the finance literature. The study documents a significant association between LLM scores and subsequent daily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the highest accuracy in sentiment prediction with an accuracy of 74.4%, slightly ahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald dictionary model demonstrates considerably lower effectiveness with only 50.1% accuracy. Regression analyses highlight a robust positive impact of OPT model scores on next-day stock returns, with coefficients of 0.274 and 0.254 in different model specifications. BERT and FINBERT also exhibit predictive relevance, though to a lesser extent. Notably, we do not observe a significant relationship between the Loughran-McDonald dictionary model scores and stock returns, challenging the efficacy of this traditional method in the current financial context. In portfolio performance, the long-short OPT strategy excels with a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT long-short strategies. Strategies based on the Loughran-McDonald dictionary yield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior performance of advanced LLMs, especially OPT, in financial market prediction and portfolio management, marking a significant shift in the landscape of financial analysis tools with implications to financial regulation and policy analysis.",
        "tags": [
            "BERT",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "158",
        "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
        "author": [
            "Emiru Tsunoo",
            "Yuki Saito",
            "Wataru Nakata",
            "Hiroshi Saruwatari"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19248",
        "abstract": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "159",
        "title": "VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware Speech Synthesis",
        "author": [
            "Jaemin Jung",
            "Junseok Ahn",
            "Chaeyoung Jung",
            "Tan Dat Nguyen",
            "Youngjoon Jang",
            "Joon Son Chung"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19259",
        "abstract": "We present VoiceDiT, a multi-modal generative model for producing environment-aware speech and audio from text and visual prompts. While aligning speech with text is crucial for intelligible speech, achieving this alignment in noisy conditions remains a significant and underexplored challenge in the field. To address this, we present a novel audio generation pipeline named VoiceDiT. This pipeline includes three key components: (1) the creation of a large-scale synthetic speech dataset for pre-training and a refined real-world speech dataset for fine-tuning, (2) the Dual-DiT, a model designed to efficiently preserve aligned speech information while accurately reflecting environmental conditions, and (3) a diffusion-based Image-to-Audio Translator that allows the model to bridge the gap between audio and image, facilitating the generation of environmental sound that aligns with the multi-modal prompts. Extensive experimental results demonstrate that VoiceDiT outperforms previous models on real-world datasets, showcasing significant improvements in both audio quality and modality integration.",
        "tags": [
            "DiT",
            "Diffusion",
            "Diffusion Transformer",
            "Transformer"
        ]
    },
    {
        "id": "160",
        "title": "Low-Rank Contextual Reinforcement Learning from Heterogeneous Human Feedback",
        "author": [
            "Seong Jin Lee",
            "Will Wei Sun",
            "Yufeng Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19436",
        "abstract": "Reinforcement learning from human feedback (RLHF) has become a cornerstone for aligning large language models with human preferences. However, the heterogeneity of human feedback, driven by diverse individual contexts and preferences, poses significant challenges for reward learning. To address this, we propose a Low-rank Contextual RLHF (LoCo-RLHF) framework that integrates contextual information to better model heterogeneous feedback while maintaining computational efficiency. Our approach builds on a contextual preference model, leveraging the intrinsic low-rank structure of the interaction between user contexts and query-answer pairs to mitigate the high dimensionality of feature representations. Furthermore, we address the challenge of distributional shifts in feedback through our Pessimism in Reduced Subspace (PRS) policy, inspired by pessimistic offline reinforcement learning techniques. We theoretically demonstrate that our policy achieves a tighter sub-optimality gap compared to existing methods. Extensive experiments validate the effectiveness of LoCo-RLHF, showcasing its superior performance in personalized RLHF settings and its robustness to distribution shifts.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "161",
        "title": "Deep Linear Hawkes Processes",
        "author": [
            "Yuxin Chang",
            "Alex Boyd",
            "Cao Xiao",
            "Taha Kass-Hout",
            "Parminder Bhatia",
            "Padhraic Smyth",
            "Andrew Warrington"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19634",
        "abstract": "Marked temporal point processes (MTPPs) are used to model sequences of different types of events with irregular arrival times, with broad applications ranging from healthcare and social networks to finance. We address shortcomings in existing point process models by drawing connections between modern deep state-space models (SSMs) and linear Hawkes processes (LHPs), culminating in an MTPP that we call the deep linear Hawkes process (DLHP). The DLHP modifies the linear differential equations in deep SSMs to be stochastic jump differential equations, akin to LHPs. After discretizing, the resulting recurrence can be implemented efficiently using a parallel scan. This brings parallelism and linear scaling to MTPP models. This contrasts with attention-based MTPPs, which scale quadratically, and RNN-based MTPPs, which do not parallelize across the sequence length. We show empirically that DLHPs match or outperform existing models across a broad range of metrics on eight real-world datasets. Our proposed DLHP model is the first instance of the unique architectural capabilities of SSMs being leveraged to construct a new class of MTPP models.",
        "tags": [
            "RNN",
            "SSMs",
            "State Space Models"
        ]
    }
]