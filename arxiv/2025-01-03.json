[
    {
        "id": "1",
        "title": "Mathematical modelling of flow and adsorption in a gas chromatograph",
        "author": [
            "A. Cabrera-Codony",
            "A. Valverde",
            "K. Born",
            "O.A.I. Noreldin",
            "T.G. Myers"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00001",
        "abstract": "In this paper, a mathematical model is developed to describe the evolution of the concentration of compounds through a gas chromatography column. The model couples mass balances and kinetic equations for all components. Both single and multiple-component cases are considered with constant or variable velocity. Non-dimensionalisation indicates the small effect of diffusion. The system where diffusion is neglected is analysed using Laplace transforms. In the multiple-component case, it is demonstrated that the competition between the compounds is negligible and the equations may be decoupled. This reduces the problem to solving a single integral equation to determine the concentration profile for all components (since they are scaled versions of each other). For a given analyte, we then only two parameters need to be fitted to the data. To verify this approach, the full governing equations are also solved numerically using the finite difference method and a global adaptive quadrature method to integrate the Laplace transformation. Comparison with the Laplace solution verifies the high degree of accuracy of the simpler Laplace form. The Laplace solution is then verified against experimental data from BTEX chromatography. This novel method, which involves solving a single equation and fitting parameters in pairs for individual components, is highly efficient. It is significantly faster and simpler than the full numerical solution and avoids the computationally expensive methods that would normally be used to fit all curves at the same time.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "2",
        "title": "SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models",
        "author": [
            "Linqin Wang",
            "Yaping Liu",
            "Zhengtao Yu",
            "Shengxiang Gao",
            "Cunli Mao",
            "Yuxin Huang",
            "Wenjun Wang",
            "Ling Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00018",
        "abstract": "With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "3",
        "title": "A Breadth-First Catalog of Text Processing, Speech Processing and Multimodal Research in South Asian Languages",
        "author": [
            "Pranav Gupta"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00029",
        "abstract": "We review the recent literature (January 2022- October 2024) in South Asian languages on text-based language processing, multimodal models, and speech processing, and provide a spotlight analysis focused on 21 low-resource South Asian languages, namely Saraiki, Assamese, Balochi, Bhojpuri, Bodo, Burmese, Chhattisgarhi, Dhivehi, Gujarati, Kannada, Kashmiri, Konkani, Khasi, Malayalam, Meitei, Nepali, Odia, Pashto, Rajasthani, Sindhi, and Telugu. We identify trends, challenges, and future research directions, using a step-wise approach that incorporates relevance classification and clustering based on large language models (LLMs). Our goal is to provide a breadth-first overview of the recent developments in South Asian language technologies to NLP researchers interested in working with South Asian languages.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "4",
        "title": "Highly Optimized Kernels and Fine-Grained Codebooks for LLM Inference on Arm CPUs",
        "author": [
            "Dibakar Gope",
            "David Mansell",
            "Danny Loh",
            "Ian Bratt"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00032",
        "abstract": "Large language models (LLMs) have transformed the way we think about language understanding and generation, enthralling both researchers and developers. However, deploying LLMs for inference has been a significant challenge due to their unprecedented size and resource requirements. While quantizing model weights to sub-byte precision has emerged as a promising solution to ease memory pressure, the group quantization formats commonly used for LLM quantization have significant compute overheads and a resource-intensive dequantization process. As a result, a higher proportion of compute instructions do not perform multiplies, i.e., real work, rendering them unsuitable for meeting the required latency requirements for LLMs deployed on commodity CPUs. In this work, we propose a set of highly optimized kernels to accelerate LLM inference and unleash the full potential of CPUs, particularly Arm CPUs. These kernels amortize the cost of loading the operands and the cost of weight unpacking across multiple output rows. This, along with the introduction of an optimized interleaved group data layout for weights and decompression path optimizations to reduce unnecessary operations and dequantization overhead while maximizing the use of vector and matrix multiply operations, significantly improves the efficiency of MAC operations. Furthermore, we present a groupwise non-uniform codebook-based quantization method for ultra-low-precision quantization of LLMs to better match non-uniform patterns in their weight distributions, demonstrating better throughput during token generation while ensuring better quality than the state-of-the-art. Applying these improvements to 4-bit LLMs results in a 3-3.2x improvement in prompt processing and a 2x improvement in autoregressive decoding on Arm CPUs, compared to http://LLaMA.cpp-based solution. The optimized kernels are available at https://github.com/ggerganov/llama.cpp.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "5",
        "title": "Resource-Efficient Transformer Architecture: Optimizing Memory and Execution Time for Real-Time Applications",
        "author": [
            "Krisvarish V",
            "Priyadarshini T",
            "K P Abhishek Sri Saai",
            "Vaidehi Vijayakumar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00042",
        "abstract": "This paper describes a memory-efficient transformer model designed to drive a reduction in memory usage and execution time by substantial orders of magnitude without impairing the model's performance near that of the original model. Recently, new architectures of transformers were presented, focused on parameter efficiency and computational optimization; however, such models usually require considerable resources in terms of hardware when deployed in real-world applications on edge devices. This approach addresses this concern by halving embedding size and applying targeted techniques such as parameter pruning and quantization to optimize the memory footprint with minimum sacrifices in terms of accuracy. Experimental results include a 52% reduction in memory usage and a 33% decrease in execution time, resulting in better efficiency than state-of-the-art models. This work compared our model with existing compelling architectures, such as MobileBERT and DistilBERT, and proved its feasibility in the domain of resource-friendly deep learning architectures, mainly for applications in real-time and in resource-constrained applications.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "6",
        "title": "Seq2Seq Model-Based Chatbot with LSTM and Attention Mechanism for Enhanced User Interaction",
        "author": [
            "Lamya Benaddi",
            "Charaf Ouaddi",
            "Adnane Souha",
            "Abdeslam Jakimi",
            "Mohamed Rahouti",
            "Mohammed Aledhari",
            "Diogo Oliveira",
            "Brahim Ouchao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00049",
        "abstract": "A chatbot is an intelligent software application that automates conversations and engages users in natural language through messaging platforms. Leveraging artificial intelligence (AI), chatbots serve various functions, including customer service, information gathering, and casual conversation. Existing virtual assistant chatbots, such as ChatGPT and Gemini, demonstrate the potential of AI in Natural Language Processing (NLP). However, many current solutions rely on predefined APIs, which can result in vendor lock-in and high costs. To address these challenges, this work proposes a chatbot developed using a Sequence-to-Sequence (Seq2Seq) model with an encoder-decoder architecture that incorporates attention mechanisms and Long Short-Term Memory (LSTM) cells. By avoiding predefined APIs, this approach ensures flexibility and cost-effectiveness. The chatbot is trained, validated, and tested on a dataset specifically curated for the tourism sector in Draa-Tafilalet, Morocco. Key evaluation findings indicate that the proposed Seq2Seq model-based chatbot achieved high accuracies: approximately 99.58% in training, 98.03% in validation, and 94.12% in testing. These results demonstrate the chatbot's effectiveness in providing relevant and coherent responses within the tourism domain, highlighting the potential of specialized AI applications to enhance user experience and satisfaction in niche markets.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "7",
        "title": "DDD-GenDT: Dynamic Data-driven Generative Digital Twin Framework",
        "author": [
            "Yu-Zheng Lin",
            "Qinxuan Shi",
            "Zhanglong Yang",
            "Banafsheh Saber Latibari",
            "Sicong Shao",
            "Soheil Salehi",
            "Pratik Satam"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00051",
        "abstract": "Digital twin (DT) technology has emerged as a transformative approach to simulate, predict, and optimize the behavior of physical systems, with applications that span manufacturing, healthcare, climate science, and more. However, the development of DT models often faces challenges such as high data requirements, integration complexity, and limited adaptability to dynamic changes in physical systems. This paper presents a new method inspired by dynamic data-driven applications systems (DDDAS), called the dynamic data-driven generative of digital twins framework (DDD-GenDT), which combines the physical system with LLM, allowing LLM to act as DT to interact with the physical system operating status and generate the corresponding physical behaviors. We apply DDD-GenDT to the computer numerical control (CNC) machining process, and we use the spindle current measurement data in the NASA milling wear data set as an example to enable LLMs to forecast the physical behavior from historical data and interact with current observations. Experimental results show that in the zero-shot prediction setting, the LLM-based DT can adapt to the change in the system, and the average RMSE of the GPT-4 prediction is 0.479A, which is 4.79% of the maximum spindle motor current measurement of 10A, with little training data and instructions required. Furthermore, we analyze the performance of DDD-GenDT in this specific application and their potential to construct digital twins. We also discuss the limitations and challenges that may arise in practical implementations.",
        "tags": [
            "GPT",
            "LLMs"
        ]
    },
    {
        "id": "8",
        "title": "AdvAnchor: Enhancing Diffusion Model Unlearning with Adversarial Anchors",
        "author": [
            "Mengnan Zhao",
            "Lihe Zhang",
            "Xingyi Yang",
            "Tianhang Zheng",
            "Baocai Yin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00054",
        "abstract": "Security concerns surrounding text-to-image diffusion models have driven researchers to unlearn inappropriate concepts through fine-tuning. Recent fine-tuning methods typically align the prediction distributions of unsafe prompts with those of predefined text anchors. However, these techniques exhibit a considerable performance trade-off between eliminating undesirable concepts and preserving other concepts. In this paper, we systematically analyze the impact of diverse text anchors on unlearning performance. Guided by this analysis, we propose AdvAnchor, a novel approach that generates adversarial anchors to alleviate the trade-off issue. These adversarial anchors are crafted to closely resemble the embeddings of undesirable concepts to maintain overall model performance, while selectively excluding defining attributes of these concepts for effective erasure. Extensive experiments demonstrate that AdvAnchor outperforms state-of-the-art methods. Our code is publicly available at https://anonymous.4open.science/r/AdvAnchor.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "9",
        "title": "LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models",
        "author": [
            "Miao Yu",
            "Junfeng Fang",
            "Yingjie Zhou",
            "Xing Fan",
            "Kun Wang",
            "Shirui Pan",
            "Qingsong Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00055",
        "abstract": "While safety-aligned large language models (LLMs) are increasingly used as the cornerstone for powerful systems such as multi-agent frameworks to solve complex real-world problems, they still suffer from potential adversarial queries, such as jailbreak attacks, which attempt to induce harmful content. Researching attack methods allows us to better understand the limitations of LLM and make trade-offs between helpfulness and safety. However, existing jailbreak attacks are primarily based on opaque optimization techniques (e.g. token-level gradient descent) and heuristic search methods like LLM refinement, which fall short in terms of transparency, transferability, and computational cost. In light of these limitations, we draw inspiration from the evolution and infection processes of biological viruses and propose LLM-Virus, a jailbreak attack method based on evolutionary algorithm, termed evolutionary jailbreak. LLM-Virus treats jailbreak attacks as both an evolutionary and transfer learning problem, utilizing LLMs as heuristic evolutionary operators to ensure high attack efficiency, transferability, and low time cost. Our experimental results on multiple safety benchmarks show that LLM-Virus achieves competitive or even superior performance compared to existing attack methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "10",
        "title": "Large Language Models for Mathematical Analysis",
        "author": [
            "Ziye Chen",
            "Hao Qi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00059",
        "abstract": "Mathematical problem-solving is a key field in artificial intelligence (AI) and a critical benchmark for evaluating the capabilities of large language models (LLMs). While extensive research has focused on mathematical problem-solving, most existing work and datasets concentrate on computational tasks, leaving gaps in areas like mathematical analysis, which demands rigorous proofs and formal reasoning. We developed the DEMI-MathAnalysis dataset, comprising proof-based problems from mathematical analysis topics such as Sequences and Limits, Infinite Series, and Convex Functions. We also designed a guiding framework to rigorously enhance LLMs' ability to solve these problems. Through fine-tuning LLMs on this dataset and employing our framework, we observed significant improvements in their capability to generate logical, complete, and elegant proofs. This work addresses critical gaps in mathematical reasoning and contributes to advancing trustworthy AI capable of handling formalized mathematical language. The code is publicly accessible at LLMs for Mathematical Analysis.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "11",
        "title": "ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis",
        "author": [
            "James P. Beno"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00062",
        "abstract": "Bidirectional transformers excel at sentiment analysis, and Large Language Models (LLM) are effective zero-shot learners. Might they perform better as a team? This paper explores collaborative approaches between ELECTRA and GPT-4o for three-way sentiment classification. We fine-tuned (FT) four models (ELECTRA Base/Large, GPT-4o/4o-mini) using a mix of reviews from Stanford Sentiment Treebank (SST) and DynaSent. We provided input from ELECTRA to GPT as: predicted label, probabilities, and retrieved examples. Sharing ELECTRA Base FT predictions with GPT-4o-mini significantly improved performance over either model alone (82.74 macro F1 vs. 79.29 ELECTRA Base FT, 79.52 GPT-4o-mini) and yielded the lowest cost/performance ratio (\\$0.12/F1 point). However, when GPT models were fine-tuned, including predictions decreased performance. GPT-4o FT-M was the top performer (86.99), with GPT-4o-mini FT close behind (86.77) at much less cost (\\$0.38 vs. \\$1.59/F1 point). Our results show that augmenting prompts with predictions from fine-tuned encoders is an efficient way to boost performance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76% less cost. Both are affordable options for projects with limited resources.",
        "tags": [
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "12",
        "title": "\"Generative Models for Financial Time Series Data: Enhancing Signal-to-Noise Ratio and Addressing Data Scarcity in A-Share Market",
        "author": [
            "Guangming Che"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00063",
        "abstract": "The financial industry is increasingly seeking robust methods to address the challenges posed by data scarcity and low signal-to-noise ratios, which limit the application of deep learning techniques in stock market analysis. This paper presents two innovative generative model-based approaches to synthesize stock data, specifically tailored for different scenarios within the A-share market in China. The first method, a sector-based synthesis approach, enhances the signal-to-noise ratio of stock data by classifying the characteristics of stocks from various sectors in China's A-share market. This method employs an Approximate Non-Local Total Variation algorithm to smooth the generated data, a bandpass filtering method based on Fourier Transform to eliminate noise, and Denoising Diffusion Implicit Models to accelerate sampling speed. The second method, a recursive stock data synthesis approach based on pattern recognition, is designed to synthesize data for stocks with short listing periods and limited comparable companies. It leverages pattern recognition techniques and Markov models to learn and generate variable-length stock sequences, while introducing a sub-time-level data augmentation method to alleviate data scarcity http://issues.We validate the effectiveness of these methods through extensive experiments on various datasets, including those from the main board, STAR Market, Growth Enterprise Market Board, Beijing Stock Exchange, NASDAQ, NYSE, and AMEX. The results demonstrate that our synthesized data not only improve the performance of predictive models but also enhance the signal-to-noise ratio of individual stock signals in price trading strategies. Furthermore, the introduction of sub-time-level data significantly improves the quality of synthesized data.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "13",
        "title": "ICLR: In-Context Learning of Representations",
        "author": [
            "Core Francisco Park",
            "Andrew Lee",
            "Ekdeep Singh Lubana",
            "Yongyi Yang",
            "Maya Okawa",
            "Kento Nishi",
            "Martin Wattenberg",
            "Hidenori Tanaka"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00070",
        "abstract": "Recent work has demonstrated that semantics specified by pretraining data influence how representations of different concepts are organized in a large language model (LLM). However, given the open-ended nature of LLMs, e.g., their ability to in-context learn, we can ask whether models alter these pretraining semantics to adopt alternative, context-specified ones. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, do models reorganize their representations in accordance with these novel semantics? To answer this question, we take inspiration from the theory of conceptual role semantics and define a toy \"graph tracing\" task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.) and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization from pretrained semantic representations to in-context representations aligned with the graph structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, providing evidence towards an implicit optimization process to infer context-specified semantics. Overall, our findings indicate scaling context-size can flexibly re-organize model representations, possibly unlocking novel capabilities.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "14",
        "title": "Position Information Emerges in Causal Transformers Without Positional Encodings via Similarity of Nearby Embeddings",
        "author": [
            "Chunsheng Zuo",
            "Pavel Guerzhoy",
            "Michael Guerzhoy"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00073",
        "abstract": "Transformers with causal attention can solve tasks that require positional information without using positional encodings. In this work, we propose and investigate a new hypothesis about how positional information can be stored without using explicit positional encoding. We observe that nearby embeddings are more similar to each other than faraway embeddings, allowing the transformer to potentially reconstruct the positions of tokens. We show that this pattern can occur in both the trained and the randomly initialized Transformer models with causal attention and no positional encodings over a common range of hyperparameters.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "15",
        "title": "AI Agent for Education: von Neumann Multi-Agent System Framework",
        "author": [
            "Yuan-Hao Jiang",
            "Ruijia Li",
            "Yizhou Zhou",
            "Changyong Qi",
            "Hanglei Hu",
            "Yuang Wei",
            "Bo Jiang",
            "Yonghe Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00083",
        "abstract": "The development of large language models has ushered in new paradigms for education. This paper centers on the multi-Agent system in education and proposes the von Neumann multi-Agent system framework. It breaks down each AI Agent into four modules: control unit, logic unit, storage unit, and input-output devices, defining four types of operations: task deconstruction, self-reflection, memory processing, and tool invocation. Furthermore, it introduces related technologies such as Chain-of-Thought, Reson+Act, and Multi-Agent Debate associated with these four types of operations. The paper also discusses the ability enhancement cycle of a multi-Agent system for education, including the outer circulation for human learners to promote knowledge construction and the inner circulation for LLM-based-Agents to enhance swarm intelligence. Through collaboration and reflection, the multi-Agent system can better facilitate human learners' learning and enhance their teaching abilities in this process.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "16",
        "title": "CaseSumm: A Large-Scale Dataset for Long-Context Summarization from U.S. Supreme Court Opinions",
        "author": [
            "Mourad Heddaya",
            "Kyle MacMillan",
            "Anup Malani",
            "Hongyuan Mei",
            "Chenhao Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00097",
        "abstract": "This paper introduces CaseSumm, a novel dataset for long-context summarization in the legal domain that addresses the need for longer and more complex datasets for summarization evaluation. We collect 25.6K U.S. Supreme Court (SCOTUS) opinions and their official summaries, known as \"syllabuses.\" Our dataset is the largest open legal case summarization dataset, and is the first to include summaries of SCOTUS decisions dating back to 1815.\nWe also present a comprehensive evaluation of LLM-generated summaries using both automatic metrics and expert human evaluation, revealing discrepancies between these assessment methods. Our evaluation shows Mistral 7b, a smaller open-source model, outperforms larger models on most automatic metrics and successfully generates syllabus-like summaries. In contrast, human expert annotators indicate that Mistral summaries contain hallucinations. The annotators consistently rank GPT-4 summaries as clearer and exhibiting greater sensitivity and specificity. Further, we find that LLM-based evaluations are not more correlated with human evaluations than traditional automatic metrics. Furthermore, our analysis identifies specific hallucinations in generated summaries, including precedent citation errors and misrepresentations of case facts. These findings demonstrate the limitations of current automatic evaluation methods for legal summarization and highlight the critical role of human evaluation in assessing summary quality, particularly in complex, high-stakes domains.\nCaseSumm is available at https://huggingface.co/datasets/ChicagoHAI/CaseSumm",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "17",
        "title": "LTX-Video: Realtime Video Latent Diffusion",
        "author": [
            "Yoav HaCohen",
            "Nisan Chiprut",
            "Benny Brazowski",
            "Daniel Shalem",
            "Dudu Moshe",
            "Eitan Richardson",
            "Eran Levin",
            "Guy Shiran",
            "Nir Zabari",
            "Ori Gordon",
            "Poriya Panet",
            "Sapir Weissbuch",
            "Victor Kulikov",
            "Yaki Bitterman",
            "Zeev Melumian",
            "Ofir Bibi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00103",
        "abstract": "We introduce LTX-Video, a transformer-based latent diffusion model that adopts a holistic approach to video generation by seamlessly integrating the responsibilities of the Video-VAE and the denoising transformer. Unlike existing methods, which treat these components as independent, LTX-Video aims to optimize their interaction for improved efficiency and quality. At its core is a carefully designed Video-VAE that achieves a high compression ratio of 1:192, with spatiotemporal downscaling of 32 x 32 x 8 pixels per token, enabled by relocating the patchifying operation from the transformer's input to the VAE's input. Operating in this highly compressed latent space enables the transformer to efficiently perform full spatiotemporal self-attention, which is essential for generating high-resolution videos with temporal consistency. However, the high compression inherently limits the representation of fine details. To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space. This approach preserves the ability to generate fine details without incurring the runtime cost of a separate upsampling module. Our model supports diverse use cases, including text-to-video and image-to-video generation, with both capabilities trained simultaneously. It achieves faster-than-real-time generation, producing 5 seconds of 24 fps video at 768x512 resolution in just 2 seconds on an Nvidia H100 GPU, outperforming all existing models of similar scale. The source code and pre-trained models are publicly available, setting a new benchmark for accessible and scalable video generation.",
        "tags": [
            "Diffusion",
            "Text-to-Video",
            "Transformer",
            "VAE",
            "Video Generation"
        ]
    },
    {
        "id": "18",
        "title": "AltGen: AI-Driven Alt Text Generation for Enhancing EPUB Accessibility",
        "author": [
            "Yixian Shen",
            "Hang Zhang",
            "Yanxin Shen",
            "Lun Wang",
            "Chuanqi Shi",
            "Shaoshuai Du",
            "Yiyi Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00113",
        "abstract": "Digital accessibility is a cornerstone of inclusive content delivery, yet many EPUB files fail to meet fundamental accessibility standards, particularly in providing descriptive alt text for images. Alt text plays a critical role in enabling visually impaired users to understand visual content through assistive technologies. However, generating high-quality alt text at scale is a resource-intensive process, creating significant challenges for organizations aiming to ensure accessibility compliance. This paper introduces AltGen, a novel AI-driven pipeline designed to automate the generation of alt text for images in EPUB files. By integrating state-of-the-art generative models, including advanced transformer-based architectures, AltGen achieves contextually relevant and linguistically coherent alt text descriptions. The pipeline encompasses multiple stages, starting with data preprocessing to extract and prepare relevant content, followed by visual analysis using computer vision models such as CLIP and ViT. The extracted visual features are enriched with contextual information from surrounding text, enabling the fine-tuned language models to generate descriptive and accurate alt text. Validation of the generated output employs both quantitative metrics, such as cosine similarity and BLEU scores, and qualitative feedback from visually impaired users.\nExperimental results demonstrate the efficacy of AltGen across diverse datasets, achieving a 97.5% reduction in accessibility errors and high scores in similarity and linguistic fidelity metrics. User studies highlight the practical impact of AltGen, with participants reporting significant improvements in document usability and comprehension. Furthermore, comparative analyses reveal that AltGen outperforms existing approaches in terms of accuracy, relevance, and scalability.",
        "tags": [
            "CLIP",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "19",
        "title": "Text-to-Image GAN with Pretrained Representations",
        "author": [
            "Xiaozhou You",
            "Jian Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00116",
        "abstract": "Generating desired images conditioned on given text descriptions has received lots of attention. Recently, diffusion models and autoregressive models have demonstrated their outstanding expressivity and gradually replaced GAN as the favored architectures for text-to-image synthesis. However, they still face some obstacles: slow inference speed and expensive training costs. To achieve more powerful and faster text-to-image synthesis under complex scenes, we propose TIGER, a text-to-image GAN with pretrained representations. To be specific, we propose a vision-empowered discriminator and a high-capacity generator. (i) The vision-empowered discriminator absorbs the complex scene understanding ability and the domain generalization ability from pretrained vision models to enhance model performance. Unlike previous works, we explore stacking multiple pretrained models in our discriminator to collect multiple different representations. (ii) The high-capacity generator aims to achieve effective text-image fusion while increasing the model capacity. The high-capacity generator consists of multiple novel high-capacity fusion blocks (HFBlock). And the HFBlock contains several deep fusion modules and a global fusion module, which play different roles to benefit our model. Extensive experiments demonstrate the outstanding performance of our proposed TIGER both on standard and zero-shot text-to-image synthesis tasks. On the standard text-to-image synthesis task, TIGER achieves state-of-the-art performance on two challenging datasets, which obtain a new FID 5.48 (COCO) and 9.38 (CUB). On the zero-shot text-to-image synthesis task, we achieve comparable performance with fewer model parameters, smaller training data size and faster inference speed. Additionally, more experiments and analyses are conducted in the Supplementary Material.",
        "tags": [
            "Diffusion",
            "GAN",
            "Text-to-Image"
        ]
    },
    {
        "id": "20",
        "title": "PQD: Post-training Quantization for Efficient Diffusion Models",
        "author": [
            "Jiaojiao Ye",
            "Zhen Wang",
            "Linnan Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00124",
        "abstract": "Diffusionmodels(DMs)havedemonstratedremarkableachievements in synthesizing images of high fidelity and diversity. However, the extensive computational requirements and slow generative speed of diffusion models have limited their widespread adoption. In this paper, we propose a novel post-training quantization for diffusion models (PQD), which is a time-aware optimization framework for diffusion models based on post-training quantization. The proposed framework optimizes the inference process by selecting representative samples and conducting time-aware calibration. Experimental results show that our proposed method is able to directly quantize full-precision diffusion models into 8-bit or 4-bit models while maintaining comparable performance in a training-free manner, achieving a few FID change on ImageNet for unconditional image generation. Our approach demonstrates compatibility and can also be applied to 512x512 text-guided image generation for the first time.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "21",
        "title": "Can Large Language Models Improve SE Active Learning via Warm-Starts?",
        "author": [
            "Lohith Senthilkumar",
            "Tim Menzies"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00125",
        "abstract": "When SE data is scarce, \"active learners\" use models learned from tiny samples of the data to find the next most informative example to label. In this way, effective models can be generated using very little data. For multi-objective software engineering (SE) tasks, active learning can benefit from an effective set of initial guesses (also known as \"warm starts\"). This paper explores the use of Large Language Models (LLMs) for creating warm-starts. Those results are compared against Gaussian Process Models and Tree of Parzen Estimators. For 49 SE tasks, LLM-generated warm starts significantly improved the performance of low- and medium-dimensional tasks. However, LLM effectiveness diminishes in high-dimensional problems, where Bayesian methods like Gaussian Process Models perform best.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "22",
        "title": "Measuring Large Language Models Capacity to Annotate Journalistic Sourcing",
        "author": [
            "Subramaniam Vincent",
            "Phoebe Wang",
            "Zhan Shi",
            "Sahas Koka",
            "Yi Fang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00164",
        "abstract": "Since the launch of ChatGPT in late 2022, the capacities of Large Language Models and their evaluation have been in constant discussion and evaluation both in academic research and in the industry. Scenarios and benchmarks have been developed in several areas such as law, medicine and math (Bommasani et al., 2023) and there is continuous evaluation of model variants. One area that has not received sufficient scenario development attention is journalism, and in particular journalistic sourcing and ethics. Journalism is a crucial truth-determination function in democracy (Vincent, 2023), and sourcing is a crucial pillar to all original journalistic output. Evaluating the capacities of LLMs to annotate stories for the different signals of sourcing and how reporters justify them is a crucial scenario that warrants a benchmark approach. It offers potential to build automated systems to contrast more transparent and ethically rigorous forms of journalism with everyday fare. In this paper we lay out a scenario to evaluate LLM performance on identifying and annotating sourcing in news stories on a five-category schema inspired from journalism studies (Gans, 2004). We offer the use case, our dataset and metrics and as the first step towards systematic benchmarking. Our accuracy findings indicate LLM-based approaches have more catching to do in identifying all the sourced statements in a story, and equally, in matching the type of sources. An even harder task is spotting source justifications.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "23",
        "title": "MLLM-as-a-Judge for Image Safety without Human Labeling",
        "author": [
            "Zhenting Wang",
            "Shuming Hu",
            "Shiyu Zhao",
            "Xiaowen Lin",
            "Felix Juefei-Xu",
            "Zhuowei Li",
            "Ligong Han",
            "Harihar Subramanyam",
            "Li Chen",
            "Jianfa Chen",
            "Nan Jiang",
            "Lingjuan Lyu",
            "Shiqing Ma",
            "Dimitris N. Metaxas",
            "Ankit Jain"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00192",
        "abstract": "Image content safety has become a significant challenge with the rise of visual media on online platforms. Meanwhile, in the age of AI-generated content (AIGC), many image generation models are capable of producing harmful content, such as images containing sexual or violent material. Thus, it becomes crucial to identify such unsafe images based on established safety rules. Pre-trained Multimodal Large Language Models (MLLMs) offer potential in this regard, given their strong pattern recognition abilities. Existing approaches typically fine-tune MLLMs with human-labeled datasets, which however brings a series of drawbacks. First, relying on human annotators to label data following intricate and detailed guidelines is both expensive and labor-intensive. Furthermore, users of safety judgment systems may need to frequently update safety rules, making fine-tuning on human-based annotation more challenging. This raises the research question: Can we detect unsafe images by querying MLLMs in a zero-shot setting using a predefined safety constitution (a set of safety rules)? Our research showed that simply querying pre-trained MLLMs does not yield satisfactory results. This lack of effectiveness stems from factors such as the subjectivity of safety rules, the complexity of lengthy constitutions, and the inherent biases in the models. To address these challenges, we propose a MLLM-based method includes objectifying safety rules, assessing the relevance between rules and images, making quick judgments based on debiased token probabilities with logically complete yet simplified precondition chains for safety rules, and conducting more in-depth reasoning with cascaded chain-of-thought processes if necessary. Experiment results demonstrate that our method is highly effective for zero-shot image safety judgment tasks.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "24",
        "title": "GPT-4 on Clinic Depression Assessment: An LLM-Based Pilot Study",
        "author": [
            "Giuliano Lorenzoni",
            "Pedro Elkind Velmovitsky",
            "Paulo Alencar",
            "Donald Cowan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00199",
        "abstract": "Depression has impacted millions of people worldwide and has become one of the most prevalent mental disorders. Early mental disorder detection can lead to cost savings for public health agencies and avoid the onset of other major comorbidities. Additionally, the shortage of specialized personnel is a critical issue because clinical depression diagnosis is highly dependent on expert professionals and is time consuming.\nIn this study, we explore the use of GPT-4 for clinical depression assessment based on transcript analysis. We examine the model's ability to classify patient interviews into binary categories: depressed and not depressed. A comparative analysis is conducted considering prompt complexity (e.g., using both simple and complex prompts) as well as varied temperature settings to assess the impact of prompt complexity and randomness on the model's performance.\nResults indicate that GPT-4 exhibits considerable variability in accuracy and F1-Score across configurations, with optimal performance observed at lower temperature values (0.0-0.2) for complex prompts. However, beyond a certain threshold (temperature >= 0.3), the relationship between randomness and performance becomes unpredictable, diminishing the gains from prompt complexity.\nThese findings suggest that, while GPT-4 shows promise for clinical assessment, the configuration of the prompts and model parameters requires careful calibration to ensure consistent results. This preliminary study contributes to understanding the dynamics between prompt engineering and large language models, offering insights for future development of AI-powered tools in clinical settings.",
        "tags": [
            "Detection",
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "25",
        "title": "The Potential of LLMs in Automating Software Testing: From Generation to Reporting",
        "author": [
            "Betim Sherifi",
            "Khaled Slhoub",
            "Fitzroy Nembhard"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00217",
        "abstract": "Having a high quality software is essential in software engineering, which requires robust validation and verification processes during testing activities. Manual testing, while effective, can be time consuming and costly, leading to an increased demand for automated methods. Recent advancements in Large Language Models (LLMs) have significantly influenced software engineering, particularly in areas like requirements analysis, test automation, and debugging. This paper explores an agent-oriented approach to automated software testing, using LLMs to reduce human intervention and enhance testing efficiency. The proposed framework integrates LLMs to generate unit tests, visualize call graphs, and automate test execution and reporting. Evaluations across multiple applications in Python and Java demonstrate the system's high test coverage and efficient operation. This research underscores the potential of LLM-powered agents to streamline software testing workflows while addressing challenges in scalability and accuracy.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "26",
        "title": "Extracting effective solutions hidden in large language models via generated comprehensive specialists: case studies in developing electronic devices",
        "author": [
            "Hikari Tomita",
            "Nobuhiro Nakamura",
            "Shoichi Ishida",
            "Toshio Kamiya",
            "Kei Terayama"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00224",
        "abstract": "Recently, many studies have increasingly explored the use of large language models (LLMs) to generate research ideas and scientific hypotheses. However, real-world research and development often require solving complex, interdisciplinary challenges where solutions may not be readily found through existing knowledge related to the problem. Therefore, it is desirable to leverage the vast, comprehensive knowledge of LLMs to generate effective, breakthrough solutions by integrating various perspectives from other disciplines. Here, we propose SELLM (Solution Enumeration via comprehensive List and LLM), a framework leveraging LLMs and structured guidance using MECE (Mutually Exclusive, Collectively Exhaustive) principles, such as International Patent Classification (IPC) and the periodic table of elements. SELLM systematically constructs comprehensive expert agents from the list to generate cross-disciplinary and effective solutions. To evaluate SELLM's practicality, we applied it to two challenges: improving light extraction in organic light-emitting diode (OLED) lighting and developing electrodes for next-generation memory materials. The results demonstrate that SELLM significantly facilitates the generation of effective solutions compared to cases without specific customization or effort, showcasing the potential of SELLM to enable LLMs to generate effective solutions even for challenging problems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "27",
        "title": "Generative Emergent Communication: Large Language Model is a Collective World Model",
        "author": [
            "Tadahiro Taniguchi",
            "Ryo Ueda",
            "Tomoaki Nakamura",
            "Masahiro Suzuki",
            "Akira Taniguchi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00226",
        "abstract": "This study proposes a unifying theoretical framework called generative emergent communication (generative EmCom) that bridges emergent communication, world models, and large language models (LLMs) through the lens of collective predictive coding (CPC). The proposed framework formalizes the emergence of language and symbol systems through decentralized Bayesian inference across multiple agents, extending beyond conventional discriminative model-based approaches to emergent communication. This study makes the following two key contributions: First, we propose generative EmCom as a novel framework for understanding emergent communication, demonstrating how communication emergence in multi-agent reinforcement learning (MARL) can be derived from control as inference while clarifying its relationship to conventional discriminative approaches. Second, we propose a mathematical formulation showing the interpretation of LLMs as collective world models that integrate multiple agents' experiences through CPC. The framework provides a unified theoretical foundation for understanding how shared symbol systems emerge through collective predictive coding processes, bridging individual cognitive development and societal language evolution. Through mathematical formulations and discussion on prior works, we demonstrate how this framework explains fundamental aspects of language emergence and offers practical insights for understanding LLMs and developing sophisticated AI systems for improving human-AI interaction and multi-agent systems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "28",
        "title": "Zero-Shot Strategies for Length-Controllable Summarization",
        "author": [
            "Fabian Retkowski",
            "Alexander Waibel"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00233",
        "abstract": "Large language models (LLMs) struggle with precise length control, particularly in zero-shot settings. We conduct a comprehensive study evaluating LLMs' length control capabilities across multiple measures and propose practical methods to improve controllability. Our experiments with LLaMA 3 reveal stark differences in length adherence across measures and highlight inherent biases of the model. To address these challenges, we introduce a set of methods: length approximation, target adjustment, sample filtering, and automated revisions. By combining these methods, we demonstrate substantial improvements in length compliance while maintaining or enhancing summary quality, providing highly effective zero-shot strategies for precise length control without the need for model fine-tuning or architectural changes. With our work, we not only advance our understanding of LLM behavior in controlled text generation but also pave the way for more reliable and adaptable summarization systems in real-world applications.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "29",
        "title": "Exploring Variability in Fine-Tuned Models for Text Classification with DistilBERT",
        "author": [
            "Giuliano Lorenzoni",
            "Ivens Portugal",
            "Paulo Alencar",
            "Donald Cowan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00241",
        "abstract": "This study evaluates fine-tuning strategies for text classification using the DistilBERT model, specifically the distilbert-base-uncased-finetuned-sst-2-english variant. Through structured experiments, we examine the influence of hyperparameters such as learning rate, batch size, and epochs on accuracy, F1-score, and loss. Polynomial regression analyses capture foundational and incremental impacts of these hyperparameters, focusing on fine-tuning adjustments relative to a baseline model.\nResults reveal variability in metrics due to hyperparameter configurations, showing trade-offs among performance metrics. For example, a higher learning rate reduces loss in relative analysis (p=0.027) but challenges accuracy improvements. Meanwhile, batch size significantly impacts accuracy and F1-score in absolute regression (p=0.028 and p=0.005) but has limited influence on loss optimization (p=0.170). The interaction between epochs and batch size maximizes F1-score (p=0.001), underscoring the importance of hyperparameter interplay.\nThese findings highlight the need for fine-tuning strategies addressing non-linear hyperparameter interactions to balance performance across metrics. Such variability and metric trade-offs are relevant for tasks beyond text classification, including NLP and computer vision. This analysis informs fine-tuning strategies for large language models and promotes adaptive designs for broader model applicability.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "30",
        "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained Image Recognition",
        "author": [
            "Edwin Arkel Rios",
            "Jansen Christopher Yuanda",
            "Vincent Leon Ghanz",
            "Cheng-Wei Yu",
            "Bo-Cheng Lai",
            "Min-Chun Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00243",
        "abstract": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that involves classifying images within a macro-category. While traditional FGIR deals with classifying different species, UFGIR goes beyond by classifying sub-categories within a species such as cultivars of a plant. In recent times the usage of Vision Transformer-based backbones has allowed methods to obtain outstanding recognition performances in this task but this comes at a significant cost in terms of computation specially since this task significantly benefits from incorporating higher resolution images. Therefore, techniques such as token reduction have emerged to reduce the computational cost. However, dropping tokens leads to loss of essential information for fine-grained categories, specially as the token keep rate is reduced. Therefore, to counteract the loss of information brought by the usage of token reduction we propose a novel Cross-Layer Aggregation Classification Head and a Cross-Layer Cache mechanism to recover and access information from previous layers in later locations. Extensive experiments covering more than 2000 runs across diverse settings including 5 datasets, 9 backbones, 7 token reduction methods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the proposed plug-and-play modules and allow us to push the boundaries of accuracy vs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to 10\\% while maintaining a competitive accuracy to state-of-the-art models. Code is available at: \\url{https://github.com/arkel23/CLCA}",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "31",
        "title": "Have We Designed Generalizable Structural Knowledge Promptings? Systematic Evaluation and Rethinking",
        "author": [
            "Yichi Zhang",
            "Zhuo Chen",
            "Lingbing Guo",
            "Yajing Xu",
            "Shaokai Chen",
            "Mengshu Sun",
            "Binbin Hu",
            "Zhiqiang Zhang",
            "Lei Liang",
            "Wen Zhang",
            "Huajun Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00244",
        "abstract": "Large language models (LLMs) have demonstrated exceptional performance in text generation within current NLP research. However, the lack of factual accuracy is still a dark cloud hanging over the LLM skyscraper. Structural knowledge prompting (SKP) is a prominent paradigm to integrate external knowledge into LLMs by incorporating structural representations, achieving state-of-the-art results in many knowledge-intensive tasks. However, existing methods often focus on specific problems, lacking a comprehensive exploration of the generalization and capability boundaries of SKP. This paper aims to evaluate and rethink the generalization capability of the SKP paradigm from four perspectives including Granularity, Transferability, Scalability, and Universality. To provide a thorough evaluation, we introduce a novel multi-granular, multi-level benchmark called SUBARU, consisting of 9 different tasks with varying levels of granularity and difficulty.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "32",
        "title": "EQUATOR: A Deterministic Framework for Evaluating LLM Reasoning with Open-Ended Questions. # v1.0.0-beta",
        "author": [
            "Raymond Bernard",
            "Shaina Raza",
            "Subhabrata Das",
            "Rahul Murugan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00257",
        "abstract": "Despite the remarkable coherence of Large Language Models (LLMs), existing evaluation methods often suffer from fluency bias and rely heavily on multiple-choice formats, making it difficult to assess factual accuracy and complex reasoning effectively. LLMs thus frequently generate factually inaccurate responses, especially in complex reasoning tasks, highlighting two prominent challenges: (1) the inadequacy of existing methods to evaluate reasoning and factual accuracy effectively, and (2) the reliance on human evaluators for nuanced judgment, as illustrated by Williams and Huckle (2024)[1], who found manual grading indispensable despite automated grading advancements.\nTo address evaluation gaps in open-ended reasoning tasks, we introduce the EQUATOR Evaluator (Evaluation of Question Answering Thoroughness in Open-ended Reasoning). This framework combines deterministic scoring with a focus on factual accuracy and robust reasoning assessment. Using a vector database, EQUATOR pairs open-ended questions with human-evaluated answers, enabling more precise and scalable evaluations. In practice, EQUATOR significantly reduces reliance on human evaluators for scoring and improves scalability compared to Williams and Huckle's (2004)[1] methods.\nOur results demonstrate that this framework significantly outperforms traditional multiple-choice evaluations while maintaining high accuracy standards. Additionally, we introduce an automated evaluation process leveraging smaller, locally hosted LLMs. We used LLaMA 3.2B, running on the Ollama binaries to streamline our assessments. This work establishes a new paradigm for evaluating LLM performance, emphasizing factual accuracy and reasoning ability, and provides a robust methodological foundation for future research.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "33",
        "title": "A structure-preserving collisional particle method for the Landau kinetic equation",
        "author": [
            "Kai Du",
            "Lei Li",
            "Yongle Xie",
            "Yang Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00263",
        "abstract": "In this paper, we propose and implement a structure-preserving stochastic particle method for the Landau equation. The method is based on a particle system for the Landau equation, where pairwise grazing collisions are modeled as diffusion processes. By exploiting the unique structure of the particle system and a spherical Brownian motion sampling, the method avoids additional temporal discretization of the particle system, ensuring that the discrete-time particle distributions exactly match their continuous-time counterparts. The method achieves $O(N)$ complexity per time step and preserves fundamental physical properties, including the conservation of mass, momentum and energy, as well as entropy dissipation. It demonstrates strong long-time accuracy and stability in numerical experiments. Furthermore, we also apply the method to the spatially non-homogeneous equations through a case study of the Vlasov--Poisson--Landau equation.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "34",
        "title": "A review of faithfulness metrics for hallucination assessment in Large Language Models",
        "author": [
            "Ben Malin",
            "Tatiana Kalganova",
            "Nikoloas Boulgouris"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00269",
        "abstract": "This review examines the means with which faithfulness has been evaluated across open-ended summarization, question-answering and machine translation tasks. We find that the use of LLMs as a faithfulness evaluator is commonly the metric that is most highly correlated with human judgement. The means with which other studies have mitigated hallucinations is discussed, with both retrieval augmented generation (RAG) and prompting framework approaches having been linked with superior faithfulness, whilst other recommendations for mitigation are provided. Research into faithfulness is integral to the continued widespread use of LLMs, as unfaithful responses can pose major risks to many areas whereby LLMs would otherwise be suitable. Furthermore, evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which can help in advancing the trust that can be placed within LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "35",
        "title": "Echoes in AI: Quantifying Lack of Plot Diversity in LLM Outputs",
        "author": [
            "Weijia Xu",
            "Nebojsa Jojic",
            "Sudha Rao",
            "Chris Brockett",
            "Bill Dolan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00273",
        "abstract": "With rapid advances in large language models (LLMs), there has been an increasing application of LLMs in creative content ideation and generation. A critical question emerges: can current LLMs provide ideas that are diverse enough to truly bolster the collective creativity? We examine two state-of-the-art LLMs, GPT-4 and LLaMA-3, on story generation and discover that LLM-generated stories often consist of plot elements that are echoed across a number of generations. To quantify this phenomenon, we introduce the Sui Generis score, which estimates how unlikely a plot element is to appear in alternative storylines generated by the same LLM. Evaluating on 100 short stories, we find that LLM-generated stories often contain combinations of idiosyncratic plot elements echoed frequently across generations, while the original human-written stories are rarely recreated or even echoed in pieces. Moreover, our human evaluation shows that the ranking of Sui Generis scores among story segments correlates moderately with human judgment of surprise level, even though score computation is completely automatic without relying on human judgment.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "36",
        "title": "ReFormer: Generating Radio Fakes for Data Augmentation",
        "author": [
            "Yagna Kaasaragadda",
            "Silvija Kokalj-Filipovic"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00282",
        "abstract": "We present ReFormer, a generative AI (GAI) model that can efficiently generate synthetic radio-frequency (RF) data, or RF fakes, statistically similar to the data it was trained on, or with modified statistics, in order to augment datasets collected in real-world experiments. For applications like this, adaptability and scalability are important issues. This is why ReFormer leverages transformer-based autoregressive generation, trained on learned discrete representations of RF signals. By using prompts, such GAI can be made to generate the data which complies with specific constraints or conditions, particularly useful for training channel estimation and modeling. It may also leverage the data from a source system to generate training data for a target system. We show how different transformer architectures and other design choices affect the quality of generated RF fakes, evaluated using metrics such as precision and recall, classification accuracy and signal constellation diagrams.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "37",
        "title": "Dual Diffusion for Unified Image Generation and Understanding",
        "author": [
            "Zijie Li",
            "Henry Li",
            "Yichun Shi",
            "Amir Barati Farimani",
            "Yuval Kluger",
            "Linjie Yang",
            "Peng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00289",
        "abstract": "Diffusion models have gained tremendous success in text-to-image generation, yet still lag behind with visual understanding tasks, an area dominated by autoregressive vision-language models. We propose a large-scale and fully end-to-end diffusion model for multi-modal understanding and generation that significantly improves on existing diffusion-based multimodal models, and is the first of its kind to support the full suite of vision-language modeling capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and recent advances in discrete diffusion language modeling, we leverage a cross-modal maximum likelihood estimation framework that simultaneously trains the conditional likelihoods of both images and text jointly under a single loss function, which is back-propagated through both branches of the diffusion transformer. The resulting model is highly flexible and capable of a wide range of tasks including image generation, captioning, and visual question answering. Our model attained competitive performance compared to recent unified image understanding and generation models, demonstrating the potential of multimodal diffusion modeling as a promising alternative to autoregressive next-token prediction models.",
        "tags": [
            "DiT",
            "Diffusion",
            "Diffusion Transformer",
            "Text-to-Image",
            "Transformer"
        ]
    },
    {
        "id": "38",
        "title": "diffIRM: A Diffusion-Augmented Invariant Risk Minimization Framework for Spatiotemporal Prediction over Graphs",
        "author": [
            "Zhaobin Mo",
            "Haotian Xiang",
            "Xuan Di"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00305",
        "abstract": "Spatiotemporal prediction over graphs (STPG) is challenging, because real-world data suffers from the Out-of-Distribution (OOD) generalization problem, where test data follow different distributions from training ones. To address this issue, Invariant Risk Minimization (IRM) has emerged as a promising approach for learning invariant representations across different environments. However, IRM and its variants are originally designed for Euclidean data like images, and may not generalize well to graph-structure data such as spatiotemporal graphs due to spatial correlations in graphs. To overcome the challenge posed by graph-structure data, the existing graph OOD methods adhere to the principles of invariance existence, or environment diversity. However, there is little research that combines both principles in the STPG problem. A combination of the two is crucial for efficiently distinguishing between invariant features and spurious ones. In this study, we fill in this research gap and propose a diffusion-augmented invariant risk minimization (diffIRM) framework that combines these two principles for the STPG problem. Our diffIRM contains two processes: i) data augmentation and ii) invariant learning. In the data augmentation process, a causal mask generator identifies causal features and a graph-based diffusion model acts as an environment augmentor to generate augmented spatiotemporal graph data. In the invariant learning process, an invariance penalty is designed using the augmented data, and then serves as a regularizer for training the spatiotemporal prediction model. The real-world experiment uses three human mobility datasets, i.e. SafeGraph, PeMS04, and PeMS08. Our proposed diffIRM outperforms baselines.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "39",
        "title": "MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models",
        "author": [
            "Mahir Labib Dihan",
            "Md Tanvir Hassan",
            "Md Tanvir Parvez",
            "Md Hasebul Hasan",
            "Md Almash Alam",
            "Muhammad Aamir Cheema",
            "Mohammed Eunus Ali",
            "Md Rizwan Parvez"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00316",
        "abstract": "Recent advancements in foundation models have enhanced AI systems' capabilities in autonomous tool usage and reasoning. However, their ability in location or map-based reasoning - which improves daily life by optimizing navigation, facilitating resource discovery, and streamlining logistics - has not been systematically studied. To bridge this gap, we introduce MapEval, a benchmark designed to assess diverse and complex map-based user queries with geo-spatial reasoning. MapEval features three task types (textual, API-based, and visual) that require collecting world information via map tools, processing heterogeneous geo-spatial contexts (e.g., named entities, travel distances, user reviews or ratings, images), and compositional reasoning, which all state-of-the-art foundation models find challenging. Comprising 700 unique multiple-choice questions about locations across 180 cities and 54 countries, MapEval evaluates foundation models' ability to handle spatial relationships, map infographics, travel planning, and navigation challenges. Using MapEval, we conducted a comprehensive evaluation of 28 prominent foundation models. While no single model excelled across all tasks, Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro achieved competitive performance overall. However, substantial performance gaps emerged, particularly in MapEval, where agents with Claude-3.5-Sonnet outperformed GPT-4o and Gemini-1.5-Pro by 16% and 21%, respectively, and the gaps became even more amplified when compared to open-source LLMs. Our detailed analyses provide insights into the strengths and weaknesses of current models, though all models still fall short of human performance by more than 20% on average, struggling with complex map images and rigorous geo-spatial reasoning. This gap highlights MapEval's critical role in advancing general-purpose foundation models with stronger geo-spatial understanding.",
        "tags": [
            "GPT",
            "LLMs"
        ]
    },
    {
        "id": "40",
        "title": "Exploring the Implicit Semantic Ability of Multimodal Large Language Models: A Pilot Study on Entity Set Expansion",
        "author": [
            "Hebin Wang",
            "Yangning Li",
            "Yinghui Li",
            "Hai-Tao Zheng",
            "Wenhao Jiang",
            "Hong-Gee Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00330",
        "abstract": "The rapid development of multimodal large language models (MLLMs) has brought significant improvements to a wide range of tasks in real-world applications. However, LLMs still exhibit certain limitations in extracting implicit semantic information. In this paper, we apply MLLMs to the Multi-modal Entity Set Expansion (MESE) task, which aims to expand a handful of seed entities with new entities belonging to the same semantic class, and multi-modal information is provided with each entity. We explore the capabilities of MLLMs to understand implicit semantic information at the entity-level granularity through the MESE task, introducing a listwise ranking method LUSAR that maps local scores to global rankings. Our LUSAR demonstrates significant improvements in MLLM's performance on the MESE task, marking the first use of generative MLLM for ESE tasks and extending the applicability of listwise ranking.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "41",
        "title": "MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation",
        "author": [
            "Chia-Yuan Chang",
            "Zhimeng Jiang",
            "Vineeth Rakesh",
            "Menghai Pan",
            "Chin-Chia Michael Yeh",
            "Guanchu Wang",
            "Mingzhi Hu",
            "Zhichao Xu",
            "Yan Zheng",
            "Mahashweta Das",
            "Na Zou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00332",
        "abstract": "Large Language Models (LLMs) are becoming essential tools for various natural language processing tasks but often suffer from generating outdated or incorrect information. Retrieval-Augmented Generation (RAG) addresses this issue by incorporating external, real-time information retrieval to ground LLM responses. However, the existing RAG systems frequently struggle with the quality of retrieval documents, as irrelevant or noisy documents degrade performance, increase computational overhead, and undermine response reliability. To tackle this problem, we propose Multi-Agent Filtering Retrieval-Augmented Generation (MAIN-RAG), a training-free RAG framework that leverages multiple LLM agents to collaboratively filter and score retrieved documents. Specifically, MAIN-RAG introduces an adaptive filtering mechanism that dynamically adjusts the relevance filtering threshold based on score distributions, effectively minimizing noise while maintaining high recall of relevant documents. The proposed approach leverages inter-agent consensus to ensure robust document selection without requiring additional training data or fine-tuning. Experimental results across four QA benchmarks demonstrate that MAIN-RAG consistently outperforms traditional RAG approaches, achieving a 2-11% improvement in answer accuracy while reducing the number of irrelevant retrieved documents. Quantitative analysis further reveals that our approach achieves superior response consistency and answer accuracy over baseline methods, offering a competitive and practical alternative to training-based solutions.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "42",
        "title": "Rethinking Layer Removal: Preserving Critical Components with Task-Aware Singular Value Decomposition",
        "author": [
            "Kainan Liu",
            "Yong Zhang",
            "Ning Cheng",
            "Zhitao Li",
            "Shaojun Wang",
            "Jing Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00339",
        "abstract": "Layer removal has emerged as a promising approach for compressing large language models (LLMs) by leveraging redundancy within layers to reduce model size and accelerate inference. However, this technique often compromises internal consistency, leading to performance degradation and instability, with varying impacts across different model architectures. In this work, we propose Taco-SVD, a task-aware framework that retains task-critical singular value directions, preserving internal consistency while enabling efficient compression. Unlike direct layer removal, Taco-SVD preserves task-critical transformations to mitigate performance degradation. By leveraging gradient-based attribution methods, Taco-SVD aligns singular values with downstream task objectives. Extensive evaluations demonstrate that Taco-SVD outperforms existing methods in perplexity and task performance across different architectures while ensuring minimal computational overhead.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "43",
        "title": "Dynamic Prompt Adjustment for Multi-Label Class-Incremental Learning",
        "author": [
            "Haifeng Zhao",
            "Yuguang Jin",
            "Leilei Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00340",
        "abstract": "Significant advancements have been made in single label incremental learning (SLCIL),yet the more practical and challenging multi label class incremental learning (MLCIL) remains understudied. Recently,visual language models such as CLIP have achieved good results in classification tasks. However,directly using CLIP to solve MLCIL issue can lead to catastrophic forgetting. To tackle this issue, we integrate an improved data replay mechanism and prompt loss to curb knowledge forgetting. Specifically,our model enhances the prompt information to better adapt to multi-label classification tasks and employs confidence-based replay strategy to select representative samples. Moreover, the prompt loss significantly reduces the model's forgetting of previous knowledge. Experimental results demonstrate that our method has substantially improved the performance of MLCIL tasks across multiple benchmark datasets,validating its effectiveness.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "44",
        "title": "SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians",
        "author": [
            "Yiwen Wang",
            "Siyuan Chen",
            "Ran Yi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00342",
        "abstract": "3D Gaussian Splatting is emerging as a state-of-the-art technique in novel view synthesis, recognized for its impressive balance between visual quality, speed, and rendering efficiency. However, reliance on third-degree spherical harmonics for color representation introduces significant storage demands and computational overhead, resulting in a large memory footprint and slower rendering speed. We introduce SG-Splatting with Spherical Gaussians based color representation, a novel approach to enhance rendering speed and quality in novel view synthesis. Our method first represents view-dependent color using Spherical Gaussians, instead of three degree spherical harmonics, which largely reduces the number of parameters used for color representation, and significantly accelerates the rendering process. We then develop an efficient strategy for organizing multiple Spherical Gaussians, optimizing their arrangement to achieve a balanced and accurate scene representation. To further improve rendering quality, we propose a mixed representation that combines Spherical Gaussians with low-degree spherical harmonics, capturing both high- and low-frequency color information effectively. SG-Splatting also has plug-and-play capability, allowing it to be easily integrated into existing systems. This approach improves computational efficiency and overall visual fidelity, making it a practical solution for real-time applications.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "45",
        "title": "Chunk-Distilled Language Modeling",
        "author": [
            "Yanhong Li",
            "Karen Livescu",
            "Jiawei Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00343",
        "abstract": "We introduce Chunk-Distilled Language Modeling (CD-LM), an approach to text generation that addresses two challenges in current large language models (LLMs): the inefficiency of token-level generation, and the difficulty of adapting to new data and knowledge. Our method combines deep network-based LLMs with a straightforward retrieval module, which allows the generation of multi-token text chunks at a single decoding step. Our retrieval framework enables flexible construction of model- or domain-specific datastores, either leveraging the internal knowledge of existing models, or incorporating expert insights from human-annotated corpora. This adaptability allows for enhanced control over the language model's distribution without necessitating additional training. We present the CD-LM formulation along with performance metrics demonstrating its ability to improve language model performance and efficiency across a diverse set of downstream tasks. Code and data will be made publicly available.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "46",
        "title": "PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM",
        "author": [
            "Runnan Chen",
            "Zhaoqing Wang",
            "Jiepeng Wang",
            "Yuexin Ma",
            "Mingming Gong",
            "Wenping Wang",
            "Tongliang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00352",
        "abstract": "Understanding geometric, semantic, and instance information in 3D scenes from sequential video data is essential for applications in robotics and augmented reality. However, existing Simultaneous Localization and Mapping (SLAM) methods generally focus on either geometric or semantic reconstruction. In this paper, we introduce PanoSLAM, the first SLAM system to integrate geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation within a unified framework. Our approach builds upon 3D Gaussian Splatting, modified with several critical components to enable efficient rendering of depth, color, semantic, and instance information from arbitrary viewpoints. To achieve panoptic 3D scene reconstruction from sequential RGB-D videos, we propose an online Spatial-Temporal Lifting (STL) module that transfers 2D panoptic predictions from vision models into 3D Gaussian representations. This STL module addresses the challenges of label noise and inconsistencies in 2D predictions by refining the pseudo labels across multi-view inputs, creating a coherent 3D representation that enhances segmentation accuracy. Our experiments show that PanoSLAM outperforms recent semantic SLAM methods in both mapping and tracking accuracy. For the first time, it achieves panoptic 3D reconstruction of open-world environments directly from the RGB-D video. (https://github.com/runnanchen/PanoSLAM)",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Robotics",
            "SLAM",
            "Segmentation"
        ]
    },
    {
        "id": "47",
        "title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented Instructions",
        "author": [
            "Wanlong Liu",
            "Junying Chen",
            "Ke Ji",
            "Li Zhou",
            "Wenyu Chen",
            "Benyou Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00353",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a key paradigm for enhancing large language models (LLMs) by incorporating external knowledge. However, current RAG methods face two limitations: (1) they only cover limited RAG scenarios. (2) They suffer from limited task diversity due to the lack of a general RAG dataset. To address these limitations, we propose RAG-Instruct, a general method for synthesizing diverse and high-quality RAG instruction data based on any source corpus. Our approach leverages (1) five RAG paradigms, which encompass diverse query-document relationships, and (2) instruction simulation, which enhances instruction diversity and quality by utilizing the strengths of existing instruction datasets. Using this method, we construct a 40K instruction dataset from Wikipedia, comprehensively covering diverse RAG scenarios and tasks. Experiments demonstrate that RAG-Instruct effectively enhances LLMs' RAG capabilities, achieving strong zero-shot performance and significantly outperforming various RAG baselines across a diverse set of tasks. RAG-Instruct is publicly available at https://github.com/FreedomIntelligence/RAG-Instruct.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "48",
        "title": "\"I Recall the Past\": Exploring How People Collaborate with Generative AI to Create Cultural Heritage Narratives",
        "author": [
            "Zhiting He",
            "Jiayi Su",
            "Li Chen",
            "Tianqi Wang",
            "Ray LC"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00359",
        "abstract": "Visitors to cultural heritage sites often encounter official information, while local people's unofficial stories remain invisible. To explore expression of local narratives, we conducted a workshop with 20 participants utilizing Generative AI (GenAI) to support visual narratives, asking them to use Stable Diffusion to create images of familiar cultural heritage sites, as well as images of unfamiliar ones for comparison. The results revealed three narrative strategies and highlighted GenAI's strengths in illuminating, amplifying, and reinterpreting personal narratives. However, GenAI showed limitations in meeting detailed requirements, portraying cultural features, and avoiding bias, which were particularly pronounced with unfamiliar sites due to participants' lack of local knowledge. To address these challenges, we recommend providing detailed explanations, prompt engineering, and fine-tuning AI models to reduce uncertainties, using objective references to mitigate inaccuracies from participants' inability to recognize errors or misconceptions, and curating datasets to train AI models capable of accurately portraying cultural features.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "49",
        "title": "SPDZCoder: Teaching LLMs to Synthesize Privacy Computing Code without Massive Training Data",
        "author": [
            "Xiaoning Dong",
            "Peilin Xin",
            "Wei Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00363",
        "abstract": "Privacy computing receives increasing attention but writing privacy computing code remains challenging for developers due to limited library functions that necessitate extensive function implementation from scratch as well as the data-oblivious requirement which contradicts intuitive thinking and usual practices of programmers. Large language models (LLMs) have demonstrated surprising capabilities in coding tasks and achieved state-of-the-art performance across many benchmarks. However, even with extensive prompting, existing LLMs struggle with code translation task for privacy computing, such as translating Python to MP-SPDZ, due to the scarcity of MP-SPDZ data required for effective pre-training or fine-tuning. To address the limitation, this paper proposes SPDZCoder, a rule-based framework to teach LLMs to synthesize privacy computing code without asking experts to write tons of code and by leveraging the instruction-following and in-context learning ability of LLMs. Specifically, SPDZCoder decouples the translation task into the refactoring stage and the generation stage, which can mitigate the semantic-expressing differences at different levels. In addition, SPDZCoder can further improve its performance by a feedback stage. SPDZCoder does not require fine-tuning since it adopts an in-context learning paradigm of LLMs. To evaluate SPDZCoder, we manually created a benchmark dataset, named SPDZEval, containing six classes of difficult tasks to implement in MP-SPDZ. We conduct experiments on SPDZEval and the experimental results shows that SPDZCoder achieves the state-of-the-art performance in pass@1 and pass@2 across six data splits. Specifically, SPDZCoder achieves an overall correctness of 85.94% and 92.01% in pass@1 and pass@2, respectively, significantly surpassing baselines (at most 30.35% and 49.84% in pass@1 and pass@2, respectively) by a large margin.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "50",
        "title": "Low-Rank Adaptation for Foundation Models: A Comprehensive Review",
        "author": [
            "Menglin Yang",
            "Jialin Chen",
            "Yifei Zhang",
            "Jiahong Liu",
            "Jiasheng Zhang",
            "Qiyao Ma",
            "Harshit Verma",
            "Qianru Zhang",
            "Min Zhou",
            "Irwin King",
            "Rex Ying"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00365",
        "abstract": "The rapid advancement of foundation modelslarge-scale neural networks trained on diverse, extensive datasetshas revolutionized artificial intelligence, enabling unprecedented advancements across domains such as natural language processing, computer vision, and scientific discovery. However, the substantial parameter count of these models, often reaching billions or trillions, poses significant challenges in adapting them to specific downstream tasks. Low-Rank Adaptation (LoRA) has emerged as a highly promising approach for mitigating these challenges, offering a parameter-efficient mechanism to fine-tune foundation models with minimal computational overhead. This survey provides the first comprehensive review of LoRA techniques beyond large Language Models to general foundation models, including recent techniques foundations, emerging frontiers and applications of low-rank adaptation across multiple domains. Finally, this survey discusses key challenges and future research directions in theoretical understanding, scalability, and robustness. This survey serves as a valuable resource for researchers and practitioners working with efficient foundation model adaptation.",
        "tags": [
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "51",
        "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable Diffusion for Free",
        "author": [
            "Evelyn Zhang",
            "Bang Xiao",
            "Jiayi Tang",
            "Qianli Ma",
            "Chang Zou",
            "Xuefei Ning",
            "Xuming Hu",
            "Linfeng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00375",
        "abstract": "Stable Diffusion has achieved remarkable success in the field of text-to-image generation, with its powerful generative capabilities and diverse generation results making a lasting impact. However, its iterative denoising introduces high computational costs and slows generation speed, limiting broader adoption. The community has made numerous efforts to reduce this computational burden, with methods like feature caching attracting attention due to their effectiveness and simplicity. Nonetheless, simply reusing features computed at previous timesteps causes the features across adjacent timesteps to become similar, reducing the dynamics of features over time and ultimately compromising the quality of generated images. In this paper, we introduce a dynamics-aware token pruning (DaTo) approach that addresses the limitations of feature caching. DaTo selectively prunes tokens with lower dynamics, allowing only high-dynamic tokens to participate in self-attention layers, thereby extending feature dynamics across timesteps. DaTo combines feature caching with token pruning in a training-free manner, achieving both temporal and token-wise information reuse. Applied to Stable Diffusion on the ImageNet, our approach delivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced image quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled with a notable FID reduction of 2.17.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "52",
        "title": "S-Diff: An Anisotropic Diffusion Model for Collaborative Filtering in Spectral Domain",
        "author": [
            "Rui Xia",
            "Yanhua Cheng",
            "Yongxiang Tang",
            "Xiaocheng Liu",
            "Xialong Liu",
            "Lisong Wang",
            "Peng Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00384",
        "abstract": "Recovering user preferences from user-item interaction matrices is a key challenge in recommender systems. While diffusion models can sample and reconstruct preferences from latent distributions, they often fail to capture similar users' collective preferences effectively. Additionally, latent variables degrade into pure Gaussian noise during the forward process, lowering the signal-to-noise ratio, which in turn degrades performance. To address this, we propose S-Diff, inspired by graph-based collaborative filtering, better to utilize low-frequency components in the graph spectral domain. S-Diff maps user interaction vectors into the spectral domain and parameterizes diffusion noise to align with graph frequency. This anisotropic diffusion retains significant low-frequency components, preserving a high signal-to-noise ratio. S-Diff further employs a conditional denoising network to encode user interactions, recovering true preferences from noisy data. This method achieves strong results across multiple datasets.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "53",
        "title": "Efficient Relational Context Perception for Knowledge Graph Completion",
        "author": [
            "Wenkai Tu",
            "Guojia Wan",
            "Zhengchun Shang",
            "Bo Du"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00397",
        "abstract": "Knowledge Graphs (KGs) provide a structured representation of knowledge but often suffer from challenges of incompleteness. To address this, link prediction or knowledge graph completion (KGC) aims to infer missing new facts based on existing facts in KGs. Previous knowledge graph embedding models are limited in their ability to capture expressive features, especially when compared to deeper, multi-layer models. These approaches also assign a single static embedding to each entity and relation, disregarding the fact that entities and relations can exhibit different behaviors in varying graph contexts. Due to complex context over a fact triple of a KG, existing methods have to leverage complex non-linear context encoder, like transformer, to project entity and relation into low dimensional representations, resulting in high computation cost. To overcome these limitations, we propose Triple Receptance Perception (TRP) architecture to model sequential information, enabling the learning of dynamic context of entities and relations. Then we use tensor decomposition to calculate triple scores, providing robust relational decoding capabilities. This integration allows for more expressive representations. Experiments on benchmark datasets such as YAGO3-10, UMLS, FB15k, and FB13 in link prediction and triple classification tasks demonstrate that our method performs better than several state-of-the-art models, proving the effectiveness of the integration.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "54",
        "title": "Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models",
        "author": [
            "Martin Pawelczyk",
            "Lillian Sun",
            "Zhenting Qi",
            "Aounon Kumar",
            "Himabindu Lakkaraju"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00418",
        "abstract": "The rapid proliferation of generative AI, especially large language models, has led to their integration into a variety of applications. A key phenomenon known as weak-to-strong generalization - where a strong model trained on a weak model's outputs surpasses the weak model in task performance - has gained significant attention. Yet, whether critical trustworthiness properties such as robustness, fairness, and privacy can generalize similarly remains an open question. In this work, we study this question by examining if a stronger model can inherit trustworthiness properties when fine-tuned on a weaker model's outputs, a process we term weak-to-strong trustworthiness generalization. To address this, we introduce two foundational training strategies: 1) Weak Trustworthiness Finetuning (Weak TFT), which leverages trustworthiness regularization during the fine-tuning of the weak model, and 2) Weak and Weak-to-Strong Trustworthiness Finetuning (Weak+WTS TFT), which extends regularization to both weak and strong models. Our experimental evaluation on real-world datasets reveals that while some trustworthiness properties, such as fairness, adversarial, and OOD robustness, show significant improvement in transfer when both models were regularized, others like privacy do not exhibit signs of weak-to-strong trustworthiness. As the first study to explore trustworthiness generalization via weak-to-strong generalization, our work provides valuable insights into the potential and limitations of weak-to-strong generalization.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "55",
        "title": "Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and Reflection agents",
        "author": [
            "Chengbo He",
            "Bochao Zou",
            "Xin Li",
            "Jiansheng Chen",
            "Junliang Xing",
            "Huimin Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00430",
        "abstract": "Agents have demonstrated their potential in scientific reasoning tasks through large language models. However, they often face challenges such as insufficient accuracy and degeneration of thought when handling complex reasoning tasks, which impede their performance. To overcome these issues, we propose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP) Framework, aimed at enhancing the reasoning capabilities of LLMs. Our approach improves scientific reasoning accuracy by employing a multi-path reasoning mechanism where each path consists of a reactive agent and a reflection agent that collaborate to prevent degeneration of thought inherent in single-agent reliance. Additionally, the RR-MP framework does not require additional training; it utilizes multiple dialogue instances for each reasoning path and a separate summarizer to consolidate insights from all paths. This design integrates diverse perspectives and strengthens reasoning across each path. We conducted zero-shot and few-shot evaluations on tasks involving moral scenarios, college-level physics, and mathematics. Experimental results demonstrate that our method outperforms baseline approaches, highlighting the effectiveness and advantages of the RR-MP framework in managing complex scientific reasoning tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "56",
        "title": "OV-HHIR: Open Vocabulary Human Interaction Recognition Using Cross-modal Integration of Large Language Models",
        "author": [
            "Lala Shakti Swarup Ray",
            "Bo Zhou",
            "Sungho Suh",
            "Paul Lukowicz"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00432",
        "abstract": "Understanding human-to-human interactions, especially in contexts like public security surveillance, is critical for monitoring and maintaining safety. Traditional activity recognition systems are limited by fixed vocabularies, predefined labels, and rigid interaction categories that often rely on choreographed videos and overlook concurrent interactive groups. These limitations make such systems less adaptable to real-world scenarios, where interactions are diverse and unpredictable. In this paper, we propose an open vocabulary human-to-human interaction recognition (OV-HHIR) framework that leverages large language models to generate open-ended textual descriptions of both seen and unseen human interactions in open-world settings without being confined to a fixed vocabulary. Additionally, we create a comprehensive, large-scale human-to-human interaction dataset by standardizing and combining existing public human interaction datasets into a unified benchmark. Extensive experiments demonstrate that our method outperforms traditional fixed-vocabulary classification systems and existing cross-modal language models for video understanding, setting the stage for more intelligent and adaptable visual understanding systems in surveillance and beyond.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "57",
        "title": "Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning",
        "author": [
            "Jianjie Luo",
            "Jingwen Chen",
            "Yehao Li",
            "Yingwei Pan",
            "Jianlin Feng",
            "Hongyang Chao",
            "Ting Yao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00437",
        "abstract": "Recently, zero-shot image captioning has gained increasing attention, where only text data is available for training. The remarkable progress in text-to-image diffusion model presents the potential to resolve this task by employing synthetic image-caption pairs generated by this pre-trained prior. Nonetheless, the defective details in the salient regions of the synthetic images introduce semantic misalignment between the synthetic image and text, leading to compromised results. To address this challenge, we propose a novel Patch-wise Cross-modal feature Mix-up (PCM) mechanism to adaptively mitigate the unfaithful contents in a fine-grained manner during training, which can be integrated into most of encoder-decoder frameworks, introducing our PCM-Net. Specifically, for each input image, salient visual concepts in the image are first detected considering the image-text similarity in CLIP space. Next, the patch-wise visual features of the input image are selectively fused with the textual features of the salient visual concepts, leading to a mixed-up feature map with less defective content. Finally, a visual-semantic encoder is exploited to refine the derived feature map, which is further incorporated into the sentence decoder for caption generation. Additionally, to facilitate the model training with synthetic data, a novel CLIP-weighted cross-entropy loss is devised to prioritize the high-quality image-text pairs over the low-quality counterparts. Extensive experiments on MSCOCO and Flickr30k datasets demonstrate the superiority of our PCM-Net compared with state-of-the-art VLMs-based approaches. It is noteworthy that our PCM-Net ranks first in both in-domain and cross-domain zero-shot image captioning. The synthetic dataset SynthImgCap and code are available at https://jianjieluo.github.io/SynthImgCap.",
        "tags": [
            "CLIP",
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "58",
        "title": "Unrolled Creative Adversarial Network For Generating Novel Musical Pieces",
        "author": [
            "Pratik Nag"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00452",
        "abstract": "Music generation has been established as a prominent topic in artificial intelligence and machine learning over recent years. In most recent works on RNN-based neural network methods have been applied for sequence generation. In contrast, generative adversarial networks (GANs) and their counterparts have been explored by very few researchersfor music generation.\nIn this paper, a classical system was employed alongside a new system to generate creative music. Both systems were designed based on adversarial networks to generate music by learning from examples. The classical system was trained to learn a set of music pieces without differentiating between classes, whereas the new system was trained to learn the different composers and their styles to generate a creative music piece by deviating from the learned composers' styles.\nThe base structure utilized was generative adversarial networks (GANs), which are capable of generating novel outputs given a set of inputs to learn from and mimic their distribution. It has been shown in previous work that GANs are limited in their original design with respect to creative outputs. Building on the Creative Adversarial Networks (CAN) , this work applied them in the music domain rather than the visual art domain. Additionally, unrolled CAN was introduced to prevent mode collapse. Experiments were conducted on both GAN and CAN for generating music, and their capabilities were measured in terms of deviation from the input set.",
        "tags": [
            "GAN",
            "RNN"
        ]
    },
    {
        "id": "59",
        "title": "Differentiable Prompt Learning for Vision Language Models",
        "author": [
            "Zhenhan Huang",
            "Tejaswini Pedapati",
            "Pin-Yu Chen",
            "Jianxi Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00457",
        "abstract": "Prompt learning is an effective way to exploit the potential of large-scale pre-trained foundational models. Continuous prompts parameterize context tokens in prompts by turning them into differentiable vectors. Deep continuous prompts insert prompts not only in the input but also in the intermediate hidden representations. Manually designed deep continuous prompts exhibit a remarkable improvement compared to the zero-shot pre-trained model on downstream tasks. How to automate the continuous prompt design is an underexplored area, and a fundamental question arises, is manually designed deep prompt strategy optimal? To answer this question, we propose a method dubbed differentiable prompt learning (DPL). The DPL method is formulated as an optimization problem to automatically determine the optimal context length of the prompt to be added to each layer, where the objective is to maximize the performance. We test the DPL method on the pre-trained CLIP. We empirically find that by using only limited data, our DPL method can find deep continuous prompt configuration with high confidence. The performance on the downstream tasks exhibits the superiority of the automatic design: our method boosts the average test accuracy by 2.60% on 11 datasets compared to baseline methods. Besides, our method focuses only on the prompt configuration (i.e. context length for each layer), which means that our method is compatible with the baseline methods that have sophisticated designs to boost the performance. The DPL method can be deployed to large language models or computer vision models at no cost.",
        "tags": [
            "CLIP",
            "Large Language Models"
        ]
    },
    {
        "id": "60",
        "title": "SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion Models with Self-Augmented Training",
        "author": [
            "Lu Zhang",
            "Liang Zeng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00463",
        "abstract": "The proliferation of AI-generated images necessitates effective watermarking to protect intellectual property and identify fake content. While existing training-based watermarking methods show promise, they often struggle with generalization across diverse prompts and tend to produce noticeable artifacts. To this end, we introduce a provably generalizable image watermarking method for Latent Diffusion Models with Self-Augmented Training (SAT-LDM), which aligns the training and testing phases by a free generation distribution to bolster the watermarking module's generalization capabilities. We theoretically consolidate our method by proving that the free generation distribution contributes to its tight generalization bound without the need to collect new data. Extensive experimental results show that SAT-LDM achieves robust watermarking while significantly improving the quality of watermarked images across diverse prompts. Furthermore, we conduct experimental analyses to demonstrate the strong generalization abilities of SAT-LDM. We hope our method offers a practical and convenient solution for securing high-fidelity AI-generated content.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "61",
        "title": "Addressing Challenges in Data Quality and Model Generalization for Malaria Detection",
        "author": [
            "Kiswendsida Kisito Kabore",
            "Desire Guel"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00464",
        "abstract": "Malaria remains a significant global health burden, particularly in resource-limited regions where timely and accurate diagnosis is critical to effective treatment and control. Deep Learning (DL) has emerged as a transformative tool for automating malaria detection and it offers high accuracy and scalability. However, the effectiveness of these models is constrained by challenges in data quality and model generalization including imbalanced datasets, limited diversity and annotation variability. These issues reduce diagnostic reliability and hinder real-world applicability.\nThis article provides a comprehensive analysis of these challenges and their implications for malaria detection performance. Key findings highlight the impact of data imbalances which can lead to a 20\\% drop in F1-score and regional biases which significantly hinder model generalization. Proposed solutions, such as GAN-based augmentation, improved accuracy by 15-20\\% by generating synthetic data to balance classes and enhance dataset diversity. Domain adaptation techniques, including transfer learning, further improved cross-domain robustness by up to 25\\% in sensitivity.\nAdditionally, the development of diverse global datasets and collaborative data-sharing frameworks is emphasized as a cornerstone for equitable and reliable malaria diagnostics. The role of explainable AI techniques in improving clinical adoption and trustworthiness is also underscored. By addressing these challenges, this work advances the field of AI-driven malaria detection and provides actionable insights for researchers and practitioners. The proposed solutions aim to support the development of accessible and accurate diagnostic tools, particularly for resource-constrained populations.",
        "tags": [
            "Detection",
            "GAN"
        ]
    },
    {
        "id": "62",
        "title": "Score-Based Metropolis-Hastings Algorithms",
        "author": [
            "Ahmed Aloui",
            "Ali Hasan",
            "Juncheng Dong",
            "Zihao Wu",
            "Vahid Tarokh"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00467",
        "abstract": "In this paper, we introduce a new approach for integrating score-based models with the Metropolis-Hastings algorithm. While traditional score-based diffusion models excel in accurately learning the score function from data points, they lack an energy function, making the Metropolis-Hastings adjustment step inaccessible. Consequently, the unadjusted Langevin algorithm is often used for sampling using estimated score functions. The lack of an energy function then prevents the application of the Metropolis-adjusted Langevin algorithm and other Metropolis-Hastings methods, limiting the wealth of other algorithms developed that use acceptance functions. We address this limitation by introducing a new loss function based on the \\emph{detailed balance condition}, allowing the estimation of the Metropolis-Hastings acceptance probabilities given a learned score function. We demonstrate the effectiveness of the proposed method for various scenarios, including sampling from heavy-tail distributions.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "63",
        "title": "Fine-grained Video-Text Retrieval: A New Benchmark and Method",
        "author": [
            "Yifan Xu",
            "Xinhao Li",
            "Yichun Yang",
            "Rui Huang",
            "Limin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00513",
        "abstract": "The ability of perceiving fine-grained spatial and temporal information is crucial for video-language retrieval. However, the existing video retrieval benchmarks, such as MSRVTT and MSVD, fail to efficiently evaluate the fine-grained retrieval ability of video-language models (VLMs) due to a lack of detailed annotations. To address this problem, we present FIBER, a FIne-grained BEnchmark for text to video Retrieval, containing 1,000 videos sourced from the FineAction dataset. Uniquely, our FIBER benchmark provides detailed human-annotated spatial annotations and temporal annotations for each video, making it possible to independently evaluate the spatial and temporal bias of VLMs on video retrieval task. Besides, we employ a text embedding method to unlock the capability of fine-grained video-language understanding of Multimodal Large Language Models (MLLMs). Surprisingly, the experiment results show that our Video Large Language Encoder (VLLE) performs comparably to CLIP-based models on traditional benchmarks and has a stronger capability of fine-grained representation with lower spatial-temporal bias. Project page: https://fiber-bench.github.io.",
        "tags": [
            "CLIP",
            "Large Language Models",
            "Text-to-Video"
        ]
    },
    {
        "id": "64",
        "title": "Is Segment Anything Model 2 All You Need for Surgery Video Segmentation? A Systematic Evaluation",
        "author": [
            "Cheng Yuan",
            "Jian Jiang",
            "Kunyi Yang",
            "Lv Wu",
            "Rui Wang",
            "Zi Meng",
            "Haonan Ping",
            "Ziyu Xu",
            "Yifan Zhou",
            "Wanli Song",
            "Hesheng Wang",
            "Qi Dou",
            "Yutong Ban"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00525",
        "abstract": "Surgery video segmentation is an important topic in the surgical AI field. It allows the AI model to understand the spatial information of a surgical scene. Meanwhile, due to the lack of annotated surgical data, surgery segmentation models suffer from limited performance. With the emergence of SAM2 model, a large foundation model for video segmentation trained on natural videos, zero-shot surgical video segmentation became more realistic but meanwhile remains to be explored. In this paper, we systematically evaluate the performance of SAM2 model in zero-shot surgery video segmentation task. We conducted experiments under different configurations, including different prompting strategies, robustness, etc. Moreover, we conducted an empirical evaluation over the performance, including 9 datasets with 17 different types of surgeries.",
        "tags": [
            "Segment Anything",
            "Segmentation"
        ]
    },
    {
        "id": "65",
        "title": "Sinhala Transliteration: A Comparative Analysis Between Rule-based and Seq2Seq Approaches",
        "author": [
            "Yomal De Mel",
            "Kasun Wickramasinghe",
            "Nisansa de Silva",
            "Surangika Ranathunga"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00529",
        "abstract": "Due to reasons of convenience and lack of tech literacy, transliteration (i.e., Romanizing native scripts instead of using localization tools) is eminently prevalent in the context of low-resource languages such as Sinhala, which have their own writing script. In this study, our focus is on Romanized Sinhala transliteration. We propose two methods to address this problem: Our baseline is a rule-based method, which is then compared against our second method where we approach the transliteration problem as a sequence-to-sequence task akin to the established Neural Machine Translation (NMT) task. For the latter, we propose a Transformer-based Encode-Decoder solution. We witnessed that the Transformer-based method could grab many ad-hoc patterns within the Romanized scripts compared to the rule-based method. The code base associated with this paper is available on GitHub - https://github.com/kasunw22/Sinhala-Transliterator/",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "66",
        "title": "Superposition in Transformers: A Novel Way of Building Mixture of Experts",
        "author": [
            "Ayoub Ben Chaliah",
            "Hela Dellagi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00530",
        "abstract": "Catastrophic forgetting remains a major challenge when adapting large language models (LLMs) to new tasks or domains. Conventional fine-tuning often overwrites existing knowledge, causing performance degradation on original tasks. We introduce Superposition in Transformers, a novel architecture that leverages autoencoders to superimpose the hidden representations of a base model and a fine-tuned model within a shared parameter space. By using B-spline-based blending coefficients and autoencoders that adaptively reconstruct hidden states based on the input data distribution, our method effectively mitigates catastrophic forgetting and enables a new paradigm of \"in-model\" superposition. This approach preserves original model capabilities while allowing compact domain-specific expertise to be added, and it supports dynamic switching between model states during inference.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "67",
        "title": "MCP-Solver: Integrating Language Models with Constraint Programming Systems",
        "author": [
            "Stefan Szeider"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00539",
        "abstract": "While Large Language Models (LLMs) perform exceptionally well at natural language tasks, they often struggle with precise formal reasoning and the rigorous specification of problems. We present MCP-Solver, a prototype implementation of the Model Context Protocol that demonstrates the potential for systematic integration between LLMs and constraint programming systems. Our implementation provides interfaces for the creation, editing, and validation of a constraint model. Through an item-based editing approach with integrated validation, the system ensures model consistency at every modification step and enables structured iterative refinement. The system handles concurrent solving sessions and maintains a persistent knowledge base of modeling insights. Initial experiments suggest that this integration can effectively combine LLMs' natural language understanding with constraint-solving capabilities. Our open-source implementation is proof of concept for integrating formal reasoning systems with LLMs through standardized protocols. While further research is needed to establish comprehensive formal guarantees, this work takes a first step toward principled integration of natural language processing with constraint-based reasoning.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "68",
        "title": "Monty Hall and Optimized Conformal Prediction to Improve Decision-Making with LLMs",
        "author": [
            "Harit Vishwakarma",
            "Alan Mishler",
            "Thomas Cook",
            "Niccol Dalmasso",
            "Natraj Raman",
            "Sumitra Ganesh"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00555",
        "abstract": "Large language models (LLMs) are empowering decision-making in several applications, including tool or API usage and answering multiple-choice questions (MCQs). However, they often make overconfident, incorrect predictions, which can be risky in high-stakes settings like healthcare and finance. To mitigate these risks, recent works have used conformal prediction (CP), a model-agnostic framework for distribution-free uncertainty quantification. CP transforms a \\emph{score function} into prediction sets that contain the true answer with high probability. While CP provides this coverage guarantee for arbitrary scores, the score quality significantly impacts prediction set sizes. Prior works have relied on LLM logits or other heuristic scores, lacking quality guarantees. We address this limitation by introducing CP-OPT, an optimization framework to learn scores that minimize set sizes while maintaining coverage. Furthermore, inspired by the Monty Hall problem, we extend CP's utility beyond uncertainty quantification to improve accuracy. We propose \\emph{conformal revision of questions} (CROQ) to revise the problem by narrowing down the available choices to those in the prediction set. The coverage guarantee of CP ensures that the correct choice is in the revised question prompt with high probability, while the smaller number of choices increases the LLM's chances of answering it correctly. Experiments on MMLU, ToolAlpaca, and TruthfulQA datasets with Gemma-2, Llama-3 and Phi-3 models show that CP-OPT significantly reduces set sizes while maintaining coverage, and CROQ improves accuracy over the standard inference, especially when paired with CP-OPT scores. Together, CP-OPT and CROQ offer a robust framework for improving both the safety and accuracy of LLM-driven decision-making.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "69",
        "title": "AraSTEM: A Native Arabic Multiple Choice Question Benchmark for Evaluating LLMs Knowledge In STEM Subjects",
        "author": [
            "Ahmad Mustapha",
            "Hadi Al-Khansa",
            "Hadi Al-Mubasher",
            "Aya Mourad",
            "Ranam Hamoud",
            "Hasan El-Husseini",
            "Marwah Al-Sakkaf",
            "Mariette Awad"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00559",
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities, not only in generating human-like text, but also in acquiring knowledge. This highlights the need to go beyond the typical Natural Language Processing downstream benchmarks and asses the various aspects of LLMs including knowledge and reasoning. Numerous benchmarks have been developed to evaluate LLMs knowledge, but they predominantly focus on the English language. Given that many LLMs are multilingual, relying solely on benchmarking English knowledge is insufficient. To address this issue, we introduce AraSTEM, a new Arabic multiple-choice question dataset aimed at evaluating LLMs knowledge in STEM subjects. The dataset spans a range of topics at different levels which requires models to demonstrate a deep understanding of scientific Arabic in order to achieve high accuracy. Our findings show that publicly available models of varying sizes struggle with this dataset, and underscores the need for more localized language models. The dataset is freely accessible on Hugging Face.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "70",
        "title": "Re-evaluating Automatic LLM System Ranking for Alignment with Human Preference",
        "author": [
            "Mingqi Gao",
            "Yixin Liu",
            "Xinyu Hu",
            "Xiaojun Wan",
            "Jonathan Bragg",
            "Arman Cohan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00560",
        "abstract": "Evaluating and ranking the capabilities of different LLMs is crucial for understanding their performance and alignment with human preferences. Due to the high cost and time-consuming nature of human evaluations, an automatic LLM bencher (i.e., an automatic evaluation framework that aims to rank LLMs based on their alignment with human preferences) is indispensable. An automatic LLM bencher consists of four components: the input set (e.g., a user instruction), the evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise comparison), and the aggregation method (e.g., the ELO rating system). However, previous work has not thoroughly explored how to select these components or how their different combinations influence the results. In this work, through controlled experiments, we provide a series of recommendations on how to choose each component to better automate the evaluation of LLMs. Furthermore, we discovered that when evaluating LLMs with similar performance, the performance of the automatic LLM bencher declines sharply, underscoring the limitations of current benchers and calling for future work. Lastly, we found that the evaluation models' performance at the instance level (e.g., the accuracy of selecting the best output) does not always align with their effectiveness when used as a component of a bencher, highlighting the importance of dedicated system-level evaluation of benchers.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "71",
        "title": "An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems",
        "author": [
            "Hashmath Shaik",
            "Alex Doboli"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00562",
        "abstract": "Large Language Models offer new opportunities to devise automated implementation generation methods that can tackle problem solving activities beyond traditional methods, which require algorithmic specifications and can use only static domain knowledge, like performance metrics and libraries of basic building blocks. Large Language Models could support creating new methods to support problem solving activities for open-ended problems, like problem framing, exploring possible solving approaches, feature elaboration and combination, more advanced implementation assessment, and handling unexpected situations. This report summarized the current work on Large Language Models, including model prompting, Reinforcement Learning, and Retrieval-Augmented Generation. Future research requirements were also discussed.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "72",
        "title": "Probing Visual Language Priors in VLMs",
        "author": [
            "Tiange Luo",
            "Ang Cao",
            "Gunhee Lee",
            "Justin Johnson",
            "Honglak Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00569",
        "abstract": "Despite recent advances in Vision-Language Models (VLMs), many still over-rely on visual language priors present in their training data rather than true visual reasoning. To examine the situation, we introduce ViLP, a visual question answering (VQA) benchmark that pairs each question with three potential answers and three corresponding images: one image whose answer can be inferred from text alone, and two images that demand visual reasoning. By leveraging image generative models, we ensure significant variation in texture, shape, conceptual combinations, hallucinated elements, and proverb-based contexts, making our benchmark images distinctly out-of-distribution. While humans achieve near-perfect accuracy, modern VLMs falter; for instance, GPT-4 achieves only 66.17% on ViLP. To alleviate this, we propose a self-improving framework in which models generate new VQA pairs and images, then apply pixel-level and semantic corruptions to form \"good-bad\" image pairs for self-training. Our training objectives compel VLMs to focus more on actual visual inputs and have demonstrated their effectiveness in enhancing the performance of open-source VLMs, including LLaVA-v1.5 and Cambrian.",
        "tags": [
            "GPT",
            "LLaVA"
        ]
    },
    {
        "id": "73",
        "title": "VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling",
        "author": [
            "Xinhao Li",
            "Yi Wang",
            "Jiashuo Yu",
            "Xiangyu Zeng",
            "Yuhan Zhu",
            "Haian Huang",
            "Jianfei Gao",
            "Kunchang Li",
            "Yinan He",
            "Chenting Wang",
            "Yu Qiao",
            "Yali Wang",
            "Limin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00574",
        "abstract": "Long-context modeling is a critical capability for multimodal large language models (MLLMs), enabling them to process long-form contents with implicit memorization. Despite its advances, handling extremely long videos remains challenging due to the difficulty in maintaining crucial features over extended sequences. This paper introduces a Hierarchical visual token Compression (HiCo) method designed for high-fidelity representation and a practical context modeling system VideoChat-Flash tailored for multimodal long-sequence processing. HiCo capitalizes on the redundancy of visual information in long videos to compress long video context from the clip-level to the video-level, reducing the compute significantly while preserving essential details. VideoChat-Flash features a multi-stage short-to-long learning scheme, a rich dataset of real-world long videos named LongVid, and an upgraded \"Needle-In-A-video-Haystack\" (NIAH) for evaluating context capacities. In extensive experiments, VideoChat-Flash shows the leading performance on both mainstream long and short video benchmarks at the 7B model scale. It firstly gets 99.1% accuracy over 10,000 frames in NIAH among open-source models.",
        "tags": [
            "CLIP",
            "Large Language Models"
        ]
    },
    {
        "id": "74",
        "title": "Causal Graph Guided Steering of LLM Values via Prompts and Sparse Autoencoders",
        "author": [
            "Yipeng Kang",
            "Junqi Wang",
            "Yexin Li",
            "Fangwei Zhong",
            "Xue Feng",
            "Mengmeng Wang",
            "Wenming Tu",
            "Quansen Wang",
            "Hengli Li",
            "Zilong Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00581",
        "abstract": "As large language models (LLMs) become increasingly integrated into critical applications, aligning their behavior with human values presents significant challenges. Current methods, such as Reinforcement Learning from Human Feedback (RLHF), often focus on a limited set of values and can be resource-intensive. Furthermore, the correlation between values has been largely overlooked and remains underutilized. Our framework addresses this limitation by mining a causal graph that elucidates the implicit relationships among various values within the LLMs. Leveraging the causal graph, we implement two lightweight mechanisms for value steering: prompt template steering and Sparse Autoencoder feature steering, and analyze the effects of altering one value dimension on others. Extensive experiments conducted on Gemma-2B-IT and Llama3-8B-IT demonstrate the effectiveness and controllability of our steering methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "75",
        "title": "Online Video Understanding: A Comprehensive Benchmark and Memory-Augmented Method",
        "author": [
            "Zhenpeng Huang",
            "Xinhao Li",
            "Jiaqi Li",
            "Jing Wang",
            "Xiangyu Zeng",
            "Cheng Liang",
            "Tao Wu",
            "Xi Chen",
            "Liang Li",
            "Limin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00584",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown significant progress in offline video understanding. However, applying these models to real-world scenarios, such as autonomous driving and human-computer interaction, presents unique challenges due to the need for real-time processing of continuous online video streams. To this end, this paper presents systematic efforts from three perspectives: evaluation benchmark, model architecture, and training strategy. First, we introduce OVBench, a comprehensive question-answering benchmark specifically designed to evaluate models' ability to perceive, memorize, and reason within online video contexts. It features six core task types across three temporal contexts-past, present, and future-forming 16 subtasks from diverse datasets. Second, we propose a new Pyramid Memory Bank (PMB) that effectively retains key spatiotemporal information in video streams. Third, we proposed an offline-to-online learning paradigm, designing an interleaved dialogue format for online video data and constructing an instruction-tuning dataset tailored for online video training. This framework led to the development of VideoChat-Online, a robust and efficient model for online video understanding. Despite the lower computational cost and higher efficiency, VideoChat-Online outperforms existing state-of-the-art offline and online models across popular offline video benchmarks and OVBench, demonstrating the effectiveness of our model architecture and training strategy.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "76",
        "title": "Sidewalk Hazard Detection Using Variational Autoencoder and One-Class SVM",
        "author": [
            "Edgar Guzman",
            "Robert D. Howe"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00585",
        "abstract": "The unpredictable nature of outdoor settings introduces numerous safety concerns, making hazard detection crucial for safe navigation. This paper introduces a novel system for sidewalk safety navigation utilizing a hybrid approach that combines a Variational Autoencoder (VAE) with a One-Class Support Vector Machine (OCSVM). The system is designed to detect anomalies on sidewalks that could potentially pose walking hazards. A dataset comprising over 15,000 training frames and 5,000 testing frames was collected using video recordings, capturing various sidewalk scenarios, including normal and hazardous conditions. During deployment, the VAE utilizes its reconstruction mechanism to detect anomalies within a frame. Poor reconstruction by the VAE implies the presence of an anomaly, after which the OCSVM is used to confirm whether the anomaly is hazardous or non-hazardous. The proposed VAE model demonstrated strong performance, with a high Area Under the Curve (AUC) of 0.94, effectively distinguishing anomalies that could be potential hazards. The OCSVM is employed to reduce the detection of false hazard anomalies, such as manhole or water valve covers. This approach achieves an accuracy of 91.4%, providing a highly reliable system for distinguishing between hazardous and non-hazardous scenarios. These results suggest that the proposed system offers a robust solution for hazard detection in uncertain environments.",
        "tags": [
            "Detection",
            "VAE"
        ]
    },
    {
        "id": "77",
        "title": "Setting Standards in Turkish NLP: TR-MMLU for Large Language Model Evaluation",
        "author": [
            "M. Ali Bayram",
            "Ali Arda Fincan",
            "Ahmet Semih G\"um\"u",
            "Banu Diri",
            "Sava Yldrm",
            "\"Oner Ayta"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00593",
        "abstract": "Language models have made remarkable advancements in understanding and generating human language, achieving notable success across a wide array of applications. However, evaluating these models remains a significant challenge, particularly for resource-limited languages such as Turkish. To address this gap, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive evaluation framework designed to assess the linguistic and conceptual capabilities of large language models (LLMs) in Turkish. TR-MMLU is constructed from a carefully curated dataset comprising 6200 multiple-choice questions across 62 sections, selected from a pool of 280000 questions spanning 67 disciplines and over 800 topics within the Turkish education system. This benchmark provides a transparent, reproducible, and culturally relevant tool for evaluating model performance. It serves as a standard framework for Turkish NLP research, enabling detailed analyses of LLMs' capabilities in processing Turkish text and fostering the development of more robust and accurate language models. In this study, we evaluate state-of-the-art LLMs on TR-MMLU, providing insights into their strengths and limitations for Turkish-specific tasks. Our findings reveal critical challenges, such as the impact of tokenization and fine-tuning strategies, and highlight areas for improvement in model design. By setting a new standard for evaluating Turkish language models, TR-MMLU aims to inspire future innovations and support the advancement of Turkish NLP research.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "78",
        "title": "Unbiased GNN Learning via Fairness-Aware Subgraph Diffusion",
        "author": [
            "Abdullah Alchihabi",
            "Yuhong Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00595",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable efficacy in tackling a wide array of graph-related tasks across diverse domains. However, a significant challenge lies in their propensity to generate biased predictions, particularly with respect to sensitive node attributes such as age and gender. These biases, inherent in many machine learning models, are amplified in GNNs due to the message-passing mechanism, which allows nodes to influence each other, rendering the task of making fair predictions notably challenging. This issue is particularly pertinent in critical domains where model fairness holds paramount importance. In this paper, we propose a novel generative Fairness-Aware Subgraph Diffusion (FASD) method for unbiased GNN learning. The method initiates by strategically sampling small subgraphs from the original large input graph, and then proceeds to conduct subgraph debiasing via generative fairness-aware graph diffusion processes based on stochastic differential equations (SDEs). To effectively diffuse unfairness in the input data, we introduce additional adversary bias perturbations to the subgraphs during the forward diffusion process, and train score-based models to predict these applied perturbations, enabling them to learn the underlying dynamics of the biases present in the data. Subsequently, the trained score-based models are utilized to further debias the original subgraph samples through the reverse diffusion process. Finally, FASD induces fair node predictions on the input graph by performing standard GNN learning on the debiased subgraphs. Experimental results demonstrate the superior performance of the proposed method over state-of-the-art Fair GNN baselines across multiple benchmark datasets.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "79",
        "title": "Per Subject Complexity in Eye Movement Prediction",
        "author": [
            "Kateryna Melnyk",
            "Dmytro Katrychuk",
            "Lee Friedman",
            "Oleg Komogortsev"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00597",
        "abstract": "Eye movement prediction is a promising area of research to compensate for the latency introduced by eye-tracking systems in virtual reality devices. In this study, we comprehensively analyze the complexity of the eye movement prediction task associated with subjects. We use three fundamentally different models within the analysis: the lightweight Long Short-Term Memory network (LSTM), the transformer-based network for multivariate time series representation learning (TST), and the Oculomotor Plant Mathematical Model wrapped in the Kalman Filter framework (OPKF). Each solution is assessed following a sample-to-event evaluation strategy and employing the new event-to-subject metrics. Our results show that the different models maintained similar prediction performance trends pertaining to subjects. We refer to these outcomes as per-subject complexity since some subjects' data pose a more significant challenge for models. Along with the detailed correlation analysis, this report investigates the source of the per-subject complexity and discusses potential solutions to overcome it.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "80",
        "title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM",
        "author": [
            "Yuqian Yuan",
            "Hang Zhang",
            "Wentong Li",
            "Zesen Cheng",
            "Boqiang Zhang",
            "Long Li",
            "Xin Li",
            "Deli Zhao",
            "Wenqiao Zhang",
            "Yueting Zhuang",
            "Jianke Zhu",
            "Lidong Bing"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00599",
        "abstract": "Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "81",
        "title": "DreamDrive: Generative 4D Scene Modeling from Street View Images",
        "author": [
            "Jiageng Mao",
            "Boyi Li",
            "Boris Ivanovic",
            "Yuxiao Chen",
            "Yan Wang",
            "Yurong You",
            "Chaowei Xiao",
            "Danfei Xu",
            "Marco Pavone",
            "Yue Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00601",
        "abstract": "Synthesizing photo-realistic visual observations from an ego vehicle's driving trajectory is a critical step towards scalable training of self-driving models. Reconstruction-based methods create 3D scenes from driving logs and synthesize geometry-consistent driving videos through neural rendering, but their dependence on costly object annotations limits their ability to generalize to in-the-wild driving scenarios. On the other hand, generative models can synthesize action-conditioned driving videos in a more generalizable way but often struggle with maintaining 3D visual consistency. In this paper, we present DreamDrive, a 4D spatial-temporal scene generation approach that combines the merits of generation and reconstruction, to synthesize generalizable 4D driving scenes and dynamic driving videos with 3D consistency. Specifically, we leverage the generative power of video diffusion models to synthesize a sequence of visual references and further elevate them to 4D with a novel hybrid Gaussian representation. Given a driving trajectory, we then render 3D-consistent driving videos via Gaussian splatting. The use of generative priors allows our method to produce high-quality 4D scenes from in-the-wild driving data, while neural rendering ensures 3D-consistent video generation from the 4D scenes. Extensive experiments on nuScenes and street view images demonstrate that DreamDrive can generate controllable and generalizable 4D driving scenes, synthesize novel views of driving videos with high fidelity and 3D consistency, decompose static and dynamic elements in a self-supervised manner, and enhance perception and planning tasks for autonomous driving.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting",
            "Video Generation"
        ]
    },
    {
        "id": "82",
        "title": "STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes",
        "author": [
            "Jiawei Yang",
            "Jiahui Huang",
            "Yuxiao Chen",
            "Yan Wang",
            "Boyi Li",
            "Yurong You",
            "Apoorva Sharma",
            "Maximilian Igl",
            "Peter Karkus",
            "Danfei Xu",
            "Boris Ivanovic",
            "Yue Wang",
            "Marco Pavone"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00602",
        "abstract": "We present STORM, a spatio-temporal reconstruction model designed for reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods often rely on per-scene optimization, dense observations across space and time, and strong motion supervision, resulting in lengthy optimization times, limited generalization to novel views or scenes, and degenerated quality caused by noisy pseudo-labels for dynamics. To address these challenges, STORM leverages a data-driven Transformer architecture that directly infers dynamic 3D scene representations--parameterized by 3D Gaussians and their velocities--in a single forward pass. Our key design is to aggregate 3D Gaussians from all frames using self-supervised scene flows, transforming them to the target timestep to enable complete (i.e., \"amodal\") reconstructions from arbitrary viewpoints at any moment in time. As an emergent property, STORM automatically captures dynamic instances and generates high-quality masks using only reconstruction losses. Extensive experiments on public datasets show that STORM achieves precise dynamic scene reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3 to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time rendering, and outperforms competitors in scene flow estimation, improving 3D EPE by 0.422m and Acc5 by 28.02%. Beyond reconstruction, we showcase four additional applications of our model, illustrating the potential of self-supervised learning for broader dynamic scene understanding.",
        "tags": [
            "3D",
            "Transformer"
        ]
    },
    {
        "id": "83",
        "title": "DiC: Rethinking Conv3x3 Designs in Diffusion Models",
        "author": [
            "Yuchuan Tian",
            "Jing Han",
            "Chengcheng Wang",
            "Yuchen Liang",
            "Chao Xu",
            "Hanting Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00603",
        "abstract": "Diffusion models have shown exceptional performance in visual generation tasks. Recently, these models have shifted from traditional U-Shaped CNN-Attention hybrid structures to fully transformer-based isotropic architectures. While these transformers exhibit strong scalability and performance, their reliance on complicated self-attention operation results in slow inference speeds. Contrary to these works, we rethink one of the simplest yet fastest module in deep learning, 3x3 Convolution, to construct a scaled-up purely convolutional diffusion model. We first discover that an Encoder-Decoder Hourglass design outperforms scalable isotropic architectures for Conv3x3, but still under-performing our expectation. Further improving the architecture, we introduce sparse skip connections to reduce redundancy and improve scalability. Based on the architecture, we introduce conditioning improvements including stage-specific embeddings, mid-block condition injection, and conditional gating. These improvements lead to our proposed Diffusion CNN (DiC), which serves as a swift yet competitive diffusion architecture baseline. Experiments on various scales and settings show that DiC surpasses existing diffusion transformers by considerable margins in terms of performance while keeping a good speed advantage. Project page: https://github.com/YuchuanTian/DiC",
        "tags": [
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "84",
        "title": "A Study on Context Length and Efficient Transformers for Biomedical Image Analysis",
        "author": [
            "Sarah M. Hooper",
            "Hui Xue"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00619",
        "abstract": "Biomedical imaging modalities often produce high-resolution, multi-dimensional images that pose computational challenges for deep neural networks. These computational challenges are compounded when training transformers due to the self-attention operator, which scales quadratically with context length. Recent developments in long-context models have potential to alleviate these difficulties and enable more efficient application of transformers to large biomedical images, although a systematic evaluation on this topic is lacking. In this study, we investigate the impact of context length on biomedical image analysis and we evaluate the performance of recently proposed long-context models. We first curate a suite of biomedical imaging datasets, including 2D and 3D data for segmentation, denoising, and classification tasks. We then analyze the impact of context length on network performance using the Vision Transformer and Swin Transformer by varying patch size and attention window size. Our findings reveal a strong relationship between context length and performance, particularly for pixel-level prediction tasks. Finally, we show that recent long-context models demonstrate significant improvements in efficiency while maintaining comparable performance, though we highlight where gaps remain. This work underscores the potential and challenges of using long-context models in biomedical imaging.",
        "tags": [
            "3D",
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "85",
        "title": "Gaussian Building Mesh (GBM): Extract a Building's 3D Mesh with Google Earth and Gaussian Splatting",
        "author": [
            "Kyle Gao",
            "Liangzhi Li",
            "Hongjie He",
            "Dening Lu",
            "Linlin Xu",
            "Jonathan Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00625",
        "abstract": "Recently released open-source pre-trained foundational image segmentation and object detection models (SAM2+GroundingDINO) allow for geometrically consistent segmentation of objects of interest in multi-view 2D images. Users can use text-based or click-based prompts to segment objects of interest without requiring labeled training datasets. Gaussian Splatting allows for the learning of the 3D representation of a scene's geometry and radiance based on 2D images. Combining Google Earth Studio, SAM2+GroundingDINO, 2D Gaussian Splatting, and our improvements in mask refinement based on morphological operations and contour simplification, we created a pipeline to extract the 3D mesh of any building based on its name, address, or geographic coordinates.",
        "tags": [
            "3D",
            "Detection",
            "Gaussian Splatting",
            "Segmentation"
        ]
    },
    {
        "id": "86",
        "title": "Flash-Split: 2D Reflection Removal with Flash Cues and Latent Diffusion Separation",
        "author": [
            "Tianfu Wang",
            "Mingyang Xie",
            "Haoming Cai",
            "Sachin Shah",
            "Christopher A. Metzler"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00637",
        "abstract": "Transparent surfaces, such as glass, create complex reflections that obscure images and challenge downstream computer vision applications. We introduce Flash-Split, a robust framework for separating transmitted and reflected light using a single (potentially misaligned) pair of flash/no-flash images. Our core idea is to perform latent-space reflection separation while leveraging the flash cues. Specifically, Flash-Split consists of two stages. Stage 1 separates apart the reflection latent and transmission latent via a dual-branch diffusion model conditioned on an encoded flash/no-flash latent pair, effectively mitigating the flash/no-flash misalignment issue. Stage 2 restores high-resolution, faithful details to the separated latents, via a cross-latent decoding process conditioned on the original images before separation. By validating Flash-Split on challenging real-world scenes, we demonstrate state-of-the-art reflection separation performance and significantly outperform the baseline methods.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "87",
        "title": "Enabling New HDLs with Agents",
        "author": [
            "Mark Zakharov",
            "Farzaneh Rabiei Kashanaki",
            "Jose Renau"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00642",
        "abstract": "Large Language Models (LLMs) based agents are transforming the programming language landscape by facilitating learning for beginners, enabling code generation, and optimizing documentation workflows. Hardware Description Languages (HDLs), with their smaller user community, stand to benefit significantly from the application of LLMs as tools for learning new HDLs. This paper investigates the challenges and solutions of enabling LLMs for HDLs, particularly for HDLs that LLMs have not been previously trained on. This work introduces HDLAgent, an AI agent optimized for LLMs with limited knowledge of various HDLs. It significantly enhances off-the-shelf LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "88",
        "title": "SoundBrush: Sound as a Brush for Visual Scene Editing",
        "author": [
            "Kim Sung-Bin",
            "Kim Jun-Seong",
            "Junseok Ko",
            "Yewon Kim",
            "Tae-Hyun Oh"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00645",
        "abstract": "We propose SoundBrush, a model that uses sound as a brush to edit and manipulate visual scenes. We extend the generative capabilities of the Latent Diffusion Model (LDM) to incorporate audio information for editing visual scenes. Inspired by existing image-editing works, we frame this task as a supervised learning problem and leverage various off-the-shelf models to construct a sound-paired visual scene dataset for training. This richly generated dataset enables SoundBrush to learn to map audio features into the textual space of the LDM, allowing for visual scene editing guided by diverse in-the-wild sound. Unlike existing methods, SoundBrush can accurately manipulate the overall scenery or even insert sounding objects to best match the audio inputs while preserving the original content. Furthermore, by integrating with novel view synthesis techniques, our framework can be extended to edit 3D scenes, facilitating sound-driven 3D scene manipulation. Demos are available at https://soundbrush.github.io/.",
        "tags": [
            "3D",
            "Diffusion",
            "Image Editing"
        ]
    },
    {
        "id": "89",
        "title": "Taming Feed-forward Reconstruction Models as Latent Encoders for 3D Generative Models",
        "author": [
            "Suttisak Wizadwongsa",
            "Jinfan Zhou",
            "Edward Li",
            "Jeong Joon Park"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00651",
        "abstract": "Recent AI-based 3D content creation has largely evolved along two paths: feed-forward image-to-3D reconstruction approaches and 3D generative models trained with 2D or 3D supervision. In this work, we show that existing feed-forward reconstruction methods can serve as effective latent encoders for training 3D generative models, thereby bridging these two paradigms. By reusing powerful pre-trained reconstruction models, we avoid computationally expensive encoder network training and obtain rich 3D latent features for generative modeling for free. However, the latent spaces of reconstruction models are not well-suited for generative modeling due to their unstructured nature. To enable flow-based model training on these latent features, we develop post-processing pipelines, including protocols to standardize the features and spatial weighting to concentrate on important regions. We further incorporate a 2D image space perceptual rendering loss to handle the high-dimensional latent spaces. Finally, we propose a multi-stream transformer-based rectified flow architecture to achieve linear scaling and high-quality text-conditioned 3D generation. Our framework leverages the advancements of feed-forward reconstruction models to enhance the scalability of 3D generative modeling, achieving both high computational efficiency and state-of-the-art performance in text-to-3D generation.",
        "tags": [
            "3D",
            "Image-to-3D",
            "Rectified Flow",
            "Text-to-3D",
            "Transformer"
        ]
    },
    {
        "id": "90",
        "title": "ICONS: Influence Consensus for Vision-Language Data Selection",
        "author": [
            "Xindi Wu",
            "Mengzhou Xia",
            "Rulin Shao",
            "Zhiwei Deng",
            "Pang Wei Koh",
            "Olga Russakovsky"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00654",
        "abstract": "Visual Instruction Tuning typically requires a large amount of vision-language training data. This data often containing redundant information that increases computational costs without proportional performance gains. In this work, we introduce ICONS, a gradient-driven Influence CONsensus approach for vision-language data Selection that selects a compact training dataset for efficient multi-task training. The key element of our approach is cross-task influence consensus, which uses majority voting across task-specific influence matrices to identify samples that are consistently valuable across multiple tasks, allowing us to effectively prioritize data that optimizes for overall performance. Experiments show that models trained on our selected data (20% of LLaVA-665K) achieve 98.6% of the relative performance obtained using the full dataset. Additionally, we release this subset, LLaVA-ICONS-133K, a compact yet highly informative subset of LLaVA-665K visual instruction tuning data, preserving high impact training data for efficient vision-language model development.",
        "tags": [
            "LLaVA"
        ]
    },
    {
        "id": "91",
        "title": "Finding Missed Code Size Optimizations in Compilers using LLMs",
        "author": [
            "Davide Italiano",
            "Chris Cummins"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00655",
        "abstract": "Compilers are complex, and significant effort has been expended on testing them. Techniques such as random program generation and differential testing have proved highly effective and have uncovered thousands of bugs in production compilers. The majority of effort has been expended on validating that a compiler produces correct code for a given input, while less attention has been paid to ensuring that the compiler produces performant code.\nIn this work we adapt differential testing to the task of identifying missed optimization opportunities in compilers. We develop a novel testing approach which combines large language models (LLMs) with a series of differential testing strategies and use them to find missing code size optimizations in C / C++ compilers.\nThe advantage of our approach is its simplicity. We offload the complex task of generating random code to an off-the-shelf LLM, and use heuristics and analyses to identify anomalous compiler behavior. Our approach requires fewer than 150 lines of code to implement. This simplicity makes it extensible. By simply changing the target compiler and initial LLM prompt we port the approach from C / C++ to Rust and Swift, finding bugs in both. To date we have reported 24 confirmed bugs in production compilers, and conclude that LLM-assisted testing is a promising avenue for detecting optimization bugs in real world compilers.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "92",
        "title": "2 OLMo 2 Furious",
        "author": [
            "Team OLMo",
            "Pete Walsh",
            "Luca Soldaini",
            "Dirk Groeneveld",
            "Kyle Lo",
            "Shane Arora",
            "Akshita Bhagia",
            "Yuling Gu",
            "Shengyi Huang",
            "Matt Jordan",
            "Nathan Lambert",
            "Dustin Schwenk",
            "Oyvind Tafjord",
            "Taira Anderson",
            "David Atkinson",
            "Faeze Brahman",
            "Christopher Clark",
            "Pradeep Dasigi",
            "Nouha Dziri",
            "Michal Guerquin",
            "Hamish Ivison",
            "Pang Wei Koh",
            "Jiacheng Liu",
            "Saumya Malik",
            "William Merrill",
            "Lester James V. Miranda",
            "Jacob Morrison",
            "Tyler Murray",
            "Crystal Nam",
            "Valentina Pyatkin",
            "Aman Rangapur",
            "Michael Schmitz",
            "Sam Skjonsberg",
            "David Wadden",
            "Christopher Wilhelm",
            "Michael Wilson",
            "Luke Zettlemoyer",
            "Ali Farhadi",
            "Noah A. Smith",
            "Hannaneh Hajishirzi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00656",
        "abstract": "We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from Tlu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "93",
        "title": "Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing",
        "author": [
            "Peihao Wang",
            "Ruisi Cai",
            "Yuehao Wang",
            "Jiajun Zhu",
            "Pragya Srivastava",
            "Zhangyang Wang",
            "Pan Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00658",
        "abstract": "Structured State Space Models (SSMs) have emerged as alternatives to transformers. While SSMs are often regarded as effective in capturing long-sequence dependencies, we rigorously demonstrate that they are inherently limited by strong recency bias. Our empirical studies also reveal that this bias impairs the models' ability to recall distant information and introduces robustness issues. Our scaling experiments then discovered that deeper structures in SSMs can facilitate the learning of long contexts. However, subsequent theoretical analysis reveals that as SSMs increase in depth, they exhibit another inevitable tendency toward over-smoothing, e.g., token representations becoming increasingly indistinguishable. This fundamental dilemma between recency and over-smoothing hinders the scalability of existing SSMs. Inspired by our theoretical findings, we propose to polarize two channels of the state transition matrices in SSMs, setting them to zero and one, respectively, simultaneously addressing recency bias and over-smoothing. Experiments demonstrate that our polarization technique consistently enhances the associative recall accuracy of long-range tokens and unlocks SSMs to benefit further from deeper architectures. All source codes are released at https://github.com/VITA-Group/SSM-Bottleneck.",
        "tags": [
            "SSMs",
            "State Space Models"
        ]
    },
    {
        "id": "94",
        "title": "Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? Revisiting a Petroglyph",
        "author": [
            "Kazuki Irie"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00659",
        "abstract": "Do autoregressive Transformer language models require explicit positional encodings (PEs)? The answer is \"no\" as long as they have more than one layer -- they can distinguish sequences with permuted tokens without requiring explicit PEs. This property has been known since early efforts (those contemporary with GPT-2) adopting the Transformer for language modeling. However, this result does not appear to have been well disseminated and was even rediscovered recently. This may be partially due to a sudden growth of the language modeling community after the advent of GPT-2, but perhaps also due to the lack of a clear explanation in prior publications, despite being commonly understood by practitioners in the past. Here we review this long-forgotten explanation why explicit PEs are nonessential for multi-layer autoregressive Transformers (in contrast, one-layer models require PEs to discern order information of their input tokens). We also review the origin of this result, and hope to re-establish it as a common knowledge.",
        "tags": [
            "GPT",
            "Transformer"
        ]
    },
    {
        "id": "95",
        "title": "IGC: Integrating a Gated Calculator into an LLM to Solve Arithmetic Tasks Reliably and Efficiently",
        "author": [
            "Florian Dietz",
            "Dietrich Klakow"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00684",
        "abstract": "Solving arithmetic tasks is a simple and fundamental skill, yet modern Large Language Models (LLMs) have great difficulty with them. We introduce the Integrated Gated Calculator (IGC), a module that enables LLMs to perform arithmetic by emulating a calculator on the GPU. We finetune a Llama model with our module and test it on the BigBench Arithmetic benchmark, where it beats the State of the Art, outperforming all models on the benchmark, including models almost two orders of magnitude larger. Our approach takes only a single iteration to run and requires no external tools. It performs arithmetic operations entirely inside the LLM without the need to produce intermediate tokens. It is computationally efficient, interpretable, and avoids side-effects on tasks that do not require arithmetic operations. It reliably achieves 98\\% to 99\\% accuracy across multiple training runs and for all subtasks, including the substantially harder subtask of multiplication, which was previously unsolved.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "96",
        "title": "Labels Generated by Large Language Model Helps Measuring People's Empathy in Vitro",
        "author": [
            "Md Rakibul Hasan",
            "Yue Yao",
            "Md Zakir Hossain",
            "Aneesh Krishna",
            "Imre Rudas",
            "Shafin Rahman",
            "Tom Gedeon"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00691",
        "abstract": "Large language models (LLMs) have revolutionised numerous fields, with LLM-as-a-service (LLMSaaS) having a strong generalisation ability that offers accessible solutions directly without the need for costly training. In contrast to the widely studied prompt engineering for task solving directly (in vivo), this paper explores its potential in in-vitro applications. These involve using LLM to generate labels to help the supervised training of mainstream models by (1) noisy label correction and (2) training data augmentation with LLM-generated labels. In this paper, we evaluate this approach in the emerging field of empathy computing -- automating the prediction of psychological questionnaire outcomes from inputs like text sequences. Specifically, crowdsourced datasets in this domain often suffer from noisy labels that misrepresent underlying empathy. By leveraging LLM-generated labels to train pre-trained language models (PLMs) like RoBERTa, we achieve statistically significant accuracy improvements over baselines, achieving a state-of-the-art Pearson correlation coefficient of 0.648 on NewsEmp benchmarks. In addition, we bring insightful discussions, including current challenges in empathy computing, data biases in training data and evaluation metric selection. Code and LLM-generated data are available at https://github.com/hasan-rakibul/LLMPathy (available once the paper is accepted).",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "97",
        "title": "Adjoint sharding for very long context training of state space models",
        "author": [
            "Xingzi Xu",
            "Amir Tavanaei",
            "Kavosh Asadi",
            "Karim Bouyarmane"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00692",
        "abstract": "Despite very fast progress, efficiently training large language models (LLMs) in very long contexts remains challenging. Existing methods fall back to training LLMs with short contexts (a maximum of a few thousands tokens in training) and use inference time techniques when evaluating on long contexts (above 1M tokens context window at inference). As opposed to long-context-inference, training on very long context input prompts is quickly limited by GPU memory availability and by the prohibitively long training times it requires on state-of-the-art hardware. Meanwhile, many real-life applications require not only inference but also training/fine-tuning with long context on specific tasks. Such applications include, for example, augmenting the context with various sources of raw reference information for fact extraction, fact summarization, or fact reconciliation tasks. We propose adjoint sharding, a novel technique that comprises sharding gradient calculation during training to reduce memory requirements by orders of magnitude, making training on very long context computationally tractable. Adjoint sharding is based on the adjoint method and computes equivalent gradients to backpropagation. We also propose truncated adjoint sharding to speed up the algorithm while maintaining performance. We provide a distributed version, and a paralleled version of adjoint sharding to further speed up training. Empirical results show the proposed adjoint sharding algorithm reduces memory usage by up to 3X with a 1.27B parameter large language model on 1M context length training. This allows to increase the maximum context length during training or fine-tuning of a 1.27B parameter model from 35K tokens to above 100K tokens on a training infrastructure composed of five AWS P4 instances.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "State Space Models"
        ]
    },
    {
        "id": "98",
        "title": "PANDA -- Paired Anti-hate Narratives Dataset from Asia: Using an LLM-as-a-Judge to Create the First Chinese Counterspeech Dataset",
        "author": [
            "Michael Bennie",
            "Demi Zhang",
            "Bushi Xiao",
            "Jing Cao",
            "Chryseis Xinyi Liu",
            "Jian Meng",
            "Alayo Tripp"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00697",
        "abstract": "Despite the global prevalence of Modern Standard Chinese language, counterspeech (CS) resources for Chinese remain virtually nonexistent. To address this gap in East Asian counterspeech research we introduce the a corpus of Modern Standard Mandarin counterspeech that focuses on combating hate speech in Mainland China. This paper proposes a novel approach of generating CS by using an LLM-as-a-Judge, simulated annealing, LLMs zero-shot CN generation and a round-robin algorithm. This is followed by manual verification for quality and contextual relevance. This paper details the methodology for creating effective counterspeech in Chinese and other non-Eurocentric languages, including unique cultural patterns of which groups are maligned and linguistic patterns in what kinds of discourse markers are programmatically marked as hate speech (HS). Analysis of the generated corpora, we provide strong evidence for the lack of open-source, properly labeled Chinese hate speech data and the limitations of using an LLM-as-Judge to score possible answers in Chinese. Moreover, the present corpus serves as the first East Asian language based CS corpus and provides an essential resource for future research on counterspeech generation and evaluation.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "99",
        "title": "Knowledge-Guided Prompt Learning for Deepfake Facial Image Detection",
        "author": [
            "Hao Wang",
            "Cheng Deng",
            "Zhidong Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00700",
        "abstract": "Recent generative models demonstrate impressive performance on synthesizing photographic images, which makes humans hardly to distinguish them from pristine ones, especially on realistic-looking synthetic facial images. Previous works mostly focus on mining discriminative artifacts from vast amount of visual data. However, they usually lack the exploration of prior knowledge and rarely pay attention to the domain shift between training categories (e.g., natural and indoor objects) and testing ones (e.g., fine-grained human facial images), resulting in unsatisfactory detection performance. To address these issues, we propose a novel knowledge-guided prompt learning method for deepfake facial image detection. Specifically, we retrieve forgery-related prompts from large language models as expert knowledge to guide the optimization of learnable prompts. Besides, we elaborate test-time prompt tuning to alleviate the domain shift, achieving significant performance improvement and boosting the application in real-world scenarios. Extensive experiments on DeepFakeFaceForensics dataset show that our proposed approach notably outperforms state-of-the-art methods.",
        "tags": [
            "Detection",
            "Large Language Models"
        ]
    },
    {
        "id": "100",
        "title": "Kolmogorov GAM Networks are all you need!",
        "author": [
            "Sarah Polson",
            "Vadim Sokolov"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00704",
        "abstract": "Kolmogorov GAM (K-GAM) networks are shown to be an efficient architecture for training and inference. They are an additive model with an embedding that is independent of the function of interest. They provide an alternative to the transformer architecture. They are the machine learning version of Kolmogorov's Superposition Theorem (KST) which provides an efficient representations of a multivariate function. Such representations have use in machine learning for encoding dictionaries (a.k.a. \"look-up\" tables). KST theory also provides a representation based on translates of the Kppen function. The goal of our paper is to interpret this representation in a machine learning context for applications in Artificial Intelligence (AI). Our architecture is equivalent to a topological embedding which is independent of the function together with an additive layer that uses a Generalized Additive Model (GAM). This provides a class of learning procedures with far fewer parameters than current deep learning algorithms. Implementation can be parallelizable which makes our algorithms computationally attractive. To illustrate our methodology, we use the Iris data from statistical learning. We also show that our additive model with non-linear embedding provides an alternative to transformer architectures which from a statistical viewpoint are kernel smoothers. Additive KAN models therefore provide a natural alternative to transformers. Finally, we conclude with directions for future research.",
        "tags": [
            "KAN",
            "Transformer"
        ]
    },
    {
        "id": "101",
        "title": "On Importance of Layer Pruning for Smaller BERT Models and Low Resource Languages",
        "author": [
            "Mayur Shirke",
            "Amey Shembade",
            "Madhushri Wagh",
            "Pavan Thorat",
            "Raviraj Joshi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00733",
        "abstract": "This study explores the effectiveness of layer pruning for developing more efficient BERT models tailored to specific downstream tasks in low-resource languages. Our primary objective is to evaluate whether pruned BERT models can maintain high performance while reducing model size and complexity. We experiment with several BERT variants, including MahaBERT-v2 and Google-Muril, applying different pruning strategies and comparing their performance to smaller, scratch-trained models like MahaBERT-Small and MahaBERT-Smaller. We fine-tune these models on Marathi datasets, specifically Short Headlines Classification (SHC), Long Paragraph Classification (LPC) and Long Document Classification (LDC), to assess their classification accuracy. Our findings demonstrate that pruned models, despite having fewer layers, achieve comparable performance to their fully-layered counterparts while consistently outperforming scratch-trained models of similar size. Notably, pruning layers from the middle of the model proves to be the most effective strategy, offering performance competitive with pruning from the top and bottom. However, there is no clear winner, as different pruning strategies perform better in different model and dataset combinations. Additionally, monolingual BERT models outperform multilingual ones in these experiments. This approach, which reduces computational demands, provides a faster and more efficient alternative to training smaller models from scratch, making advanced NLP models more accessible for low-resource languages without compromising classification accuracy.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "102",
        "title": "RORem: Training a Robust Object Remover with Human-in-the-Loop",
        "author": [
            "Ruibin Li",
            "Tao Yang",
            "Song Guo",
            "Lei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00740",
        "abstract": "Despite the significant advancements, existing object removal methods struggle with incomplete removal, incorrect content synthesis and blurry synthesized regions, resulting in low success rates. Such issues are mainly caused by the lack of high-quality paired training data, as well as the self-supervised training paradigm adopted in these methods, which forces the model to in-paint the masked regions, leading to ambiguity between synthesizing the masked objects and restoring the background. To address these issues, we propose a semi-supervised learning strategy with human-in-the-loop to create high-quality paired training data, aiming to train a Robust Object Remover (RORem). We first collect 60K training pairs from open-source datasets to train an initial object removal model for generating removal samples, and then utilize human feedback to select a set of high-quality object removal pairs, with which we train a discriminator to automate the following training data generation process. By iterating this process for several rounds, we finally obtain a substantial object removal dataset with over 200K pairs. Fine-tuning the pre-trained stable diffusion model with this dataset, we obtain our RORem, which demonstrates state-of-the-art object removal performance in terms of both reliability and image quality. Particularly, RORem improves the object removal success rate over previous methods by more than 18\\%. The dataset, source code and trained model are available at https://github.com/leeruibin/RORem.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "103",
        "title": "DIVE: Diversified Iterative Self-Improvement",
        "author": [
            "Yiwei Qin",
            "Yixiu Liu",
            "Pengfei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00747",
        "abstract": "Recent advances in large language models (LLMs) have demonstrated the effectiveness of Iterative Self-Improvement (ISI) techniques. However, continuous training on self-generated data leads to reduced output diversity, a limitation particularly critical in reasoning tasks where diverse solution paths are essential. We present DIVE (Diversified Iterative Self-Improvement), a novel framework that addresses this challenge through two key components: Sample Pool Expansion for broader solution exploration, and Data Selection for balancing diversity and quality in preference pairs. Experiments on MATH and GSM8k datasets show that DIVE achieves a 10% to 45% relative increase in output diversity metrics while maintaining performance quality compared to vanilla ISI. Our ablation studies confirm both components' significance in achieving these improvements. Code is available at https://github.com/qinyiwei/DIVE.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "104",
        "title": "Beyond Text: Implementing Multimodal Large Language Model-Powered Multi-Agent Systems Using a No-Code Platform",
        "author": [
            "Cheonsu Jeong"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00750",
        "abstract": "This study proposes the design and implementation of a multimodal LLM-based Multi-Agent System (MAS) leveraging a No-Code platform to address the practical constraints and significant entry barriers associated with AI adoption in enterprises. Advanced AI technologies, such as Large Language Models (LLMs), often pose challenges due to their technical complexity and high implementation costs, making them difficult for many organizations to adopt. To overcome these limitations, this research develops a No-Code-based Multi-Agent System designed to enable users without programming knowledge to easily build and manage AI systems. The study examines various use cases to validate the applicability of AI in business processes, including code generation from image-based notes, Advanced RAG-based question-answering systems, text-based image generation, and video generation using images and prompts. These systems lower the barriers to AI adoption, empowering not only professional developers but also general users to harness AI for significantly improved productivity and efficiency. By demonstrating the scalability and accessibility of No-Code platforms, this study advances the democratization of AI technologies within enterprises and validates the practical applicability of Multi-Agent Systems, ultimately contributing to the widespread adoption of AI across various industries.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG",
            "Video Generation"
        ]
    },
    {
        "id": "105",
        "title": "Enhancing Transformers for Generalizable First-Order Logical Entailment",
        "author": [
            "Tianshi Zheng",
            "Jiazheng Wang",
            "Zihao Wang",
            "Jiaxin Bai",
            "Hang Yin",
            "Zheye Deng",
            "Yangqiu Song",
            "Jianxin Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00759",
        "abstract": "Transformers, as a fundamental deep learning architecture, have demonstrated remarkable capabilities in reasoning. This paper investigates the generalizable first-order logical reasoning ability of transformers with their parameterized knowledge and explores ways to improve it. The first-order reasoning capability of transformers is assessed through their ability to perform first-order logical entailment, which is quantitatively measured by their performance in answering knowledge graph queries. We establish connections between (1) two types of distribution shifts studied in out-of-distribution generalization and (2) the unseen knowledge and query settings discussed in the task of knowledge graph query answering, enabling a characterization of fine-grained generalizability. Results on our comprehensive dataset show that transformers outperform previous methods specifically designed for this task and provide detailed empirical evidence on the impact of input query syntax, token embedding, and transformer architectures on the reasoning capability of transformers. Interestingly, our findings reveal a mismatch between positional encoding and other design choices in transformer architectures employed in prior practices. This discovery motivates us to propose a more sophisticated, logic-aware architecture, TEGA, to enhance the capability for generalizable first-order logical entailment in transformers.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "106",
        "title": "Beyond Words: AuralLLM and SignMST-C for Precise Sign Language Production and Bidirectional Accessibility",
        "author": [
            "Yulong Li",
            "Yuxuan Zhang",
            "Feilong Tang",
            "Mian Zhou",
            "Zhixiang Lu",
            "Haochen Xue",
            "Yifang Wang",
            "Kang Dang",
            "Jionglong Su"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00765",
        "abstract": "Although sign language recognition aids non-hearing-impaired understanding, many hearing-impaired individuals still rely on sign language alone due to limited literacy, underscoring the need for advanced sign language production and translation (SLP and SLT) systems. In the field of sign language production, the lack of adequate models and datasets restricts practical applications. Existing models face challenges in production accuracy and pose control, making it difficult to provide fluent sign language expressions across diverse scenarios. Additionally, data resources are scarce, particularly high-quality datasets with complete sign vocabulary and pose annotations. To address these issues, we introduce CNText2Sign and CNSign, comprehensive datasets to benchmark SLP and SLT, respectively, with CNText2Sign covering gloss and landmark mappings for SLP, and CNSign providing extensive video-to-text data for SLT. To improve the accuracy and applicability of sign language systems, we propose the AuraLLM and SignMST-C models. AuraLLM, incorporating LoRA and RAG techniques, achieves a BLEU-4 score of 50.41 on the CNText2Sign dataset, enabling precise control over gesture semantics and motion. SignMST-C employs self-supervised rapid motion video pretraining, achieving a BLEU-4 score of 31.03/32.08 on the PHOENIX2014-T benchmark, setting a new state-of-the-art. These models establish robust baselines for the datasets released for their respective tasks.",
        "tags": [
            "LoRA",
            "RAG"
        ]
    },
    {
        "id": "107",
        "title": "Using Large Language Model to Support Flexible and Structural Inductive Qualitative Analysis",
        "author": [
            "Jie Gao",
            "Zhiyao Shu",
            "Shun Yi Yeo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00775",
        "abstract": "Traditional qualitative analysis requires significant effort and collaboration to achieve consensus through formal coding processes, including open coding, discussions, and codebook merging. However, in scenarios where such rigorous and time-intensive methods are unnecessary-such as summarizing meetings or personal ideation-quick yet structual insights are more practical. To address this need, we proposed MindCoder, a tool inspired by the \"Codes-to-theory\" model and developed through an iterative design process to support flexible and structural inductive qualitative analysis. With OpenAI's GPT-4o model, MindCoder supports data preprocessing, automatic open coding, automatic axial coding, and automatic concept development, ultimately presenting a report to support insights presentation. An evaluation with 12 participants highlights its effectiveness in enabling flexible yet structured analysis and its advantages over ChatGPT and http://Atlas.ti Web AI coding function.",
        "tags": [
            "ChatGPT",
            "GPT"
        ]
    },
    {
        "id": "108",
        "title": "FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation",
        "author": [
            "Qianli Wang",
            "Nils Feldhus",
            "Simon Ostermann",
            "Luis Felipe Villa-Arenas",
            "Sebastian Mller",
            "Vera Schmitt"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00777",
        "abstract": "Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF's core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "109",
        "title": "Decoding the Flow: CauseMotion for Emotional Causality Analysis in Long-form Conversations",
        "author": [
            "Yuxuan Zhang",
            "Yulong Li",
            "Zichen Yu",
            "Feilong Tang",
            "Zhixiang Lu",
            "Chong Li",
            "Kang Dang",
            "Jionglong Su"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00778",
        "abstract": "Long-sequence causal reasoning seeks to uncover causal relationships within extended time series data but is hindered by complex dependencies and the challenges of validating causal links. To address the limitations of large-scale language models (e.g., GPT-4) in capturing intricate emotional causality within extended dialogues, we propose CauseMotion, a long-sequence emotional causal reasoning framework grounded in Retrieval-Augmented Generation (RAG) and multimodal fusion. Unlike conventional methods relying only on textual information, CauseMotion enriches semantic representations by incorporating audio-derived features-vocal emotion, emotional intensity, and speech rate-into textual modalities. By integrating RAG with a sliding window mechanism, it effectively retrieves and leverages contextually relevant dialogue segments, thus enabling the inference of complex emotional causal chains spanning multiple conversational turns. To evaluate its effectiveness, we constructed the first benchmark dataset dedicated to long-sequence emotional causal reasoning, featuring dialogues with over 70 turns. Experimental results demonstrate that the proposed RAG-based multimodal integrated approach, the efficacy of substantially enhances both the depth of emotional understanding and the causal inference capabilities of large-scale language models. A GLM-4 integrated with CauseMotion achieves an 8.7% improvement in causal accuracy over the original model and surpasses GPT-4o by 1.2%. Additionally, on the publicly available DiaASQ dataset, CauseMotion-GLM-4 achieves state-of-the-art results in accuracy, F1 score, and causal reasoning accuracy.",
        "tags": [
            "GPT",
            "RAG"
        ]
    },
    {
        "id": "110",
        "title": "REM: A Scalable Reinforced Multi-Expert Framework for Multiplex Influence Maximization",
        "author": [
            "Huyen Nguyen",
            "Hieu Dam",
            "Nguyen Do",
            "Cong Tran",
            "Cuong Pham"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00779",
        "abstract": "In social online platforms, identifying influential seed users to maximize influence spread is a crucial as it can greatly diminish the cost and efforts required for information dissemination. While effective, traditional methods for Multiplex Influence Maximization (MIM) have reached their performance limits, prompting the emergence of learning-based approaches. These novel methods aim for better generalization and scalability for more sizable graphs but face significant challenges, such as (1) inability to handle unknown diffusion patterns and (2) reliance on high-quality training samples. To address these issues, we propose the Reinforced Expert Maximization framework (REM). REM leverages a Propagation Mixture of Experts technique to encode dynamic propagation of large multiplex networks effectively in order to generate enhanced influence propagation. Noticeably, REM treats a generative model as a policy to autonomously generate different seed sets and learn how to improve them from a Reinforcement Learning perspective. Extensive experiments on several real-world datasets demonstrate that REM surpasses state-of-the-art methods in terms of influence spread, scalability, and inference time in influence maximization tasks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "111",
        "title": "Navigating Nuance: In Quest for Political Truth",
        "author": [
            "Soumyadeep Sar",
            "Dwaipayan Roy"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00782",
        "abstract": "This study investigates the several nuanced rationales for countering the rise of political bias. We evaluate the performance of the Llama-3 (70B) language model on the Media Bias Identification Benchmark (MBIB), based on a novel prompting technique that incorporates subtle reasons for identifying political leaning. Our findings underscore the challenges of detecting political bias and highlight the potential of transfer learning methods to enhance future models. Through our framework, we achieve a comparable performance with the supervised and fully fine-tuned ConvBERT model, which is the state-of-the-art model, performing best among other baseline models for the political bias task on MBIB. By demonstrating the effectiveness of our approach, we contribute to the development of more robust tools for mitigating the spread of misinformation and polarization. Our codes and dataset are made publicly available in github.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "112",
        "title": "Solid-state dewetting of axisymmetric thin film on axisymmetric curved-surface substrates: modeling and simulation",
        "author": [
            "Zhenghua Duan",
            "Meng Li",
            "Chunjie Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00783",
        "abstract": "In this work,we consider the solid-state dewetting of an axisymmetric thin film on a curved-surface substrate,with the assumption that the substrate morphology is also http://axisymmetric.Under the assumptions of axisymmetry,the surface evolution problem on a curved-surface substrate can be reduced to a curve evolution problem on a static curved http://substrate.Based on the thermodynamic variation of the anisotropic surface energy,we thoroughly derive a sharp-interface model that is governed by anisotropic surface diffusion,along with appropriate boundary http://conditions.The continuum system satisfies the laws of energy decay and volume conservation,which motivates the design of a structure-preserving numerical algorithm for simulating the mathematical http://model.By introducing a symmetrized surface energy matrix, we derive a novel symmetrized variational formulation. Then, by carefully discretizing the boundary terms of the variational formulation, we establish an unconditionally energy-stable parametric finite element approximation of the axisymmetric system. By applying an ingenious correction method, we further develop another structure-preserving method that can preserve both the energy stability and volume conservation properties. Finally, we present extensive numerical examples to demonstrate the convergence and structure-preserving properties of our proposed numerical scheme. Additionally, several interesting phenomena are explored, including the migration of 'small' particles on a curved-surface substrate generated by curves with positive or negative curvature, pinch-off events, and edge retraction.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "113",
        "title": "Shifting-Merging: Secure, High-Capacity and Efficient Steganography via Large Language Models",
        "author": [
            "Minhao Bai",
            "Jinshuai Yang",
            "Kaiyi Pang",
            "Yongfeng Huang",
            "Yue Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00786",
        "abstract": "In the face of escalating surveillance and censorship within the cyberspace, the sanctity of personal privacy has come under siege, necessitating the development of steganography, which offers a way to securely hide messages within innocent-looking texts. Previous methods alternate the texts to hide private massages, which is not secure. Large Language Models (LLMs) provide high-quality and explicit distribution, which is an available mathematical tool for secure steganography methods. However, existing attempts fail to achieve high capacity, time efficiency and correctness simultaneously, and their strongly coupling designs leave little room for refining them to achieve better performance. To provide a secure, high-capacity and efficient steganography method, we introduce ShiMer. Specifically, ShiMer pseudorandomly shifts the probability interval of the LLM's distribution to obtain a private distribution, and samples a token according to the private bits. ShiMer produced steganographic texts are indistinguishable in quality from the normal texts directly generated by the language model. To further enhance the capacity of ShiMer, we design a reordering algorithm to minimize the occurrence of interval splitting during decoding phase. Experimental results indicate that our method achieves the highest capacity and efficiency among existing secure steganography techniques.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "114",
        "title": "Creating, Using and Assessing a Generative-AI-Based Human-Chatbot-Dialogue Dataset with User-Interaction Learning Capabilities",
        "author": [
            "Alfredo Cuzzocrea",
            "Giovanni Pilato",
            "Pablo Garcia Bringas"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00791",
        "abstract": "The study illustrates a first step towards an ongoing work aimed at developing a dataset of dialogues potentially useful for customer service conversation management between humans and AI chatbots. The approach exploits ChatGPT 3.5 to generate dialogues. One of the requirements is that the dialogue is characterized by a specific language proficiency level of the user; the other one is that the user expresses a specific emotion during the interaction. The generated dialogues were then evaluated for overall quality. The complexity of the language used by both humans and AI agents, has been evaluated by using standard complexity measurements. Furthermore, the attitudes and interaction patterns exhibited by the chatbot at each turn have been stored for further detection of common conversation patterns in specific emotional contexts. The methodology could improve human-AI dialogue effectiveness and serve as a basis for systems that can learn from user interactions.",
        "tags": [
            "ChatGPT",
            "Detection"
        ]
    },
    {
        "id": "115",
        "title": "Multimodal Large Models Are Effective Action Anticipators",
        "author": [
            "Binglu Wang",
            "Yao Tian",
            "Shunzhou Wang",
            "Le Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00795",
        "abstract": "The task of long-term action anticipation demands solutions that can effectively model temporal dynamics over extended periods while deeply understanding the inherent semantics of actions. Traditional approaches, which primarily rely on recurrent units or Transformer layers to capture long-term dependencies, often fall short in addressing these challenges. Large Language Models (LLMs), with their robust sequential modeling capabilities and extensive commonsense knowledge, present new opportunities for long-term action anticipation. In this work, we introduce the ActionLLM framework, a novel approach that treats video sequences as successive tokens, leveraging LLMs to anticipate future actions. Our baseline model simplifies the LLM architecture by setting future tokens, incorporating an action tuning module, and reducing the textual decoder layer to a linear layer, enabling straightforward action prediction without the need for complex instructions or redundant descriptions. To further harness the commonsense reasoning of LLMs, we predict action categories for observed frames and use sequential textual clues to guide semantic understanding. In addition, we introduce a Cross-Modality Interaction Block, designed to explore the specificity within each modality and capture interactions between vision and textual modalities, thereby enhancing multimodal tuning. Extensive experiments on benchmark datasets demonstrate the superiority of the proposed ActionLLM framework, encouraging a promising direction to explore LLMs in the context of action anticipation. Code is available at https://github.com/2tianyao1/ActionLLM.git.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "116",
        "title": "Regression Guided Strategy to Automated Facial Beauty Optimization through Image Synthesis",
        "author": [
            "Erik Nguyen",
            "Spencer Htin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00811",
        "abstract": "The use of beauty filters on social media, which enhance the appearance of individuals in images, is a well-researched area, with existing methods proving to be highly effective. Traditionally, such enhancements are performed using rule-based approaches that leverage domain knowledge of facial features associated with attractiveness, applying very specific transformations to maximize these attributes. In this work, we present an alternative approach that projects facial images as points on the latent space of a pre-trained GAN, which are then optimized to produce beautiful faces. The movement of the latent points is guided by a newly developed facial beauty evaluation regression network, which learns to distinguish attractive facial features, outperforming many existing facial beauty evaluation models in this domain. By using this data-driven approach, our method can automatically capture holistic patterns in beauty directly from data rather than relying on predefined rules, enabling more dynamic and potentially broader applications of facial beauty editing. This work demonstrates a potential new direction for automated aesthetic enhancement, offering a complementary alternative to existing methods.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "117",
        "title": "MixSA: Training-free Reference-based Sketch Extraction via Mixture-of-Self-Attention",
        "author": [
            "Rui Yang",
            "Xiaojun Wu",
            "Shengfeng He"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00816",
        "abstract": "Current sketch extraction methods either require extensive training or fail to capture a wide range of artistic styles, limiting their practical applicability and versatility. We introduce Mixture-of-Self-Attention (MixSA), a training-free sketch extraction method that leverages strong diffusion priors for enhanced sketch perception. At its core, MixSA employs a mixture-of-self-attention technique, which manipulates self-attention layers by substituting the keys and values with those from reference sketches. This allows for the seamless integration of brushstroke elements into initial outline images, offering precise control over texture density and enabling interpolation between styles to create novel, unseen styles. By aligning brushstroke styles with the texture and contours of colored images, particularly in late decoder layers handling local textures, MixSA addresses the common issue of color averaging by adjusting initial outlines. Evaluated with various perceptual metrics, MixSA demonstrates superior performance in sketch quality, flexibility, and applicability. This approach not only overcomes the limitations of existing methods but also empowers users to generate diverse, high-fidelity sketches that more accurately reflect a wide range of artistic expressions.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "118",
        "title": "Decoupling Knowledge and Reasoning in Transformers: A Modular Architecture with Generalized Cross-Attention",
        "author": [
            "Zhenyu Guo",
            "Wenguang Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00823",
        "abstract": "Transformers have achieved remarkable success across diverse domains, but their monolithic architecture presents challenges in interpretability, adaptability, and scalability. This paper introduces a novel modular Transformer architecture that explicitly decouples knowledge and reasoning through a generalized cross-attention mechanism to a shared knowledge base, specifically designed for effective knowledge retrieval. Critically, we provide a rigorous mathematical derivation demonstrating that the Feed-Forward Network (FFN) in a standard Transformer is a specialized case (a closure) of this generalized cross-attention, revealing its role in implicit knowledge retrieval and validating our design. This theoretical framework provides a new lens for understanding FFNs and lays the foundation for future research exploring enhanced interpretability, adaptability, and scalability, enabling richer interplay with external knowledge bases and other systems.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "119",
        "title": "Embedding Style Beyond Topics: Analyzing Dispersion Effects Across Different Language Models",
        "author": [
            "Benjamin Icard",
            "Evangelia Zve",
            "Lila Sainero",
            "Alice Breton",
            "Jean-Gabriel Ganascia"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00828",
        "abstract": "This paper analyzes how writing style affects the dispersion of embedding vectors across multiple, state-of-the-art language models. While early transformer models primarily aligned with topic modeling, this study examines the role of writing style in shaping embedding spaces. Using a literary corpus that alternates between topics and styles, we compare the sensitivity of language models across French and English. By analyzing the particular impact of style on embedding dispersion, we aim to better understand how language models process stylistic information, contributing to their overall interpretability.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "120",
        "title": "An LLM-Empowered Adaptive Evolutionary Algorithm For Multi-Component Deep Learning Systems",
        "author": [
            "Haoxiang Tian",
            "Xingshuo Han",
            "Guoquan Wu",
            "An Guo",
            "Yuan Zhou. Jie Zhang",
            "Shuo Li",
            "Jun Wei",
            "Tianwei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00829",
        "abstract": "Multi-objective evolutionary algorithms (MOEAs) are widely used for searching optimal solutions in complex multi-component applications. Traditional MOEAs for multi-component deep learning (MCDL) systems face challenges in enhancing the search efficiency while maintaining the diversity. To combat these, this paper proposes $\\mu$MOEA, the first LLM-empowered adaptive evolutionary search algorithm to detect safety violations in MCDL systems. Inspired by the context-understanding ability of Large Language Models (LLMs), $\\mu$MOEA promotes the LLM to comprehend the optimization problem and generate an initial population tailed to evolutionary objectives. Subsequently, it employs adaptive selection and variation to iteratively produce offspring, balancing the evolutionary efficiency and diversity. During the evolutionary process, to navigate away from the local optima, $\\mu$MOEA integrates the evolutionary experience back into the LLM. This utilization harnesses the LLM's quantitative reasoning prowess to generate differential seeds, breaking away from current optimal solutions. We evaluate $\\mu$MOEA in finding safety violations of MCDL systems, and compare its performance with state-of-the-art MOEA methods. Experimental results show that $\\mu$MOEA can significantly improve the efficiency and diversity of the evolutionary search.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "121",
        "title": "LLM+AL: Bridging Large Language Models and Action Languages for Complex Reasoning about Actions",
        "author": [
            "Adam Ishay",
            "Joohyung Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00830",
        "abstract": "Large Language Models (LLMs) have made significant strides in various intelligent tasks but still struggle with complex action reasoning tasks that require systematic search. To address this limitation, we propose a method that bridges the natural language understanding capabilities of LLMs with the symbolic reasoning strengths of action languages. Our approach, termed \"LLM+AL,\" leverages the LLM's strengths in semantic parsing and commonsense knowledge generation alongside the action language's proficiency in automated reasoning based on encoded knowledge. We compare LLM+AL against state-of-the-art LLMs, including ChatGPT-4, Claude 3 Opus, Gemini Ultra 1.0, and o1-preview, using benchmarks for complex reasoning about actions. Our findings indicate that, although all methods exhibit errors, LLM+AL, with relatively minimal human corrections, consistently leads to correct answers, whereas standalone LLMs fail to improve even with human feedback. LLM+AL also contributes to automated generation of action languages.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "122",
        "title": "Spatially-guided Temporal Aggregation for Robust Event-RGB Optical Flow Estimation",
        "author": [
            "Qianang Zhou",
            "Junhui Hou",
            "Meiyi Yang",
            "Yongjian Deng",
            "Youfu Li",
            "Junlin Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00838",
        "abstract": "Current optical flow methods exploit the stable appearance of frame (or RGB) data to establish robust correspondences across time. Event cameras, on the other hand, provide high-temporal-resolution motion cues and excel in challenging scenarios. These complementary characteristics underscore the potential of integrating frame and event data for optical flow estimation. However, most cross-modal approaches fail to fully utilize the complementary advantages, relying instead on simply stacking information. This study introduces a novel approach that uses a spatially dense modality to guide the aggregation of the temporally dense event modality, achieving effective cross-modal fusion. Specifically, we propose an event-enhanced frame representation that preserves the rich texture of frames and the basic structure of events. We use the enhanced representation as the guiding modality and employ events to capture temporally dense motion information. The robust motion features derived from the guiding modality direct the aggregation of motion information from events. To further enhance fusion, we propose a transformer-based module that complements sparse event motion features with spatially rich frame information and enhances global information propagation. Additionally, a mix-fusion encoder is designed to extract comprehensive spatiotemporal contextual features from both modalities. Extensive experiments on the MVSEC and DSEC-Flow datasets demonstrate the effectiveness of our framework. Leveraging the complementary strengths of frames and events, our method achieves leading performance on the DSEC-Flow dataset. Compared to the event-only model, frame guidance improves accuracy by 10\\%. Furthermore, it outperforms the state-of-the-art fusion-based method with a 4\\% accuracy gain and a 45\\% reduction in inference time.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "123",
        "title": "IllusionBench: A Large-scale and Comprehensive Benchmark for Visual Illusion Understanding in Vision-Language Models",
        "author": [
            "Yiming Zhang",
            "Zicheng Zhang",
            "Xinyi Wei",
            "Xiaohong Liu",
            "Guangtao Zhai",
            "Xiongkuo Min"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00848",
        "abstract": "Current Visual Language Models (VLMs) show impressive image understanding but struggle with visual illusions, especially in real-world scenarios. Existing benchmarks focus on classical cognitive illusions, which have been learned by state-of-the-art (SOTA) VLMs, revealing issues such as hallucinations and limited perceptual abilities. To address this gap, we introduce IllusionBench, a comprehensive visual illusion dataset that encompasses not only classic cognitive illusions but also real-world scene illusions. This dataset features 1,051 images, 5,548 question-answer pairs, and 1,051 golden text descriptions that address the presence, causes, and content of the illusions. We evaluate ten SOTA VLMs on this dataset using true-or-false, multiple-choice, and open-ended tasks. In addition to real-world illusions, we design trap illusions that resemble classical patterns but differ in reality, highlighting hallucination issues in SOTA models. The top-performing model, GPT-4o, achieves 80.59% accuracy on true-or-false tasks and 76.75% on multiple-choice questions, but still lags behind human performance. In the semantic description task, GPT-4o's hallucinations on classical illusions result in low scores for trap illusions, even falling behind some open-source models. IllusionBench is, to the best of our knowledge, the largest and most comprehensive benchmark for visual illusions in VLMs to date.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "124",
        "title": "DiffETM: Diffusion Process Enhanced Embedded Topic Model",
        "author": [
            "Wei Shao",
            "Mingyang Liu",
            "Linqi Song"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00862",
        "abstract": "The embedded topic model (ETM) is a widely used approach that assumes the sampled document-topic distribution conforms to the logistic normal distribution for easier optimization. However, this assumption oversimplifies the real document-topic distribution, limiting the model's performance. In response, we propose a novel method that introduces the diffusion process into the sampling process of document-topic distribution to overcome this limitation and maintain an easy optimization process. We validate our method through extensive experiments on two mainstream datasets, proving its effectiveness in improving topic modeling performance.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "125",
        "title": "Large Language Models Are Read/Write Policy-Makers for Simultaneous Generation",
        "author": [
            "Shoutao Guo",
            "Shaolei Zhang",
            "Zhengrui Ma",
            "Yang Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00868",
        "abstract": "Simultaneous generation models write generation results while reading streaming inputs, necessitating a policy-maker to determine the appropriate output timing. Existing simultaneous generation methods generally adopt the traditional encoder-decoder architecture and learn the generation and policy-making capabilities through complex dynamic programming techniques. Although LLMs excel at text generation, they face challenges in taking on the role of policy-makers through traditional training methods, limiting their exploration in simultaneous generation. To overcome these limitations, we propose a novel LLM-driven Simultaneous Generation (LSG) framework, which allows the off-the-shelf LLM to decide the generation timing and produce output concurrently. Specifically, LSG selects the generation policy that minimizes latency as the baseline policy. Referring to the baseline policy, LSG enables the LLM to devise an improved generation policy that better balances latency and generation quality, and writes generation results accordingly. Experiments on simultaneous translation and streaming automatic speech recognition tasks show that our method can achieve state-of-the-art performance utilizing the open-source LLMs and demonstrate practicality in real-world scenarios.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "126",
        "title": "Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation",
        "author": [
            "Mingjia Li",
            "Shuang Li",
            "Tongrui Su",
            "Longhui Yuan",
            "Jian Liang",
            "Wei Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00873",
        "abstract": "Capitalizing on the complementary advantages of generative and discriminative models has always been a compelling vision in machine learning, backed by a growing body of research. This work discloses the hidden semantic structure within score-based generative models, unveiling their potential as effective discriminative priors. Inspired by our theoretical findings, we propose DUSA to exploit the structured semantic priors underlying diffusion score to facilitate the test-time adaptation of image classifiers or dense predictors. Notably, DUSA extracts knowledge from a single timestep of denoising diffusion, lifting the curse of Monte Carlo-based likelihood estimation over timesteps. We demonstrate the efficacy of our DUSA in adapting a wide variety of competitive pre-trained discriminative models on diverse test-time scenarios. Additionally, a thorough ablation study is conducted to dissect the pivotal elements in DUSA. Code is publicly available at https://github.com/BIT-DA/DUSA.",
        "tags": [
            "Diffusion",
            "Score-Based Generative"
        ]
    },
    {
        "id": "127",
        "title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models",
        "author": [
            "Hieu Man",
            "Nghia Trung Ngo",
            "Viet Dac Lai",
            "Ryan A. Rossi",
            "Franck Dernoncourt",
            "Thien Huu Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00874",
        "abstract": "Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largely unexplored. To address this limitation, we present LUSIFER, a novel zero-shot approach that adapts LLM-based embedding models for multilingual tasks without requiring multilingual supervision. LUSIFER's architecture combines a multilingual encoder, serving as a language-universal learner, with an LLM-based embedding model optimized for embedding-specific tasks. These components are seamlessly integrated through a minimal set of trainable parameters that act as a connector, effectively transferring the multilingual encoder's language understanding capabilities to the specialized embedding model. Additionally, to comprehensively evaluate multilingual embedding performance, we introduce a new benchmark encompassing 5 primary embedding tasks, 123 diverse datasets, and coverage across 14 languages. Extensive experimental results demonstrate that LUSIFER significantly enhances the multilingual performance across various embedding tasks, particularly for medium and low-resource languages, without requiring explicit multilingual training data.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "128",
        "title": "FGAseg: Fine-Grained Pixel-Text Alignment for Open-Vocabulary Semantic Segmentation",
        "author": [
            "Bingyu Li",
            "Da Zhang",
            "Zhiyuan Zhao",
            "Junyu Gao",
            "Xuelong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00877",
        "abstract": "Open-vocabulary segmentation aims to identify and segment specific regions and objects based on text-based descriptions. A common solution is to leverage powerful vision-language models (VLMs), such as CLIP, to bridge the gap between vision and text information. However, VLMs are typically pretrained for image-level vision-text alignment, focusing on global semantic features. In contrast, segmentation tasks require fine-grained pixel-level alignment and detailed category boundary information, which VLMs alone cannot provide. As a result, information extracted directly from VLMs can't meet the requirements of segmentation tasks. To address this limitation, we propose FGAseg, a model designed for fine-grained pixel-text alignment and category boundary supplementation. The core of FGAseg is a Pixel-Level Alignment module that employs a cross-modal attention mechanism and a text-pixel alignment loss to refine the coarse-grained alignment from CLIP, achieving finer-grained pixel-text semantic alignment. Additionally, to enrich category boundary information, we introduce the alignment matrices as optimizable pseudo-masks during forward propagation and propose Category Information Supplementation module. These pseudo-masks, derived from cosine and convolutional similarity, provide essential global and local boundary information between different categories. By combining these two strategies, FGAseg effectively enhances pixel-level alignment and category boundary information, addressing key challenges in open-vocabulary segmentation. Extensive experiments demonstrate that FGAseg outperforms existing methods on open-vocabulary semantic segmentation benchmarks.",
        "tags": [
            "CLIP",
            "Segmentation"
        ]
    },
    {
        "id": "129",
        "title": "TrustRAG: Enhancing Robustness and Trustworthiness in RAG",
        "author": [
            "Huichi Zhou",
            "Kin-Hei Lee",
            "Zhonghao Zhan",
            "Yue Chen",
            "Zhenhao Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00879",
        "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user queries. However, these systems remain vulnerable to corpus poisoning attacks that can significantly degrade LLM performance through the injection of malicious content. To address these challenges, we propose TrustRAG, a robust framework that systematically filters compromised and irrelevant content before it reaches the language model. Our approach implements a two-stage defense mechanism: first, it employs K-means clustering to identify potential attack patterns in retrieved documents based on their semantic embeddings, effectively isolating suspicious content. Second, it leverages cosine similarity and ROUGE metrics to detect malicious documents while resolving discrepancies between the model's internal knowledge and external information through a self-assessment process. TrustRAG functions as a plug-and-play, training-free module that integrates seamlessly with any language model, whether open or closed-source, maintaining high contextual relevance while strengthening defenses against attacks. Through extensive experimental validation, we demonstrate that TrustRAG delivers substantial improvements in retrieval accuracy, efficiency, and attack resistance compared to existing approaches across multiple model architectures and datasets. We have made TrustRAG available as open-source software at \\url{https://github.com/HuichiZhou/TrustRAG}.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "130",
        "title": "Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction",
        "author": [
            "Teng Hu",
            "Jiangning Zhang",
            "Ran Yi",
            "Jieyu Weng",
            "Yabiao Wang",
            "Xianfang Zeng",
            "Zhucun Xue",
            "Lizhuang Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00880",
        "abstract": "Employing LLMs for visual generation has recently become a research focus. However, the existing methods primarily transfer the LLM architecture to visual generation but rarely investigate the fundamental differences between language and vision. This oversight may lead to suboptimal utilization of visual generation capabilities within the LLM framework. In this paper, we explore the characteristics of visual embedding space under the LLM framework and discover that the correlation between visual embeddings can help achieve more stable and robust generation results. We present IAR, an Improved AutoRegressive Visual Generation Method that enhances the training efficiency and generation quality of LLM-based visual generation models. Firstly, we propose a Codebook Rearrangement strategy that uses balanced k-means clustering algorithm to rearrange the visual codebook into clusters, ensuring high similarity among visual features within each cluster. Leveraging the rearranged codebook, we propose a Cluster-oriented Cross-entropy Loss that guides the model to correctly predict the cluster where the token is located. This approach ensures that even if the model predicts the wrong token index, there is a high probability the predicted token is located in the correct cluster, which significantly enhances the generation quality and robustness. Extensive experiments demonstrate that our method consistently enhances the model training efficiency and performance from 100M to 1.4B, reducing the training time by half while achieving the same FID. Additionally, our approach can be applied to various LLM-based visual generation models and adheres to the scaling law, providing a promising direction for future research in LLM-based visual generation.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "131",
        "title": "FullTransNet: Full Transformer with Local-Global Attention for Video Summarization",
        "author": [
            "Libin Lan",
            "Lu Jiang",
            "Tianshu Yu",
            "Xiaojuan Liu",
            "Zhongshi He"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00882",
        "abstract": "Video summarization mainly aims to produce a compact, short, informative, and representative synopsis of raw videos, which is of great importance for browsing, analyzing, and understanding video content. Dominant video summarization approaches are generally based on recurrent or convolutional neural networks, even recent encoder-only transformers. We propose using full transformer as an alternative architecture to perform video summarization. The full transformer with an encoder-decoder structure, specifically designed for handling sequence transduction problems, is naturally suitable for video summarization tasks. This work considers supervised video summarization and casts it as a sequence-to-sequence learning problem. Our key idea is to directly apply the full transformer to the video summarization task, which is intuitively sound and effective. Also, considering the efficiency problem, we replace full attention with the combination of local and global sparse attention, which enables modeling long-range dependencies while reducing computational costs. Based on this, we propose a transformer-like architecture, named FullTransNet, which has a full encoder-decoder structure with local-global sparse attention for video summarization. Specifically, both the encoder and decoder in FullTransNet are stacked the same way as ones in the vanilla transformer, and the local-global sparse attention is used only at the encoder side. Extensive experiments on two public multimedia benchmark datasets SumMe and TVSum demonstrate that our proposed model can outperform other video summarization approaches, achieving F-Measures of 54.4% on SumMe and 63.9% on TVSum with relatively lower compute and memory requirements, verifying its effectiveness and efficiency. The code and models are publicly available on GitHub.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "132",
        "title": "Representation in large language models",
        "author": [
            "Cameron C. Yetman"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00885",
        "abstract": "The extraordinary success of recent Large Language Models (LLMs) on a diverse array of tasks has led to an explosion of scientific and philosophical theorizing aimed at explaining how they do what they do. Unfortunately, disagreement over fundamental theoretical issues has led to stalemate, with entrenched camps of LLM optimists and pessimists often committed to very different views of how these systems work. Overcoming stalemate requires agreement on fundamental questions, and the goal of this paper is to address one such question, namely: is LLM behavior driven partly by representation-based information processing of the sort implicated in biological cognition, or is it driven entirely by processes of memorization and stochastic table look-up? This is a question about what kind of algorithm LLMs implement, and the answer carries serious implications for higher level questions about whether these systems have beliefs, intentions, concepts, knowledge, and understanding. I argue that LLM behavior is partially driven by representation-based information processing, and then I describe and defend a series of practical techniques for investigating these representations and developing explanations on their basis. The resulting account provides a groundwork for future theorizing about language models and their successors.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "133",
        "title": "Unfolding the Headline: Iterative Self-Questioning for News Retrieval and Timeline Summarization",
        "author": [
            "Weiqi Wu",
            "Shen Huang",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Hai Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00888",
        "abstract": "In the fast-changing realm of information, the capacity to construct coherent timelines from extensive event-related content has become increasingly significant and challenging. The complexity arises in aggregating related documents to build a meaningful event graph around a central topic. This paper proposes CHRONOS - Causal Headline Retrieval for Open-domain News Timeline SummarizatiOn via Iterative Self-Questioning, which offers a fresh perspective on the integration of Large Language Models (LLMs) to tackle the task of Timeline Summarization (TLS). By iteratively reflecting on how events are linked and posing new questions regarding a specific news topic to gather information online or from an offline knowledge base, LLMs produce and refresh chronological summaries based on documents retrieved in each round. Furthermore, we curate Open-TLS, a novel dataset of timelines on recent news topics authored by professional journalists to evaluate open-domain TLS where information overload makes it impossible to find comprehensive relevant documents from the web. Our experiments indicate that CHRONOS is not only adept at open-domain timeline summarization, but it also rivals the performance of existing state-of-the-art systems designed for closed-domain applications, where a related news corpus is provided for summarization.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "134",
        "title": "Spatial Temporal Attention based Target Vehicle Trajectory Prediction for Internet of Vehicles",
        "author": [
            "Ouhan Huang",
            "Huanle Rao",
            "Xiaowen Cai",
            "Tianyun Wang",
            "Aolong Sun",
            "Sizhe Xing",
            "Yifan Sun",
            "Gangyong Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00890",
        "abstract": "Forecasting vehicle behavior within complex traffic environments is pivotal within Intelligent Transportation Systems (ITS). Though this technology plays a significant role in alleviating the prevalent operational difficulties in logistics and transportation systems, the precise prediction of vehicle trajectories still poses a substantial challenge. To address this, our study introduces the Spatio Temporal Attention-based methodology for Target Vehicle Trajectory Prediction (STATVTPred). This approach integrates Global Positioning System(GPS) localization technology to track target movement and dynamically predict the vehicle's future path using comprehensive spatio-temporal trajectory data. We map the vehicle trajectory onto a directed graph, after which spatial attributes are extracted via a Graph Attention Networks(GATs). The Transformer technology is employed to yield temporal features from the sequence. These elements are then amalgamated with local road network structure maps to filter and deliver a smooth trajectory sequence, resulting in precise vehicle trajectory http://prediction.This study validates our proposed STATVTPred method on T-Drive and Chengdu taxi-trajectory datasets. The experimental results demonstrate that STATVTPred achieves 6.38% and 10.55% higher Average Match Rate (AMR) than the Transformer model on the Beijing and Chengdu datasets, respectively. Compared to the LSTM Encoder-Decoder model, STATVTPred boosts AMR by 37.45% and 36.06% on the same datasets. This is expected to establish STATVTPred as a new approach for handling trajectory prediction of targets in logistics and transportation scenarios, thereby enhancing prediction accuracy.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "135",
        "title": "Large Language Model Based Multi-Agent System Augmented Complex Event Processing Pipeline for Internet of Multimedia Things",
        "author": [
            "Talha Zeeshan",
            "Abhishek Kumar",
            "Susanna Pirttikangas",
            "Sasu Tarkoma"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00906",
        "abstract": "This paper presents the development and evaluation of a Large Language Model (LLM), also known as foundation models, based multi-agent system framework for complex event processing (CEP) with a focus on video query processing use cases. The primary goal is to create a proof-of-concept (POC) that integrates state-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub) tools to address the integration of LLMs with current CEP systems. Utilizing the Autogen framework in conjunction with Kafka message brokers, the system demonstrates an autonomous CEP pipeline capable of handling complex workflows. Extensive experiments evaluate the system's performance across varying configurations, complexities, and video resolutions, revealing the trade-offs between functionality and latency. The results show that while higher agent count and video complexities increase latency, the system maintains high consistency in narrative coherence. This research builds upon and contributes to, existing novel approaches to distributed AI systems, offering detailed insights into integrating such systems into existing infrastructures.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "136",
        "title": "Population Aware Diffusion for Time Series Generation",
        "author": [
            "Yang Li",
            "Han Meng",
            "Zhenyu Bi",
            "Ingolv T. Urnes",
            "Haipeng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00910",
        "abstract": "Diffusion models have shown promising ability in generating high-quality time series (TS) data. Despite the initial success, existing works mostly focus on the authenticity of data at the individual level, but pay less attention to preserving the population-level properties on the entire dataset. Such population-level properties include value distributions for each dimension and distributions of certain functional dependencies (e.g., cross-correlation, CC) between different dimensions. For instance, when generating house energy consumption TS data, the value distributions of the outside temperature and the kitchen temperature should be preserved, as well as the distribution of CC between them. Preserving such TS population-level properties is critical in maintaining the statistical insights of the datasets, mitigating model bias, and augmenting downstream tasks like TS prediction. Yet, it is often overlooked by existing models. Hence, data generated by existing models often bear distribution shifts from the original data. We propose Population-aware Diffusion for Time Series (PaD-TS), a new TS generation model that better preserves the population-level properties. The key novelties of PaD-TS include 1) a new training method explicitly incorporating TS population-level property preservation, and 2) a new dual-channel encoder model architecture that better captures the TS data structure. Empirical results in major benchmark datasets show that PaD-TS can improve the average CC distribution shift score between real and synthetic data by 5.9x while maintaining a performance comparable to state-of-the-art models on individual-level authenticity.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "137",
        "title": "Aligning LLMs with Domain Invariant Reward Models",
        "author": [
            "David Wu",
            "Sanjiban Choudhury"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00911",
        "abstract": "Aligning large language models (LLMs) to human preferences is challenging in domains where preference data is unavailable. We address the problem of learning reward models for such target domains by leveraging feedback collected from simpler source domains, where human preferences are easier to obtain. Our key insight is that, while domains may differ significantly, human preferences convey \\emph{domain-agnostic} concepts that can be effectively captured by a reward model. We propose \\method, a framework that trains domain-invariant reward models by optimizing a dual loss: a domain loss that minimizes the divergence between source and target distribution, and a source loss that optimizes preferences on the source domain. We show \\method is a general approach that we evaluate and analyze across 4 distinct settings: (1) Cross-lingual transfer (accuracy: $0.621 \\rightarrow 0.661$), (2) Clean-to-noisy (accuracy: $0.671 \\rightarrow 0.703$), (3) Few-shot-to-full transfer (accuracy: $0.845 \\rightarrow 0.920$), and (4) Simple-to-complex tasks transfer (correlation: $0.508 \\rightarrow 0.556$). Our code, models and data are available at \\url{https://github.com/portal-cornell/dial}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "138",
        "title": "AutoPresent: Designing Structured Visuals from Scratch",
        "author": [
            "Jiaxin Ge",
            "Zora Zhiruo Wang",
            "Xuhui Zhou",
            "Yi-Hao Peng",
            "Sanjay Subramanian",
            "Qinyue Tan",
            "Maarten Sap",
            "Alane Suhr",
            "Daniel Fried",
            "Graham Neubig",
            "Trevor Darrell"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00912",
        "abstract": "Designing structured visuals such as presentation slides is essential for communicative needs, necessitating both content creation and visual planning skills. In this work, we tackle the challenge of automated slide generation, where models produce slide presentations from natural language (NL) instructions. We first introduce the SlidesBench benchmark, the first benchmark for slide generation with 7k training and 585 testing examples derived from 310 slide decks across 10 domains. SlidesBench supports evaluations that are (i)reference-based to measure similarity to a target slide, and (ii)reference-free to measure the design quality of generated slides alone. We benchmark end-to-end image generation and program generation methods with a variety of models, and find that programmatic methods produce higher-quality slides in user-interactable formats. Built on the success of program generation, we create AutoPresent, an 8B Llama-based model trained on 7k pairs of instructions paired with code for slide generation, and achieve results comparable to the closed-source model GPT-4o. We further explore iterative design refinement where the model is tasked to self-refine its own output, and we found that this process improves the slide's quality. We hope that our work will provide a basis for future work on generating structured visuals.",
        "tags": [
            "GPT",
            "LLaMA"
        ]
    },
    {
        "id": "139",
        "title": "Diffusion Policies for Generative Modeling of Spacecraft Trajectories",
        "author": [
            "Julia Briden",
            "Breanna Johnson",
            "Richard Linares",
            "Abhishek Cauligi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00915",
        "abstract": "Machine learning has demonstrated remarkable promise for solving the trajectory generation problem and in paving the way for online use of trajectory optimization for resource-constrained spacecraft. However, a key shortcoming in current machine learning-based methods for trajectory generation is that they require large datasets and even small changes to the original trajectory design requirements necessitate retraining new models to learn the parameter-to-solution mapping. In this work, we leverage compositional diffusion modeling to efficiently adapt out-of-distribution data and problem variations in a few-shot framework for 6 degree-of-freedom (DoF) powered descent trajectory generation. Unlike traditional deep learning methods that can only learn the underlying structure of one specific trajectory optimization problem, diffusion models are a powerful generative modeling framework that represents the solution as a probability density function (PDF) and this allows for the composition of PDFs encompassing a variety of trajectory design specifications and constraints. We demonstrate the capability of compositional diffusion models for inference-time 6 DoF minimum-fuel landing site selection and composable constraint representations. Using these samples as initial guesses for 6 DoF powered descent guidance enables dynamically feasible and computationally efficient trajectory generation.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "140",
        "title": "Hierarchical Vision-Language Alignment for Text-to-Image Generation via Diffusion Models",
        "author": [
            "Emily Johnson",
            "Noah Wilson"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00917",
        "abstract": "Text-to-image generation has witnessed significant advancements with the integration of Large Vision-Language Models (LVLMs), yet challenges remain in aligning complex textual descriptions with high-quality, visually coherent images. This paper introduces the Vision-Language Aligned Diffusion (VLAD) model, a generative framework that addresses these challenges through a dual-stream strategy combining semantic alignment and hierarchical diffusion. VLAD utilizes a Contextual Composition Module (CCM) to decompose textual prompts into global and local representations, ensuring precise alignment with visual features. Furthermore, it incorporates a multi-stage diffusion process with hierarchical guidance to generate high-fidelity images. Experiments conducted on MARIO-Eval and INNOVATOR-Eval benchmarks demonstrate that VLAD significantly outperforms state-of-the-art methods in terms of image quality, semantic alignment, and text rendering accuracy. Human evaluations further validate the superior performance of VLAD, making it a promising approach for text-to-image generation in complex scenarios.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "141",
        "title": "Multiscaled Multi-Head Attention-based Video Transformer Network for Hand Gesture Recognition",
        "author": [
            "Mallika Garg",
            "Debashis Ghosh",
            "Pyari Mohan Pradhan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00935",
        "abstract": "Dynamic gesture recognition is one of the challenging research areas due to variations in pose, size, and shape of the signer's hand. In this letter, Multiscaled Multi-Head Attention Video Transformer Network (MsMHA-VTN) for dynamic hand gesture recognition is proposed. A pyramidal hierarchy of multiscale features is extracted using the transformer multiscaled head attention model. The proposed model employs different attention dimensions for each head of the transformer which enables it to provide attention at the multiscale level. Further, in addition to single modality, recognition performance using multiple modalities is examined. Extensive experiments demonstrate the superior performance of the proposed MsMHA-VTN with an overall accuracy of 88.22\\% and 99.10\\% on NVGesture and Briareo datasets, respectively.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "142",
        "title": "Overlapping Schwarz methods are not anisotropy-robust multigrid smoothers",
        "author": [
            "Oliver A. Krzysik",
            "Ben S. Southworth",
            "Bobby Philip"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00938",
        "abstract": "We analyze overlapping multiplicative Schwarz methods as smoothers in the geometric multigrid solution of two-dimensional anisotropic diffusion problems. For diffusion equations, it is well known that the smoothing properties of point-wise smoothers, such as Gauss--Seidel, rapidly deteriorate as the strength of anisotropy increases. On the other hand, global smoothers based on line smoothing are known to generally provide good smoothing for diffusion problems, independent of the anisotropy strength. A natural question is whether global methods are really necessary to achieve good smoothing in such problems, or whether it can be obtained with locally overlapping block smoothers using sufficiently large blocks and overlap. Through local Fourier analysis and careful numerical experimentation, we show that global methods are indeed necessary to achieve anisotropy-robust smoothing. Specifically, for any fixed block size bounded sufficiently far away from the global domain size, we find that the smoothing properties of overlapping multiplicative Schwarz rapidly deteriorate with increasing anisotropy, irrespective of the amount of overlap between blocks. Moreover, our results indicate that anisotropy-robust smoothing requires blocks of diameter ${\\cal O}(\\epsilon^{-1/2})$ for anisotropy ratio $\\epsilon \\in (0,1]$.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "143",
        "title": "SPADE: Enhancing Adaptive Cyber Deception Strategies with Generative AI and Structured Prompt Engineering",
        "author": [
            "Shihab Ahmed",
            "A B M Mohaimenur Rahman",
            "Md Morshed Alam",
            "Md Sajidul Islam Sajid"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00940",
        "abstract": "The rapid evolution of modern malware presents significant challenges to the development of effective defense mechanisms. Traditional cyber deception techniques often rely on static or manually configured parameters, limiting their adaptability to dynamic and sophisticated threats. This study leverages Generative AI (GenAI) models to automate the creation of adaptive cyber deception ploys, focusing on structured prompt engineering (PE) to enhance relevance, actionability, and deployability. We introduce a systematic framework (SPADE) to address inherent challenges large language models (LLMs) pose to adaptive deceptions, including generalized outputs, ambiguity, under-utilization of contextual information, and scalability constraints. Evaluations across diverse malware scenarios using metrics such as Recall, Exact Match (EM), BLEU Score, and expert quality assessments identified ChatGPT-4o as the top performer. Additionally, it achieved high engagement (93%) and accuracy (96%) with minimal refinements. Gemini and ChatGPT-4o Mini demonstrated competitive performance, with Llama3.2 showing promise despite requiring further optimization. These findings highlight the transformative potential of GenAI in automating scalable, adaptive deception strategies and underscore the critical role of structured PE in advancing real-world cybersecurity applications.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "144",
        "title": "A Novel Diffusion Model for Pairwise Geoscience Data Generation with Unbalanced Training Dataset",
        "author": [
            "Junhuan Yang",
            "Yuzhou Zhang",
            "Yi Sheng",
            "Youzuo Lin",
            "Lei Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00941",
        "abstract": "Recently, the advent of generative AI technologies has made transformational impacts on our daily lives, yet its application in scientific applications remains in its early stages. Data scarcity is a major, well-known barrier in data-driven scientific computing, so physics-guided generative AI holds significant promise. In scientific computing, most tasks study the conversion of multiple data modalities to describe physical phenomena, for example, spatial and waveform in seismic imaging, time and frequency in signal processing, and temporal and spectral in climate modeling; as such, multi-modal pairwise data generation is highly required instead of single-modal data generation, which is usually used in natural images (e.g., faces, scenery). Moreover, in real-world applications, the unbalance of available data in terms of modalities commonly exists; for example, the spatial data (i.e., velocity maps) in seismic imaging can be easily simulated, but real-world seismic waveform is largely lacking. While the most recent efforts enable the powerful diffusion model to generate multi-modal data, how to leverage the unbalanced available data is still unclear. In this work, we use seismic imaging in subsurface geophysics as a vehicle to present ``UB-Diff'', a novel diffusion model for multi-modal paired scientific data generation. One major innovation is a one-in-two-out encoder-decoder network structure, which can ensure pairwise data is obtained from a co-latent representation. Then, the co-latent representation will be used by the diffusion process for pairwise data generation. Experimental results on the OpenFWI dataset show that UB-Diff significantly outperforms existing techniques in terms of Frchet Inception Distance (FID) score and pairwise evaluation, indicating the generation of reliable and useful multi-modal pairwise data.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "145",
        "title": "Diffusion Prism: Enhancing Diversity and Morphology Consistency in Mask-to-Image Diffusion",
        "author": [
            "Hao Wang",
            "Xiwen Chen",
            "Ashish Bastola",
            "Jiayou Qin",
            "Abolfazl Razi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00944",
        "abstract": "The emergence of generative AI and controllable diffusion has made image-to-image synthesis increasingly practical and efficient. However, when input images exhibit low entropy and sparse, the inherent characteristics of diffusion models often result in limited diversity. This constraint significantly interferes with data augmentation. To address this, we propose Diffusion Prism, a training-free framework that efficiently transforms binary masks into realistic and diverse samples while preserving morphological features. We explored that a small amount of artificial noise will significantly assist the image-denoising process. To prove this novel mask-to-image concept, we use nano-dendritic patterns as an example to demonstrate the merit of our method compared to existing controllable diffusion models. Furthermore, we extend the proposed framework to other biological patterns, highlighting its potential applications across various fields.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "146",
        "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant Computation Elimination in Diffusion Model",
        "author": [
            "Omid Saghatchian",
            "Atiyeh Gh. Moghadam",
            "Ahmad Nickabadi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00946",
        "abstract": "Diffusion models have emerged as a promising approach for generating high-quality, high-dimensional images. Nevertheless, these models are hindered by their high computational cost and slow inference, partly due to the quadratic computational complexity of the self-attention mechanisms with respect to input size. Various approaches have been proposed to address this drawback. One such approach focuses on reducing the number of tokens fed into the self-attention, known as token merging (ToMe). In our method, which is called cached adaptive token merging(CA-ToMe), we calculate the similarity between tokens and then merge the r proportion of the most similar tokens. However, due to the repetitive patterns observed in adjacent steps and the variation in the frequency of similarities, we aim to enhance this approach by implementing an adaptive threshold for merging tokens and adding a caching mechanism that stores similar pairs across several adjacent steps. Empirical results demonstrate that our method operates as a training-free acceleration method, achieving a speedup factor of 1.24 in the denoising process while maintaining the same FID scores compared to existing approaches.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "147",
        "title": "Incremental Dialogue Management: Survey, Discussion, and Implications for HRI",
        "author": [
            "Casey Kennington",
            "Pierre Lison",
            "David Schlangen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00953",
        "abstract": "Efforts towards endowing robots with the ability to speak have benefited from recent advancements in NLP, in particular large language models. However, as powerful as current models have become, they still operate on sentence or multi-sentence level input, not on the word-by-word input that humans operate on, affecting the degree of responsiveness that they offer, which is critical in situations where humans interact with robots using speech. In this paper, we review the literature on interactive systems that operate incrementally (i.e., at the word level or below it). We motivate the need for incremental systems, survey incremental modeling of important aspects of dialogue like speech recognition and language generation. Primary focus is on the part of the system that makes decisions, known as the dialogue manager. We find that there is very little research on incremental dialogue management, offer some requirements for practical incremental dialogue management, and the implications of incremental dialogue for embodied, robotic platforms.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "148",
        "title": "Generative AI and LLMs in Industry: A text-mining Analysis and Critical Evaluation of Guidelines and Policy Statements Across Fourteen Industrial Sectors",
        "author": [
            "Junfeng Jiao",
            "Saleh Afroogh",
            "Kevin Chen",
            "David Atkinson",
            "Amit Dhurandhar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00957",
        "abstract": "The rise of Generative AI (GAI) and Large Language Models (LLMs) has transformed industrial landscapes, offering unprecedented opportunities for efficiency and innovation while raising critical ethical, regulatory, and operational challenges. This study conducts a text-based analysis of 160 guidelines and policy statements across fourteen industrial sectors, utilizing systematic methods and text-mining techniques to evaluate the governance of these technologies. By examining global directives, industry practices, and sector-specific policies, the paper highlights the complexities of balancing innovation with ethical accountability and equitable access. The findings provide actionable insights and recommendations for fostering responsible, transparent, and safe integration of GAI and LLMs in diverse industry contexts.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "149",
        "title": "IGGA: A Dataset of Industrial Guidelines and Policy Statements for Generative AIs",
        "author": [
            "Junfeng Jiao",
            "Saleh Afroogh",
            "Kevin Chen",
            "David Atkinson",
            "Amit Dhurandhar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00959",
        "abstract": "This paper introduces IGGA, a dataset of 160 industry guidelines and policy statements for the use of Generative AIs (GAIs) and Large Language Models (LLMs) in industry and workplace settings, collected from official company websites, and trustworthy news sources. The dataset contains 104,565 words and serves as a valuable resource for natural language processing tasks commonly applied in requirements engineering, such as model synthesis, abstraction identification, and document structure assessment. Additionally, IGGA can be further annotated to function as a benchmark for various tasks, including ambiguity detection, requirements categorization, and the identification of equivalent requirements. Our methodologically rigorous approach ensured a thorough examination, with a selection of reputable and influential companies that represent a diverse range of global institutions across six continents. The dataset captures perspectives from fourteen industry sectors, including technology, finance, and both public and private institutions, offering a broad spectrum of insights into the integration of GAIs and LLMs in industry.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "150",
        "title": "OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes",
        "author": [
            "Sepehr Dehdashtian",
            "Gautam Sreekumar",
            "Vishnu Naresh Boddeti"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00962",
        "abstract": "Images generated by text-to-image (T2I) models often exhibit visual biases and stereotypes of concepts such as culture and profession. Existing quantitative measures of stereotypes are based on statistical parity that does not align with the sociological definition of stereotypes and, therefore, incorrectly categorizes biases as stereotypes. Instead of oversimplifying stereotypes as biases, we propose a quantitative measure of stereotypes that aligns with its sociological definition. We then propose OASIS to measure the stereotypes in a generated dataset and understand their origins within the T2I model. OASIS includes two scores to measure stereotypes from a generated image dataset: (M1) Stereotype Score to measure the distributional violation of stereotypical attributes, and (M2) WALS to measure spectral variance in the images along a stereotypical attribute. OASIS also includes two methods to understand the origins of stereotypes in T2I models: (U1) StOP to discover attributes that the T2I model internally associates with a given concept, and (U2) SPI to quantify the emergence of stereotypical attributes in the latent space of the T2I model during image generation. Despite the considerable progress in image fidelity, using OASIS, we conclude that newer T2I models such as FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts and still generate images with widespread stereotypical attributes. Additionally, the quantity of stereotypes worsens for nationalities with lower Internet footprints.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "151",
        "title": "Optimizing Noise Schedules of Generative Models in High Dimensionss",
        "author": [
            "Santiago Aranguri",
            "Giulio Biroli",
            "Marc Mezard",
            "Eric Vanden-Eijnden"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00988",
        "abstract": "Recent works have shown that diffusion models can undergo phase transitions, the resolution of which is needed for accurately generating samples. This has motivated the use of different noise schedules, the two most common choices being referred to as variance preserving (VP) and variance exploding (VE). Here we revisit these schedules within the framework of stochastic interpolants. Using the Gaussian Mixture (GM) and Curie-Weiss (CW) data distributions as test case models, we first investigate the effect of the variance of the initial noise distribution and show that VP recovers the low-level feature (the distribution of each mode) but misses the high-level feature (the asymmetry between modes), whereas VE performs oppositely. We also show that this dichotomy, which happens when denoising by a constant amount in each step, can be avoided by using noise schedules specific to VP and VE that allow for the recovery of both high- and low-level features. Finally we show that these schedules yield generative models for the GM and CW model whose probability flow ODE can be discretized using $\\Theta_d(1)$ steps in dimension $d$ instead of the $\\Theta_d(\\sqrt{d})$ steps required by constant denoising.",
        "tags": [
            "Diffusion",
            "ODE"
        ]
    },
    {
        "id": "152",
        "title": "Exploring Information Processing in Large Language Models: Insights from Information Bottleneck Theory",
        "author": [
            "Zhou Yang",
            "Zhengyu Qi",
            "Zhaochun Ren",
            "Zhikai Jia",
            "Haizhou Sun",
            "Xiaofei Zhu",
            "Xiangwen Liao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00999",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks by understanding input information and predicting corresponding outputs. However, the internal mechanisms by which LLMs comprehend input and make effective predictions remain poorly understood. In this paper, we explore the working mechanism of LLMs in information processing from the perspective of Information Bottleneck Theory. We propose a non-training construction strategy to define a task space and identify the following key findings: (1) LLMs compress input information into specific task spaces (e.g., sentiment space, topic space) to facilitate task understanding; (2) they then extract and utilize relevant information from the task space at critical moments to generate accurate predictions. Based on these insights, we introduce two novel approaches: an Information Compression-based Context Learning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances reasoning performance and inference efficiency by compressing retrieved example information into the task space. TS-FT employs a space-guided loss to fine-tune LLMs, encouraging the learning of more effective compression and selection mechanisms. Experiments across multiple datasets validate the effectiveness of task space construction. Additionally, IC-ICL not only improves performance but also accelerates inference speed by over 40\\%, while TS-FT achieves superior results with a minimal strategy adjustment.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "153",
        "title": "EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy",
        "author": [
            "Ao Gao",
            "Luosong Guo",
            "Tao Chen",
            "Zhao Wang",
            "Ying Tai",
            "Jian Yang",
            "Zhenyu Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01003",
        "abstract": "3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene representation. Despite their impressive performance, they confront challenges due to the limitation of structure-from-motion (SfM) methods on acquiring accurate scene initialization, or the inefficiency of densification strategy. In this paper, we introduce a novel framework EasySplat to achieve high-quality 3DGS modeling. Instead of using SfM for scene initialization, we employ a novel method to release the power of large-scale pointmap approaches. Specifically, we propose an efficient grouping strategy based on view similarity, and use robust pointmap priors to obtain high-quality point clouds and camera poses for 3D scene initialization. After obtaining a reliable scene structure, we propose a novel densification approach that adaptively splits Gaussian primitives based on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme. In this way, the proposed method tackles the limitation on initialization and optimization, leading to an efficient and accurate 3DGS modeling. Extensive experiments demonstrate that EasySplat outperforms the current state-of-the-art (SOTA) in handling novel view synthesis.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "154",
        "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving",
        "author": [
            "Zihao Ye",
            "Lequn Chen",
            "Ruihang Lai",
            "Wuwei Lin",
            "Yineng Zhang",
            "Stephanie Wang",
            "Tianqi Chen",
            "Baris Kasikci",
            "Vinod Grover",
            "Arvind Krishnamurthy",
            "Luis Ceze"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01005",
        "abstract": "Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "155",
        "title": "CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction",
        "author": [
            "Mohammad Shahab Sepehri",
            "Asal Mehradfar",
            "Mahdi Soltanolkotabi",
            "Salman Avestimehr"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01010",
        "abstract": "Predicting Bitcoin price remains a challenging problem due to the high volatility and complex non-linear dynamics of cryptocurrency markets. Traditional time-series models, such as ARIMA and GARCH, and recurrent neural networks, like LSTMs, have been widely applied to this task but struggle to capture the regime shifts and long-range dependencies inherent in the data. In this work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM) architecture designed to effectively capture long-range dependencies in financial time-series data. Our experiments show that CryptoMamba not only provides more accurate predictions but also offers enhanced generalizability across different market conditions, surpassing the limitations of previous models. Coupled with trading algorithms for real-world scenarios, CryptoMamba demonstrates its practical utility by translating accurate forecasts into financial outcomes. Our findings signal a huge advantage for SSMs in stock and cryptocurrency price forecasting tasks.",
        "tags": [
            "Mamba",
            "SSMs",
            "State Space Models"
        ]
    },
    {
        "id": "156",
        "title": "MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based on Large language Model",
        "author": [
            "Chengze Zhang",
            "Changshan Li",
            "Shiyang Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01014",
        "abstract": "The exponential growth of data and advancements in big data technologies have created a demand for more efficient and automated approaches to data analysis and storytelling. However, automated data analysis systems still face challenges in leveraging large language models (LLMs) for data insight discovery, augmented analysis, and data storytelling. This paper introduces the Multidimensional Data Storytelling Framework (MDSF) based on large language models for automated insight generation and context-aware storytelling. The framework incorporates advanced preprocessing techniques, augmented analysis algorithms, and a unique scoring mechanism to identify and prioritize actionable insights. The use of fine-tuned LLMs enhances contextual understanding and generates narratives with minimal manual intervention. The architecture also includes an agent-based mechanism for real-time storytelling continuation control. Key findings reveal that MDSF outperforms existing methods across various datasets in terms of insight ranking accuracy, descriptive quality, and narrative coherence. The experimental evaluation demonstrates MDSF's ability to automate complex analytical tasks, reduce interpretive biases, and improve user satisfaction. User studies further underscore its practical utility in enhancing content structure, conclusion extraction, and richness of detail.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "157",
        "title": "Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo Matching Transformer",
        "author": [
            "Ziyang Chen",
            "Yongjun Zhang",
            "Wenting Li",
            "Bingshu Wang",
            "Yabo Wu",
            "Yong Zhao",
            "C.L. Philip Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01023",
        "abstract": "In light of the advancements in transformer technology, extant research posits the construction of stereo transformers as a potential solution to the binocular stereo matching challenge. However, constrained by the low-rank bottleneck and quadratic complexity of attention mechanisms, stereo transformers still fail to demonstrate sufficient nonlinear expressiveness within a reasonable inference time. The lack of focus on key homonymous points renders the representations of such methods vulnerable to challenging conditions, including reflections and weak textures. Furthermore, a slow computing speed is not conducive to the application. To overcome these difficulties, we present the \\textbf{H}adamard \\textbf{A}ttention \\textbf{R}ecurrent Stereo \\textbf{T}ransformer (HART) that incorporates the following components: 1) For faster inference, we present a Hadamard product paradigm for the attention mechanism, achieving linear computational complexity. 2) We designed a Dense Attention Kernel (DAK) to amplify the differences between relevant and irrelevant feature responses. This allows HART to focus on important details. DAK also converts zero elements to non-zero elements to mitigate the reduced expressiveness caused by the low-rank bottleneck. 3) To compensate for the spatial and channel interaction missing in the Hadamard product, we propose MKOI to capture both global and local information through the interleaving of large and small kernel convolutions. Experimental results demonstrate the effectiveness of our HART. In reflective area, HART ranked \\textbf{1st} on the KITTI 2012 benchmark among all published methods at the time of submission. Code is available at \\url{https://github.com/ZYangChen/HART}.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "158",
        "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model",
        "author": [
            "Xinshuo Hu",
            "Zifei Shan",
            "Xinping Zhao",
            "Zetian Sun",
            "Zhenyu Liu",
            "Dongfang Li",
            "Shaolin Ye",
            "Xinyuan Wei",
            "Qian Chen",
            "Baotian Hu",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01028",
        "abstract": "As retrieval-augmented generation prevails in large language models, embedding models are becoming increasingly crucial. Despite the growing number of general embedding models, prior work often overlooks the critical role of training data quality. In this work, we introduce KaLM-Embedding, a general multilingual embedding model that leverages a large quantity of cleaner, more diverse, and domain-specific training data. Our model has been trained with key techniques proven to enhance performance: (1) persona-based synthetic data to create diversified examples distilled from LLMs, (2) ranking consistency filtering to remove less informative samples, and (3) semi-homogeneous task batch sampling to improve training efficacy. Departing from traditional BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model, facilitating the adaptation of auto-regressive language models for general embedding tasks. Extensive evaluations of the MTEB benchmark across multiple languages show that our model outperforms others of comparable size, setting a new standard for multilingual embedding models with <1B parameters.",
        "tags": [
            "BERT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "159",
        "title": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning",
        "author": [
            "Wonduk Seo",
            "Zonghao Yuan",
            "Yi Bu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01031",
        "abstract": "Cultural values alignment in Large Language Models (LLMs) is a critical challenge due to their tendency to embed Western-centric biases from training data, leading to misrepresentations and fairness issues in cross-cultural contexts. Recent approaches, such as role-assignment and few-shot learning, often struggle with reliable cultural alignment as they heavily rely on pre-trained knowledge, lack scalability, and fail to capture nuanced cultural values effectively. To address these issues, we propose ValuesRAG, a novel and effective framework that applies Retrieval-Augmented Generation (RAG) with in-context learning to integrate cultural and demographic knowledge dynamically during text generation. Leveraging the World Values Survey (WVS) dataset, ValuesRAG first generates summaries of values for each individual. Subsequently, we curated several representative regional datasets to serve as test datasets and retrieve relevant summaries of values based on demographic features, followed by a reranking step to select the top-k relevant summaries. ValuesRAG consistently outperforms baseline methods, both in the main experiment and in the ablation study where only the values summary was provided, highlighting ValuesRAG's potential to foster culturally aligned AI systems and enhance the inclusivity of AI-driven applications.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "160",
        "title": "Advancing Singlish Understanding: Bridging the Gap with Datasets and Multimodal Models",
        "author": [
            "Bin Wang",
            "Xunlong Zou",
            "Shuo Sun",
            "Wenyu Zhang",
            "Yingxu He",
            "Zhuohan Liu",
            "Chengwei Wei",
            "Nancy F. Chen",
            "AiTi Aw"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01034",
        "abstract": "Singlish, a Creole language rooted in English, is a key focus in linguistic research within multilingual and multicultural contexts. However, its spoken form remains underexplored, limiting insights into its linguistic structure and applications. To address this gap, we standardize and annotate the largest spoken Singlish corpus, introducing the Multitask National Speech Corpus (MNSC). These datasets support diverse tasks, including Automatic Speech Recognition (ASR), Spoken Question Answering (SQA), Spoken Dialogue Summarization (SDS), and Paralinguistic Question Answering (PQA). We release standardized splits and a human-verified test set to facilitate further research. Additionally, we propose SingAudioLLM, a multi-task multimodal model leveraging multimodal large language models to handle these tasks concurrently. Experiments reveal our models adaptability to Singlish context, achieving state-of-the-art performance and outperforming prior models by 10-30% in comparison with other AudioLLMs and cascaded solutions.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "161",
        "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
        "author": [
            "Yixing Xu",
            "Shivank Nag",
            "Dong Li",
            "Lu Tian",
            "Emad Barsoum"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01039",
        "abstract": "Transformer-based LLMs have achieved exceptional performance across a wide range of NLP tasks. However, the standard self-attention mechanism suffers from quadratic time complexity and linearly increased cache size. Sliding window attention (SWA) solves this problem by restricting the attention range to a fixed-size local context window. Nevertheless, SWA employs a uniform window size for each head in each layer, making it inefficient in capturing context of varying scales. To mitigate this limitation, we propose Multi-Scale Window Attention (MSWA) which applies diverse window sizes across heads and layers in the Transformer. It not only allows for different window sizes among heads within the same layer but also progressively increases window size allocation from shallow to deep layers, thus enabling the model to capture contextual information with different lengths and distances. Experimental results on language modeling and common-sense reasoning tasks substantiate that MSWA outperforms traditional local attention in both effectiveness and efficiency.",
        "tags": [
            "LLMs",
            "Transformer"
        ]
    },
    {
        "id": "162",
        "title": "Event Masked Autoencoder: Point-wise Action Recognition with Event-Based Cameras",
        "author": [
            "Jingkai Sun",
            "Qiang Zhang",
            "Jiaxu Wang",
            "Jiahang Cao",
            "Renjing Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01040",
        "abstract": "Dynamic vision sensors (DVS) are bio-inspired devices that capture visual information in the form of asynchronous events, which encode changes in pixel intensity with high temporal resolution and low latency. These events provide rich motion cues that can be exploited for various computer vision tasks, such as action recognition. However, most existing DVS-based action recognition methods lose temporal information during data transformation or suffer from noise and outliers caused by sensor imperfections or environmental factors. To address these challenges, we propose a novel framework that preserves and exploits the spatiotemporal structure of event data for action recognition. Our framework consists of two main components: 1) a point-wise event masked autoencoder (MAE) that learns a compact and discriminative representation of event patches by reconstructing them from masked raw event camera points data; 2) an improved event points patch generation algorithm that leverages an event data inlier model and point-wise data augmentation techniques to enhance the quality and diversity of event points patches. To the best of our knowledge, our approach introduces the pre-train method into event camera raw points data for the first time, and we propose a novel event points patch embedding to utilize transformer-based models on event cameras.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "163",
        "title": "Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs",
        "author": [
            "Linhao Huang",
            "Xue Jiang",
            "Zhiqiang Wang",
            "Wentao Mo",
            "Xi Xiao",
            "Bo Han",
            "Yongjie Yin",
            "Feng Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01042",
        "abstract": "Video-based multimodal large language models (V-MLLMs) have shown vulnerability to adversarial examples in video-text multimodal tasks. However, the transferability of adversarial videos to unseen models--a common and practical real world scenario--remains unexplored. In this paper, we pioneer an investigation into the transferability of adversarial video samples across V-MLLMs. We find that existing adversarial attack methods face significant limitations when applied in black-box settings for V-MLLMs, which we attribute to the following shortcomings: (1) lacking generalization in perturbing video features, (2) focusing only on sparse key-frames, and (3) failing to integrate multimodal information. To address these limitations and deepen the understanding of V-MLLM vulnerabilities in black-box scenarios, we introduce the Image-to-Video MLLM (I2V-MLLM) attack. In I2V-MLLM, we utilize an image-based multimodal model (IMM) as a surrogate model to craft adversarial video samples. Multimodal interactions and temporal information are integrated to disrupt video representations within the latent space, improving adversarial transferability. In addition, a perturbation propagation technique is introduced to handle different unknown frame sampling strategies. Experimental results demonstrate that our method can generate adversarial examples that exhibit strong transferability across different V-MLLMs on multiple video-text multimodal tasks. Compared to white-box attacks on these models, our black-box attacks (using BLIP-2 as surrogate model) achieve competitive performance, with average attack success rates of 55.48% on MSVD-QA and 58.26% on MSRVTT-QA for VideoQA tasks, respectively. Our code will be released upon acceptance.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "164",
        "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
        "author": [
            "Youngjun Son",
            "Chaewon Kim",
            "Jaejin Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01046",
        "abstract": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving training performance and efficiency of LLMs. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework \\sys that optimizes MinHash LSH for GPU clusters and leverages computationally efficient and partially reusable non-cryptographic hash functions. \\sys significantly outperforms the CPU-based deduplication tool included in SlimPajama by up to 58.3 times and the GPU-based deduplication tool included in NVIDIA NeMo Curator by up to 8.6 times when processing 1 million documents with a node of four GPUs. Deduplication of 1.2 trillion tokens is completed in just 5.1 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (https://github.com/mcrl/FED).",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "165",
        "title": "Dynamic Scaling of Unit Tests for Code Reward Modeling",
        "author": [
            "Zeyao Ma",
            "Xiaokang Zhang",
            "Jing Zhang",
            "Jifan Yu",
            "Sijia Luo",
            "Jie Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01054",
        "abstract": "Current large language models (LLMs) often struggle to produce accurate responses on the first attempt for complex reasoning tasks like code generation. Prior research tackles this challenge by generating multiple candidate solutions and validating them with LLM-generated unit tests. The execution results of unit tests serve as reward signals to identify correct solutions. As LLMs always confidently make mistakes, these unit tests are not reliable, thereby diminishing the quality of reward signals. Motivated by the observation that scaling the number of solutions improves LLM performance, we explore the impact of scaling unit tests to enhance reward signal quality. Our pioneer experiment reveals a positive correlation between the number of unit tests and reward signal quality, with greater benefits observed in more challenging problems. Based on these insights, we propose CodeRM-8B, a lightweight yet effective unit test generator that enables efficient and high-quality unit test scaling. Additionally, we implement a dynamic scaling mechanism that adapts the number of unit tests based on problem difficulty, further improving efficiency. Experimental results show that our approach significantly improves performance across various models on three benchmarks (e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on HumanEval Plus).",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "166",
        "title": "Risks of Cultural Erasure in Large Language Models",
        "author": [
            "Rida Qadri",
            "Aida M. Davani",
            "Kevin Robinson",
            "Vinodkumar Prabhakaran"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01056",
        "abstract": "Large language models are increasingly being integrated into applications that shape the production and discovery of societal knowledge such as search, online education, and travel planning. As a result, language models will shape how people learn about, perceive and interact with global cultures making it important to consider whose knowledge systems and perspectives are represented in models. Recognizing this importance, increasingly work in Machine Learning and NLP has focused on evaluating gaps in global cultural representational distribution within outputs. However, more work is needed on developing benchmarks for cross-cultural impacts of language models that stem from a nuanced sociologically-aware conceptualization of cultural impact or harm. We join this line of work arguing for the need of metricizable evaluations of language technologies that interrogate and account for historical power inequities and differential impacts of representation on global cultures, particularly for cultures already under-represented in the digital corpora. We look at two concepts of erasure: omission: where cultures are not represented at all and simplification i.e. when cultural complexity is erased by presenting one-dimensional views of a rich culture. The former focuses on whether something is represented, and the latter on how it is represented. We focus our analysis on two task contexts with the potential to influence global cultural production. First, we probe representations that a language model produces about different places around the world when asked to describe these contexts. Second, we analyze the cultures represented in the travel recommendations produced by a set of language model applications. Our study shows ways in which the NLP community and application developers can begin to operationalize complex socio-cultural considerations into standard evaluations and benchmarks.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "167",
        "title": "Dynamic Attention-Guided Context Decoding for Mitigating Context Faithfulness Hallucinations in Large Language Models",
        "author": [
            "Yanwen Huang",
            "Yong Zhang",
            "Ning Cheng",
            "Zhitao Li",
            "Shaojun Wang",
            "Jing Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01059",
        "abstract": "Large language models (LLMs) often suffer from context faithfulness hallucinations, where outputs deviate from retrieved information due to insufficient context utilization and high output uncertainty. Our uncertainty evaluation experiments reveal a strong correlation between high uncertainty and hallucinations. We hypothesize that attention mechanisms encode signals indicative of contextual utilization, validated through probing analysis. Based on these insights, we propose Dynamic Attention-Guided Context Decoding (DAGCD), a lightweight framework that integrates attention distributions and uncertainty signals in a single-pass decoding process. Experiments across QA datasets demonstrate DAGCD's effectiveness, achieving significant improvements in faithfulness and robustness while maintaining computational efficiency.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "168",
        "title": "BeliN: A Novel Corpus for Bengali Religious News Headline Generation using Contextual Feature Fusion",
        "author": [
            "Md Osama",
            "Ashim Dey",
            "Kawsar Ahmed",
            "Muhammad Ashad Kabir"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01069",
        "abstract": "Automatic text summarization, particularly headline generation, remains a critical yet underexplored area for Bengali religious news. Existing approaches to headline generation typically rely solely on the article content, overlooking crucial contextual features such as sentiment, category, and aspect. This limitation significantly hinders their effectiveness and overall performance. This study addresses this limitation by introducing a novel corpus, BeliN (Bengali Religious News) - comprising religious news articles from prominent Bangladeshi online newspapers, and MultiGen - a contextual multi-input feature fusion headline generation approach. Leveraging transformer-based pre-trained language models such as BanglaT5, mBART, mT5, and mT0, MultiGen integrates additional contextual features - including category, aspect, and sentiment - with the news content. This fusion enables the model to capture critical contextual information often overlooked by traditional methods. Experimental results demonstrate the superiority of MultiGen over the baseline approach that uses only news content, achieving a BLEU score of 18.61 and ROUGE-L score of 24.19, compared to baseline approach scores of 16.08 and 23.08, respectively. These findings underscore the importance of incorporating contextual features in headline generation for low-resource languages. By bridging linguistic and cultural gaps, this research advances natural language processing for Bengali and other underrepresented languages. To promote reproducibility and further exploration, the dataset and implementation code are publicly accessible at https://github.com/akabircs/BeliN.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "169",
        "title": "Bridging Simplicity and Sophistication using GLinear: A Novel Architecture for Enhanced Time Series Prediction",
        "author": [
            "Syed Tahir Hussain Rizvi",
            "Neel Kanwal",
            "Muddasar Naeem",
            "Alfredo Cuzzocrea",
            "Antonio Coronato"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01087",
        "abstract": "Time Series Forecasting (TSF) is an important application across many fields. There is a debate about whether Transformers, despite being good at understanding long sequences, struggle with preserving temporal relationships in time series data. Recent research suggests that simpler linear models might outperform or at least provide competitive performance compared to complex Transformer-based models for TSF tasks. In this paper, we propose a novel data-efficient architecture, GLinear, for multivariate TSF that exploits periodic patterns to provide better accuracy. It also provides better prediction accuracy by using a smaller amount of historical data compared to other state-of-the-art linear predictors. Four different datasets (ETTh1, Electricity, Traffic, and Weather) are used to evaluate the performance of the proposed predictor. A performance comparison with state-of-the-art linear architectures (such as NLinear, DLinear, and RLinear) and transformer-based time series predictor (Autoformer) shows that the GLinear, despite being parametrically efficient, significantly outperforms the existing architectures in most cases of multivariate TSF. We hope that the proposed GLinear opens new fronts of research and development of simpler and more sophisticated architectures for data and computationally efficient time-series analysis. The source code is publicly available on GitHub.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "170",
        "title": "EliGen: Entity-Level Controlled Image Generation with Regional Attention",
        "author": [
            "Hong Zhang",
            "Zhongjie Duan",
            "Xingjun Wang",
            "Yingda Chen",
            "Yu Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01097",
        "abstract": "Recent advancements in diffusion models have significantly advanced text-to-image generation, yet global text prompts alone remain insufficient for achieving fine-grained control over individual entities within an image. To address this limitation, we present EliGen, a novel framework for Entity-Level controlled Image Generation. We introduce regional attention, a mechanism for diffusion transformers that requires no additional parameters, seamlessly integrating entity prompts and arbitrary-shaped spatial masks. By contributing a high-quality dataset with fine-grained spatial and semantic entity-level annotations, we train EliGen to achieve robust and accurate entity-level manipulation, surpassing existing methods in both positional control precision and image quality. Additionally, we propose an inpainting fusion pipeline, extending EliGen to multi-entity image inpainting tasks. We further demonstrate its flexibility by integrating it with community models such as IP-Adapter and MLLM, unlocking new creative possibilities. The source code, dataset, and model will be released publicly.",
        "tags": [
            "Diffusion",
            "Inpainting",
            "Text-to-Image"
        ]
    },
    {
        "id": "171",
        "title": "Deformable Gaussian Splatting for Efficient and High-Fidelity Reconstruction of Surgical Scenes",
        "author": [
            "Jiwei Shan",
            "Zeyu Cai",
            "Cheng-Tai Hsieh",
            "Shing Shin Cheng",
            "Hesheng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01101",
        "abstract": "Efficient and high-fidelity reconstruction of deformable surgical scenes is a critical yet challenging task. Building on recent advancements in 3D Gaussian splatting, current methods have seen significant improvements in both reconstruction quality and rendering speed. However, two major limitations remain: (1) difficulty in handling irreversible dynamic changes, such as tissue shearing, which are common in surgical scenes; and (2) the lack of hierarchical modeling for surgical scene deformation, which reduces rendering speed. To address these challenges, we introduce EH-SurGS, an efficient and high-fidelity reconstruction algorithm for deformable surgical scenes. We propose a deformation modeling approach that incorporates the life cycle of 3D Gaussians, effectively capturing both regular and irreversible deformations, thus enhancing reconstruction quality. Additionally, we present an adaptive motion hierarchy strategy that distinguishes between static and deformable regions within the surgical scene. This strategy reduces the number of 3D Gaussians passing through the deformation field, thereby improving rendering speed. Extensive experiments demonstrate that our method surpasses existing state-of-the-art approaches in both reconstruction quality and rendering speed. Ablation studies further validate the effectiveness and necessity of our proposed components. We will open-source our code upon acceptance of the paper.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "172",
        "title": "FAST: Fast Audio Spectrogram Transformer",
        "author": [
            "Anugunj Naman",
            "Gaibo Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01104",
        "abstract": "In audio classification, developing efficient and robust models is critical for real-time applications. Inspired by the design principles of MobileViT, we present FAST (Fast Audio Spectrogram Transformer), a new architecture that combines convolutional neural networks (CNNs) and transformers to capitalize on the strengths of both. FAST integrates the local feature extraction efficiencies of CNNs with the global context modeling capabilities of transformers, resulting in a model that is powerful yet lightweight, well-suited to a real-time or mobile use case. Additionally, we incorporate Lipschitz continuous attention mechanisms to improve training stability and accelerate convergence. We evaluate FAST on the ADIMA dataset, a multilingual corpus towards real-time profanity and abuse detection, as well as on the more traditional AudioSet. Our results show that FAST achieves state-of-the-art performance on both the ADIMA and AudioSet classification tasks and in some cases surpasses existing benchmarks while using up to 150x fewer parameters.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "173",
        "title": "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization",
        "author": [
            "Haina Zhu",
            "Yizhi Zhou",
            "Hangting Chen",
            "Jianwei Yu",
            "Ziyang Ma",
            "Rongzhi Gu",
            "Wei Tan",
            "Xie Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01108",
        "abstract": "Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more. In this paper, we propose a self-supervised music representation learning model for music understanding. Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance. Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data. Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance. To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints are open source in https://github.com/tencent-ailab/MuQ.",
        "tags": [
            "Detection",
            "Vector Quantization"
        ]
    },
    {
        "id": "174",
        "title": "MalCL: Leveraging GAN-Based Generative Replay to Combat Catastrophic Forgetting in Malware Classification",
        "author": [
            "Jimin Park",
            "AHyun Ji",
            "Minji Park",
            "Mohammad Saidur Rahman",
            "Se Eun Oh"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01110",
        "abstract": "Continual Learning (CL) for malware classification tackles the rapidly evolving nature of malware threats and the frequent emergence of new types. Generative Replay (GR)-based CL systems utilize a generative model to produce synthetic versions of past data, which are then combined with new data to retrain the primary model. Traditional machine learning techniques in this domain often struggle with catastrophic forgetting, where a model's performance on old data degrades over time.\nIn this paper, we introduce a GR-based CL system that employs Generative Adversarial Networks (GANs) with feature matching loss to generate high-quality malware samples. Additionally, we implement innovative selection schemes for replay samples based on the model's hidden representations.\nOur comprehensive evaluation across Windows and Android malware datasets in a class-incremental learning scenario -- where new classes are introduced continuously over multiple tasks -- demonstrates substantial performance improvements over previous methods. For example, our system achieves an average accuracy of 55% on Windows malware samples, significantly outperforming other GR-based models by 28%. This study provides practical insights for advancing GR-based malware classification systems. The implementation is available at \\url {https://github.com/MalwareReplayGAN/MalCL}\\footnote{The code will be made public upon the presentation of the paper}.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "175",
        "title": "Graph2text or Graph2token: A Perspective of Large Language Models for Graph Learning",
        "author": [
            "Shuo Yu",
            "Yingbo Wang",
            "Ruolin Li",
            "Guchun Liu",
            "Yanming Shen",
            "Shaoxiong Ji",
            "Bowen Li",
            "Fengling Han",
            "Xiuzhen Zhang",
            "Feng Xia"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01124",
        "abstract": "Graphs are data structures used to represent irregular networks and are prevalent in numerous real-world applications. Previous methods directly model graph structures and achieve significant success. However, these methods encounter bottlenecks due to the inherent irregularity of graphs. An innovative solution is converting graphs into textual representations, thereby harnessing the powerful capabilities of Large Language Models (LLMs) to process and comprehend graphs. In this paper, we present a comprehensive review of methodologies for applying LLMs to graphs, termed LLM4graph. The core of LLM4graph lies in transforming graphs into texts for LLMs to understand and analyze. Thus, we propose a novel taxonomy of LLM4graph methods in the view of the transformation. Specifically, existing methods can be divided into two paradigms: Graph2text and Graph2token, which transform graphs into texts or tokens as the input of LLMs, respectively. We point out four challenges during the transformation to systematically present existing methods in a problem-oriented perspective. For practical concerns, we provide a guideline for researchers on selecting appropriate models and LLMs for different graphs and hardware constraints. We also identify five future research directions for LLM4graph.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "176",
        "title": "DuMo: Dual Encoder Modulation Network for Precise Concept Erasure",
        "author": [
            "Feng Han",
            "Kai Chen",
            "Chao Gong",
            "Zhipeng Wei",
            "Jingjing Chen",
            "Yu-Gang Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01125",
        "abstract": "The exceptional generative capability of text-to-image models has raised substantial safety concerns regarding the generation of Not-Safe-For-Work (NSFW) content and potential copyright infringement. To address these concerns, previous methods safeguard the models by eliminating inappropriate concepts. Nonetheless, these models alter the parameters of the backbone network and exert considerable influences on the structural (low-frequency) components of the image, which undermines the model's ability to retain non-target concepts. In this work, we propose our Dual encoder Modulation network (DuMo), which achieves precise erasure of inappropriate target concepts with minimum impairment to non-target concepts. In contrast to previous methods, DuMo employs the Eraser with PRior Knowledge (EPR) module which modifies the skip connection features of the U-NET and primarily achieves concept erasure on details (high-frequency) components of the image. To minimize the damage to non-target concepts during erasure, the parameters of the backbone U-NET are frozen and the prior knowledge from the original skip connection features is introduced to the erasure process. Meanwhile, the phenomenon is observed that distinct erasing preferences for the image structure and details are demonstrated by the EPR at different timesteps and layers. Therefore, we adopt a novel Time-Layer MOdulation process (TLMO) that adjusts the erasure scale of EPR module's outputs across different layers and timesteps, automatically balancing the erasure effects and model's generative ability. Our method achieves state-of-the-art performance on Explicit Content Erasure, Cartoon Concept Removal and Artistic Style Erasure, clearly outperforming alternative methods. Code is available at https://github.com/Maplebb/DuMo",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "177",
        "title": "Missing Data as Augmentation in the Earth Observation Domain: A Multi-View Learning Approach",
        "author": [
            "Francisco Mena",
            "Diego Arenas",
            "Andreas Dengel"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01132",
        "abstract": "Multi-view learning (MVL) leverages multiple sources or views of data to enhance machine learning model performance and robustness. This approach has been successfully used in the Earth Observation (EO) domain, where views have a heterogeneous nature and can be affected by missing data. Despite the negative effect that missing data has on model predictions, the ML literature has used it as an augmentation technique to improve model generalization, like masking the input data. Inspired by this, we introduce novel methods for EO applications tailored to MVL with missing views. Our methods integrate the combination of a set to simulate all combinations of missing views as different training samples. Instead of replacing missing data with a numerical value, we use dynamic merge functions, like average, and more complex ones like Transformer. This allows the MVL model to entirely ignore the missing views, enhancing its predictive robustness. We experiment on four EO datasets with temporal and static views, including state-of-the-art methods from the EO domain. The results indicate that our methods improve model robustness under conditions of moderate missingness, and improve the predictive performance when all views are present. The proposed methods offer a single adaptive solution to operate effectively with any combination of available views.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "178",
        "title": "Semantics-Guided Diffusion for Deep Joint Source-Channel Coding in Wireless Image Transmission",
        "author": [
            "Maojun Zhang",
            "Haotian Wu",
            "Guangxu Zhu",
            "Richeng Jin",
            "Xiaoming Chen",
            "Deniz Gndz"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01138",
        "abstract": "Joint source-channel coding (JSCC) offers a promising avenue for enhancing transmission efficiency by jointly incorporating source and channel statistics into the system design. A key advancement in this area is the deep joint source and channel coding (DeepJSCC) technique that designs a direct mapping of input signals to channel symbols parameterized by a neural network, which can be trained for arbitrary channel models and semantic quality metrics. This paper advances the DeepJSCC framework toward a semantics-aligned, high-fidelity transmission approach, called semantics-guided diffusion DeepJSCC (SGD-JSCC). Existing schemes that integrate diffusion models (DMs) with JSCC face challenges in transforming random generation into accurate reconstruction and adapting to varying channel conditions. SGD-JSCC incorporates two key innovations: (1) utilizing some inherent information that contributes to the semantics of an image, such as text description or edge map, to guide the diffusion denoising process; and (2) enabling seamless adaptability to varying channel conditions with the help of a semantics-guided DM for channel denoising. The DM is guided by diverse semantic information and integrates seamlessly with DeepJSCC. In a slow fading channel, SGD-JSCC dynamically adapts to the instantaneous signal-to-noise ratio (SNR) directly estimated from the channel output, thereby eliminating the need for additional pilot transmissions for channel estimation. In a fast fading channel, we introduce a training-free denoising strategy, allowing SGD-JSCC to effectively adjust to fluctuations in channel gains. Numerical results demonstrate that, guided by semantic information and leveraging the powerful DM, our method outperforms existing DeepJSCC schemes, delivering satisfactory reconstruction performance even at extremely poor channel conditions.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "179",
        "title": "Embodied AI-Enhanced Vehicular Networks: An Integrated Large Language Models and Reinforcement Learning Method",
        "author": [
            "Ruichen Zhang",
            "Changyuan Zhao",
            "Hongyang Du",
            "Dusit Niyato",
            "Jiacheng Wang",
            "Suttinee Sawadsitang",
            "Xuemin Shen",
            "Dong In Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01141",
        "abstract": "This paper investigates adaptive transmission strategies in embodied AI-enhanced vehicular networks by integrating large language models (LLMs) for semantic information extraction and deep reinforcement learning (DRL) for decision-making. The proposed framework aims to optimize both data transmission efficiency and decision accuracy by formulating an optimization problem that incorporates the Weber-Fechner law, serving as a metric for balancing bandwidth utilization and quality of experience (QoE). Specifically, we employ the large language and vision assistant (LLAVA) model to extract critical semantic information from raw image data captured by embodied AI agents (i.e., vehicles), reducing transmission data size by approximately more than 90\\% while retaining essential content for vehicular communication and decision-making. In the dynamic vehicular environment, we employ a generalized advantage estimation-based proximal policy optimization (GAE-PPO) method to stabilize decision-making under uncertainty. Simulation results show that attention maps from LLAVA highlight the model's focus on relevant image regions, enhancing semantic representation accuracy. Additionally, our proposed transmission strategy improves QoE by up to 36\\% compared to DDPG and accelerates convergence by reducing required steps by up to 47\\% compared to pure PPO. Further analysis indicates that adapting semantic symbol length provides an effective trade-off between transmission quality and bandwidth, achieving up to a 61.4\\% improvement in QoE when scaling from 4 to 8 vehicles.",
        "tags": [
            "LLMs",
            "LLaVA",
            "Large Language Models"
        ]
    },
    {
        "id": "180",
        "title": "BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient LLM Inference",
        "author": [
            "Wonsuk Jang",
            "Thierry Tambe"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01144",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success, but their increasing size poses significant challenges in memory usage and computational costs. Quantizing both weights and activations can address these issues, with fine-grained block-wise quantization emerging as a promising hardware-supported solution to mitigate outliers. However, existing methods struggle to capture nuanced block data distributions. To address this, we propose BlockDialect, a block-wise fine-grained mixed format technique that assigns a per-block optimal number format from formatbook for better data representation. Additionally, we introduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that adapt to diverse data distributions. Importantly, DialectFP4 ensures hardware efficiency by selecting representable values as scaled integers compatible with low-precision integer arithmetic. Furthermore, we propose a two-stage approach for online DialectFP4 activation quantization. BlockDialect achieves 11.40% (6.90%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with a comparable bit usage per data, while being only 5.89% (3.31%) below full precision even when quantizing full-path matrix multiplication. Focusing on how to represent over how to scale, our work presents a promising path for energy-efficient LLM inference.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "181",
        "title": "A3: Android Agent Arena for Mobile GUI Agents",
        "author": [
            "Yuxiang Chai",
            "Hanhao Li",
            "Jiayu Zhang",
            "Liang Liu",
            "Guozhi Wang",
            "Shuai Ren",
            "Siyuan Huang",
            "Hongsheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01149",
        "abstract": "AI agents have become increasingly prevalent in recent years, driven by significant advancements in the field of large language models (LLMs). Mobile GUI agents, a subset of AI agents, are designed to autonomously perform tasks on mobile devices. While numerous studies have introduced agents, datasets, and benchmarks to advance mobile GUI agent research, many existing datasets focus on static frame evaluations and fail to provide a comprehensive platform for assessing performance on real-world, in-the-wild tasks. To address this gap, we present Android Agent Arena (A3), a novel evaluation platform. Unlike existing in-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as real-time online information retrieval and operational instructions; (2) a larger, more flexible action space, enabling compatibility with agents trained on any dataset; and (3) automated business-level LLM-based evaluation process. A3 includes 21 widely used general third-party apps and 201 tasks representative of common user scenarios, providing a robust foundation for evaluating mobile GUI agents in real-world situations and a new autonomous evaluation process for less human labor and coding expertise. The project is available at \\url{https://yuxiangchai.github.io/Android-Agent-Arena/}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "182",
        "title": "TexAVi: Generating Stereoscopic VR Video Clips from Text Descriptions",
        "author": [
            "Vriksha Srihari",
            "R. Bhavya",
            "Shruti Jayaraman",
            "V. Mary Anita Rajam"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01156",
        "abstract": "While generative models such as text-to-image, large language models and text-to-video have seen significant progress, the extension to text-to-virtual-reality remains largely unexplored, due to a deficit in training data and the complexity of achieving realistic depth and motion in virtual environments. This paper proposes an approach to coalesce existing generative systems to form a stereoscopic virtual reality video from text.\nCarried out in three main stages, we start with a base text-to-image model that captures context from an input text. We then employ Stable Diffusion on the rudimentary image produced, to generate frames with enhanced realism and overall quality. These frames are processed with depth estimation algorithms to create left-eye and right-eye views, which are stitched side-by-side to create an immersive viewing experience. Such systems would be highly beneficial in virtual reality production, since filming and scene building often require extensive hours of work and post-production effort.\nWe utilize image evaluation techniques, specifically Frchet Inception Distance and CLIP Score, to assess the visual quality of frames produced for the video. These quantitative measures establish the proficiency of the proposed method.\nOur work highlights the exciting possibilities of using natural language-driven graphics in fields like virtual reality simulations.",
        "tags": [
            "CLIP",
            "Depth Estimation",
            "Diffusion",
            "Large Language Models",
            "Text-to-Image",
            "Text-to-Video"
        ]
    },
    {
        "id": "183",
        "title": "3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer",
        "author": [
            "Jiajun Deng",
            "Tianyu He",
            "Li Jiang",
            "Tianyu Wang",
            "Feras Dayoub",
            "Ian Reid"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01163",
        "abstract": "Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential in 3D-vision-based dialogue and reasoning. However, how to further enhance 3D LMMs to achieve fine-grained scene understanding and facilitate flexible human-agent interaction remains a challenging problem. In this work, we introduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an intelligent assistant in comprehending, reasoning, and interacting with the 3D world. Unlike existing top-performing methods that rely on complicated pipelines-such as offline multi-view feature extraction or additional task-specific heads-3D-LLaVA adopts a minimalist design with integrated architecture and only takes point clouds as input. At the core of 3D-LLaVA is a new Omni Superpoint Transformer (OST), which integrates three functionalities: (1) a visual feature selector that converts and selects visual tokens, (2) a visual prompt encoder that embeds interactive visual prompts into the visual token space, and (3) a referring mask decoder that produces 3D masks based on text description. This versatile OST is empowered by the hybrid pretraining to obtain perception priors and leveraged as the visual connector that bridges the 3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA reports impressive results on various benchmarks. The code and model will be released to promote future exploration.",
        "tags": [
            "3D",
            "LLaVA",
            "Transformer"
        ]
    },
    {
        "id": "184",
        "title": "Towards Interactive Deepfake Analysis",
        "author": [
            "Lixiong Qin",
            "Ning Jiang",
            "Yang Zhang",
            "Yuhan Qiu",
            "Dingheng Zeng",
            "Jiani Hu",
            "Weihong Deng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01164",
        "abstract": "Existing deepfake analysis methods are primarily based on discriminative models, which significantly limit their application scenarios. This paper aims to explore interactive deepfake analysis by performing instruction tuning on multi-modal large language models (MLLMs). This will face challenges such as the lack of datasets and benchmarks, and low training efficiency. To address these issues, we introduce (1) a GPT-assisted data construction process resulting in an instruction-following dataset called DFA-Instruct, (2) a benchmark named DFA-Bench, designed to comprehensively evaluate the capabilities of MLLMs in deepfake detection, deepfake classification, and artifact description, and (3) construct an interactive deepfake analysis system called DFA-GPT, as a strong baseline for the community, with the Low-Rank Adaptation (LoRA) module. The dataset and code will be made available at https://github.com/lxq1000/DFA-Instruct to facilitate further research.",
        "tags": [
            "Detection",
            "GPT",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "185",
        "title": "RingFormer: A Neural Vocoder with Ring Attention and Convolution-Augmented Transformer",
        "author": [
            "Seongho Hong",
            "Yong-Hoon Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01182",
        "abstract": "While transformers demonstrate outstanding performance across various audio tasks, their application to neural vocoders remains challenging. Neural vocoders require the generation of long audio signals at the sample level, which demands high temporal resolution. This results in significant computational costs for attention map generation and limits their ability to efficiently process both global and local information. Additionally, the sequential nature of sample generation in neural vocoders poses difficulties for real-time processing, making the direct adoption of transformers impractical. To address these challenges, we propose RingFormer, a neural vocoder that incorporates the ring attention mechanism into a lightweight transformer variant, the convolution-augmented transformer (Conformer). Ring attention effectively captures local details while integrating global information, making it well-suited for processing long sequences and enabling real-time audio generation. RingFormer is trained using adversarial training with two discriminators. The proposed model is applied to the decoder of the text-to-speech model VITS and compared with state-of-the-art vocoders such as HiFi-GAN, iSTFT-Net, and BigVGAN under identical conditions using various objective and subjective metrics. Experimental results show that RingFormer achieves comparable or superior performance to existing models, particularly excelling in real-time audio generation. Our code and audio samples are available on GitHub.",
        "tags": [
            "GAN",
            "Transformer"
        ]
    },
    {
        "id": "186",
        "title": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education",
        "author": [
            "Annika Bush",
            "Amin Alibakhshi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01192",
        "abstract": "Early childhood science education is crucial for developing scientific literacy, yet translating complex scientific concepts into age-appropriate content remains challenging for educators. Our study evaluates four leading Large Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their ability to generate preschool-appropriate scientific explanations across biology, chemistry, and physics. Through systematic evaluation by 30 nursery teachers using established pedagogical criteria, we identify significant differences in the models' capabilities to create engaging, accurate, and developmentally appropriate content. Unexpectedly, Claude outperformed other models, particularly in biological topics, while all LLMs struggled with abstract chemical concepts. Our findings provide practical insights for educators leveraging AI in early science education and offer guidance for developers working to enhance LLMs' educational applications. The results highlight the potential and current limitations of using LLMs to bridge the early science literacy gap.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "187",
        "title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects",
        "author": [
            "Abdullah Mushtaq",
            "Muhammad Rafay Naeem",
            "Ibrahim Ghaznavi",
            "Muhammad Imran Taj",
            "Imran Hashmi",
            "Junaid Qadir"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01205",
        "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings. Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. In this paper, we explore the use of Multi-Agent LLMs in supporting these senior design projects undertaken by engineering students, which often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach. This implementation leverages standard multi-agent system (MAS) concepts such as coordination, cooperation, and negotiation, incorporating prompt engineering to develop diverse personas for each agent. These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution. We adapt these techniques to create a collaboration structure for LLM agents, encouraging interdisciplinary reasoning and negotiation similar to real-world senior design projects. To assess the efficacy of this framework, we collected six proposals of engineering and computer science of...",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "188",
        "title": "Real-time Cross-modal Cybersickness Prediction in Virtual Reality",
        "author": [
            "Yitong Zhu",
            "Tangyao Li",
            "Yuyang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01212",
        "abstract": "Cybersickness remains a significant barrier to the widespread adoption of immersive virtual reality (VR) experiences, as it can greatly disrupt user engagement and comfort. Research has shown that cybersickness can significantly be reflected in head and eye tracking data, along with other physiological data (e.g., TMP, EDA, and BMP). Despite the application of deep learning techniques such as CNNs and LSTMs, these models often struggle to capture the complex interactions between multiple data modalities and lack the capacity for real-time inference, limiting their practical application. Addressing this gap, we propose a lightweight model that leverages a transformer-based encoder with sparse self-attention to process bio-signal features and a PP-TSN network for video feature extraction. These features are then integrated via a cross-modal fusion module, creating a video-aware bio-signal representation that supports cybersickness prediction based on both visual and bio-signal inputs. Our model, trained with a lightweight framework, was validated on a public dataset containing eye and head tracking data, physiological data, and VR video, and demonstrated state-of-the-art performance in cybersickness prediction, achieving a high accuracy of 93.13\\% using only VR video inputs. These findings suggest that our approach not only enables effective, real-time cybersickness prediction but also addresses the longstanding issue of modality interaction in VR environments. This advancement provides a foundation for future research on multimodal data integration in VR, potentially leading to more personalized, comfortable and widely accessible VR experiences.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "189",
        "title": "Conditional Consistency Guided Image Translation and Enhancement",
        "author": [
            "A. V. Subramanyam",
            "Amil Bhagat",
            "Milind Jain"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01223",
        "abstract": "Consistency models have emerged as a promising alternative to diffusion models, offering high-quality generative capabilities through single-step sample generation. However, their application to multi-domain image translation tasks, such as cross-modal translation and low-light image enhancement remains largely unexplored. In this paper, we introduce Conditional Consistency Models (CCMs) for multi-domain image translation by incorporating additional conditional inputs. We implement these modifications by introducing task-specific conditional inputs that guide the denoising process, ensuring that the generated outputs retain structural and contextual information from the corresponding input domain. We evaluate CCMs on 10 different datasets demonstrating their effectiveness in producing high-quality translated images across multiple domains. Code is available at https://github.com/amilbhagat/Conditional-Consistency-Models.",
        "tags": [
            "Consistency Models",
            "Diffusion"
        ]
    },
    {
        "id": "190",
        "title": "Exploiting Latent Properties to Optimize Neural Codecs",
        "author": [
            "Muhammet Balcilar",
            "Bharath Bhushan Damodaran",
            "Karam Naser",
            "Franck Galpin",
            "Pierre Hellier"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01231",
        "abstract": "End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "191",
        "title": "SVFR: A Unified Framework for Generalized Video Face Restoration",
        "author": [
            "Zhiyao Wang",
            "Xu Chen",
            "Chengming Xu",
            "Junwei Zhu",
            "Xiaobin Hu",
            "Jiangning Zhang",
            "Chengjie Wang",
            "Yuqi Liu",
            "Yiyi Zhou",
            "Rongrong Ji"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01235",
        "abstract": "Face Restoration (FR) is a crucial area within image and video processing, focusing on reconstructing high-quality portraits from degraded inputs. Despite advancements in image FR, video FR remains relatively under-explored, primarily due to challenges related to temporal consistency, motion artifacts, and the limited availability of high-quality video data. Moreover, traditional face restoration typically prioritizes enhancing resolution and may not give as much consideration to related tasks such as facial colorization and inpainting. In this paper, we propose a novel approach for the Generalized Video Face Restoration (GVFR) task, which integrates video BFR, inpainting, and colorization tasks that we empirically show to benefit each other. We present a unified framework, termed as stable video face restoration (SVFR), which leverages the generative and motion priors of Stable Video Diffusion (SVD) and incorporates task-specific information through a unified face restoration framework. A learnable task embedding is introduced to enhance task identification. Meanwhile, a novel Unified Latent Regularization (ULR) is employed to encourage the shared feature representation learning among different subtasks. To further enhance the restoration quality and temporal stability, we introduce the facial prior learning and the self-referred refinement as auxiliary strategies used for both training and inference. The proposed framework effectively combines the complementary strengths of these tasks, enhancing temporal coherence and achieving superior restoration quality. This work advances the state-of-the-art in video FR and establishes a new paradigm for generalized video face restoration.",
        "tags": [
            "Diffusion",
            "Inpainting"
        ]
    },
    {
        "id": "192",
        "title": "Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants",
        "author": [
            "Lixiong Qin",
            "Shilong Ou",
            "Miaoxuan Zhang",
            "Jiangning Wei",
            "Yuhang Zhang",
            "Xiaoshuai Song",
            "Yuchen Liu",
            "Mei Wang",
            "Weiran Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01243",
        "abstract": "Faces and humans are crucial elements in social interaction and are widely included in everyday photos and videos. Therefore, a deep understanding of faces and humans will enable multi-modal assistants to achieve improved response quality and broadened application scope. Currently, the multi-modal assistant community lacks a comprehensive and scientific evaluation of face and human understanding abilities. In this paper, we first propose a hierarchical ability taxonomy that includes three levels of abilities. Then, based on this taxonomy, we collect images and annotations from publicly available datasets in the face and human community and build a semi-automatic data pipeline to produce problems for the new benchmark. Finally, the obtained Face-Human-Bench comprises a development set with 900 problems and a test set with 1800 problems, supporting both English and Chinese. We conduct evaluations over 25 mainstream multi-modal large language models (MLLMs) with our Face-Human-Bench, focusing on the correlation between abilities, the impact of the relative position of targets on performance, and the impact of Chain of Thought (CoT) prompting on performance. Moreover, inspired by multi-modal agents, we also explore which abilities of MLLMs need to be supplemented by specialist models.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "193",
        "title": "SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization",
        "author": [
            "Yongle Huang",
            "Haodong Chen",
            "Zhenbang Xu",
            "Zihan Jia",
            "Haozhou Sun",
            "Dian Shao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01245",
        "abstract": "Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover a wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., \"salto backward tucked with 1 turn\"). Given the high costs of annotating fine-grained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design a new strong augmentation strategy for the Teacher-Student learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher model's predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semi-supervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "194",
        "title": "Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base Completion",
        "author": [
            "Qiyuan He",
            "Jianfei Yu",
            "Wenya Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01246",
        "abstract": "Integrating large language models (LLMs) with rule-based reasoning offers a powerful solution for improving the flexibility and reliability of Knowledge Base Completion (KBC). Traditional rule-based KBC methods offer verifiable reasoning yet lack flexibility, while LLMs provide strong semantic understanding yet suffer from hallucinations. With the aim of combining LLMs' understanding capability with the logical and rigor of rule-based approaches, we propose a novel framework consisting of a Subgraph Extractor, an LLM Proposer, and a Rule Reasoner. The Subgraph Extractor first samples subgraphs from the KB. Then, the LLM uses these subgraphs to propose diverse and meaningful rules that are helpful for inferring missing facts. To effectively avoid hallucination in LLMs' generations, these proposed rules are further refined by a Rule Reasoner to pinpoint the most significant rules in the KB for Knowledge Base Completion. Our approach offers several key benefits: the utilization of LLMs to enhance the richness and diversity of the proposed rules and the integration with rule-based reasoning to improve reliability. Our method also demonstrates strong performance across diverse KB datasets, highlighting the robustness and generalizability of the proposed framework.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "195",
        "title": "Bayesian Active Learning By Distribution Disagreement",
        "author": [
            "Thorben Werner",
            "Lars Schmidt-Thieme"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01248",
        "abstract": "Active Learning (AL) for regression has been systematically under-researched due to the increased difficulty of measuring uncertainty in regression models. Since normalizing flows offer a full predictive distribution instead of a point forecast, they facilitate direct usage of known heuristics for AL like Entropy or Least-Confident sampling. However, we show that most of these heuristics do not work well for normalizing flows in pool-based AL and we need more sophisticated algorithms to distinguish between aleatoric and epistemic uncertainty. In this work we propose BALSA, an adaptation of the BALD algorithm, tailored for regression with normalizing flows. With this work we extend current research on uncertainty quantification with normalizing flows \\cite{berry2023normalizing, berry2023escaping} to real world data and pool-based AL with multiple acquisition functions and query sizes. We report SOTA results for BALSA across 4 different datasets and 2 different architectures.",
        "tags": [
            "Normalizing Flows"
        ]
    },
    {
        "id": "196",
        "title": "Digital Guardians: Can GPT-4, Perspective API, and Moderation API reliably detect hate speech in reader comments of German online newspapers?",
        "author": [
            "Manuel Weber",
            "Moritz Huber",
            "Maximilian Auch",
            "Alexander Dschl",
            "Max-Emanuel Keller",
            "Peter Mandl"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01256",
        "abstract": "In recent years, toxic content and hate speech have become widespread phenomena on the internet. Moderators of online newspapers and forums are now required, partly due to legal regulations, to carefully review and, if necessary, delete reader comments. This is a labor-intensive process. Some providers of large language models already offer solutions for automated hate speech detection or the identification of toxic content. These include GPT-4o from OpenAI, Jigsaw's (Google) Perspective API, and OpenAI's Moderation API. Based on the selected German test dataset HOCON34k, which was specifically created for developing tools to detect hate speech in reader comments of online newspapers, these solutions are compared with each other and against the HOCON34k baseline. The test dataset contains 1,592 annotated text samples. For GPT-4o, three different promptings are used, employing a Zero-Shot, One-Shot, and Few-Shot approach. The results of the experiments demonstrate that GPT-4o outperforms both the Perspective API and the Moderation API, and exceeds the HOCON34k baseline by approximately 5 percentage points, as measured by a combined metric of MCC and F2-score.",
        "tags": [
            "Detection",
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "197",
        "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings",
        "author": [
            "Shanghaoran Quan",
            "Jiaxi Yang",
            "Bowen Yu",
            "Bo Zheng",
            "Dayiheng Liu",
            "An Yang",
            "Xuancheng Ren",
            "Bofei Gao",
            "Yibo Miao",
            "Yunlong Feng",
            "Zekun Wang",
            "Jian Yang",
            "Zeyu Cui",
            "Yang Fan",
            "Yichang Zhang",
            "Binyuan Hui",
            "Junyang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01257",
        "abstract": "With the increasing code reasoning capabilities of existing large language models (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3, there is a growing need to develop more challenging and comprehensive benchmarks that effectively test their sophisticated competition-level coding abilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to the unavailability of private test cases, lack of support for special judges, and misaligned execution environments. To bridge this gap, we introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. We compile the recent six months of contest problems on CodeForces with detailed information such as contest divisions, problem difficulty ratings, and problem algorithm tags. We introduce a unique judging method in which problems are submitted directly to the platform and develop a reliable Elo rating calculation system that aligns with the platform and is comparable with human participants but has lower variance. By testing on our CodeElo, we provide the Elo ratings of 30 existing popular open-source and 3 proprietary LLMs for the first time. The results show that o1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of 1578 and 1261, respectively, while other models struggle even with the easiest problems, placing in the lowest 20 percent among all human participants. Detailed analysis experiments are also conducted to provide insights into performance across algorithms and comparisons between using C++ and Python, which can suggest directions for future studies.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "198",
        "title": "Detail Matters: Mamba-Inspired Joint Unfolding Network for Snapshot Spectral Compressive Imaging",
        "author": [
            "Mengjie Qin",
            "Yuchao Feng",
            "Zongliang Wu",
            "Yulun Zhang",
            "Xin Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01262",
        "abstract": "In the coded aperture snapshot spectral imaging system, Deep Unfolding Networks (DUNs) have made impressive progress in recovering 3D hyperspectral images (HSIs) from a single 2D measurement. However, the inherent nonlinear and ill-posed characteristics of HSI reconstruction still pose challenges to existing methods in terms of accuracy and stability. To address this issue, we propose a Mamba-inspired Joint Unfolding Network (MiJUN), which integrates physics-embedded DUNs with learning-based HSI imaging. Firstly, leveraging the concept of trapezoid discretization to expand the representation space of unfolding networks, we introduce an accelerated unfolding network scheme. This approach can be interpreted as a generalized accelerated half-quadratic splitting with a second-order differential equation, which reduces the reliance on initial optimization stages and addresses challenges related to long-range interactions. Crucially, within the Mamba framework, we restructure the Mamba-inspired global-to-local attention mechanism by incorporating a selective state space model and an attention mechanism. This effectively reinterprets Mamba as a variant of the Transformer} architecture, improving its adaptability and efficiency. Furthermore, we refine the scanning strategy with Mamba by integrating the tensor mode-$k$ unfolding into the Mamba network. This approach emphasizes the low-rank properties of tensors along various modes, while conveniently facilitating 12 scanning directions. Numerical and visual comparisons on both simulation and real datasets demonstrate the superiority of our proposed MiJUN, and achieving overwhelming detail representation.",
        "tags": [
            "3D",
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "199",
        "title": "ProgCo: Program Helps Self-Correction of Large Language Models",
        "author": [
            "Xiaoshuai Song",
            "Yanan Wu",
            "Weixun Wang",
            "Jiaheng Liu",
            "Wenbo Su",
            "Bo Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01264",
        "abstract": "Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "200",
        "title": "Does a Large Language Model Really Speak in Human-Like Language?",
        "author": [
            "Mose Park",
            "Yunjin Choi",
            "Jong-June Jeon"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01273",
        "abstract": "Large Language Models (LLMs) have recently emerged, attracting considerable attention due to their ability to generate highly natural, human-like text. This study compares the latent community structures of LLM-generated text and human-written text within a hypothesis testing procedure. Specifically, we analyze three text sets: original human-written texts ($\\mathcal{O}$), their LLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set ($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key questions: (1) Is the difference in latent community structures between $\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and $\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as the LLM parameter controlling text variability is adjusted? The first question is based on the assumption that if LLM-generated text truly resembles human language, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should be similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both pairs consist of an original text and its paraphrase. The second question examines whether the degree of similarity between LLM-generated and human text varies with changes in the breadth of text generation. To address these questions, we propose a statistical hypothesis testing framework that leverages the fact that each text has corresponding parts across all datasets due to their paraphrasing relationship. This relationship enables the mapping of one dataset's relative position to another, allowing two datasets to be mapped to a third dataset. As a result, both mapped datasets can be quantified with respect to the space characterized by the third dataset, facilitating a direct comparison between them. Our results indicate that GPT-generated text remains distinct from human-authored text.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "201",
        "title": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking",
        "author": [
            "Xiaoxue Cheng",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01306",
        "abstract": "Large language models (LLMs) demonstrate exceptional capabilities, yet still face the hallucination issue. Typical text generation approaches adopt an auto-regressive generation without deliberate reasoning, which often results in untrustworthy and factually inaccurate responses. In this paper, we propose HaluSearch, a novel framework that incorporates tree search-based algorithms (e.g. MCTS) to enable an explicit slow thinking generation process for mitigating hallucinations of LLMs during inference. Specifically, HaluSearch frames text generation as a step-by-step reasoning process, using a self-evaluation reward model to score each generation step and guide the tree search towards the most reliable generation pathway for fully exploiting the internal knowledge of LLMs. To balance efficiency and quality, we introduce a hierarchical thinking system switch mechanism inspired by the dual process theory in cognitive science, which dynamically alternates between fast and slow thinking modes at both the instance and step levels, adapting to the complexity of questions and reasoning states. We conduct extensive experiments on both English and Chinese datasets and the results show that our approach significantly outperforms baseline approaches.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "202",
        "title": "SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration",
        "author": [
            "Jianyi Wang",
            "Zhijie Lin",
            "Meng Wei",
            "Yang Zhao",
            "Ceyuan Yang",
            "Chen Change Loy",
            "Lu Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01320",
        "abstract": "Video restoration poses non-trivial challenges in maintaining fidelity while recovering temporally consistent details from unknown degradations in the wild. Despite recent advances in diffusion-based restoration, these methods often face limitations in generation capability and sampling efficiency. In this work, we present SeedVR, a diffusion transformer designed to handle real-world video restoration with arbitrary length and resolution. The core design of SeedVR lies in the shifted window attention that facilitates effective restoration on long video sequences. SeedVR further supports variable-sized windows near the boundary of both spatial and temporal dimensions, overcoming the resolution constraints of traditional window attention. Equipped with contemporary practices, including causal video autoencoder, mixed image and video training, and progressive training, SeedVR achieves highly-competitive performance on both synthetic and real-world benchmarks, as well as AI-generated videos. Extensive experiments demonstrate SeedVR's superiority over existing methods for generic video restoration.",
        "tags": [
            "Diffusion",
            "Diffusion Transformer",
            "Transformer"
        ]
    },
    {
        "id": "203",
        "title": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation",
        "author": [
            "Shuzheng Gao",
            "Chaozheng Wang",
            "Cuiyun Gao",
            "Xiaoqian Jiao",
            "Chun Yong Chong",
            "Shan Gao",
            "Michael Lyu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01329",
        "abstract": "Test cases are essential for validating the reliability and quality of software applications. Recent studies have demonstrated the capability of Large Language Models (LLMs) to generate useful test cases for given source code. However, the existing work primarily relies on human-written plain prompts, which often leads to suboptimal results since the performance of LLMs can be highly influenced by the prompts. Moreover, these approaches use the same prompt for all LLMs, overlooking the fact that different LLMs might be best suited to different prompts. Given the wide variety of possible prompt formulations, automatically discovering the optimal prompt for each LLM presents a significant challenge. Although there are methods on automated prompt optimization in the natural language processing field, they are hard to produce effective prompts for the test case generation task. First, the methods iteratively optimize prompts by simply combining and mutating existing ones without proper guidance, resulting in prompts that lack diversity and tend to repeat the same errors in the generated test cases. Second, the prompts are generally lack of domain contextual knowledge, limiting LLMs' performance in the task.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "204",
        "title": "Decoding Knowledge in Large Language Models: A Framework for Categorization and Comprehension",
        "author": [
            "Yanbo Fang",
            "Ruixiang Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01332",
        "abstract": "Understanding how large language models (LLMs) acquire, retain, and apply knowledge remains an open challenge. This paper introduces a novel framework, K-(CSA)^2, which categorizes LLM knowledge along two dimensions: correctness and confidence. The framework defines six categories of knowledge, ranging from highly confident correctness to confidently held misconceptions, enabling a nuanced evaluation of model comprehension beyond binary accuracy. Using this framework, we demonstrate how techniques like chain-of-thought prompting and reinforcement learning with human feedback fundamentally alter the knowledge structures of internal (pre-trained) and external (context-dependent) knowledge in LLMs. CoT particularly enhances base model performance and shows synergistic benefits when applied to aligned LLMs. Moreover, our layer-wise analysis reveals that higher layers in LLMs encode more high-confidence knowledge, while low-confidence knowledge tends to emerge in middle-to-lower layers.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "205",
        "title": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models",
        "author": [
            "Johan Wahrus",
            "Ahmed Mohamed Hussain",
            "Panos Papadimitratos"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01335",
        "abstract": "Numerous studies have investigated methods for jailbreaking Large Language Models (LLMs) to generate harmful content. Typically, these methods are evaluated using datasets of malicious prompts designed to bypass security policies established by LLM providers. However, the generally broad scope and open-ended nature of existing datasets can complicate the assessment of jailbreaking effectiveness, particularly in specific domains, notably cybersecurity. To address this issue, we present and publicly release CySecBench, a comprehensive dataset containing 12662 prompts specifically designed to evaluate jailbreaking techniques in the cybersecurity domain. The dataset is organized into 10 distinct attack-type categories, featuring close-ended prompts to enable a more consistent and accurate assessment of jailbreaking attempts. Furthermore, we detail our methodology for dataset generation and filtration, which can be adapted to create similar datasets in other domains. To demonstrate the utility of CySecBench, we propose and evaluate a jailbreaking approach based on prompt obfuscation. Our experimental results show that this method successfully elicits harmful content from commercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT and 88% with Gemini; in contrast, Claude demonstrated greater resilience with a jailbreaking SR of 17%. Compared to existing benchmark approaches, our method shows superior performance, highlighting the value of domain-specific evaluation datasets for assessing LLM security measures. Moreover, when evaluated using prompts from a widely used dataset (i.e., AdvBench), it achieved an SR of 78.5%, higher than the state-of-the-art methods.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "206",
        "title": "Aligning Large Language Models for Faithful Integrity Against Opposing Argument",
        "author": [
            "Yong Zhao",
            "Yang Deng",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01336",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks. However, they can be easily misled by unfaithful arguments during conversations, even when their original statements are correct. To this end, we investigate the problem of maintaining faithful integrity in LLMs. This involves ensuring that LLMs adhere to their faithful statements in the face of opposing arguments and are able to correct their incorrect statements when presented with faithful arguments. In this work, we propose a novel framework, named Alignment for Faithful Integrity with Confidence Estimation (AFICE), which aims to align the LLM responses with faithful integrity. Specifically, AFICE first designs a Bilateral Confidence Estimation (BCE) approach for estimating the uncertainty of each response generated by the LLM given a specific context, which simultaneously estimate the model's confidence to the question based on the internal states during decoding as well as to the answer based on cumulative probability ratios. With the BCE, we construct a conversational preference dataset composed of context, original statement, and argument, which is adopted for aligning the LLM for faithful integrity using Direct Preference Optimization (DPO). Extensive experimental results on a wide range of benchmarks demonstrate significant improvements in the LLM's ability to maintain faithful responses when encountering opposing arguments, ensuring both the practical utility and trustworthiness of LLMs in complex interactive settings. Code and data will be released via https://github.com/zhaoy777/AFICE.git",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "207",
        "title": "DeepFilter: An Instrumental Baseline for Accurate and Efficient Process Monitoring",
        "author": [
            "Hao Wang",
            "Zhichao Chen",
            "Licheng Pan",
            "Xiaoyu Jiang",
            "Yichen Song",
            "Qunshan He",
            "Xinggao Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01342",
        "abstract": "Effective process monitoring is increasingly vital in industrial automation for ensuring operational safety, necessitating both high accuracy and efficiency. Although Transformers have demonstrated success in various fields, their canonical form based on the self-attention mechanism is inadequate for process monitoring due to two primary limitations: (1) the step-wise correlations captured by self-attention mechanism are difficult to capture discriminative patterns in monitoring logs due to the lacking semantics of each step, thus compromising accuracy; (2) the quadratic computational complexity of self-attention hampers efficiency. To address these issues, we propose DeepFilter, a Transformer-style framework for process monitoring. The core innovation is an efficient filtering layer that excel capturing long-term and periodic patterns with reduced complexity. Equipping with the global filtering layer, DeepFilter enhances both accuracy and efficiency, meeting the stringent demands of process monitoring. Experimental results on real-world process monitoring datasets validate DeepFilter's superiority in terms of accuracy and efficiency compared to existing state-of-the-art models.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "208",
        "title": "AdaptVC: High Quality Voice Conversion with Adaptive Learning",
        "author": [
            "Jaehun Kim",
            "Ji-Hoon Kim",
            "Yeunju Choi",
            "Tan Dat Nguyen",
            "Seongkyu Mun",
            "Joon Son Chung"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01347",
        "abstract": "The goal of voice conversion is to transform the speech of a source speaker to sound like that of a reference speaker while preserving the original content. A key challenge is to extract disentangled linguistic content from the source and voice style from the reference. While existing approaches leverage various methods to isolate the two, a generalization still requires further attention, especially for robustness in zero-shot scenarios. In this paper, we achieve successful disentanglement of content and speaker features by tuning self-supervised speech features with adapters. The adapters are trained to dynamically encode nuanced features from rich self-supervised features, and the decoder fuses them to produce speech that accurately resembles the reference with minimal loss of content. Moreover, we leverage a conditional flow matching decoder with cross-attention speaker conditioning to further boost the synthesis quality and efficiency. Subjective and objective evaluations in a zero-shot scenario demonstrate that the proposed method outperforms existing models in speech quality and similarity to the reference speech.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "209",
        "title": "Test-time Controllable Image Generation by Explicit Spatial Constraint Enforcement",
        "author": [
            "Z. Zhang",
            "B. Liu",
            "J. Bao",
            "L. Chen",
            "S. Zhu",
            "J. Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01368",
        "abstract": "Recent text-to-image generation favors various forms of spatial conditions, e.g., masks, bounding boxes, and key points. However, the majority of the prior art requires form-specific annotations to fine-tune the original model, leading to poor test-time generalizability. Meanwhile, existing training-free methods work well only with simplified prompts and spatial conditions. In this work, we propose a novel yet generic test-time controllable generation method that aims at natural text prompts and complex conditions. Specifically, we decouple spatial conditions into semantic and geometric conditions and then enforce their consistency during the image-generation process individually. As for the former, we target bridging the gap between the semantic condition and text prompts, as well as the gap between such condition and the attention map from diffusion models. To achieve this, we propose to first complete the prompt w.r.t. semantic condition, and then remove the negative impact of distracting prompt words by measuring their statistics in attention maps as well as distances in word space w.r.t. this condition. To further cope with the complex geometric conditions, we introduce a geometric transform module, in which Region-of-Interests will be identified in attention maps and further used to translate category-wise latents w.r.t. geometric condition. More importantly, we propose a diffusion-based latents-refill method to explicitly remove the impact of latents at the RoI, reducing the artifacts on generated images. Experiments on Coco-stuff dataset showcase 30$\\%$ relative boost compared to SOTA training-free methods on layout consistency evaluation metrics.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "210",
        "title": "CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering",
        "author": [
            "Ben Vardi",
            "Oron Nir",
            "Ariel Shamir"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01371",
        "abstract": "Recent Vision-Language Models (VLMs) have demonstrated remarkable capabilities in visual understanding and reasoning, and in particular on multiple-choice Visual Question Answering (VQA). Still, these models can make distinctly unnatural errors, for example, providing (wrong) answers to unanswerable VQA questions, such as questions asking about objects that do not appear in the image. To address this issue, we propose CLIP-UP: CLIP-based Unanswerable Problem detection, a novel lightweight method for equipping VLMs with the ability to withhold answers to unanswerable questions. By leveraging CLIP to extract question-image alignment information, CLIP-UP requires only efficient training of a few additional layers, while keeping the original VLMs' weights unchanged. Tested across LLaVA models, CLIP-UP achieves state-of-the-art results on the MM-UPD benchmark for assessing unanswerability in multiple-choice VQA, while preserving the original performance on other tasks.",
        "tags": [
            "CLIP",
            "Detection",
            "LLaVA"
        ]
    },
    {
        "id": "211",
        "title": "OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data for Diverse Scenarios",
        "author": [
            "Xize Cheng",
            "Dongjie Fu",
            "Xiaoda Yang",
            "Minghui Fang",
            "Ruofan Hu",
            "Jingyu Lu",
            "Bai Jionghao",
            "Zehan Wang",
            "Shengpeng Ji",
            "Rongjie Huang",
            "Linjun Li",
            "Yu Chen",
            "Tao Jin",
            "Zhou Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01384",
        "abstract": "With the rapid development of large language models, researchers have created increasingly advanced spoken dialogue systems that can naturally converse with humans. However, these systems still struggle to handle the full complexity of real-world conversations, including audio events, musical contexts, and emotional expressions, mainly because current dialogue datasets are constrained in both scale and scenario diversity. In this paper, we propose leveraging synthetic data to enhance the dialogue models across diverse scenarios. We introduce ShareChatX, the first comprehensive, large-scale dataset for spoken dialogue that spans diverse scenarios. Based on this dataset, we introduce OmniChat, a multi-turn dialogue system with a heterogeneous feature fusion module, designed to optimize feature selection in different dialogue contexts. In addition, we explored critical aspects of training dialogue systems using synthetic data. Through comprehensive experimentation, we determined the ideal balance between synthetic and real data, achieving state-of-the-art results on the real-world dialogue dataset DailyTalk. We also highlight the crucial importance of synthetic data in tackling diverse, complex dialogue scenarios, especially those involving audio and music. For more details, please visit our demo page at \\url{https://sharechatx.github.io/}.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "212",
        "title": "A Unified Hyperparameter Optimization Pipeline for Transformer-Based Time Series Forecasting Models",
        "author": [
            "Jingjing Xu",
            "Caesar Wu",
            "Yuan-Fang Li",
            "Grgoire Danoy",
            "Pascal Bouvry"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01394",
        "abstract": "Transformer-based models for time series forecasting (TSF) have attracted significant attention in recent years due to their effectiveness and versatility. However, these models often require extensive hyperparameter optimization (HPO) to achieve the best possible performance, and a unified pipeline for HPO in transformer-based TSF remains lacking. In this paper, we present one such pipeline and conduct extensive experiments on several state-of-the-art (SOTA) transformer-based TSF models. These experiments are conducted on standard benchmark datasets to evaluate and compare the performance of different models, generating practical insights and examples. Our pipeline is generalizable beyond transformer-based architectures and can be applied to other SOTA models, such as Mamba and TimeMixer, as demonstrated in our experiments. The goal of this work is to provide valuable guidance to both industry practitioners and academic researchers in efficiently identifying optimal hyperparameters suited to their specific domain applications. The code and complete experimental results are available on GitHub.",
        "tags": [
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "213",
        "title": "Nested Attention: Semantic-aware Attention Values for Concept Personalization",
        "author": [
            "Or Patashnik",
            "Rinon Gal",
            "Daniil Ostashev",
            "Sergey Tulyakov",
            "Kfir Aberman",
            "Daniel Cohen-Or"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01407",
        "abstract": "Personalizing text-to-image models to generate images of specific subjects across diverse scenes and styles is a rapidly advancing field. Current approaches often face challenges in maintaining a balance between identity preservation and alignment with the input text prompt. Some methods rely on a single textual token to represent a subject, which limits expressiveness, while others employ richer representations but disrupt the model's prior, diminishing prompt alignment. In this work, we introduce Nested Attention, a novel mechanism that injects a rich and expressive image representation into the model's existing cross-attention layers. Our key idea is to generate query-dependent subject values, derived from nested attention layers that learn to select relevant subject features for each region in the generated image. We integrate these nested layers into an encoder-based personalization method, and show that they enable high identity preservation while adhering to input text prompts. Our approach is general and can be trained on various domains. Additionally, its prior preservation allows us to combine multiple personalized subjects from different domains in a single image.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "214",
        "title": "On Unifying Video Generation and Camera Pose Estimation",
        "author": [
            "Chun-Hao Paul Huang",
            "Jae Shin Yoon",
            "Hyeonho Jeong",
            "Niloy Mitra",
            "Duygu Ceylan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01409",
        "abstract": "Inspired by the emergent 3D capabilities in image generators, we explore whether video generators similarly exhibit 3D awareness. Using structure-from-motion (SfM) as a benchmark for 3D tasks, we investigate if intermediate features from OpenSora, a video generation model, can support camera pose estimation. We first examine native 3D awareness in video generation features by routing raw intermediate outputs to SfM-prediction modules like DUSt3R. Then, we explore the impact of fine-tuning on camera pose estimation to enhance 3D awareness. Results indicate that while video generator features have limited inherent 3D awareness, task-specific supervision significantly boosts their accuracy for camera pose estimation, resulting in competitive performance. The proposed unified model, named JOG3R, produces camera pose estimates with competitive quality without degrading video generation quality.",
        "tags": [
            "3D",
            "Pose Estimation",
            "Video Generation"
        ]
    },
    {
        "id": "215",
        "title": "Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models",
        "author": [
            "Jingfeng Yao",
            "Xinggang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01423",
        "abstract": "Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochs--representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: https://github.com/hustvl/LightningDiT.",
        "tags": [
            "DiT",
            "Diffusion",
            "Transformer",
            "VAE"
        ]
    },
    {
        "id": "216",
        "title": "Object-level Visual Prompts for Compositional Image Generation",
        "author": [
            "Gaurav Parmar",
            "Or Patashnik",
            "Kuan-Chieh Wang",
            "Daniil Ostashev",
            "Srinivasa Narasimhan",
            "Jun-Yan Zhu",
            "Daniel Cohen-Or",
            "Kfir Aberman"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01424",
        "abstract": "We introduce a method for composing object-level visual prompts within a text-to-image diffusion model. Our approach addresses the task of generating semantically coherent compositions across diverse scenes and styles, similar to the versatility and expressiveness offered by text prompts. A key challenge in this task is to preserve the identity of the objects depicted in the input visual prompts, while also generating diverse compositions across different images. To address this challenge, we introduce a new KV-mixed cross-attention mechanism, in which keys and values are learned from distinct visual representations. The keys are derived from an encoder with a small bottleneck for layout control, whereas the values come from a larger bottleneck encoder that captures fine-grained appearance details. By mixing keys and values from these complementary sources, our model preserves the identity of the visual prompts while supporting flexible variations in object arrangement, pose, and composition. During inference, we further propose object-level compositional guidance to improve the method's identity preservation and layout correctness. Results show that our technique produces diverse scene compositions that preserve the unique characteristics of each visual prompt, expanding the creative potential of text-to-image generation.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "217",
        "title": "Free-Form Motion Control: A Synthetic Video Generation Dataset with Controllable Camera and Object Motions",
        "author": [
            "Xincheng Shuai",
            "Henghui Ding",
            "Zhenyuan Qin",
            "Hao Luo",
            "Xingjun Ma",
            "Dacheng Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01425",
        "abstract": "Controlling the movements of dynamic objects and the camera within generated videos is a meaningful yet challenging task. Due to the lack of datasets with comprehensive motion annotations, existing algorithms can not simultaneously control the motions of both camera and objects, resulting in limited controllability over generated contents. To address this issue and facilitate the research in this field, we introduce a Synthetic Dataset for Free-Form Motion Control (SynFMC). The proposed SynFMC dataset includes diverse objects and environments and covers various motion patterns according to specific rules, simulating common and complex real-world scenarios. The complete 6D pose information facilitates models learning to disentangle the motion effects from objects and the camera in a video. To validate the effectiveness and generalization of SynFMC, we further propose a method, Free-Form Motion Control (FMC). FMC enables independent or simultaneous control of object and camera movements, producing high-fidelity videos. Moreover, it is compatible with various personalized text-to-image (T2I) models for different content styles. Extensive experiments demonstrate that the proposed FMC outperforms previous methods across multiple scenarios.",
        "tags": [
            "Text-to-Image",
            "Video Generation"
        ]
    },
    {
        "id": "218",
        "title": "Unifying Specialized Visual Encoders for Video Language Models",
        "author": [
            "Jihoon Chung",
            "Tyler Zhu",
            "Max Gonzalez Saez-Diez",
            "Juan Carlos Niebles",
            "Honglu Zhou",
            "Olga Russakovsky"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01426",
        "abstract": "The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create a unified representation of a video, providing the VideoLLM with a comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle a wider range of open-ended and multiple-choice video understanding questions and outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having a better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder methods while parallelizing the visual processing. Finally, we provide qualitative evidence that MERV successfully captures domain knowledge from each of its encoders. Our results offer promising directions in utilizing multiple vision encoders for comprehensive video understanding.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "LLaVA",
            "Large Language Models"
        ]
    },
    {
        "id": "219",
        "title": "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control",
        "author": [
            "Yuanpeng Tu",
            "Hao Luo",
            "Xi Chen",
            "Sihui Ji",
            "Xiang Bai",
            "Hengshuang Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01427",
        "abstract": "Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motions at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a reweight reconstruction loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning.",
        "tags": [
            "Diffusion",
            "Talking Head",
            "Text-to-Video",
            "Video Generation",
            "Virtual Try-On"
        ]
    },
    {
        "id": "220",
        "title": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models",
        "author": [
            "Zhangyang Qi",
            "Zhixiong Zhang",
            "Ye Fang",
            "Jiaqi Wang",
            "Hengshuang Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01428",
        "abstract": "In recent years, 2D Vision-Language Models (VLMs) have made significant strides in image-text understanding tasks. However, their performance in 3D spatial comprehension, which is critical for embodied intelligence, remains limited. Recent advances have leveraged 3D point clouds and multi-view images as inputs, yielding promising results. However, we propose exploring a purely vision-based solution inspired by human perception, which merely relies on visual cues for 3D spatial understanding. This paper empirically investigates the limitations of VLMs in 3D spatial knowledge, revealing that their primary shortcoming lies in the lack of global-local correspondence between the scene and individual frames. To address this, we introduce GPT4Scene, a novel visual prompting paradigm in VLM training and inference that helps build the global-local relationship, significantly improving the 3D spatial understanding of indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV) image from the video and marks consistent object IDs across both frames and the BEV image. The model then inputs the concatenated BEV image and video frames with markers. In zero-shot evaluations, GPT4Scene improves performance over closed-source VLMs like GPT-4o. Additionally, we prepare a processed video dataset consisting of 165K text annotation to fine-tune open-source VLMs, achieving state-of-the-art performance on all 3D understanding tasks. Surprisingly, after training with the GPT4Scene paradigm, VLMs consistently improve during inference, even without visual prompting and BEV image as explicit correspondence. It demonstrates that the proposed paradigm helps VLMs develop an intrinsic ability to understand 3D scenes, which paves the way for a noninvasive approach to extending pre-trained VLMs for 3D scene understanding.",
        "tags": [
            "3D",
            "GPT"
        ]
    },
    {
        "id": "221",
        "title": "Magnetic Field Data Calibration with Transformer Model Using Physical Constraints: A Scalable Method for Satellite Missions, Illustrated by Tianwen-1",
        "author": [
            "Beibei Li",
            "Yutian Chi",
            "Yuming Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00020",
        "abstract": "This study introduces a novel approach that integrates the magnetic field data correction from the Tianwen-1 Mars mission with a neural network architecture constrained by physical principles derived from Maxwell's equation equations. By employing a Transformer based model capable of efficiently handling sequential data, the method corrects measurement anomalies caused by satellite dynamics, instrument interference, and environmental noise. As a result, it significantly improves both the accuracy and the physical consistency of the calibrated data. Compared to traditional methods that require long data segments and manual intervention often taking weeks or even months to complete this new approach can finish calibration in just minutes to hours, and predictions are made within seconds. This innovation not only accelerates the process of space weather modeling and planetary magnetospheric studies but also provides a robust framework for future planetary exploration and solar wind interaction research.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "222",
        "title": "Speech Recognition With LLMs Adapted to Disordered Speech Using Reinforcement Learning",
        "author": [
            "Chirag Nagpal",
            "Subhashini Venugopalan",
            "Jimmy Tobin",
            "Marilyn Ladewig",
            "Katherine Heller",
            "Katrin Tomanek"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00039",
        "abstract": "We introduce a large language model (LLM) capable of processing speech inputs and show that tuning it further with reinforcement learning on human preference (RLHF) enables it to adapt better to disordered speech than traditional fine-tuning. Our method replaces low-frequency text tokens in an LLM's vocabulary with audio tokens and enables the model to recognize speech by fine-tuning it on speech with transcripts. We then use RL with rewards based on syntactic and semantic accuracy measures generalizing the LLM further to recognize disordered speech. While the resulting LLM does not outperform existing systems for speech recognition, we find that tuning with reinforcement learning using custom rewards leads to substantially better performance than supervised fine-tuning of the language model, specifically when adapting to speech in a different setting. This presents a compelling alternative tuning strategy for speech recognition using large language models.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RL"
        ]
    },
    {
        "id": "223",
        "title": "GroverGPT: A Large Language Model with 8 Billion Parameters for Quantum Searching",
        "author": [
            "Haoran Wang",
            "Pingzhi Li",
            "Min Chen",
            "Jinglei Cheng",
            "Junyu Liu",
            "Tianlong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00135",
        "abstract": "Quantum computing is an exciting non-Von Neumann paradigm, offering provable speedups over classical computing for specific problems. However, the practical limits of classical simulatability for quantum circuits remain unclear, especially with current noisy quantum devices. In this work, we explore the potential of leveraging Large Language Models (LLMs) to simulate the output of a quantum Turing machine using Grover's quantum circuits, known to provide quadratic speedups over classical counterparts. To this end, we developed GroverGPT, a specialized model based on LLaMA's 8-billion-parameter architecture, trained on over 15 trillion tokens. Unlike brute-force state-vector simulations, which demand substantial computational resources, GroverGPT employs pattern recognition to approximate quantum search algorithms without explicitly representing quantum states. Analyzing 97K quantum search instances, GroverGPT consistently outperformed OpenAI's GPT-4o (45\\% accuracy), achieving nearly 100\\% accuracy on 6- and 10-qubit datasets when trained on 4-qubit or larger datasets. It also demonstrated strong generalization, surpassing 95\\% accuracy for systems with over 20 qubits when trained on 3- to 6-qubit data. Analysis indicates GroverGPT captures quantum features of Grover's search rather than classical patterns, supported by novel prompting strategies to enhance performance. Although accuracy declines with increasing system size, these findings offer insights into the practical boundaries of classical simulatability. This work suggests task-specific LLMs can surpass general-purpose models like GPT-4o in quantum algorithm learning and serve as powerful tools for advancing quantum research.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "224",
        "title": "Large Language Model-Driven Database for Thermoelectric Materials",
        "author": [
            "Suman Itani",
            "Yibo Zhang",
            "Jiadong Zang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00564",
        "abstract": "Thermoelectric materials provide a sustainable way to convert waste heat into electricity. However, data-driven discovery and optimization of these materials are challenging because of a lack of a reliable database. Here we developed a comprehensive database of 7,123 thermoelectric compounds, containing key information such as chemical composition, structural detail, seebeck coefficient, electrical and thermal conductivity, power factor, and figure of merit (ZT). We used the GPTArticleExtractor workflow, powered by large language models (LLM), to extract and curate data automatically from the scientific literature published in Elsevier journals. This process enabled the creation of a structured database that addresses the challenges of manual data collection. The open access database could stimulate data-driven research and advance thermoelectric material analysis and discovery.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "225",
        "title": "Polynomial time sampling from log-smooth distributions in fixed dimension under semi-log-concavity of the forward diffusion with application to strongly dissipative distributions",
        "author": [
            "Adrien Vacher",
            "Omar Chehab",
            "Anna Korba"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00565",
        "abstract": "In this article we provide a stochastic sampling algorithm with polynomial complexity in fixed dimension that leverages the recent advances on diffusion models where it is shown that under mild conditions, sampling can be achieved via an accurate estimation of intermediate scores across the marginals $(p_t)_{t\\ge 0}$ of the standard Ornstein-Uhlenbeck process started at the density we wish to sample from. The heart of our method consists into approaching these scores via a computationally cheap estimator and relating the variance of this estimator to the smoothness properties of the forward process. Under the assumption that the density to sample from is $L$-log-smooth and that the forward process is semi-log-concave: $-\\nabla^2 \\log(p_t) \\succeq -\\beta I_d$ for some $\\beta \\geq 0$, we prove that our algorithm achieves an expected $\\epsilon$ error in $\\text{KL}$ divergence in $O(d^7L^{d+2}\\epsilon^{-2(d+3)} (L+\\beta)^2d^{2(d+1)})$ time. In particular, our result allows to fully transfer the problem of sampling from a log-smooth distribution into a regularity estimate problem. As an application, we derive an exponential complexity improvement for the problem of sampling from a $L$-log-smooth distribution that is $\\alpha$-strongly log-concave distribution outside some ball of radius $R$: after proving that such distributions verify the semi-log-concavity assumption, a result which might be of independent interest, we recover a $poly(R,L,\\alpha^{-1}, \\epsilon^{-1})$ complexity in fixed dimension which exponentially improves upon the previously known $poly(e^{RL^2}, L,\\alpha^{-1}, \\log(\\epsilon^{-1}))$ complexity in the low precision regime.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "226",
        "title": "VoiceRestore: Flow-Matching Transformers for Speech Recording Quality Restoration",
        "author": [
            "Stanislav Kirdey"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00794",
        "abstract": "We present VoiceRestore, a novel approach to restoring the quality of speech recordings using flow-matching Transformers trained in a self-supervised manner on synthetic data. Our method tackles a wide range of degradations frequently found in both short and long-form speech recordings, including background noise, reverberation, compression artifacts, and bandwidth limitations - all within a single, unified model. Leveraging conditional flow matching and classifier free guidance, the model learns to map degraded speech to high quality recordings without requiring paired clean and degraded datasets. We describe the training process, the conditional flow matching framework, and the model's architecture. We also demonstrate the model's generalization to real-world speech restoration tasks, including both short utterances and extended monologues or dialogues. Qualitative and quantitative evaluations show that our approach provides a flexible and effective solution for enhancing the quality of speech recordings across varying lengths and degradation types.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "227",
        "title": "SLIDE: Integrating Speech Language Model with LLM for Spontaneous Spoken Dialogue Generation",
        "author": [
            "Haitian Lu",
            "Gaofeng Cheng",
            "Liuping Luo",
            "Leying Zhang",
            "Yanmin Qian",
            "Pengyuan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00805",
        "abstract": "Recently, ``textless\" speech language models (SLMs) based on speech units have made huge progress in generating naturalistic speech, including non-verbal vocalizations. However, the generated speech samples often lack semantic coherence. In this paper, we propose SLM and LLM Integration for spontaneous spoken Dialogue gEneration (SLIDE). Specifically, we first utilize an LLM to generate the textual content of spoken dialogue. Next, we convert the textual dialogues into phoneme sequences and use a two-tower transformer-based duration predictor to predict the duration of each phoneme. Finally, an SLM conditioned on the spoken phoneme sequences is used to vocalize the textual dialogue. Experimental results on the Fisher dataset demonstrate that our system can generate naturalistic spoken dialogue while maintaining high semantic coherence.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "228",
        "title": "LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management",
        "author": [
            "Yichen Luo",
            "Yebo Feng",
            "Jiahua Xu",
            "Paolo Tasca",
            "Yang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00826",
        "abstract": "Cryptocurrency investment is inherently difficult due to its shorter history compared to traditional assets, the need to integrate vast amounts of data from various modalities, and the requirement for complex reasoning. While deep learning approaches have been applied to address these challenges, their black-box nature raises concerns about trust and explainability. Recently, large language models (LLMs) have shown promise in financial applications due to their ability to understand multi-modal data and generate explainable decisions. However, single LLM faces limitations in complex, comprehensive tasks such as asset investment. These limitations are even more pronounced in cryptocurrency investment, where LLMs have less domain-specific knowledge in their training corpora.\nTo overcome these challenges, we propose an explainable, multi-modal, multi-agent framework for cryptocurrency investment. Our framework uses specialized agents that collaborate within and across teams to handle subtasks such as data analysis, literature integration, and investment decision-making for the top 30 cryptocurrencies by market capitalization. The expert training module fine-tunes agents using multi-modal historical data and professional investment literature, while the multi-agent investment module employs real-time data to make informed cryptocurrency investment decisions. Unique intrateam and interteam collaboration mechanisms enhance prediction accuracy by adjusting final predictions based on confidence levels within agent teams and facilitating information sharing between teams. Empirical evaluation using data from November 2023 to September 2024 demonstrates that our framework outperforms single-agent models and market benchmarks in classification, asset pricing, portfolio, and explainability performance.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "229",
        "title": "Tight Constraint Prediction of Six-Degree-of-Freedom Transformer-based Powered Descent Guidance",
        "author": [
            "Julia Briden",
            "Trey Gurga",
            "Breanna Johnson",
            "Abhishek Cauligi",
            "Richard Linares"
        ],
        "pdf": "https://arxiv.org/pdf/2501.00930",
        "abstract": "This work introduces Transformer-based Successive Convexification (T-SCvx), an extension of Transformer-based Powered Descent Guidance (T-PDG), generalizable for efficient six-degree-of-freedom (DoF) fuel-optimal powered descent trajectory generation. Our approach significantly enhances the sample efficiency and solution quality for nonconvex-powered descent guidance by employing a rotation invariant transformation of the sampled dataset. T-PDG was previously applied to the 3-DoF minimum fuel powered descent guidance problem, improving solution times by up to an order of magnitude compared to lossless convexification (LCvx). By learning to predict the set of tight or active constraints at the optimal control problem's solution, Transformer-based Successive Convexification (T-SCvx) creates the minimal reduced-size problem initialized with only the tight constraints, then uses the solution of this reduced problem to warm-start the direct optimization solver. 6-DoF powered descent guidance is known to be challenging to solve quickly and reliably due to the nonlinear and non-convex nature of the problem, the discretization scheme heavily influencing solution validity, and reference trajectory initialization determining algorithm convergence or divergence. Our contributions in this work address these challenges by extending T-PDG to learn the set of tight constraints for the successive convexification (SCvx) formulation of the 6-DoF powered descent guidance problem. In addition to reducing the problem size, feasible and locally optimal reference trajectories are also learned to facilitate convergence from the initial guess. T-SCvx enables onboard computation of real-time guidance trajectories, demonstrated by a 6-DoF Mars powered landing application problem.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "230",
        "title": "Disambiguation of Chinese Polyphones in an End-to-End Framework with Semantic Features Extracted by Pre-trained BERT",
        "author": [
            "Dongyang Dai",
            "Zhiyong Wu",
            "Shiyin Kang",
            "Xixin Wu",
            "Jia Jia",
            "Dan Su",
            "Dong Yu",
            "Helen Meng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01102",
        "abstract": "Grapheme-to-phoneme (G2P) conversion serves as an essential component in Chinese Mandarin text-to-speech (TTS) system, where polyphone disambiguation is the core issue. In this paper, we propose an end-to-end framework to predict the pronunciation of a polyphonic character, which accepts sentence containing polyphonic character as input in the form of Chinese character sequence without the necessity of any preprocessing. The proposed method consists of a pre-trained bidirectional encoder representations from Transformers (BERT) model and a neural network (NN) based classifier. The pre-trained BERT model extracts semantic features from a raw Chinese character sequence and the NN based classifier predicts the polyphonic character's pronunciation according to BERT output. In out experiments, we implemented three classifiers, a fully-connected network based classifier, a long short-term memory (LSTM) network based classifier and a Transformer block based classifier. The experimental results compared with the baseline approach based on LSTM demonstrate that, the pre-trained model extracts effective semantic features, which greatly enhances the performance of polyphone disambiguation. In addition, we also explored the impact of contextual information on polyphone disambiguation.",
        "tags": [
            "BERT",
            "Transformer"
        ]
    },
    {
        "id": "231",
        "title": "Learning Spectral Methods by Transformers",
        "author": [
            "Yihan He",
            "Yuan Cao",
            "Hong-Yu Chen",
            "Dennis Wu",
            "Jianqing Fan",
            "Han Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01312",
        "abstract": "Transformers demonstrate significant advantages as the building block of modern LLMs. In this work, we study the capacities of Transformers in performing unsupervised learning. We show that multi-layered Transformers, given a sufficiently large set of pre-training instances, are able to learn the algorithms themselves and perform statistical estimation tasks given new instances. This learning paradigm is distinct from the in-context learning setup and is similar to the learning procedure of human brains where skills are learned through past experience. Theoretically, we prove that pre-trained Transformers can learn the spectral methods and use the classification of bi-class Gaussian mixture model as an example. Our proof is constructive using algorithmic design techniques. Our results are built upon the similarities of multi-layered Transformer architecture with the iterative recovery algorithms used in practice. Empirically, we verify the strong capacity of the multi-layered (pre-trained) Transformer on unsupervised learning through the lens of both the PCA and the Clustering tasks performed on the synthetic and real-world datasets.",
        "tags": [
            "LLMs",
            "Transformer"
        ]
    }
]