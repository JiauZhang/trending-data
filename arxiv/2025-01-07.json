[
    {
        "id": "1",
        "title": "INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models",
        "author": [
            "Di Jin",
            "Xing Liu",
            "Yu Liu",
            "Jia Qing Yap",
            "Andrea Wong",
            "Adriana Crespo",
            "Qi Lin",
            "Zhiyuan Yin",
            "Qiang Yan",
            "Ryan Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01973",
        "abstract": "The rapid development of large language models (LLMs) and large vision models (LVMs) have propelled the evolution of multi-modal AI systems, which have demonstrated the remarkable potential for industrial applications by emulating human-like cognition. However, they also pose significant ethical challenges, including amplifying harmful content and reinforcing societal biases. For instance, biases in some industrial image generation models highlighted the urgent need for robust fairness assessments. Most existing evaluation frameworks focus on the comprehensiveness of various aspects of the models, but they exhibit critical limitations, including insufficient attention to content generation alignment and social bias-sensitive domains. More importantly, their reliance on pixel-detection techniques is prone to inaccuracies.\nTo address these issues, this paper presents INFELM, an in-depth fairness evaluation on widely-used text-to-image models. Our key contributions are: (1) an advanced skintone classifier incorporating facial topology and refined skin pixel representation to enhance classification precision by at least 16.04%, (2) a bias-sensitive content alignment measurement for understanding societal impacts, (3) a generalizable representation bias evaluation for diverse demographic groups, and (4) extensive experiments analyzing large-scale text-to-image model outputs across six social-bias-sensitive domains. We find that existing models in the study generally do not meet the empirical fairness criteria, and representation bias is generally more pronounced than alignment errors. INFELM establishes a robust benchmark for fairness assessment, supporting the development of multi-modal AI systems that align with ethical and human-centric principles.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models",
            "Text-to-Image"
        ]
    },
    {
        "id": "2",
        "title": "ECG-guided individual identification via PPG",
        "author": [
            "Riling Wei",
            "Hanjie Chen",
            "Kelu Yao",
            "Chuanguang Yang",
            "Jun Wang",
            "Chao Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01983",
        "abstract": "Photoplethsmography (PPG)-based individual identification aiming at recognizing humans via intrinsic cardiovascular activities has raised extensive attention due to its high security and resistance to mimicry. However, this kind of technology witnesses unpromising results due to the limitation of low information density. To this end, electrocardiogram (ECG) signals have been introduced as a novel modality to enhance the density of input information. Specifically, a novel cross-modal knowledge distillation framework is implemented to propagate discriminate knowledge from ECG modality to PPG modality without incurring additional computational demands at the inference phase. Furthermore, to ensure efficient knowledge propagation, Contrastive Language-Image Pre-training (CLIP)-based knowledge alignment and cross-knowledge assessment modules are proposed respectively. Comprehensive experiments are conducted and results show our framework outperforms the baseline model with the improvement of 2.8% and 3.0% in terms of overall accuracy on seen- and unseen individual recognitions.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "3",
        "title": "FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models",
        "author": [
            "Tianyu Fu",
            "Tengxuan Liu",
            "Qinghao Han",
            "Guohao Dai",
            "Shengen Yan",
            "Huazhong Yang",
            "Xuefei Ning",
            "Yu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01986",
        "abstract": "The increasing demand to process long and high-resolution videos significantly burdens Large Vision-Language Models (LVLMs) due to the enormous number of visual tokens. Existing token reduction methods primarily focus on importance-based token pruning, which overlooks the redundancy caused by frame resemblance and repetitive visual elements. In this paper, we analyze the high vision token similarities in LVLMs. We reveal that token similarity distribution condenses as layers deepen while maintaining ranking consistency. Leveraging the unique properties of similarity over importance, we introduce FrameFusion, a novel approach that combines similarity-based merging with importance-based pruning for better token reduction in LVLMs. FrameFusion identifies and merges similar tokens before pruning, opening up a new perspective for token reduction. We evaluate FrameFusion on diverse LVLMs, including Llava-Video-{7B,32B,72B}, and MiniCPM-V-8B, on video understanding, question-answering, and retrieval benchmarks. Experiments show that FrameFusion reduces vision tokens by 70$\\%$, achieving 3.4-4.4x LLM speedups and 1.6-1.9x end-to-end speedups, with an average performance impact of less than 3$\\%$. Our code is available at https://github.com/thu-nics/FrameFusion.",
        "tags": [
            "LLaVA"
        ]
    },
    {
        "id": "4",
        "title": "Gender Bias in Text-to-Video Generation Models: A case study of Sora",
        "author": [
            "Mohammad Nadeem",
            "Shahab Saquib Sohail",
            "Erik Cambria",
            "Bj√∂rn W. Schuller",
            "Amir Hussain"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01987",
        "abstract": "The advent of text-to-video generation models has revolutionized content creation as it produces high-quality videos from textual prompts. However, concerns regarding inherent biases in such models have prompted scrutiny, particularly regarding gender representation. Our study investigates the presence of gender bias in OpenAI's Sora, a state-of-the-art text-to-video generation model. We uncover significant evidence of bias by analyzing the generated videos from a diverse set of gender-neutral and stereotypical prompts. The results indicate that Sora disproportionately associates specific genders with stereotypical behaviors and professions, which reflects societal prejudices embedded in its training data.",
        "tags": [
            "Sora",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "5",
        "title": "CRRG-CLIP: Automatic Generation of Chest Radiology Reports and Classification of Chest Radiographs",
        "author": [
            "Jianfei Xu",
            "Thanet Markchom",
            "Huizhi Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01989",
        "abstract": "The complexity of stacked imaging and the massive number of radiographs make writing radiology reports complex and inefficient. Even highly experienced radiologists struggle to maintain accuracy and consistency in interpreting radiographs under prolonged high-intensity work. To address these issues, this work proposes the CRRG-CLIP Model (Chest Radiology Report Generation and Radiograph Classification Model), an end-to-end model for automated report generation and radiograph classification. The model consists of two modules: the radiology report generation module and the radiograph classification module. The generation module uses Faster R-CNN to identify anatomical regions in radiographs, a binary classifier to select key regions, and GPT-2 to generate semantically coherent reports. The classification module uses the unsupervised Contrastive Language Image Pretraining (CLIP) model, addressing the challenges of high-cost labelled datasets and insufficient features. The results show that the generation module performs comparably to high-performance baseline models on BLEU, METEOR, and ROUGE-L metrics, and outperformed the GPT-4o model on BLEU-2, BLEU-3, BLEU-4, and ROUGE-L metrics. The classification module significantly surpasses the state-of-the-art model in AUC and Accuracy. This demonstrates that the proposed model achieves high accuracy, readability, and fluency in report generation, while multimodal contrastive training with unlabelled radiograph-report pairs enhances classification performance.",
        "tags": [
            "CLIP",
            "GPT"
        ]
    },
    {
        "id": "6",
        "title": "Towards Sustainable Large Language Model Serving",
        "author": [
            "Sophia Nguyen",
            "Beihao Zhou",
            "Yi Ding",
            "Sihang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01990",
        "abstract": "In this work, we study LLMs from a carbon emission perspective, addressing both operational and embodied emissions, and paving the way for sustainable LLM serving. We characterize the performance and energy of LLaMA with 1B, 3B, and 7B parameters using two Nvidia GPU types, a latest-generation RTX6000 Ada and an older-generation T4. We analytically model operational carbon emissions based on energy consumption and carbon intensities from three grid regions -- each representing a different energy source mix, and embodied carbon emissions based on chip area and memory size. Our characterization and modeling provide us with an in-depth understanding of the performance, energy, and carbon emissions of LLM serving. Our findings highlight the potential for optimizing sustainable LLM serving systems by considering both operational and embodied carbon emissions simultaneously.",
        "tags": [
            "LLMs",
            "LLaMA"
        ]
    },
    {
        "id": "7",
        "title": "SmartSpatial: Enhancing the 3D Spatial Arrangement Capabilities of Stable Diffusion Models and Introducing a Novel 3D Spatial Evaluation Framework",
        "author": [
            "Mao Xun Huang",
            "Hen-Hsen Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.01998",
        "abstract": "Stable Diffusion models have made remarkable strides in generating photorealistic images from text prompts but often falter when tasked with accurately representing complex spatial arrangements, particularly involving intricate 3D relationships. To address this limitation, we introduce SmartSpatial, an innovative approach that enhances the spatial arrangement capabilities of Stable Diffusion models through 3D-aware conditioning and attention-guided mechanisms. SmartSpatial incorporates depth information and employs cross-attention control to ensure precise object placement, delivering notable improvements in spatial accuracy metrics. In conjunction with SmartSpatial, we present SmartSpatialEval, a comprehensive evaluation framework designed to assess spatial relationships. This framework utilizes vision-language models and graph-based dependency parsing for performance analysis. Experimental results on the COCO and SpatialPrompts datasets show that SmartSpatial significantly outperforms existing methods, setting new benchmarks for spatial arrangement accuracy in image generation.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "8",
        "title": "TART: Token-based Architecture Transformer for Neural Network Performance Prediction",
        "author": [
            "Yannis Y. He"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02007",
        "abstract": "In the realm of neural architecture design, achieving high performance is largely reliant on the manual expertise of researchers. Despite the emergence of Neural Architecture Search (NAS) as a promising technique for automating this process, current NAS methods still require human input to expand the search space and cannot generate new architectures. This paper explores the potential of Transformers in comprehending neural architectures and their performance, with the objective of establishing the foundation for utilizing Transformers to generate novel networks. We propose the Token-based Architecture Transformer (TART), which predicts neural network performance without the need to train candidate networks. TART attains state-of-the-art performance on the DeepNets-1M dataset for performance prediction tasks without edge information, indicating the potential of Transformers to aid in discovering novel and high-performing neural architectures.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "9",
        "title": "Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts",
        "author": [
            "Youcheng Huang",
            "Chen Huang",
            "Duanyu Feng",
            "Wenqiang Lei",
            "Jiancheng Lv"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02009",
        "abstract": "Understanding the inner workings of Large Language Models (LLMs) is a critical research frontier. Prior research has shown that a single LLM's concept representations can be captured as steering vectors (SVs), enabling the control of LLM behavior (e.g., towards generating harmful content). Our work takes a novel approach by exploring the intricate relationships between concept representations across different LLMs, drawing an intriguing parallel to Plato's Allegory of the Cave. In particular, we introduce a linear transformation method to bridge these representations and present three key findings: 1) Concept representations across different LLMs can be effectively aligned using simple linear transformations, enabling efficient cross-model transfer and behavioral control via SVs. 2) This linear transformation generalizes across concepts, facilitating alignment and control of SVs representing different concepts across LLMs. 3) A weak-to-strong transferability exists between LLM concept representations, whereby SVs extracted from smaller LLMs can effectively control the behavior of larger LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "10",
        "title": "Enhancing Uncertainty Modeling with Semantic Graph for Hallucination Detection",
        "author": [
            "Kedi Chen",
            "Qin Chen",
            "Jie Zhou",
            "Xinqi Tao",
            "Bowen Ding",
            "Jingwen Xie",
            "Mingchen Xie",
            "Peilong Li",
            "Feng Zheng",
            "Liang He"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02020",
        "abstract": "Large Language Models (LLMs) are prone to hallucination with non-factual or unfaithful statements, which undermines the applications in real-world scenarios. Recent researches focus on uncertainty-based hallucination detection, which utilizes the output probability of LLMs for uncertainty calculation and does not rely on external knowledge or frequent sampling from LLMs. Whereas, most approaches merely consider the uncertainty of each independent token, while the intricate semantic relations among tokens and sentences are not well studied, which limits the detection of hallucination that spans over multiple tokens and sentences in the passage. In this paper, we propose a method to enhance uncertainty modeling with semantic graph for hallucination detection. Specifically, we first construct a semantic graph that well captures the relations among entity tokens and sentences. Then, we incorporate the relations between two entities for uncertainty propagation to enhance sentence-level hallucination detection. Given that hallucination occurs due to the conflict between sentences, we further present a graph-based uncertainty calibration method that integrates the contradiction probability of the sentence with its neighbors in the semantic graph for uncertainty calculation. Extensive experiments on two datasets show the great advantages of our proposed approach. In particular, we obtain substantial improvements with 19.78% in passage-level hallucination detection.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "11",
        "title": "Recursive Decomposition of Logical Thoughts: Framework for Superior Reasoning and Knowledge Propagation in Large Language Models",
        "author": [
            "Kaleem Ullah Qasim",
            "Jiashu Zhang",
            "Tariq Alsahfi",
            "Ateeq Ur Rehman Butt"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02026",
        "abstract": "Enhancing the reasoning capabilities of Large Language Models remains a critical challenge in artificial intelligence. We introduce RDoLT, Recursive Decomposition of Logical Thought prompting, a novel framework that significantly boosts LLM reasoning performance. RDoLT is built on three key innovations: (1) recursively breaking down complex reasoning tasks into sub-tasks of progressive complexity; (2) employing an advanced selection and scoring mechanism to identify the most promising reasoning thoughts; and (3) integrating a knowledge propagation module that mimics human learning by keeping track of strong and weak thoughts for information propagation. Our approach was evaluated across multiple benchmarks, including GSM8K, SVAMP, MultiArith, LastLetterConcatenation, and Gaokao2023 Math. The results demonstrate that RDoLT consistently outperforms existing state-of-the-art techniques, achieving a 90.98 percent accuracy on GSM8K with ChatGPT-4, surpassing state-of-the-art techniques by 6.28 percent. Similar improvements were observed on other benchmarks, with accuracy gains ranging from 5.5 percent to 6.75 percent. These findings highlight RDoLT's potential to advance prompt engineering, offering a more effective and generalizable approach to complex reasoning tasks.",
        "tags": [
            "ChatGPT",
            "Large Language Models"
        ]
    },
    {
        "id": "12",
        "title": "Detecting Music Performance Errors with Transformers",
        "author": [
            "Benjamin Shiue-Hal Chou",
            "Purvish Jajal",
            "Nicholas John Eliopoulos",
            "Tim Nadolsky",
            "Cheng-Yun Yang",
            "Nikita Ravi",
            "James C. Davis",
            "Kristen Yeon-Ji Yun",
            "Yung-Hsiang Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02030",
        "abstract": "Beginner musicians often struggle to identify specific errors in their performances, such as playing incorrect notes or rhythms. There are two limitations in existing tools for music error detection: (1) Existing approaches rely on automatic alignment; therefore, they are prone to errors caused by small deviations between alignment targets.; (2) There is a lack of sufficient data to train music error detection models, resulting in over-reliance on heuristics. To address (1), we propose a novel transformer model, Polytune, that takes audio inputs and outputs annotated music scores. This model can be trained end-to-end to implicitly align and compare performance audio with music scores through latent space representations. To address (2), we present a novel data generation technique capable of creating large-scale synthetic music error datasets. Our approach achieves a 64.1% average Error Detection F1 score, improving upon prior work by 40 percentage points across 14 instruments. Additionally, compared with existing transcription methods repurposed for music error detection, our model can handle multiple instruments. Our source code and datasets are available at https://github.com/ben2002chou/Polytune.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "13",
        "title": "CarbonChat: Large Language Model-Based Corporate Carbon Emission Analysis and Climate Knowledge Q&A System",
        "author": [
            "Zhixuan Cao",
            "Ming Han",
            "Jingtao Wang",
            "Meng Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02031",
        "abstract": "As the impact of global climate change intensifies, corporate carbon emissions have become a focal point of global attention. In response to issues such as the lag in climate change knowledge updates within large language models, the lack of specialization and accuracy in traditional augmented generation architectures for complex problems, and the high cost and time consumption of sustainability report analysis, this paper proposes CarbonChat: Large Language Model-based corporate carbon emission analysis and climate knowledge Q&A system, aimed at achieving precise carbon emission analysis and policy http://understanding.First, a diversified index module construction method is proposed to handle the segmentation of rule-based and long-text documents, as well as the extraction of structured data, thereby optimizing the parsing of key http://information.Second, an enhanced self-prompt retrieval-augmented generation architecture is designed, integrating intent recognition, structured reasoning chains, hybrid retrieval, and Text2SQL, improving the efficiency of semantic understanding and query http://conversion.Next, based on the greenhouse gas accounting framework, 14 dimensions are established for carbon emission analysis, enabling report summarization, relevance evaluation, and customized http://responses.Finally, through a multi-layer chunking mechanism, timestamps, and hallucination detection features, the accuracy and verifiability of the analysis results are ensured, reducing hallucination rates and enhancing the precision of the responses.",
        "tags": [
            "Detection",
            "Large Language Models",
            "Segmentation"
        ]
    },
    {
        "id": "14",
        "title": "An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage",
        "author": [
            "Fan Bu",
            "Zheng Wang",
            "Siyi Wang",
            "Ziyao Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02039",
        "abstract": "As Large Language Models (LLMs) become increasingly prevalent in tasks related to cultural heritage, such as generating descriptions of historical monuments, translating ancient texts, preserving oral traditions, and creating educational content, their ability to produce accurate and culturally aligned texts is being increasingly relied upon by users and researchers. However, cultural value misalignments may exist in generated texts, such as the misrepresentation of historical facts, the erosion of cultural identity, and the oversimplification of complex cultural narratives, which may lead to severe consequences. Therefore, investigating value misalignment in the context of LLM for cultural heritage is crucial for mitigating these risks, yet there has been a significant lack of systematic and comprehensive study and investigation in this area. To fill this gap, we systematically assess the reliability of LLMs in generating culturally aligned texts for cultural heritage-related tasks. We conduct a comprehensive evaluation by compiling an extensive set of 1066 query tasks covering 5 widely recognized categories with 17 aspects within the knowledge framework of cultural heritage across 5 open-source LLMs, and examine both the type and rate of cultural value misalignments in the generated texts. Using both automated and manual approaches, we effectively detect and analyze the cultural value misalignments in LLM-generated texts. Our findings are concerning: over 65% of the generated texts exhibit notable cultural misalignments, with certain tasks demonstrating almost complete misalignment with key cultural values. Beyond these findings, this paper introduces a benchmark dataset and a comprehensive evaluation workflow that can serve as a valuable resource for future research aimed at enhancing the cultural sensitivity and reliability of LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "15",
        "title": "A Separable Self-attention Inspired by the State Space Model for Computer Vision",
        "author": [
            "Juntao Zhang",
            "Shaogeng Liu",
            "Kun Bian",
            "You Zhou",
            "Pei Zhang",
            "Jianning Liu",
            "Jun Zhou",
            "Bingyan Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02040",
        "abstract": "Mamba is an efficient State Space Model (SSM) with linear computational complexity. Although SSMs are not suitable for handling non-causal data, Vision Mamba (ViM) methods still demonstrate good performance in tasks such as image classification and object detection. Recent studies have shown that there is a rich theoretical connection between state space models and attention variants. We propose a novel separable self attention method, for the first time introducing some excellent design concepts of Mamba into separable self-attention. To ensure a fair comparison with ViMs, we introduce VMINet, a simple yet powerful prototype architecture, constructed solely by stacking our novel attention modules with the most basic down-sampling layers. Notably, VMINet differs significantly from the conventional Transformer architecture. Our experiments demonstrate that VMINet has achieved competitive results on image classification and high-resolution dense prediction http://tasks.Code is available at: \\url{https://github.com/yws-wxs/VMINet}.",
        "tags": [
            "Detection",
            "Mamba",
            "SSMs",
            "State Space Models",
            "Transformer"
        ]
    },
    {
        "id": "16",
        "title": "MRG: A Multi-Robot Manufacturing Digital Scene Generation Method Using Multi-Instance Point Cloud Registration",
        "author": [
            "Songjie Han",
            "Yinhua Liu",
            "Yanzheng Li",
            "Hua Chen",
            "Dongmei Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02041",
        "abstract": "A high-fidelity digital simulation environment is crucial for accurately replicating physical operational processes. However, inconsistencies between simulation and physical environments result in low confidence in simulation outcomes, limiting their effectiveness in guiding real-world production. Unlike the traditional step-by-step point cloud \"segmentation-registration\" generation method, this paper introduces, for the first time, a novel Multi-Robot Manufacturing Digital Scene Generation (MRG) method that leverages multi-instance point cloud registration, specifically within manufacturing scenes. Tailored to the characteristics of industrial robots and manufacturing settings, an instance-focused transformer module is developed to delineate instance boundaries and capture correlations between local regions. Additionally, a hypothesis generation module is proposed to extract target instances while preserving key features. Finally, an efficient screening and optimization algorithm is designed to refine the final registration results. Experimental evaluations on the Scan2CAD and Welding-Station datasets demonstrate that: (1) the proposed method outperforms existing multi-instance point cloud registration techniques; (2) compared to state-of-the-art methods, the Scan2CAD dataset achieves improvements in MR and MP by 12.15% and 17.79%, respectively; and (3) on the Welding-Station dataset, MR and MP are enhanced by 16.95% and 24.15%, respectively. This work marks the first application of multi-instance point cloud registration in manufacturing scenes, significantly advancing the precision and reliability of digital simulation environments for industrial applications.",
        "tags": [
            "Robot",
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "17",
        "title": "AGGA: A Dataset of Academic Guidelines for Generative AI and Large Language Models",
        "author": [
            "Junfeng Jiao",
            "Saleh Afroogh",
            "Kevin Chen",
            "David Atkinson",
            "Amit Dhurandhar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02063",
        "abstract": "This study introduces AGGA, a dataset comprising 80 academic guidelines for the use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic settings, meticulously collected from official university websites. The dataset contains 188,674 words and serves as a valuable resource for natural language processing tasks commonly applied in requirements engineering, such as model synthesis, abstraction identification, and document structure assessment. Additionally, AGGA can be further annotated to function as a benchmark for various tasks, including ambiguity detection, requirements categorization, and the identification of equivalent requirements. Our methodologically rigorous approach ensured a thorough examination, with a selection of universities that represent a diverse range of global institutions, including top-ranked universities across six continents. The dataset captures perspectives from a variety of academic fields, including humanities, technology, and both public and private institutions, offering a broad spectrum of insights into the integration of GAIs and LLMs in academia.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "18",
        "title": "ArtCrafter: Text-Image Aligning Style Transfer via Embedding Reframing",
        "author": [
            "Nisha Huang",
            "Kaer Huang",
            "Yifan Pu",
            "Jiangshan Wang",
            "Jie Guo",
            "Yiqiang Yan",
            "Xiu Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02064",
        "abstract": "Recent years have witnessed significant advancements in text-guided style transfer, primarily attributed to innovations in diffusion models. These models excel in conditional guidance, utilizing text or images to direct the sampling process. However, despite their capabilities, direct conditional guidance approaches often face challenges in balancing the expressiveness of textual semantics with the diversity of output results while capturing stylistic features. To address these challenges, we introduce ArtCrafter, a novel framework for text-to-image style transfer. Specifically, we introduce an attention-based style extraction module, meticulously engineered to capture the subtle stylistic elements within an image. This module features a multi-layer architecture that leverages the capabilities of perceiver attention mechanisms to integrate fine-grained information. Additionally, we present a novel text-image aligning augmentation component that adeptly balances control over both modalities, enabling the model to efficiently map image and text embeddings into a shared feature space. We achieve this through attention operations that enable smooth information flow between modalities. Lastly, we incorporate an explicit modulation that seamlessly blends multimodal enhanced embeddings with original embeddings through an embedding reframing design, empowering the model to generate diverse outputs. Extensive experiments demonstrate that ArtCrafter yields impressive results in visual stylization, exhibiting exceptional levels of stylistic intensity, controllability, and diversity.",
        "tags": [
            "Diffusion",
            "Style Transfer",
            "Text-to-Image"
        ]
    },
    {
        "id": "19",
        "title": "Instruction-Following Pruning for Large Language Models",
        "author": [
            "Bairu Hou",
            "Qibin Chen",
            "Jianyu Wang",
            "Guoli Yin",
            "Chong Wang",
            "Nan Du",
            "Ruoming Pang",
            "Shiyu Chang",
            "Tao Lei"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02086",
        "abstract": "With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed \"instruction-following pruning\", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "20",
        "title": "Online Detection of Water Contamination Under Concept Drift",
        "author": [
            "Jin Li",
            "Kleanthis Malialis",
            "Stelios G. Vrachimis",
            "Marios M. Polycarpou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02107",
        "abstract": "Water Distribution Networks (WDNs) are vital infrastructures, and contamination poses serious public health risks. Harmful substances can interact with disinfectants like chlorine, making chlorine monitoring essential for detecting contaminants. However, chlorine sensors often become unreliable and require frequent calibration. This study introduces the Dual-Threshold Anomaly and Drift Detection (AD&DD) method, an unsupervised approach combining a dual-threshold drift detection mechanism with an LSTM-based Variational Autoencoder(LSTM-VAE) for real-time contamination detection. Tested on two realistic WDNs, AD&DD effectively identifies anomalies with sensor offsets as concept drift, and outperforms other methods. A proposed decentralized architecture enables accurate contamination detection and localization by deploying AD&DD on selected nodes.",
        "tags": [
            "Detection",
            "VAE"
        ]
    },
    {
        "id": "21",
        "title": "Fastest mixing reversible Markov chain on friendship graph: Trade-off between transition probabilities among friends and convergence rate",
        "author": [
            "Saber Jafarizadeh"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02117",
        "abstract": "A long-standing goal of social network research has been to alter the properties of network to achieve the desired outcome. In doing so, DeGroot's consensus model has served as the popular choice for modeling the information diffusion and opinion formation in social networks. Achieving a trade-off between the cost associated with modifications made to the network and the speed of convergence to the desired state has shown to be a critical factor. This has been treated as the Fastest Mixing Markov Chain (FMMC) problem over a graph with given transition probabilities over a subset of edges. Addressing this multi-objective optimization problem over the friendship graph, this paper has provided the corresponding Pareto optimal points or the Pareto frontier. In the case of friendship graph with at least three blades, it is shown that the Pareto frontier is reduced to a global minimum point which is same as the optimal point corresponding to the minimum spanning tree of the friendship graph, i.e., the star topology. Furthermore, a lower limit for transition probabilities among friends has been provided, where values higher than this limit do not have any impact on the convergence rate.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "22",
        "title": "AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs",
        "author": [
            "Sanjoy Chowdhury",
            "Sayan Nag",
            "Subhrajyoti Dasgupta",
            "Yaoting Wang",
            "Mohamed Elhoseiny",
            "Ruohan Gao",
            "Dinesh Manocha"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02135",
        "abstract": "With the rapid advancement of Multi-modal Large Language Models (MLLMs), several diagnostic benchmarks have recently been developed to assess these models' multi-modal reasoning proficiency. However, these benchmarks are restricted to assessing primarily the visual aspect and do not examine the holistic audio-visual (AV) understanding. Moreover, currently, there are no benchmarks that investigate the capabilities of AVLLMs to calibrate their responses when presented with perturbed inputs. To this end, we introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 13 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks. We will publicly release our code and benchmark to facilitate future research in this direction.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "23",
        "title": "Effective LLM-Driven Code Generation with Pythoness",
        "author": [
            "Kyla H. Levin",
            "Kyle Gwilt",
            "Emery D. Berger",
            "Stephen N. Freund"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02138",
        "abstract": "The advent of large language models (LLMs) has paved the way for a new era of programming tools with both significant capabilities and risks, as the generated code lacks guarantees of correctness and reliability. Developers using LLMs currently face the difficult task of optimizing, integrating, and maintaining code generated by AI. We propose an embedded domain-specific language (DSL), Pythoness, to address those challenges. In Pythoness, developers program with LLMs at a higher level of abstraction. Rather than interacting directly with generated code, developers using Pythoness operate at the level of behavioral specifications when writing functions, classes, or an entire program. These specifications can take the form of unit tests and property-based tests, which may be expressed formally or in natural language. Guided by these specifications, Pythoness generates code that both passes the tests and can be continuously checked during execution. We posit that the Pythoness approach lets developers harness the full potential of LLMs for code generation while substantially mitigating their inherent risks. We describe our current prototype implementation of Pythoness and demonstrate that it can successfully leverage a combination of tests and code generation to yield higher quality code than specifications alone.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "24",
        "title": "Personalized Graph-Based Retrieval for Large Language Models",
        "author": [
            "Steven Au",
            "Cameron J. Dimacali",
            "Ojasmitha Pedirappagari",
            "Namyong Park",
            "Franck Dernoncourt",
            "Yu Wang",
            "Nikos Kanakaris",
            "Hanieh Deilamsalehy",
            "Ryan A. Rossi",
            "Nesreen K. Ahmed"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02157",
        "abstract": "As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "25",
        "title": "ROLO-SLAM: Rotation-Optimized LiDAR-Only SLAM in Uneven Terrain with Ground Vehicle",
        "author": [
            "Yinchuan Wang",
            "Bin Ren",
            "Xiang Zhang",
            "Pengyu Wang",
            "Chaoqun Wang",
            "Rui Song",
            "Yibin Li",
            "Max Q.-H. Meng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02166",
        "abstract": "LiDAR-based SLAM is recognized as one effective method to offer localization guidance in rough environments. However, off-the-shelf LiDAR-based SLAM methods suffer from significant pose estimation drifts, particularly components relevant to the vertical direction, when passing to uneven terrains. This deficiency typically leads to a conspicuously distorted global map. In this article, a LiDAR-based SLAM method is presented to improve the accuracy of pose estimations for ground vehicles in rough terrains, which is termed Rotation-Optimized LiDAR-Only (ROLO) SLAM. The method exploits a forward location prediction to coarsely eliminate the location difference of consecutive scans, thereby enabling separate and accurate determination of the location and orientation at the front-end. Furthermore, we adopt a parallel-capable spatial voxelization for correspondence-matching. We develop a spherical alignment-guided rotation registration within each voxel to estimate the rotation of vehicle. By incorporating geometric alignment, we introduce the motion constraint into the optimization formulation to enhance the rapid and effective estimation of LiDAR's translation. Subsequently, we extract several keyframes to construct the submap and exploit an alignment from the current scan to the submap for precise pose estimation. Meanwhile, a global-scale factor graph is established to aid in the reduction of cumulative errors. In various scenes, diverse experiments have been conducted to evaluate our method. The results demonstrate that ROLO-SLAM excels in pose estimation of ground vehicles and outperforms existing state-of-the-art LiDAR SLAM frameworks.",
        "tags": [
            "Pose Estimation",
            "SLAM"
        ]
    },
    {
        "id": "26",
        "title": "Generating Multimodal Images with GAN: Integrating Text, Image, and Style",
        "author": [
            "Chaoyi Tan",
            "Wenqing Zhang",
            "Zhen Qi",
            "Kowei Shih",
            "Xinshi Li",
            "Ao Xiang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02167",
        "abstract": "In the field of computer vision, multimodal image generation has become a research hotspot, especially the task of integrating text, image, and style. In this study, we propose a multimodal image generation method based on Generative Adversarial Networks (GAN), capable of effectively combining text descriptions, reference images, and style information to generate images that meet multimodal requirements. This method involves the design of a text encoder, an image feature extractor, and a style integration module, ensuring that the generated images maintain high quality in terms of visual content and style consistency. We also introduce multiple loss functions, including adversarial loss, text-image consistency loss, and style matching loss, to optimize the generation process. Experimental results show that our method produces images with high clarity and consistency across multiple public datasets, demonstrating significant performance improvements compared to existing methods. The outcomes of this study provide new insights into multimodal image generation and present broad application prospects.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "27",
        "title": "Benchmark Evaluations, Applications, and Challenges of Large Vision Language Models: A Survey",
        "author": [
            "Zongxia Li",
            "Xiyang Wu",
            "Hongyang Du",
            "Huy Nghiem",
            "Guangyao Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02189",
        "abstract": "Multimodal Vision Language Models (VLMs) have emerged as a transformative technology at the intersection of computer vision and natural language processing, enabling machines to perceive and reason about the world through both visual and textual modalities. For example, models such as CLIP, Claude, and GPT-4V demonstrate strong reasoning and understanding abilities on visual and textual data and beat classical single modality vision models on zero-shot classification. Despite their rapid advancements in research and growing popularity in applications, a comprehensive survey of existing studies on VLMs is notably lacking, particularly for researchers aiming to leverage VLMs in their specific domains. To this end, we provide a systematic overview of VLMs in the following aspects: model information of the major VLMs developed over the past five years (2019-2024); the main architectures and training methods of these VLMs; summary and categorization of the popular benchmarks and evaluation metrics of VLMs; the applications of VLMs including embodied agents, robotics, and video generation; the challenges and issues faced by current VLMs such as hallucination, fairness, and safety. Detailed collections including papers and model repository links are listed in https://github.com/zli12321/Awesome-VLM-Papers-And-Models.git.",
        "tags": [
            "CLIP",
            "GPT",
            "Robotics",
            "Video Generation"
        ]
    },
    {
        "id": "28",
        "title": "EvoPath: Evolutionary Meta-path Discovery with Large Language Models for Complex Heterogeneous Information Networks",
        "author": [
            "Shixuan Liu",
            "Haoxiang Cheng",
            "Yunfei Wang",
            "Yue He",
            "Changjun Fan",
            "Zhong Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02192",
        "abstract": "Heterogeneous Information Networks (HINs) encapsulate diverse entity and relation types, with meta-paths providing essential meta-level semantics for knowledge reasoning, although their utility is constrained by discovery challenges. While Large Language Models (LLMs) offer new prospects for meta-path discovery due to their extensive knowledge encoding and efficiency, their adaptation faces challenges such as corpora bias, lexical discrepancies, and hallucination. This paper pioneers the mitigation of these challenges by presenting EvoPath, an innovative framework that leverages LLMs to efficiently identify high-quality meta-paths. EvoPath is carefully designed, with each component aimed at addressing issues that could lead to potential knowledge conflicts. With a minimal subset of HIN facts, EvoPath iteratively generates and evolves meta-paths by dynamically replaying meta-paths in the buffer with prioritization based on their scores. Comprehensive experiments on three large, complex HINs with hundreds of relations demonstrate that our framework, EvoPath, enables LLMs to generate high-quality meta-paths through effective prompting, confirming its superior performance in HIN reasoning tasks. Further ablation studies validate the effectiveness of each module within the framework.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "29",
        "title": "Ensemble-based Deep Multilayer Community Search",
        "author": [
            "Jianwei Wang",
            "Yuehai Wang",
            "Kai Wang",
            "Xuemin Lin",
            "Wenjie Zhang",
            "Ying Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02194",
        "abstract": "Multilayer graphs, consisting of multiple interconnected layers, are widely used to model diverse relationships in the real world. A community is a cohesive subgraph that offers valuable insights for analyzing (multilayer) graphs. Recently, there has been an emerging trend focused on searching query-driven communities within the multilayer graphs. However, existing methods for multilayer community search are either 1) rule-based, which suffer from structure inflexibility; or 2) learning-based, which rely on labeled data or fail to capture layer-specific characteristics. To address these, we propose EnMCS, an Ensemble-based unsupervised (i.e., label-free) Multilayer Community Search framework. EnMCS contains two key components, i.e., HoloSearch which identifies potential communities in each layer while integrating both layer-shared and layer-specific information, and EMerge which is an Expectation-Maximization (EM)-based method that synthesizes the potential communities from each layer into a consensus community. Specifically, HoloSearch first employs a graph-diffusion-based model that integrates three label-free loss functions to learn layer-specific and layer-shared representations for each node. Communities in each layer are then identified based on nodes that exhibit high similarity in layer-shared representations while demonstrating low similarity in layer-specific representations w.r.t. the query nodes. To account for the varying layer-specific characteristics of each layer when merging communities, EMerge models the error rates of layers and true community as latent variables. It then employs the EM algorithm to simultaneously minimize the error rates of layers and predict the final consensus community through iterative maximum likelihood estimation. Experiments over 10 real-world datasets highlight the superiority of EnMCS in terms of both efficiency and effectiveness.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "30",
        "title": "Can ChatGPT implement finite element models for geotechnical engineering applications?",
        "author": [
            "Taegu Kim",
            "Tae Sup Yun",
            "Hyoung Suk Suh"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02199",
        "abstract": "This study assesses the capability of ChatGPT to generate finite element code for geotechnical engineering applications from a set of prompts. We tested three different initial boundary value problems using a hydro-mechanically coupled formulation for unsaturated soils, including the dissipation of excess pore water pressure through fluid mass diffusion in one-dimensional space, time-dependent differential settlement of a strip footing, and gravity-driven seepage. For each case, initial prompting involved providing ChatGPT with necessary information for finite element implementation, such as balance and constitutive equations, problem geometry, initial and boundary conditions, material properties, and spatiotemporal discretization and solution strategies. Any errors and unexpected results were further addressed through prompt augmentation processes until the ChatGPT-generated finite element code passed the verification/validation test. Our results demonstrate that ChatGPT required minimal code revisions when using the FEniCS finite element library, owing to its high-level interfaces that enable efficient programming. In contrast, the MATLAB code generated by ChatGPT necessitated extensive prompt augmentations and/or direct human intervention, as it involves a significant amount of low-level programming required for finite element analysis, such as constructing shape functions or assembling global matrices. Given that prompt engineering for this task requires an understanding of the mathematical formulation and numerical techniques, this study suggests that while a large language model may not yet replace human programmers, it can greatly assist in the implementation of numerical models.",
        "tags": [
            "ChatGPT",
            "Diffusion"
        ]
    },
    {
        "id": "31",
        "title": "Examining the Robustness of Homogeneity Bias to Hyperparameter Adjustments in GPT-4",
        "author": [
            "Messi H.J. Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02211",
        "abstract": "Vision-Language Models trained on massive collections of human-generated data often reproduce and amplify societal stereotypes. One critical form of stereotyping reproduced by these models is homogeneity bias-the tendency to represent certain groups as more homogeneous than others. We investigate how this bias responds to hyperparameter adjustments in GPT-4, specifically examining sampling temperature and top p which control the randomness of model outputs. By generating stories about individuals from different racial and gender groups and comparing their similarities using vector representations, we assess both bias robustness and its relationship with hyperparameter values. We find that (1) homogeneity bias persists across most hyperparameter configurations, with Black Americans and women being represented more homogeneously than White Americans and men, (2) the relationship between hyperparameters and group representations shows unexpected non-linear patterns, particularly at extreme values, and (3) hyperparameter adjustments affect racial and gender homogeneity bias differently-while increasing temperature or decreasing top p can reduce racial homogeneity bias, these changes show different effects on gender homogeneity bias. Our findings suggest that while hyperparameter tuning may mitigate certain biases to some extent, it cannot serve as a universal solution for addressing homogeneity bias across different social group dimensions.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "32",
        "title": "Diffusion Model-Based Data Synthesis Aided Federated Semi-Supervised Learning",
        "author": [
            "Zhongwei Wang",
            "Tong Wu",
            "Zhiyong Chen",
            "Liang Qian",
            "Yin Xu",
            "Meixia Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02219",
        "abstract": "Federated semi-supervised learning (FSSL) is primarily challenged by two factors: the scarcity of labeled data across clients and the non-independent and identically distribution (non-IID) nature of data among clients. In this paper, we propose a novel approach, diffusion model-based data synthesis aided FSSL (DDSA-FSSL), which utilizes a diffusion model (DM) to generate synthetic data, bridging the gap between heterogeneous local data distributions and the global data distribution. In DDSA-FSSL, clients address the challenge of the scarcity of labeled data by employing a federated learning-trained classifier to perform pseudo labeling for unlabeled data. The DM is then collaboratively trained using both labeled and precision-optimized pseudo-labeled data, enabling clients to generate synthetic samples for classes that are absent in their labeled datasets. This process allows clients to generate more comprehensive synthetic datasets aligned with the global distribution. Extensive experiments conducted on multiple datasets and varying non-IID distributions demonstrate the effectiveness of DDSA-FSSL, e.g., it improves accuracy from 38.46% to 52.14% on CIFAR-10 datasets with 10% labeled data.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "33",
        "title": "Leveraging Large Language Models and Machine Learning for Smart Contract Vulnerability Detection",
        "author": [
            "S M Mostaq Hossain",
            "Amani Altarawneh",
            "Jesse Roberts"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02229",
        "abstract": "As blockchain technology and smart contracts become widely adopted, securing them throughout every stage of the transaction process is essential. The concern of improved security for smart contracts is to find and detect vulnerabilities using classical Machine Learning (ML) models and fine-tuned Large Language Models (LLM). The robustness of such work rests on a labeled smart contract dataset that includes annotated vulnerabilities on which several LLMs alongside various traditional machine learning algorithms such as DistilBERT model is trained and tested. We train and test machine learning algorithms to classify smart contract codes according to vulnerability types in order to compare model performance. Having fine-tuned the LLMs specifically for smart contract code classification should help in getting better results when detecting several types of well-known vulnerabilities, such as Reentrancy, Integer Overflow, Timestamp Dependency and Dangerous Delegatecall. From our initial experimental results, it can be seen that our fine-tuned LLM surpasses the accuracy of any other model by achieving an accuracy of over 90%, and this advances the existing vulnerability detection benchmarks. Such performance provides a great deal of evidence for LLMs ability to describe the subtle patterns in the code that traditional ML models could miss. Thus, we compared each of the ML and LLM models to give a good overview of each models strengths, from which we can choose the most effective one for real-world applications in smart contract security. Our research combines machine learning and large language models to provide a rich and interpretable framework for detecting different smart contract vulnerabilities, which lays a foundation for a more secure blockchain ecosystem.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "34",
        "title": "Survey on Question Answering over Visually Rich Documents: Methods, Challenges, and Trends",
        "author": [
            "Camille Barboule",
            "Benjamin Piwowarski",
            "Yoan Chabot"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02235",
        "abstract": "Using Large Language Models (LLMs) for Visually-rich Document Understanding (VrDU) has significantly improved performance on tasks requiring both comprehension and generation, such as question answering, albeit introducing new challenges. This survey explains how VrDU models enhanced by LLMs function, covering methods for integrating VrD features into LLMs and highlighting key challenges.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "35",
        "title": "Financial Named Entity Recognition: How Far Can LLM Go?",
        "author": [
            "Yi-Te Lu",
            "Yintong Huo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02237",
        "abstract": "The surge of large language models (LLMs) has revolutionized the extraction and analysis of crucial information from a growing volume of financial statements, announcements, and business news. Recognition for named entities to construct structured data poses a significant challenge in analyzing financial documents and is a foundational task for intelligent financial analytics. However, how effective are these generic LLMs and their performance under various prompts are yet need a better understanding. To fill in the blank, we present a systematic evaluation of state-of-the-art LLMs and prompting methods in the financial Named Entity Recognition (NER) problem. Specifically, our experimental results highlight their strengths and limitations, identify five representative failure types, and provide insights into their potential and challenges for domain-specific tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "36",
        "title": "MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control",
        "author": [
            "Mengting Wei",
            "Tuomas Varanka",
            "Xingxun Jiang",
            "Huai-Qian Khor",
            "Guoying Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02260",
        "abstract": "We address the problem of facial expression editing by controling the relative variation of facial action-unit (AU) from the same person. This enables us to edit this specific person's expression in a fine-grained, continuous and interpretable manner, while preserving their identity, pose, background and detailed facial attributes. Key to our model, which we dub MagicFace, is a diffusion model conditioned on AU variations and an ID encoder to preserve facial details of high consistency. Specifically, to preserve the facial details with the input identity, we leverage the power of pretrained Stable-Diffusion models and design an ID encoder to merge appearance features through self-attention. To keep background and pose consistency, we introduce an efficient Attribute Controller by explicitly informing the model of current background and pose of the target. By injecting AU variations into a denoising UNet, our model can animate arbitrary identities with various AU combinations, yielding superior results in high-fidelity expression editing compared to other facial expression editing works. Code is publicly available at https://github.com/weimengting/MagicFace.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "37",
        "title": "Unsupervised Class Generation to Expand Semantic Segmentation Datasets",
        "author": [
            "Javier Montalvo",
            "√Ålvaro Garc√≠a-Mart√≠n",
            "Pablo Carballeira",
            "Juan C. SanMiguel"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02264",
        "abstract": "Semantic segmentation is a computer vision task where classification is performed at a pixel level. Due to this, the process of labeling images for semantic segmentation is time-consuming and expensive. To mitigate this cost there has been a surge in the use of synthetically generated data -- usually created using simulators or videogames -- which, in combination with domain adaptation methods, can effectively learn how to segment real data. Still, these datasets have a particular limitation: due to their closed-set nature, it is not possible to include novel classes without modifying the tool used to generate them, which is often not public. Concurrently, generative models have made remarkable progress, particularly with the introduction of diffusion models, enabling the creation of high-quality images from text prompts without additional supervision.\nIn this work, we propose an unsupervised pipeline that leverages Stable Diffusion and Segment Anything Module to generate class examples with an associated segmentation mask, and a method to integrate generated cutouts for novel classes in semantic segmentation datasets, all with minimal user input. Our approach aims to improve the performance of unsupervised domain adaptation methods by introducing novel samples into the training data without modifications to the underlying algorithms. With our methods, we show how models can not only effectively learn how to segment novel classes, with an average performance of 51% IoU, but also reduce errors for other, already existing classes, reaching a higher performance level overall.",
        "tags": [
            "Diffusion",
            "Segment Anything",
            "Segmentation"
        ]
    },
    {
        "id": "38",
        "title": "LLMzSz{\\L}: a comprehensive LLM benchmark for Polish",
        "author": [
            "Krzysztof Jassem",
            "Micha≈Ç Ciesi√≥≈Çka",
            "Filip Grali≈Ñski",
            "Piotr Jab≈Ço≈Ñski",
            "Jakub Pokrywka",
            "Marek Kubis",
            "Monika Jab≈Ço≈Ñska",
            "Ryszard Staruch"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02266",
        "abstract": "This article introduces the first comprehensive benchmark for the Polish language at this scale: LLMzSz≈Å (LLMs Behind the School Desk). It is based on a coherent collection of Polish national exams, including both academic and professional tests extracted from the archives of the Polish Central Examination Board. It covers 4 types of exams, coming from 154 domains. Altogether, it consists of almost 19k closed-ended questions. We investigate the performance of open-source multilingual, English, and Polish LLMs to verify LLMs' abilities to transfer knowledge between languages. Also, the correlation between LLMs and humans at model accuracy and exam pass rate levels is examined. We show that multilingual LLMs can obtain superior results over monolingual ones; however, monolingual models may be beneficial when model size matters. Our analysis highlights the potential of LLMs in assisting with exam validation, particularly in identifying anomalies or errors in examination tasks.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "39",
        "title": "What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph",
        "author": [
            "Yutao Jiang",
            "Qiong Wu",
            "Wenhao Lin",
            "Wei Yu",
            "Yiyi Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02268",
        "abstract": "Recent Multimodal Large Language Models(MLLMs) often use a large number of visual tokens to compensate their visual shortcoming, leading to excessive computation and obvious visual redundancy. In this paper, we investigate what kind of visual tokens are needed for MLLMs, and reveal that both foreground and background tokens are critical for MLLMs given the varying difficulties of examples. Based on this observation, we propose a graph-based method towards training-free visual token pruning, termed http://G-Prune.In particular, G-Prune regards visual tokens as nodes, and construct their connections based on their semantic similarities. Afterwards, the information flow is propagated via weighted links, and the most important tokens after iterations are kept for MLLMs, which can be front or http://background.To validate G-Prune, we apply it to a recent MLLM called LLaVA-NeXT, and conduct extensive experiments on a set of http://benchmarks.The experiment results show that G-Prune can greatly reduce computation overhead while retaining high performance on both coarse- and fine-grained tasks. For instance, G-Prune can reduce 63.57\\% FLOPs of LLaVA-NeXT on VQA2.0 and TextVQA with only 0.95\\% and 2.34\\% accuracy drops, respectively.",
        "tags": [
            "LLaVA",
            "Large Language Models"
        ]
    },
    {
        "id": "40",
        "title": "TDM: Temporally-Consistent Diffusion Model for All-in-One Real-World Video Restoration",
        "author": [
            "Yizhou Li",
            "Zihua Liu",
            "Yusuke Monno",
            "Masatoshi Okutomi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02269",
        "abstract": "In this paper, we propose the first diffusion-based all-in-one video restoration method that utilizes the power of a pre-trained Stable Diffusion and a fine-tuned ControlNet. Our method can restore various types of video degradation with a single unified model, overcoming the limitation of standard methods that require specific models for each restoration task. Our contributions include an efficient training strategy with Task Prompt Guidance (TPG) for diverse restoration tasks, an inference strategy that combines Denoising Diffusion Implicit Models~(DDIM) inversion with a novel Sliding Window Cross-Frame Attention (SW-CFA) mechanism for enhanced content preservation and temporal consistency, and a scalable pipeline that makes our method all-in-one to adapt to different video restoration tasks. Through extensive experiments on five video restoration tasks, we demonstrate the superiority of our method in generalization capability to real-world videos and temporal consistency preservation over existing state-of-the-art methods. Our method advances the video restoration task by providing a unified solution that enhances video quality across multiple applications.",
        "tags": [
            "ControlNet",
            "DDIM",
            "Diffusion"
        ]
    },
    {
        "id": "41",
        "title": "On Symmetries in Analytic Input-Output Systems",
        "author": [
            "W. Steven Gray",
            "Erik I. Verriest"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02280",
        "abstract": "There are many notions of symmetry for state space models. They play a role in understanding when systems are time reversible, provide a system theoretic interpretation of thermodynamics, and have applications in certain stabilization and optimal control problems. The earliest form of symmetry for analytic input-output systems is due to Fliess who introduced systems described by an exchangeable generating series. In this case, one is able to write the output as a memoryless analytic function of the integral of each input. The first goal of this paper is to describe two new types of symmetry for such Chen--Fliess input-output systems, namely, coefficient reversible symmetry and palindromic symmetry. Each concept is then related to the notion of an exchangeable series. The second goal of the paper is to provide an in-depth analysis of Chen--Fliess input-output systems whose generating series are linear time-varying, palindromic, and have generating series coefficients growing at a maximal rate while ensuring some type of convergence. It is shown that such series have an infinite Hankel rank and Lie rank, have a certain infinite dimensional state space realization, and a description of their relative degree and zero dynamics is given.",
        "tags": [
            "State Space Models"
        ]
    },
    {
        "id": "42",
        "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
        "author": [
            "Yachao Zhao",
            "Bo Wang",
            "Yan Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02295",
        "abstract": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated bias in LLMs, prior work has predominantly focused on explicit bias, leaving the more nuanced implicit biases largely unexplored. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel \"self-reflection\" based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on state-of-the-art LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases, where explicit biases manifest as mild stereotypes while implicit biases show strong stereotypes. Furthermore, we investigate the underlying factors contributing to this explicit-implicit bias inconsistency. Our experiments examine the effects of training data scale, model parameters, and alignment techniques. Results indicate that while explicit bias diminishes with increased training data and model size, implicit bias exhibits a contrasting upward trend. Notably, contemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit bias but show limited efficacy in mitigating implicit bias. These findings suggest that while scaling up models and alignment training can address explicit bias, the challenge of implicit bias requires novel approaches beyond current methodologies.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "43",
        "title": "Design and Benchmarking of A Multi-Modality Sensor for Robotic Manipulation with GAN-Based Cross-Modality Interpretation",
        "author": [
            "Dandan Zhang",
            "Wen Fan",
            "Jialin Lin",
            "Haoran Li",
            "Qingzheng Cong",
            "Weiru Liu",
            "Nathan F. Lepora",
            "Shan Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02303",
        "abstract": "In this paper, we present the design and benchmark of an innovative sensor, ViTacTip, which fulfills the demand for advanced multi-modal sensing in a compact design. A notable feature of ViTacTip is its transparent skin, which incorporates a `see-through-skin' mechanism. This mechanism aims at capturing detailed object features upon contact, significantly improving both vision-based and proximity perception capabilities. In parallel, the biomimetic tips embedded in the sensor's skin are designed to amplify contact details, thus substantially augmenting tactile and derived force perception abilities. To demonstrate the multi-modal capabilities of ViTacTip, we developed a multi-task learning model that enables simultaneous recognition of hardness, material, and textures. To assess the functionality and validate the versatility of ViTacTip, we conducted extensive benchmarking experiments, including object recognition, contact point detection, pose regression, and grating identification. To facilitate seamless switching between various sensing modalities, we employed a Generative Adversarial Network (GAN)-based approach. This method enhances the applicability of the ViTacTip sensor across diverse environments by enabling cross-modality interpretation.",
        "tags": [
            "Detection",
            "GAN"
        ]
    },
    {
        "id": "44",
        "title": "Fourier-Gegenbauer Integral-Galerkin Method for Solving the Advection-Diffusion Equation With Periodic Boundary Conditions",
        "author": [
            "Kareem T. Elgindy"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02307",
        "abstract": "This study presents the Fourier-Gegenbauer Integral-Galerkin (FGIG) method, a novel and efficient numerical framework for solving the one-dimensional advection-diffusion equation with periodic boundary conditions. The FGIG method uniquely combines Fourier series for spatial periodicity and Gegenbauer polynomials for temporal integration within a Galerkin framework, resulting in highly accurate numerical and semi-analytical solutions. Distinctively, this approach eliminates the need for time-stepping procedures by reformulating the problem as a system of integral equations, reducing error accumulation over long-time simulations and improving computational efficiency. Key contributions include exponential convergence rates for smooth solutions, robustness under oscillatory conditions, and an inherently parallelizable structure, enabling scalable computation for large-scale problems. Additionally, the method introduces a barycentric formulation of shifted-Gegenbauer-Gauss quadrature to ensure high accuracy and stability for relatively low P√©clet numbers. Numerical experiments validate the method's superior performance over traditional techniques, demonstrating its potential for extending to higher-dimensional problems and diverse applications in computational mathematics and engineering.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "45",
        "title": "Improving the stability and efficiency of high-order operator-splitting methods",
        "author": [
            "Siqi Wei",
            "Victoria Guenter",
            "Raymond J. Spiteri"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02310",
        "abstract": "Operator-splitting methods are widely used to solve differential equations, especially those that arise from multi-scale or multi-physics models, because a monolithic (single-method) approach may be inefficient or even infeasible. The most common operator-splitting methods are the first-order Lie--Trotter (or Godunov) and the second-order Strang (Strang--Marchuk) splitting methods. High-order splitting methods with real coefficients require backward-in-time integration in each operator and hence may be adversely impacted by instability for certain operators such as diffusion. However, besides the method coefficients, there are many other ancillary aspects to an overall operator-splitting method that are important but often overlooked. For example, the operator ordering and the choice of sub-integration methods can significantly affect the stability and efficiency of an operator-splitting method. In this paper, we investigate some design principles for the construction of operator-splitting methods, including minimization of local error measure, choice of sub-integration method, maximization of linear stability, and minimization of overall computational cost. We propose a new four-stage, third-order, 2-split operator-splitting method with seven sub-integrations per step and optimized linear stability for a benchmark problem from cardiac electrophysiology. We then propose a general principle to further improve stability and efficiency of such operator-splitting methods by using low-order, explicit sub-integrators for unstable sub-integrations. We demonstrate an almost 30\\% improvement in the performance of methods derived from these design principles compared to the best-known third-order methods.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "46",
        "title": "DiffGraph: Heterogeneous Graph Diffusion Model",
        "author": [
            "Zongwei Li",
            "Lianghao Xia",
            "Hua Hua",
            "Shijie Zhang",
            "Shuangyang Wang",
            "Chao Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02313",
        "abstract": "Recent advances in Graph Neural Networks (GNNs) have revolutionized graph-structured data modeling, yet traditional GNNs struggle with complex heterogeneous structures prevalent in real-world scenarios. Despite progress in handling heterogeneous interactions, two fundamental challenges persist: noisy data significantly compromising embedding quality and learning performance, and existing methods' inability to capture intricate semantic transitions among heterogeneous relations, which impacts downstream predictions. To address these fundamental issues, we present the Heterogeneous Graph Diffusion Model (DiffGraph), a pioneering framework that introduces an innovative cross-view denoising strategy. This advanced approach transforms auxiliary heterogeneous data into target semantic spaces, enabling precise distillation of task-relevant information. At its core, DiffGraph features a sophisticated latent heterogeneous graph diffusion mechanism, implementing a novel forward and backward diffusion process for superior noise management. This methodology achieves simultaneous heterogeneous graph denoising and cross-type transition, while significantly simplifying graph generation through its latent-space diffusion capabilities. Through rigorous experimental validation on both public and industrial datasets, we demonstrate that DiffGraph consistently surpasses existing methods in link prediction and node classification tasks, establishing new benchmarks for robustness and efficiency in heterogeneous graph processing. The model implementation is publicly available at: https://github.com/HKUDS/DiffGraph.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "47",
        "title": "Validity Arguments For Constructed Response Scoring Using Generative Artificial Intelligence Applications",
        "author": [
            "Jodi M. Casabianca",
            "Daniel F. McCaffrey",
            "Matthew S. Johnson",
            "Naim Alper",
            "Vladimir Zubenko"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02334",
        "abstract": "The rapid advancements in large language models and generative artificial intelligence (AI) capabilities are making their broad application in the high-stakes testing context more likely. Use of generative AI in the scoring of constructed responses is particularly appealing because it reduces the effort required for handcrafting features in traditional AI scoring and might even outperform those methods. The purpose of this paper is to highlight the differences in the feature-based and generative AI applications in constructed response scoring systems and propose a set of best practices for the collection of validity evidence to support the use and interpretation of constructed response scores from scoring systems using generative AI. We compare the validity evidence needed in scoring systems using human ratings, feature-based natural language processing AI scoring engines, and generative AI. The evidence needed in the generative AI context is more extensive than in the feature-based NLP scoring context because of the lack of transparency and other concerns unique to generative AI such as consistency. Constructed response score data from standardized tests demonstrate the collection of validity evidence for different types of scoring systems and highlights the numerous complexities and considerations when making a validity argument for these scores. In addition, we discuss how the evaluation of AI scores might include a consideration of how a contributory scoring approach combining multiple AI scores (from different sources) will cover more of the construct in the absence of human ratings.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "48",
        "title": "AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference",
        "author": [
            "Zhuomin He",
            "Yizhen Yao",
            "Pengfei Zuo",
            "Bin Gao",
            "Qinya Li",
            "Zhenzhe Zheng",
            "Fan Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02336",
        "abstract": "Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes \\sysname, an adaptive sublayer skipping method specifically designed for long-context inference. \\sysname adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of \\sysname is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "49",
        "title": "Evaluation of the Code Generation Capabilities of ChatGPT 4: A Comparative Analysis in 19 Programming Languages",
        "author": [
            "L. C. Gilbert"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02338",
        "abstract": "This bachelor's thesis examines the capabilities of ChatGPT 4 in code generation across 19 programming languages. The study analyzed solution rates across three difficulty levels, types of errors encountered, and code quality in terms of runtime and memory efficiency through a quantitative experiment. A total of 188 programming problems were selected from the LeetCode platform, and ChatGPT 4 was given three attempts to produce a correct solution with feedback. ChatGPT 4 successfully solved 39.67% of all tasks, with success rates decreasing significantly as problem complexity increased. Notably, the model faced considerable challenges with hard problems across all languages. ChatGPT 4 demonstrated higher competence in widely used languages, likely due to a larger volume and higher quality of training data. The solution rates also revealed a preference for languages with low abstraction levels and static typing. For popular languages, the most frequent error was \"Wrong Answer,\" whereas for less popular languages, compiler and runtime errors prevailed, suggesting frequent misunderstandings and confusion regarding the structural characteristics of these languages. The model exhibited above-average runtime efficiency in all programming languages, showing a tendency toward statically typed and low-abstraction languages. Memory efficiency results varied significantly, with above-average performance in 14 languages and below-average performance in five languages. A slight preference for low-abstraction languages and a leaning toward dynamically typed languages in terms of memory efficiency were observed. Future research should include a larger number of tasks, iterations, and less popular languages. Additionally, ChatGPT 4's abilities in code interpretation and summarization, debugging, and the development of complex, practical code could be analyzed further.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "50",
        "title": "UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility",
        "author": [
            "Yonglin Tian",
            "Fei Lin",
            "Yiduo Li",
            "Tengchao Zhang",
            "Qiyao Zhang",
            "Xuan Fu",
            "Jun Huang",
            "Xingyuan Dai",
            "Yutong Wang",
            "Chunwei Tian",
            "Bai Li",
            "Yisheng Lv",
            "Levente Kov√°cs",
            "Fei-Yue Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02341",
        "abstract": "Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has introduced transformative advancements across various domains, like transportation, logistics, and agriculture. Leveraging flexible perspectives and rapid maneuverability, UAVs extend traditional systems' perception and action capabilities, garnering widespread attention from academia and industry. However, current UAV operations primarily depend on human control, with only limited autonomy in simple scenarios, and lack the intelligence and adaptability needed for more complex environments and tasks. The emergence of large language models (LLMs) demonstrates remarkable problem-solving and generalization capabilities, offering a promising pathway for advancing UAV intelligence. This paper explores the integration of LLMs and UAVs, beginning with an overview of UAV systems' fundamental components and functionalities, followed by an overview of the state-of-the-art in LLM technology. Subsequently, it systematically highlights the multimodal data resources available for UAVs, which provide critical support for training and evaluation. Furthermore, it categorizes and analyzes key tasks and application scenarios where UAVs and LLMs converge. Finally, a reference roadmap towards agentic UAVs is proposed, aiming to enable UAVs to achieve agentic intelligence through autonomous perception, memory, reasoning, and tool utilization. Related resources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "51",
        "title": "Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving",
        "author": [
            "Sanghyun Park",
            "Boris Maciejovsky",
            "Phanish Puranam"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02348",
        "abstract": "Complex problem-solving requires cognitive flexibility--the capacity to entertain multiple perspectives while preserving their distinctiveness. This flexibility replicates the \"wisdom of crowds\" within a single individual, allowing them to \"think with many minds.\" While mental simulation enables imagined deliberation, cognitive constraints limit its effectiveness. We propose synthetic deliberation, a Large Language Model (LLM)-based method that simulates discourse between agents embodying diverse perspectives, as a solution. Using a custom GPT-based model, we showcase its benefits: concurrent processing of multiple viewpoints without cognitive degradation, parallel exploration of perspectives, and precise control over viewpoint synthesis. By externalizing the deliberative process and distributing cognitive labor between parallel search and integration, synthetic deliberation transcends mental simulation's limitations. This approach shows promise for strategic planning, policymaking, and conflict resolution.",
        "tags": [
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "52",
        "title": "CorrFill: Enhancing Faithfulness in Reference-based Inpainting with Correspondence Guidance in Diffusion Models",
        "author": [
            "Kuan-Hung Liu",
            "Cheng-Kun Yang",
            "Min-Hung Chen",
            "Yu-Lun Liu",
            "Yen-Yu Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02355",
        "abstract": "In the task of reference-based image inpainting, an additional reference image is provided to restore a damaged target image to its original state. The advancement of diffusion models, particularly Stable Diffusion, allows for simple formulations in this task. However, existing diffusion-based methods often lack explicit constraints on the correlation between the reference and damaged images, resulting in lower faithfulness to the reference images in the inpainting results. In this work, we propose CorrFill, a training-free module designed to enhance the awareness of geometric correlations between the reference and target images. This enhancement is achieved by guiding the inpainting process with correspondence constraints estimated during inpainting, utilizing attention masking in self-attention layers and an objective function to update the input tensor according to the constraints. Experimental results demonstrate that CorrFill significantly enhances the performance of multiple baseline diffusion-based methods, including state-of-the-art approaches, by emphasizing faithfulness to the reference images.",
        "tags": [
            "Diffusion",
            "Inpainting"
        ]
    },
    {
        "id": "53",
        "title": "Context Aware Lemmatization and Morphological Tagging Method in Turkish",
        "author": [
            "Cagri Sayallar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02361",
        "abstract": "The smallest part of a word that defines the word is called a word root. Word roots are used to increase success in many applications since they simplify the word. In this study, the lemmatization model, which is a word root finding method, and the morphological tagging model, which predicts the grammatical knowledge of the word, are presented. The presented model was developed for Turkish, and both models make predictions by taking the meaning of the word into account. In the literature, there is no lemmatization study that is sensitive to word meaning in Turkish. For this reason, the present study shares the model and the results obtained from the model on Turkish lemmatization for the first time in the literature. In the present study, in the lemmatization and morphological tagging models, bidirectional LSTM is used for the spelling of words, and the Turkish BERT model is used for the meaning of words. The models are trained using the IMST and PUD datasets from Universal Dependencies. The results from the training of the models were compared with the results from the SIGMORPHON 2019 competition. The results of the comparisons revealed that our models were superior.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "54",
        "title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison",
        "author": [
            "Tsz Kin Lam",
            "Marco Gaido",
            "Sara Papi",
            "Luisa Bentivogli",
            "Barry Haddow"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02370",
        "abstract": "Following the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech -- the most common form in communication. To integrate speech into LLMs, one promising approach is dense feature prepending (DFP) which prepends the projected speech representations to the textual representations, allowing end-to-end training with the speech encoder. However, DFP typically requires connecting a text decoder to a speech encoder. This raises questions about the importance of having a sophisticated speech encoder for DFP, and how its performance compares with a standard encoder-decoder (i.e. cross-attention) architecture. In order to perform a controlled architectural comparison, we train all models from scratch, rather than using large pretrained models, and use comparable data and parameter settings, testing speech-to-text recognition (ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. We study the influence of a speech encoder in DFP. More importantly, we compare DFP and cross-attention under a variety of configurations, such as CTC compression, sequence-level knowledge distillation, generation speed and GPU memory footprint on monolingual, bilingual and multilingual models. Despite the prevalence of DFP over cross-attention, our overall results do not indicate a clear advantage of DFP.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "55",
        "title": "Generalizable Origin Identification for Text-Guided Image-to-Image Diffusion Models",
        "author": [
            "Wenhao Wang",
            "Yifan Sun",
            "Zongxin Yang",
            "Zhentao Tan",
            "Zhengdong Hu",
            "Yi Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02376",
        "abstract": "Text-guided image-to-image diffusion models excel in translating images based on textual prompts, allowing for precise and creative visual modifications. However, such a powerful technique can be misused for spreading misinformation, infringing on copyrights, and evading content tracing. This motivates us to introduce the task of origin IDentification for text-guided Image-to-image Diffusion models (ID$^2$), aiming to retrieve the original image of a given translated query. A straightforward solution to ID$^2$ involves training a specialized deep embedding model to extract and compare features from both query and reference images. However, due to visual discrepancy across generations produced by different diffusion models, this similarity-based approach fails when training on images from one model and testing on those from another, limiting its effectiveness in real-world applications. To solve this challenge of the proposed ID$^2$ task, we contribute the first dataset and a theoretically guaranteed method, both emphasizing generalizability. The curated dataset, OriPID, contains abundant Origins and guided Prompts, which can be used to train and test potential IDentification models across various diffusion models. In the method section, we first prove the existence of a linear transformation that minimizes the distance between the pre-trained Variational Autoencoder (VAE) embeddings of generated samples and their origins. Subsequently, it is demonstrated that such a simple linear transformation can be generalized across different diffusion models. Experimental results show that the proposed method achieves satisfying generalization performance, significantly surpassing similarity-based methods ($+31.6\\%$ mAP), even those with generalization designs.",
        "tags": [
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": "56",
        "title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
        "author": [
            "Markus J. Buehler"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02393",
        "abstract": "We present an approach to modifying Transformer architectures by integrating graph-aware relational reasoning into the attention mechanism, merging concepts from graph neural networks and language modeling. Building on the inherent connection between attention and graph theory, we reformulate the Transformer's attention mechanism as a graph operation and propose Graph-Aware Isomorphic Attention. This method leverages advanced graph modeling strategies, including Graph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA), to enrich the representation of relational structures. Our approach captures complex dependencies and generalizes across tasks, as evidenced by a reduced generalization gap and improved learning performance. Additionally, we expand the concept of graph-aware attention to introduce Sparse GIN-Attention, a fine-tuning approach that employs sparse GINs. By interpreting attention matrices as sparse adjacency graphs, this technique enhances the adaptability of pre-trained foundational models with minimal computational overhead, endowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning achieves improved training dynamics and better generalization compared to alternative methods like low-rank adaption (LoRA). We discuss latent graph-like structures within traditional attention mechanisms, offering a new lens through which Transformers can be understood. By evolving Transformers as hierarchical GIN models for relational reasoning. This perspective suggests profound implications for foundational model development, enabling the design of architectures that dynamically adapt to both local and global dependencies. Applications in bioinformatics, materials science, language modeling, and beyond could benefit from this synthesis of relational and sequential data modeling, setting the stage for interpretable and generalizable modeling strategies.",
        "tags": [
            "LoRA",
            "Transformer"
        ]
    },
    {
        "id": "57",
        "title": "GenTREC: The First Test Collection Generated by Large Language Models for Evaluating Information Retrieval Systems",
        "author": [
            "Mehmet Deniz T√ºrkmen",
            "Mucahid Kutlu",
            "Bahadir Altun",
            "Gokalp Cosgun"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02408",
        "abstract": "Building test collections for Information Retrieval evaluation has traditionally been a resource-intensive and time-consuming task, primarily due to the dependence on manual relevance judgments. While various cost-effective strategies have been explored, the development of such collections remains a significant challenge. In this paper, we present GenTREC , the first test collection constructed entirely from documents generated by a Large Language Model (LLM), eliminating the need for manual relevance judgments. Our approach is based on the assumption that documents generated by an LLM are inherently relevant to the prompts used for their generation. Based on this heuristic, we utilized existing TREC search topics to generate documents. We consider a document relevant only to the prompt that generated it, while other document-topic pairs are treated as non-relevant. To introduce realistic retrieval challenges, we also generated non-relevant documents, ensuring that IR systems are tested against a diverse and robust set of materials. The resulting GenTREC collection comprises 96,196 documents, 300 topics, and 18,964 relevance \"judgments\". We conducted extensive experiments to evaluate GenTREC in terms of document quality, relevance judgment accuracy, and evaluation reliability. Notably, our findings indicate that the ranking of IR systems using GenTREC is compatible with the evaluations conducted using traditional TREC test collections, particularly for P@100, MAP, and RPrec metrics. Overall, our results show that our proposed approach offers a promising, low-cost alternative for IR evaluation, significantly reducing the burden of building and maintaining future IR evaluation resources.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "58",
        "title": "FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance",
        "author": [
            "Haicheng Wang",
            "Zhemeng Yu",
            "Gabriele Spadaro",
            "Chen Ju",
            "Victor Qu√©tu",
            "Enzo Tartaglione"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02430",
        "abstract": "Recently, Multi-modal Large Language Models (MLLMs) have shown remarkable effectiveness for multi-modal tasks due to their abilities to generate and understand cross-modal data. However, processing long sequences of visual tokens extracted from visual backbones poses a challenge for deployment in real-time applications. To address this issue, we introduce FOLDER, a simple yet effective plug-and-play module designed to reduce the length of the visual token sequence, mitigating both computational and memory demands during training and inference. Through a comprehensive analysis of the token reduction process, we analyze the information loss introduced by different reduction strategies and develop FOLDER to preserve key information while removing visual redundancy. We showcase the effectiveness of FOLDER by integrating it into the visual backbone of several MLLMs, significantly accelerating the inference phase. Furthermore, we evaluate its utility as a training accelerator or even performance booster for MLLMs. In both contexts, FOLDER achieves comparable or even better performance than the original models, while dramatically reducing complexity by removing up to 70% of visual tokens.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "59",
        "title": "Efficient Deployment of Large Language Models on Resource-constrained Devices",
        "author": [
            "Zhiwei Yao",
            "Yang Xu",
            "Hongli Xu",
            "Yunming Liao",
            "Zuan Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02438",
        "abstract": "Deploying Large Language Models (LLMs) on resource-constrained (or weak) devices presents significant challenges due to limited resources and heterogeneous data distribution. To address the data concern, it is necessary to fine-tune LLMs using on-device private data for various downstream tasks. While Federated Learning (FL) offers a promising privacy-preserving solution, existing fine-tuning methods retain the original LLM size, leaving issues of high inference latency and excessive memory demands unresolved. Hence, we design FedSpine, an FL framework that combines Parameter- Efficient Fine-Tuning (PEFT) with structured pruning for efficient deployment of LLMs on resource-constrained devices. Specifically, FedSpine introduces an iterative process to prune and tune the parameters of LLMs. To mitigate the impact of device heterogeneity, an online Multi-Armed Bandit (MAB) algorithm is employed to adaptively determine different pruning ratios and LoRA ranks for heterogeneous devices without any prior knowledge of their computing and communication capabilities. As a result, FedSpine maintains higher inference accuracy while improving fine-tuning efficiency. Experimental results conducted on a physical platform with 80 devices demonstrate that FedSpine can speed up fine-tuning by 1.4$\\times$-6.9$\\times$ and improve final accuracy by 0.4%-4.5% under the same sparsity level compared to other baselines.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "60",
        "title": "Understand, Solve and Translate: Bridging the Multilingual Mathematical Reasoning Gap",
        "author": [
            "Hyunwoo Ko",
            "Guijin Son",
            "Dasol Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02448",
        "abstract": "Large language models (LLMs) demonstrate exceptional performance on complex reasoning tasks. However, despite their strong reasoning capabilities in high-resource languages (e.g., English and Chinese), a significant performance gap persists in other languages. To investigate this gap in Korean, we introduce HRM8K, a benchmark comprising 8,011 English-Korean parallel bilingual math problems. Through systematic analysis of model behaviors, we identify a key finding: these performance disparities stem primarily from difficulties in comprehending non-English inputs, rather than limitations in reasoning capabilities. Based on these findings, we propose UST (Understand, Solve, and Translate), a method that strategically uses English as an anchor for reasoning and solution generation. By fine-tuning the model on 130k synthetically generated data points, UST achieves a 10.91% improvement on the HRM8K benchmark and reduces the multilingual performance gap from 11.6% to 0.7%. Additionally, we show that improvements from UST generalize effectively to different Korean domains, demonstrating that capabilities acquired from machine-verifiable content can be generalized to other areas. We publicly release the benchmark, training dataset, and models.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "61",
        "title": "DeTrack: In-model Latent Denoising Learning for Visual Object Tracking",
        "author": [
            "Xinyu Zhou",
            "Jinglun Li",
            "Lingyi Hong",
            "Kaixun Jiang",
            "Pinxue Guo",
            "Weifeng Ge",
            "Wenqiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02467",
        "abstract": "Previous visual object tracking methods employ image-feature regression models or coordinate autoregression models for bounding box prediction. Image-feature regression methods heavily depend on matching results and do not utilize positional prior, while the autoregressive approach can only be trained using bounding boxes available in the training set, potentially resulting in suboptimal performance during testing with unseen data. Inspired by the diffusion model, denoising learning enhances the model's robustness to unseen data. Therefore, We introduce noise to bounding boxes, generating noisy boxes for training, thus enhancing model robustness on testing data. We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process. However, tracking algorithms are usually asked to run in real-time, directly applying the diffusion model to object tracking would severely impair tracking speed. Therefore, we decompose the denoising learning process into every denoising block within a model, not by running the model multiple times, and thus we summarize the proposed paradigm as an in-model latent denoising learning process. Specifically, we propose a denoising Vision Transformer (ViT), which is composed of multiple denoising blocks. In the denoising block, template and search embeddings are projected into every denoising block as conditions. A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process. Subsequently, we utilize image features and trajectory information to refine the denoised bounding box. Besides, we also utilize trajectory memory and visual memory to improve tracking stability. Experimental results validate the effectiveness of our approach, achieving competitive performance on several challenging datasets.",
        "tags": [
            "Diffusion",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "62",
        "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas and Ad-Hoc Networks",
        "author": [
            "Atonu Ghosh",
            "Sudip Misra"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02469",
        "abstract": "The minimal infrastructure requirements of LoRa make it suitable for deployments in remote and disaster-stricken areas. Concomitantly, the modern era is witnessing the proliferation of web applications in all aspects of human life, including IoT and other network services. Contemporary IoT and network solutions heavily rely on web applications to render services. However, despite the recent research and development pivoted around LoRa, there is still a lack of studies focusing on web application access over LoRa networks. Specifically, technical challenges like payload size limitation, low data rate, and contentions in multi-user setups limit the applicability of LoRa for web applications. Hence, we propose LoRaWeb, which enables web access over LoRa networks. The LoRaWeb hardware tethers a WiFi hotspot to which the client devices connect and access the web pages using a web browser. LoRa backbone of the network handles the web page transmission from the requester and receiver devices. LoRaWeb implements a synchronization procedure to address the aforementioned challenges for effective message exchange between requesters and responders. The system implements a caching mechanism to reduce latency and contention. Additionally, it implements a message-slicing mechanism in the application layer to overcome the hardware limitations on the message length. The actual hardware-based implementation results indicate seamless deployment, and the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and ~$6 S$ for a $10 KB$ size web page.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "63",
        "title": "Decoding News Bias: Multi Bias Detection in News Articles",
        "author": [
            "Bhushan Santosh Shah",
            "Deven Santosh Shah",
            "Vahida Attar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02482",
        "abstract": "News Articles provides crucial information about various events happening in the society but they unfortunately come with different kind of biases. These biases can significantly distort public opinion and trust in the media, making it essential to develop techniques to detect and address them. Previous works have majorly worked towards identifying biases in particular domains e.g., Political, gender biases. However, more comprehensive studies are needed to detect biases across diverse domains. Large language models (LLMs) offer a powerful way to analyze and understand natural language, making them ideal for constructing datasets and detecting these biases. In this work, we have explored various biases present in the news articles, built a dataset using LLMs and present results obtained using multiple detection techniques. Our approach highlights the importance of broad-spectrum bias detection and offers new insights for improving the integrity of news articles.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "64",
        "title": "LLMPC: Large Language Model Predictive Control",
        "author": [
            "Gabriel Maher"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02486",
        "abstract": "Recent advancements in prompting techniques for Large Language Models (LLMs) have improved their reasoning, planning, and action abilities. This paper examines these prompting techniques through the lens of model predictive control (MPC). We show that LLMs act as implicit planning cost function minimizers when planning prompts are used. Under our framework we demonstrate that LLM planning performance can be improved further by incorporating real planning cost functions and evaluators.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "MPC"
        ]
    },
    {
        "id": "65",
        "title": "ACE++: Instruction-Based Image Creation and Editing via Context-Aware Content Filling",
        "author": [
            "Chaojie Mao",
            "Jingfeng Zhang",
            "Yulin Pan",
            "Zeyinzi Jiang",
            "Zhen Han",
            "Yu Liu",
            "Jingren Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02487",
        "abstract": "We report ACE++, an instruction-based diffusion framework that tackles various image generation and editing tasks. Inspired by the input format for the inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context Condition Unit (LCU) introduced in ACE and extend this input paradigm to any editing and generation tasks. To take full advantage of image generative priors, we develop a two-stage training scheme to minimize the efforts of finetuning powerful text-to-image diffusion models like FLUX.1-dev. In the first stage, we pre-train the model using task data with the 0-ref tasks from the text-to-image model. There are many models in the community based on the post-training of text-to-image foundational models that meet this training paradigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with painting tasks and can be used as an initialization to accelerate the training process. In the second stage, we finetune the above model to support the general instructions using all tasks defined in ACE. To promote the widespread application of ACE++ in different scenarios, we provide a comprehensive set of models that cover both full finetuning and lightweight finetuning, while considering general applicability and applicability in vertical scenarios. The qualitative analysis showcases the superiority of ACE++ in terms of generating image quality and prompt following ability.",
        "tags": [
            "Diffusion",
            "Inpainting",
            "Text-to-Image"
        ]
    },
    {
        "id": "66",
        "title": "ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use",
        "author": [
            "Junjie Ye",
            "Zhengyin Du",
            "Xuesong Yao",
            "Weijian Lin",
            "Yufei Xu",
            "Zehui Chen",
            "Zaiyuan Wang",
            "Sining Zhu",
            "Zhiheng Xi",
            "Siyu Yuan",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang",
            "Jiechao Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02506",
        "abstract": "Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/bytedance-research/ToolHop.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "67",
        "title": "CHAIR-Classifier of Hallucination as Improver",
        "author": [
            "Ao Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02518",
        "abstract": "This paper presents a supervised method for detecting hallucinations in large language models. By analyzing token scores (logitis) across layers of the LLaMA model, we derive a small set, aiming to reduce overfitting, of features-including maximum, minimum, mean, standard deviation, and slope. We use logistic regression for classification and validate the model on the TruthfulQA and MMLU datasets. The results demonstrate significant performance gains, especially in zero-shot scenarios, highlighting the effectiveness and potential for generalization.",
        "tags": [
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "68",
        "title": "Layout2Scene: 3D Semantic Layout Guided Scene Generation via Geometry and Appearance Diffusion Priors",
        "author": [
            "Minglin Chen",
            "Longguang Wang",
            "Sheng Ao",
            "Ye Zhang",
            "Kai Xu",
            "Yulan Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02519",
        "abstract": "3D scene generation conditioned on text prompts has significantly progressed due to the development of 2D diffusion generation models. However, the textual description of 3D scenes is inherently inaccurate and lacks fine-grained control during training, leading to implausible scene generation. As an intuitive and feasible solution, the 3D layout allows for precise specification of object locations within the scene. To this end, we present a text-to-scene generation method (namely, Layout2Scene) using additional semantic layout as the prompt to inject precise control of 3D object positions. Specifically, we first introduce a scene hybrid representation to decouple objects and backgrounds, which is initialized via a pre-trained text-to-3D model. Then, we propose a two-stage scheme to optimize the geometry and appearance of the initialized scene separately. To fully leverage 2D diffusion priors in geometry and appearance generation, we introduce a semantic-guided geometry diffusion model and a semantic-geometry guided diffusion model which are finetuned on a scene dataset. Extensive experiments demonstrate that our method can generate more plausible and realistic scenes as compared to state-of-the-art approaches. Furthermore, the generated scene allows for flexible yet precise editing, thereby facilitating multiple downstream applications.",
        "tags": [
            "3D",
            "Diffusion",
            "Text-to-3D"
        ]
    },
    {
        "id": "69",
        "title": "Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation",
        "author": [
            "Dawei Dai",
            "Mingming Jia",
            "Yinxiu Zhou",
            "Hang Xing",
            "Chenghang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02523",
        "abstract": "Facial images have extensive practical applications. Although the current large-scale text-image diffusion models exhibit strong generation capabilities, it is challenging to generate the desired facial images using only text prompt. Image prompts are a logical choice. However, current methods of this type generally focus on general domain. In this paper, we aim to optimize image makeup techniques to generate the desired facial images. Specifically, (1) we built a dataset of 4 million high-quality face image-text pairs (FaceCaptionHQ-4M) based on LAION-Face to train our Face-MakeUp model; (2) to maintain consistency with the reference facial image, we extract/learn multi-scale content features and pose features for the facial image, integrating these into the diffusion model to enhance the preservation of facial identity features for diffusion models. Validation on two face-related test datasets demonstrates that our Face-MakeUp can achieve the best comprehensive http://performance.All codes are available at:https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "70",
        "title": "Vision-Driven Prompt Optimization for Large Language Models in Multimodal Generative Tasks",
        "author": [
            "Leo Franklin",
            "Apiradee Boonmee",
            "Kritsada Wongsuwan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02527",
        "abstract": "Vision generation remains a challenging frontier in artificial intelligence, requiring seamless integration of visual understanding and generative capabilities. In this paper, we propose a novel framework, Vision-Driven Prompt Optimization (VDPO), that leverages Large Language Models (LLMs) to dynamically generate textual prompts from visual inputs, guiding high-fidelity image synthesis. VDPO combines a visual embedding prompt tuner, a textual instruction generator, and a vision generation module to achieve state-of-the-art performance in diverse vision generation tasks. Extensive experiments on benchmarks such as COCO and Sketchy demonstrate that VDPO consistently outperforms existing methods, achieving significant improvements in FID, LPIPS, and BLEU/CIDEr scores. Additional analyses reveal the scalability, robustness, and generalization capabilities of VDPO, making it a versatile solution for in-domain and out-of-domain tasks. Human evaluations further validate the practical superiority of VDPO in generating visually appealing and semantically coherent outputs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "71",
        "title": "Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI",
        "author": [
            "Ljubisa Bojic",
            "Dylan Seychell",
            "Milan Cabarkapa"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02531",
        "abstract": "With the expansion of neural networks, such as large language models, humanity is exponentially heading towards superintelligence. As various AI systems are increasingly integrated into the fabric of societies-through recommending values, devising creative solutions, and making decisions-it becomes critical to assess how these AI systems impact humans in the long run. This research aims to contribute towards establishing a benchmark for evaluating the sentiment of various Large Language Models in socially importan issues. The methodology adopted was a Likert scale survey. Seven LLMs, including GPT-4 and Bard, were analyzed and compared against sentiment data from three independent human sample populations. Temporal variations in sentiment were also evaluated over three consecutive days. The results highlighted a diversity in sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4 recorded the most positive sentiment score towards AGI, whereas Bard was leaning towards the neutral sentiment. The human samples, contrastingly, showed a lower average sentiment of 2.97. The temporal comparison revealed differences in sentiment evolution between LLMs in three days, ranging from 1.03% to 8.21%. The study's analysis outlines the prospect of potential conflicts of interest and bias possibilities in LLMs' sentiment formation. Results indicate that LLMs, akin to human cognitive processes, could potentially develop unique sentiments and subtly influence societies' perceptions towards various opinions formed within the LLMs.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "72",
        "title": "Evaluating Large Language Models Against Human Annotators in Latent Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm",
        "author": [
            "Ljubisa Bojic",
            "Olga Zagovora",
            "Asta Zelenkauskaite",
            "Vuk Vukovic",
            "Milan Cabarkapa",
            "Selma Veseljeviƒá Jerkovic",
            "Ana Jovanƒçevic"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02532",
        "abstract": "In the era of rapid digital communication, vast amounts of textual data are generated daily, demanding efficient methods for latent content analysis to extract meaningful insights. Large Language Models (LLMs) offer potential for automating this process, yet comprehensive assessments comparing their performance to human annotators across multiple dimensions are lacking. This study evaluates the reliability, consistency, and quality of seven state-of-the-art LLMs, including variants of OpenAI's GPT-4, Gemini, Llama, and Mixtral, relative to human annotators in analyzing sentiment, political leaning, emotional intensity, and sarcasm detection. A total of 33 human annotators and eight LLM variants assessed 100 curated textual items, generating 3,300 human and 19,200 LLM annotations, with LLMs evaluated across three time points to examine temporal consistency. Inter-rater reliability was measured using Krippendorff's alpha, and intra-class correlation coefficients assessed consistency over time. The results reveal that both humans and LLMs exhibit high reliability in sentiment analysis and political leaning assessments, with LLMs demonstrating higher internal consistency than humans. In emotional intensity, LLMs displayed higher agreement compared to humans, though humans rated emotional intensity significantly higher. Both groups struggled with sarcasm detection, evidenced by low agreement. LLMs showed excellent temporal consistency across all dimensions, indicating stable performance over time. This research concludes that LLMs, especially GPT-4, can effectively replicate human analysis in sentiment and political leaning, although human expertise remains essential for emotional intensity interpretation. The findings demonstrate the potential of LLMs for consistent and high-quality performance in certain areas of latent content analysis.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "73",
        "title": "A completely uniform transformer for parity",
        "author": [
            "Alexander Kozachinskiy",
            "Tomasz Steifer"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02535",
        "abstract": "We construct a 3-layer constant-dimension transformer, recognizing the parity language, where neither parameter matrices nor the positional encoding depend on the input length. This improves upon a construction of Chiang and Cholak who use a positional encoding, depending on the input length (but their construction has 2 layers).",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "74",
        "title": "Multi-LLM Collaborative Caption Generation in Scientific Documents",
        "author": [
            "Jaeyoung Kim",
            "Jongho Lee",
            "Hong-Jun Choi",
            "Ting-Yao Hsu",
            "Chieh-Yang Huang",
            "Sungchul Kim",
            "Ryan Rossi",
            "Tong Yu",
            "Clyde Lee Giles",
            "Ting-Hao 'Kenneth' Huang",
            "Sungchul Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02552",
        "abstract": "Scientific figure captioning is a complex task that requires generating contextually appropriate descriptions of visual content. However, existing methods often fall short by utilizing incomplete information, treating the task solely as either an image-to-text or text summarization problem. This limitation hinders the generation of high-quality captions that fully capture the necessary details. Moreover, existing data sourced from arXiv papers contain low-quality captions, posing significant challenges for training large language models (LLMs). In this paper, we introduce a framework called Multi-LLM Collaborative Figure Caption Generation (MLBCAP) to address these challenges by leveraging specialized LLMs for distinct sub-tasks. Our approach unfolds in three key modules: (Quality Assessment) We utilize multimodal LLMs to assess the quality of training data, enabling the filtration of low-quality captions. (Diverse Caption Generation) We then employ a strategy of fine-tuning/prompting multiple LLMs on the captioning task to generate candidate captions. (Judgment) Lastly, we prompt a prominent LLM to select the highest quality caption from the candidates, followed by refining any remaining inaccuracies. Human evaluations demonstrate that informative captions produced by our approach rank better than human-written captions, highlighting its effectiveness. Our code is available at https://github.com/teamreboott/MLBCAP",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "75",
        "title": "LeetDecoding: A PyTorch Library for Exponentially Decaying Causal Linear Attention with CUDA Implementations",
        "author": [
            "Jiaping Wang",
            "Simiao Zhang",
            "Qiao-Chu He",
            "Yifan Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02573",
        "abstract": "The machine learning and data science community has made significant while dispersive progress in accelerating transformer-based large language models (LLMs), and one promising approach is to replace the original causal attention in a generative pre-trained transformer (GPT) with \\emph{exponentially decaying causal linear attention}. In this paper, we present LeetDecoding, which is the first Python package that provides a large set of computation routines for this fundamental operator. The launch of LeetDecoding was motivated by the current lack of (1) clear understanding of the complexity regarding this operator, (2) a comprehensive collection of existing computation methods (usually spread in seemingly unrelated fields), and (3) CUDA implementations for fast inference on GPU. LeetDecoding's design is easy to integrate with existing linear-attention LLMs, and allows for researchers to benchmark and evaluate new computation methods for exponentially decaying causal linear attention. The usage of LeetDecoding does not require any knowledge of GPU programming and the underlying complexity analysis, intentionally making LeetDecoding accessible to LLM practitioners. The source code of LeetDecoding is provided at \\href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{this GitHub repository}, and users can simply install LeetDecoding by the command \\texttt{pip install leet-decoding}.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "76",
        "title": "DepthMaster: Taming Diffusion Models for Monocular Depth Estimation",
        "author": [
            "Ziyang Song",
            "Zerong Wang",
            "Bo Li",
            "Hao Zhang",
            "Ruijie Zhu",
            "Li Liu",
            "Peng-Tao Jiang",
            "Tianzhu Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02576",
        "abstract": "Monocular depth estimation within the diffusion-denoising paradigm demonstrates impressive generalization ability but suffers from low inference speed. Recent methods adopt a single-step deterministic paradigm to improve inference efficiency while maintaining comparable performance. However, they overlook the gap between generative and discriminative features, leading to suboptimal results. In this work, we propose DepthMaster, a single-step diffusion model designed to adapt generative features for the discriminative depth estimation task. First, to mitigate overfitting to texture details introduced by generative features, we propose a Feature Alignment module, which incorporates high-quality semantic features to enhance the denoising network's representation capability. Second, to address the lack of fine-grained details in the single-step deterministic framework, we propose a Fourier Enhancement module to adaptively balance low-frequency structure and high-frequency details. We adopt a two-stage training strategy to fully leverage the potential of the two modules. In the first stage, we focus on learning the global scene structure with the Feature Alignment module, while in the second stage, we exploit the Fourier Enhancement module to improve the visual quality. Through these efforts, our model achieves state-of-the-art performance in terms of generalization and detail preservation, outperforming other diffusion-based methods across various datasets. Our project page can be found at https://indu1ge.github.io/DepthMaster_page.",
        "tags": [
            "Depth Estimation",
            "Diffusion"
        ]
    },
    {
        "id": "77",
        "title": "LP-ICP: General Localizability-Aware Point Cloud Registration for Robust Localization in Extreme Unstructured Environments",
        "author": [
            "Haosong Yue",
            "Qingyuan Xu",
            "Fei Chen",
            "Jia Pan",
            "Weihai Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02580",
        "abstract": "The Iterative Closest Point (ICP) algorithm is a crucial component of LiDAR-based SLAM algorithms. However, its performance can be negatively affected in unstructured environments that lack features and geometric structures, leading to low accuracy and poor robustness in localization and mapping. It is known that degeneracy caused by the lack of geometric constraints can lead to errors in 6-DOF pose estimation along ill-conditioned directions. Therefore, there is a need for a broader and more fine-grained degeneracy detection and handling method. This paper proposes a new point cloud registration framework, LP-ICP, that combines point-to-line and point-to-plane distance metrics in the ICP algorithm, with localizability detection and handling. LP-ICP consists of a localizability detection module and an optimization module. The localizability detection module performs localizability analysis by utilizing the correspondences between edge points (with low local smoothness) to lines and planar points (with high local smoothness) to planes between the scan and the map. The localizability contribution of individual correspondence constraints can be applied to a broader range. The optimization module adds additional soft and hard constraints to the optimization equations based on the localizability category. This allows the pose to be constrained along ill-conditioned directions, with updates either tending towards the constraint value or leaving the initial estimate unchanged. This improves accuracy and reduces fluctuations. The proposed method is extensively evaluated through experiments on both simulation and real-world datasets, demonstrating higher or comparable accuracy than the state-of-the-art methods. The dataset and code of this paper will also be open-sourced at https://github.com/xuqingyuan2000/LP-ICP.",
        "tags": [
            "Detection",
            "Pose Estimation",
            "SLAM"
        ]
    },
    {
        "id": "78",
        "title": "Empowering Bengali Education with AI: Solving Bengali Math Word Problems through Transformer Models",
        "author": [
            "Jalisha Jashim Era",
            "Bidyarthi Paul",
            "Tahmid Sattar Aothoi",
            "Mirazur Rahman Zim",
            "Faisal Muhammad Shah"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02599",
        "abstract": "Mathematical word problems (MWPs) involve the task of converting textual descriptions into mathematical equations. This poses a significant challenge in natural language processing, particularly for low-resource languages such as Bengali. This paper addresses this challenge by developing an innovative approach to solving Bengali MWPs using transformer-based models, including Basic Transformer, mT5, BanglaT5, and mBART50. To support this effort, the \"PatiGonit\" dataset was introduced, containing 10,000 Bengali math problems, and these models were fine-tuned to translate the word problems into equations accurately. The evaluation revealed that the mT5 model achieved the highest accuracy of 97.30%, demonstrating the effectiveness of transformer models in this domain. This research marks a significant step forward in Bengali natural language processing, offering valuable methodologies and resources for educational AI tools. By improving math education, it also supports the development of advanced problem-solving skills for Bengali-speaking students.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "79",
        "title": "TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms",
        "author": [
            "Jovan Stojkovic",
            "Chaojie Zhang",
            "√ç√±igo Goiri",
            "Esha Choukse",
            "Haoran Qiu",
            "Rodrigo Fonseca",
            "Josep Torrellas",
            "Ricardo Bianchini"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02600",
        "abstract": "The rising demand for generative large language models (LLMs) poses challenges for thermal and power management in cloud datacenters. Traditional techniques often are inadequate for LLM inference due to the fine-grained, millisecond-scale execution phases, each with distinct performance, thermal, and power profiles. Additionally, LLM inference workloads are sensitive to various configuration parameters (e.g., model parallelism, size, and quantization) that involve trade-offs between performance, temperature, power, and output quality. Moreover, clouds often co-locate SaaS and IaaS workloads, each with different levels of visibility and flexibility. We propose TAPAS, a thermal- and power-aware framework designed for LLM inference clusters in the cloud. TAPAS enhances cooling and power oversubscription capabilities, reducing the total cost of ownership (TCO) while effectively handling emergencies (e.g., cooling and power failures). The system leverages historical temperature and power data, along with the adaptability of SaaS workloads, to: (1) efficiently place new GPU workload VMs within cooling and power constraints, (2) route LLM inference requests across SaaS VMs, and (3) reconfigure SaaS VMs to manage load spikes and emergency situations. Our evaluation on a large GPU cluster demonstrates significant reductions in thermal and power throttling events, boosting system efficiency.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "80",
        "title": "LLMs Help Alleviate the Cross-Subject Variability in Brain Signal and Language Alignment",
        "author": [
            "Yifei Liu",
            "Hengwei Ye",
            "Shuhang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02621",
        "abstract": "Decoding human activity from EEG signals has long been a popular research topic. While recent studies have increasingly shifted focus from single-subject to cross-subject analysis, few have explored the model's ability to perform zero-shot predictions on EEG signals from previously unseen subjects. This research aims to investigate whether deep learning methods can capture subject-independent semantic information inherent in human EEG signals. Such insights are crucial for Brain-Computer Interfaces (BCI) because, on one hand, they demonstrate the model's robustness against subject-specific temporal biases, and on the other, they significantly enhance the generalizability of downstream tasks. We employ Large Language Models (LLMs) as denoising agents to extract subject-independent semantic features from noisy EEG signals. Experimental results, including ablation studies, highlight the pivotal role of LLMs in decoding subject-independent semantic information from noisy EEG data. We hope our findings will contribute to advancing BCI research and assist both academia and industry in applying EEG signals to a broader range of applications.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "81",
        "title": "HALO: Hadamard-Assisted Lossless Optimization for Efficient Low-Precision LLM Training and Fine-Tuning",
        "author": [
            "Saleh Ashkboos",
            "Mahdi Nikdan",
            "Soroush Tabesh",
            "Roberto L. Castro",
            "Torsten Hoefler",
            "Dan Alistarh"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02625",
        "abstract": "Quantized training of Large Language Models (LLMs) remains an open challenge, as maintaining accuracy while performing all matrix multiplications in low precision has proven difficult. This is particularly the case when fine-tuning pre-trained models, which often already have large weight and activation outlier values that render quantized optimization difficult. We present HALO, a novel quantization-aware training approach for Transformers that enables accurate and efficient low-precision training by combining 1) strategic placement of Hadamard rotations in both forward and backward passes, to mitigate outliers during the low-precision computation, 2) FSDP integration for low-precision communication, and 3) high-performance kernel support. Our approach ensures that all large matrix multiplications during the forward and backward passes are executed in lower precision. Applied to LLAMA-family models, HALO achieves near-full-precision-equivalent results during fine-tuning on various tasks, while delivering up to 1.31x end-to-end speedup for full fine-tuning on RTX 4090 GPUs. Our method supports both standard and parameter-efficient fine-tuning (PEFT) methods, both backed by efficient kernel implementations. Our results demonstrate the first practical approach to fully quantized LLM fine-tuning that maintains accuracy in FP8 precision, while delivering performance benefits.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "82",
        "title": "Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM Pre-Training Datasets",
        "author": [
            "Mahmoud Jahanshahi",
            "Audris Mockus"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02628",
        "abstract": "A critical part of creating code suggestion systems is the pre-training of Large Language Models on vast amounts of source code and natural language text, often of questionable origin or quality. This may contribute to the presence of bugs and vulnerabilities in code generated by LLMs. While efforts to identify bugs at or after code generation exist, it is preferable to pre-train or fine-tune LLMs on curated, high-quality, and compliant datasets. The need for vast amounts of training data necessitates that such curation be automated, minimizing human intervention.\nWe propose an automated source code autocuration technique that leverages the complete version history of open-source software projects to improve the quality of training data. This approach leverages the version history of all OSS projects to identify training data samples that have been modified or have undergone changes in at least one OSS project, and pinpoint a subset of samples that include fixes for bugs or vulnerabilities. We evaluate this method using The Stack v2 dataset, and find that 17% of the code versions in the dataset have newer versions, with 17% of those representing bug fixes, including 2.36% addressing known CVEs. The deduplicated version of Stack v2 still includes blobs vulnerable to 6,947 known CVEs. Furthermore, 58% of the blobs in the dataset were never modified after creation, suggesting they likely represent software with minimal or no use. Misidentified blob origins present an additional challenge, as they lead to the inclusion of non-permissively licensed code, raising serious compliance concerns.\nBy addressing these issues, the training of new models can avoid perpetuating buggy code patterns or license violations. We expect our results to inspire process improvements for automated data curation, with the potential to enhance the reliability of outputs generated by AI tools.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "83",
        "title": "Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense",
        "author": [
            "Yang Ouyang",
            "Hengrui Gu",
            "Shuhang Lin",
            "Wenyue Hua",
            "Jie Peng",
            "Bhavya Kailkhura",
            "Tianlong Chen",
            "Kaixiong Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02629",
        "abstract": "As large language models (LLMs) are increasingly deployed in diverse applications, including chatbot assistants and code generation, aligning their behavior with safety and ethical standards has become paramount. However, jailbreak attacks, which exploit vulnerabilities to elicit unintended or harmful outputs, threaten LLMs' safety significantly. In this paper, we introduce Layer-AdvPatcher, a novel methodology designed to defend against jailbreak attacks by utilizing an unlearning strategy to patch specific layers within LLMs through self-augmented datasets. Our insight is that certain layer(s), tend to produce affirmative tokens when faced with harmful prompts. By identifying these layers and adversarially exposing them to generate more harmful data, one can understand their inherent and diverse vulnerabilities to attacks. With these exposures, we then \"unlearn\" these issues, reducing the impact of affirmative tokens and hence minimizing jailbreak risks while keeping the model's responses to safe queries intact. We conduct extensive experiments on two models, four benchmark datasets, and multiple state-of-the-art jailbreak benchmarks to demonstrate the efficacy of our approach. Results indicate that our framework reduces the harmfulness and attack success rate of jailbreak attacks without compromising utility for benign queries compared to recent defense methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "84",
        "title": "Representation Learning of Lab Values via Masked AutoEncoder",
        "author": [
            "David Restrepo",
            "Chenwei Wu",
            "Yueran Jia",
            "Jaden K. Sun",
            "Jack Gallifant",
            "Catherine G. Bielick",
            "Yugang Jia",
            "Leo A. Celi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02648",
        "abstract": "Accurate imputation of missing laboratory values in electronic health records (EHRs) is critical to enable robust clinical predictions and reduce biases in AI systems in healthcare. Existing methods, such as variational autoencoders (VAEs) and decision tree-based approaches such as XGBoost, struggle to model the complex temporal and contextual dependencies in EHR data, mainly in underrepresented groups. In this work, we propose Lab-MAE, a novel transformer-based masked autoencoder framework that leverages self-supervised learning for the imputation of continuous sequential lab values. Lab-MAE introduces a structured encoding scheme that jointly models laboratory test values and their corresponding timestamps, enabling explicit capturing temporal dependencies. Empirical evaluation on the MIMIC-IV dataset demonstrates that Lab-MAE significantly outperforms the state-of-the-art baselines such as XGBoost across multiple metrics, including root mean square error (RMSE), R-squared (R2), and Wasserstein distance (WD). Notably, Lab-MAE achieves equitable performance across demographic groups of patients, advancing fairness in clinical predictions. We further investigate the role of follow-up laboratory values as potential shortcut features, revealing Lab-MAE's robustness in scenarios where such data is unavailable. The findings suggest that our transformer-based architecture, adapted to the characteristics of the EHR data, offers a foundation model for more accurate and fair clinical imputation models. In addition, we measure and compare the carbon footprint of Lab-MAE with the baseline XGBoost model, highlighting its environmental requirements.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "85",
        "title": "Tighnari: Multi-modal Plant Species Prediction Based on Hierarchical Cross-Attention Using Graph-Based and Vision Backbone-Extracted Features",
        "author": [
            "Haixu Liu",
            "Penghao Jiang",
            "Zerui Tao",
            "Muyan Wan",
            "Qiuzhuang Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02649",
        "abstract": "Predicting plant species composition in specific spatiotemporal contexts plays an important role in biodiversity management and conservation, as well as in improving species identification tools. Our work utilizes 88,987 plant survey records conducted in specific spatiotemporal contexts across Europe. We also use the corresponding satellite images, time series data, climate time series, and other rasterized environmental data such as land cover, human footprint, bioclimatic, and soil variables as training data to train the model to predict the outcomes of 4,716 plant surveys. We propose a feature construction and result correction method based on the graph structure. Through comparative experiments, we select the best-performing backbone networks for feature extraction in both temporal and image modalities. In this process, we built a backbone network based on the Swin-Transformer Block for extracting temporal Cubes features. We then design a hierarchical cross-attention mechanism capable of robustly fusing features from multiple modalities. During training, we adopt a 10-fold cross-fusion method based on fine-tuning and use a Threshold Top-K method for post-processing. Ablation experiments demonstrate the improvements in model performance brought by our proposed solution pipeline.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "86",
        "title": "Determination of Preferred Fiber Orientation State based on Newton-Raphson Method using Exact Jacobian",
        "author": [
            "Aigbe Awenlimobor",
            "Douglas E. Smith"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02663",
        "abstract": "Fiber orientation is an important descriptor of the intrinsic microstructure of polymer composite materials and the ability to predict the orientation state accurately and efficiently is crucial in evaluating the bulk thermo-mechanical behavior and consequently performance of printed part. Recent macroscopic fiber orientation models have employed the moment-tensor form in representing the fiber orientation state thus requiring some form of closure approximation of a higher order orientation tensor. Currently, different models have been developed to account for the added effect of rotary diffusion due to fiber-fiber and fiber matrix interactions and accurately simulate the experimentally observed slow fiber kinematics in polymer composite processing. Traditionally explicit numerical IVP-ODE transient solvers like the 4th order Runge Kutta method have been used to determine the steady-state fiber orientation. Here we propose a computationally efficient and faster method based on Newton-Raphson iterative technique for determining the preferred orientation state by evaluating the exact derivatives of the moment-tensor evolution equation with respect to the independent components of the orientation tensor. We consider various existing macroscopic-fiber orientation models and different closure approximations to ensure to ensure the robustness and reliability of the method, and we evaluate its performance and stability in determining physical solutions in various complex flow fields. Validation of the obtained exact partial derivatives of the material derivative of the orientation tensor is carried out by benchmarking with results of finite difference techniques.",
        "tags": [
            "Diffusion",
            "ODE"
        ]
    },
    {
        "id": "87",
        "title": "Decoding specialised feature neurons in LLMs with the final projection layer",
        "author": [
            "Harry J Davies"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02688",
        "abstract": "Large Language Models (LLMs) typically have billions of parameters and are thus often difficult to interpret in their operation. Such black-box models can pose a significant risk to safety when trusted to make important decisions. The lack of interpretability of LLMs is more related to their sheer size, rather than the complexity of their individual components. The TARS method for knowledge removal (Davies et al 2024) provides strong evidence for the hypothesis that that linear layer weights which act directly on the residual stream may have high correlation with different concepts encoded in the residual stream. Building upon this, we attempt to decode neuron weights directly into token probabilities through the final projection layer of the model (the LM-head). Firstly, we show that with Llama 3.1 8B we can utilise the LM-head to decode specialised feature neurons that respond strongly to certain concepts, with examples such as \"dog\" and \"California\". This is then confirmed by demonstrating that these neurons can be clamped to affect the probability of the concept in the output. This extends to the fine-tuned assistant Llama 3.1 8B instruct model, where we find that over 75% of neurons in the up-projection layers have the same top associated token compared to the pretrained model. Finally, we demonstrate that clamping the \"dog\" neuron leads the instruct model to always discuss dogs when asked about its favourite animal. Through our method, it is possible to map the entirety of Llama 3.1 8B's up-projection neurons in less than 15 minutes with no parallelization.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "88",
        "title": "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking",
        "author": [
            "Weikang Bian",
            "Zhaoyang Huang",
            "Xiaoyu Shi",
            "Yijin Li",
            "Fu-Yun Wang",
            "Hongsheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02690",
        "abstract": "4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.",
        "tags": [
            "3D",
            "DiT",
            "Diffusion",
            "Diffusion Transformer",
            "Gaussian Splatting",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "89",
        "title": "EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models",
        "author": [
            "Andr√©s Villa",
            "Juan Le√≥n Alc√°zar",
            "Motasem Alfarra",
            "Vladimir Araujo",
            "Alvaro Soto",
            "Bernard Ghanem"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02699",
        "abstract": "Large language models and vision transformers have demonstrated impressive zero-shot capabilities, enabling significant transferability in downstream tasks. The fusion of these models has resulted in multi-modal architectures with enhanced instructional capabilities. Despite incorporating vast image and language pre-training, these multi-modal architectures often generate responses that deviate from the ground truth in the image data. These failure cases are known as hallucinations. Current methods for mitigating hallucinations generally focus on regularizing the language component, improving the fusion module, or ensembling multiple visual encoders to improve visual representation. In this paper, we address the hallucination issue by directly enhancing the capabilities of the visual component. Our approach, named EAGLE, is fully agnostic to the LLM or fusion module and works as a post-pretraining approach that improves the grounding and language alignment of the visual encoder. We show that a straightforward reformulation of the original contrastive pre-training task results in an improved visual encoder that can be incorporated into the instructional multi-modal architecture without additional instructional training. As a result, EAGLE achieves a significant reduction in hallucinations across multiple challenging benchmarks and tasks.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "90",
        "title": "QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted Question Matching for Enhanced QA Performance",
        "author": [
            "Binita Saha",
            "Utsha Saha",
            "Muhammad Zubair Malik"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02702",
        "abstract": "This work presents a novel architecture for building Retrieval-Augmented Generation (RAG) systems to improve Question Answering (QA) tasks from a target corpus. Large Language Models (LLMs) have revolutionized the analyzing and generation of human-like text. These models rely on pre-trained data and lack real-time updates unless integrated with live data tools. RAG enhances LLMs by integrating online resources and databases to generate contextually appropriate responses. However, traditional RAG still encounters challenges like information dilution and hallucinations when handling vast amounts of data. Our approach addresses these challenges by converting corpora into a domain-specific dataset and RAG architecture is constructed to generate responses from the target document. We introduce QuIM-RAG (Question-to-question Inverted Index Matching), a novel approach for the retrieval mechanism in our system. This strategy generates potential questions from document chunks and matches these with user queries to identify the most relevant text chunks for generating accurate answers. We have implemented our RAG system on top of the open-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on Hugging Face. We constructed a custom corpus of 500+ pages from a high-traffic website accessed thousands of times daily for answering complex questions, along with manually prepared ground truth QA for evaluation. We compared our approach with traditional RAG models using BERT-Score and RAGAS, state-of-the-art metrics for evaluating LLM applications. Our evaluation demonstrates that our approach outperforms traditional RAG architectures on both metrics.",
        "tags": [
            "BERT",
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "91",
        "title": "Multilevel Semantic-Aware Model for AI-Generated Video Quality Assessment",
        "author": [
            "Jiaze Li",
            "Haoran Xu",
            "Shiding Zhu",
            "Junwei He",
            "Haozhao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02706",
        "abstract": "The rapid development of diffusion models has greatly advanced AI-generated videos in terms of length and consistency recently, yet assessing AI-generated videos still remains challenging. Previous approaches have often focused on User-Generated Content(UGC), but few have targeted AI-Generated Video Quality Assessment methods. In this work, we introduce MSA-VQA, a Multilevel Semantic-Aware Model for AI-Generated Video Quality Assessment, which leverages CLIP-based semantic supervision and cross-attention mechanisms. Our hierarchical framework analyzes video content at three levels: frame, segment, and video. We propose a Prompt Semantic Supervision Module using text encoder of CLIP to ensure semantic consistency between videos and conditional prompts. Additionally, we propose the Semantic Mutation-aware Module to capture subtle variations between frames. Extensive experiments demonstrate our method achieves state-of-the-art results.",
        "tags": [
            "CLIP",
            "Diffusion"
        ]
    },
    {
        "id": "92",
        "title": "KG-CF: Knowledge Graph Completion with Context Filtering under the Guidance of Large Language Models",
        "author": [
            "Zaiyi Zheng",
            "Yushun Dong",
            "Song Wang",
            "Haochen Liu",
            "Qi Wang",
            "Jundong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02711",
        "abstract": "Large Language Models (LLMs) have shown impressive performance in various tasks, including knowledge graph completion (KGC). However, current studies mostly apply LLMs to classification tasks, like identifying missing triplets, rather than ranking-based tasks, where the model ranks candidate entities based on plausibility. This focus limits the practical use of LLMs in KGC, as real-world applications prioritize highly plausible triplets. Additionally, while graph paths can help infer the existence of missing triplets and improve completion accuracy, they often contain redundant information. To address these issues, we propose KG-CF, a framework tailored for ranking-based KGC tasks. KG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts, achieving superior results on real-world datasets. The code and datasets are available at \\url{https://anonymous.4open.science/r/KG-CF}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "93",
        "title": "Artificial Intelligence in Creative Industries: Advances Prior to 2025",
        "author": [
            "Nantheera Anantrasirichai",
            "Fan Zhang",
            "David Bull"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02725",
        "abstract": "The rapid advancements in artificial intelligence (AI), particularly in generative AI and large language models (LLMs), have profoundly impacted the creative industries by enabling innovative content creation, enhancing workflows, and democratizing access to creative tools. This paper explores the significant technological shifts since our previous review in 2022, highlighting how these developments have expanded creative opportunities and efficiency. These technological advancements have enhanced the capabilities of text-to-image, text-to-video, and multimodal generation technologies. In particular, key breakthroughs in LLMs have established new benchmarks in conversational AI, while advancements in image generators have revolutionized content creation. We also discuss AI integration into post-production workflows, which has significantly accelerated and refined traditional processes. Despite these innovations, challenges remain, particularly for the media industry, due to the demands on communication traffic from creative content. We therefore include data compression and quality assessment in this paper. Furthermore, we highlight the trend toward unified AI frameworks capable of addressing multiple creative tasks and underscore the importance of human oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's future potential in the creative sector, stressing the need to navigate emerging challenges to maximize its benefits while addressing associated risks.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Text-to-Image",
            "Text-to-Video"
        ]
    },
    {
        "id": "94",
        "title": "Kolmogorov equations for evaluating the boundary hitting of degenerate diffusion with unsteady drift",
        "author": [
            "Hidekazu Yoshioka"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02729",
        "abstract": "Jacobi diffusion is a representative diffusion process whose solution is bounded in a domain under certain conditions on its drift and diffusion coefficients. However, the process without such conditions has been far less investigated. We explore a Jacobi diffusion whose drift coefficient is modulated by another process, which causes the process to hit the boundary of a domain in finite time. The Kolmogorov equation (a degenerate elliptic partial differential equation) for evaluating the boundary hitting of the proposed Jacobi diffusion is then presented and mathematically analyzed. We also investigate a related mean field game arising in tourism management, where the drift depends on the index for sensor boundary hitting, thereby confining the process in a domain with a higher probability. In this case, the Kolmogorov equation becomes nonlinear. We propose a finite difference method applicable to both the linear and nonlinear Kolmogorov equations that yields a unique numerical solution due to discrete ellipticity. The accuracy of the finite difference method critically depends on the regularity of the boundary condition, and the use of a high-order discretization method is not always effective. Finally, we computationally investigate the mean field effect.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "95",
        "title": "AFed: Algorithmic Fair Federated Learning",
        "author": [
            "Huiqiang Chen",
            "Tianqing Zhu",
            "Wanlei Zhou",
            "Wei Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02732",
        "abstract": "Federated Learning (FL) has gained significant attention as it facilitates collaborative machine learning among multiple clients without centralizing their data on a server. FL ensures the privacy of participating clients by locally storing their data, which creates new challenges in fairness. Traditional debiasing methods assume centralized access to sensitive information, rendering them impractical for the FL setting. Additionally, FL is more susceptible to fairness issues than centralized machine learning due to the diverse client data sources that may be associated with group information. Therefore, training a fair model in FL without access to client local data is important and challenging. This paper presents AFed, a straightforward yet effective framework for promoting group fairness in FL. The core idea is to circumvent restricted data access by learning the global data distribution. This paper proposes two approaches: AFed-G, which uses a conditional generator trained on the server side, and AFed-GAN, which improves upon AFed-G by training a conditional GAN on the client side. We augment the client data with the generated samples to help remove bias. Our theoretical analysis justifies the proposed methods, and empirical results on multiple real-world datasets demonstrate a substantial improvement in AFed over several baselines.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "96",
        "title": "Sequence Complementor: Complementing Transformers For Time Series Forecasting with Learnable Sequences",
        "author": [
            "Xiwen Chen",
            "Peijie Qiu",
            "Wenhui Zhu",
            "Huayu Li",
            "Hao Wang",
            "Aristeidis Sotiras",
            "Yalin Wang",
            "Abolfazl Razi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02735",
        "abstract": "Since its introduction, the transformer has shifted the development trajectory away from traditional models (e.g., RNN, MLP) in time series forecasting, which is attributed to its ability to capture global dependencies within temporal tokens. Follow-up studies have largely involved altering the tokenization and self-attention modules to better adapt Transformers for addressing special challenges like non-stationarity, channel-wise dependency, and variable correlation in time series. However, we found that the expressive capability of sequence representation is a key factor influencing Transformer performance in time forecasting after investigating several representative methods, where there is an almost linear relationship between sequence representation entropy and mean square error, with more diverse representations performing better. In this paper, we propose a novel attention mechanism with Sequence Complementors and prove feasible from an information theory perspective, where these learnable sequences are able to provide complementary information beyond current input to feed attention. We further enhance the Sequence Complementors via a diversification loss that is theoretically covered. The empirical evaluation of both long-term and short-term forecasting has confirmed its superiority over the recent state-of-the-art methods.",
        "tags": [
            "RNN",
            "Transformer"
        ]
    },
    {
        "id": "97",
        "title": "Brick-Diffusion: Generating Long Videos with Brick-to-Wall Denoising",
        "author": [
            "Yunlong Yuan",
            "Yuanfan Guo",
            "Chunwei Wang",
            "Hang Xu",
            "Li Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02741",
        "abstract": "Recent advances in diffusion models have greatly improved text-driven video generation. However, training models for long video generation demands significant computational power and extensive data, leading most video diffusion models to be limited to a small number of frames. Existing training-free methods that attempt to generate long videos using pre-trained short video diffusion models often struggle with issues such as insufficient motion dynamics and degraded video fidelity. In this paper, we present Brick-Diffusion, a novel, training-free approach capable of generating long videos of arbitrary length. Our method introduces a brick-to-wall denoising strategy, where the latent is denoised in segments, with a stride applied in subsequent iterations. This process mimics the construction of a staggered brick wall, where each brick represents a denoised segment, enabling communication between frames and improving overall video quality. Through quantitative and qualitative evaluations, we demonstrate that Brick-Diffusion outperforms existing baseline methods in generating high-fidelity videos.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "98",
        "title": "Enhancing Robot Route Optimization in Smart Logistics with Transformer and GNN Integration",
        "author": [
            "Hao Luo",
            "Jianjun Wei",
            "Shuchen Zhao",
            "Ankai Liang",
            "Zhongjin Xu",
            "Ruxue Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02749",
        "abstract": "This research delves into advanced route optimization for robots in smart logistics, leveraging a fusion of Transformer architectures, Graph Neural Networks (GNNs), and Generative Adversarial Networks (GANs). The approach utilizes a graph-based representation encompassing geographical data, cargo allocation, and robot dynamics, addressing both spatial and resource limitations to refine route efficiency. Through extensive testing with authentic logistics datasets, the proposed method achieves notable improvements, including a 15% reduction in travel distance, a 20% boost in time efficiency, and a 10% decrease in energy consumption. These findings highlight the algorithm's effectiveness, promoting enhanced performance in intelligent logistics operations.",
        "tags": [
            "Robot",
            "Transformer"
        ]
    },
    {
        "id": "99",
        "title": "CHAT: Beyond Contrastive Graph Transformer for Link Prediction in Heterogeneous Networks",
        "author": [
            "Shengming Zhang",
            "Le Zhang",
            "Jingbo Zhou",
            "Hui Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02760",
        "abstract": "Link prediction in heterogeneous networks is crucial for understanding the intricacies of network structures and forecasting their future developments. Traditional methodologies often face significant obstacles, including over-smoothing-wherein the excessive aggregation of node features leads to the loss of critical structural details-and a dependency on human-defined meta-paths, which necessitate extensive domain knowledge and can be inherently restrictive. These limitations hinder the effective prediction and analysis of complex heterogeneous networks. In response to these challenges, we propose the Contrastive Heterogeneous grAph Transformer (CHAT). CHAT introduces a novel sampling-based graph transformer technique that selectively retains nodes of interest, thereby obviating the need for predefined meta-paths. The method employs an innovative connection-aware transformer to encode node sequences and their interconnections with high fidelity, guided by a dual-faceted loss function specifically designed for heterogeneous network link prediction. Additionally, CHAT incorporates an ensemble link predictor that synthesizes multiple samplings to achieve enhanced prediction accuracy. We conducted comprehensive evaluations of CHAT using three distinct drug-target interaction (DTI) datasets. The empirical results underscore CHAT's superior performance, outperforming both general-task approaches and models specialized in DTI prediction. These findings substantiate the efficacy of CHAT in addressing the complex problem of link prediction in heterogeneous networks.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "100",
        "title": "Scaled-cPIKANs: Domain Scaling in Chebyshev-based Physics-informed Kolmogorov-Arnold Networks",
        "author": [
            "Farinaz Mostajeran",
            "Salah A Faroughi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02762",
        "abstract": "Partial Differential Equations (PDEs) are integral to modeling many scientific and engineering problems. Physics-informed Neural Networks (PINNs) have emerged as promising tools for solving PDEs by embedding governing equations into the neural network loss function. However, when dealing with PDEs characterized by strong oscillatory dynamics over large computational domains, PINNs based on Multilayer Perceptrons (MLPs) often exhibit poor convergence and reduced accuracy. To address these challenges, this paper introduces Scaled-cPIKAN, a physics-informed architecture rooted in Kolmogorov-Arnold Networks (KANs). Scaled-cPIKAN integrates Chebyshev polynomial representations with a domain scaling approach that transforms spatial variables in PDEs into the standardized domain \\([-1,1]^d\\), as intrinsically required by Chebyshev polynomials. By combining the flexibility of Chebyshev-based KANs (cKANs) with the physics-driven principles of PINNs, and the spatial domain transformation, Scaled-cPIKAN enables efficient representation of oscillatory dynamics across extended spatial domains while improving computational performance. We demonstrate Scaled-cPIKAN efficacy using four benchmark problems: the diffusion equation, the Helmholtz equation, the Allen-Cahn equation, as well as both forward and inverse formulations of the reaction-diffusion equation (with and without noisy data). Our results show that Scaled-cPIKAN significantly outperforms existing methods in all test cases. In particular, it achieves several orders of magnitude higher accuracy and faster convergence rate, making it a highly efficient tool for approximating PDE solutions that feature oscillatory behavior over large spatial domains.",
        "tags": [
            "Diffusion",
            "Kolmogorov-Arnold Networks"
        ]
    },
    {
        "id": "101",
        "title": "Visual Large Language Models for Generalized and Specialized Applications",
        "author": [
            "Yifan Li",
            "Zhixin Lai",
            "Wentao Bao",
            "Zhen Tan",
            "Anh Dao",
            "Kewei Sui",
            "Jiayi Shen",
            "Dong Liu",
            "Huan Liu",
            "Yu Kong"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02765",
        "abstract": "Visual-language models (VLM) have emerged as a powerful tool for learning a unified embedding space for vision and language. Inspired by large language models, which have demonstrated strong reasoning and multi-task capabilities, visual large language models (VLLMs) are gaining increasing attention for building general-purpose VLMs. Despite the significant progress made in VLLMs, the related literature remains limited, particularly from a comprehensive application perspective, encompassing generalized and specialized applications across vision (image, video, depth), action, and language modalities. In this survey, we focus on the diverse applications of VLLMs, examining their using scenarios, identifying ethics consideration and challenges, and discussing future directions for their development. By synthesizing these contents, we aim to provide a comprehensive guide that will pave the way for future innovations and broader applications of VLLMs. The paper list repository is available: https://github.com/JackYFL/awesome-VLLMs.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "102",
        "title": "GeAR: Generation Augmented Retrieval",
        "author": [
            "Haoyu Liu",
            "Shaohan Huang",
            "Jianfeng Liu",
            "Yuefeng Zhan",
            "Hao Sun",
            "Weiwei Deng",
            "Feng Sun",
            "Furu Wei",
            "Qi Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02772",
        "abstract": "Document retrieval techniques form the foundation for the development of large-scale information systems. The prevailing methodology is to construct a bi-encoder and compute the semantic similarity. However, such scalar similarity is difficult to reflect enough information and impedes our comprehension of the retrieval results. In addition, this computational process mainly emphasizes the global semantics and ignores the fine-grained semantic relationship between the query and the complex text in the document. In this paper, we propose a new method called $\\textbf{Ge}$neration $\\textbf{A}$ugmented $\\textbf{R}$etrieval ($\\textbf{GeAR}$) that incorporates well-designed fusion and decoding modules. This enables GeAR to generate the relevant text from documents based on the fused representation of the query and the document, thus learning to \"focus on\" the fine-grained information. Also when used as a retriever, GeAR does not add any computational burden over bi-encoders. To support the training of the new framework, we have introduced a pipeline to efficiently synthesize high-quality data by utilizing large language models. GeAR exhibits competitive retrieval and localization performance across diverse scenarios and datasets. Moreover, the qualitative analysis and the results generated by GeAR provide novel insights into the interpretation of retrieval results. The code, data, and models will be released after completing technical review to facilitate future research.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "103",
        "title": "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion",
        "author": [
            "Zhaoyi Yan",
            "Zhijie Sang",
            "Yiming Zhang",
            "Yuhao Fu",
            "Baoyi He",
            "Qi Zhou",
            "Yining Di",
            "Chunlin Ji",
            "Shengyu Zhang",
            "Fei Wu",
            "Hongxia Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02795",
        "abstract": "Large Language Models (LLMs) have demonstrated strong performance across various reasoning tasks, yet building a single model that consistently excels across all domains remains challenging. This paper addresses this problem by exploring strategies to integrate multiple domain-specialized models into an efficient pivot http://model.We propose two fusion strategies to combine the strengths of multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially distills each source model into the pivot model, followed by a weight merging step to integrate the distilled models into the final model. This method achieves strong performance but requires substantial training effort; and (2) a unified fusion approach that aggregates all source models' outputs http://simultaneously.To improve the fusion process, we introduce a novel Rate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K ratios during parameter merging for enhanced flexibility and http://stability.Furthermore, we propose an uncertainty-based weighting method for the unified approach, which dynamically balances the contributions of source models and outperforms other logits/distribution ensemble http://methods.We achieved accuracy improvements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval tasks, respectively.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "104",
        "title": "AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal Conditions and Larger Scene",
        "author": [
            "Chaoran Feng",
            "Wangbo Yu",
            "Xinhua Cheng",
            "Zhenyu Tang",
            "Junwu Zhang",
            "Li Yuan",
            "Yonghong Tian"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02807",
        "abstract": "Compared to frame-based methods, computational neuromorphic imaging using event cameras offers significant advantages, such as minimal motion blur, enhanced temporal resolution, and high dynamic range. The multi-view consistency of Neural Radiance Fields combined with the unique benefits of event cameras, has spurred recent research into reconstructing NeRF from data captured by moving event cameras. While showing impressive performance, existing methods rely on ideal conditions with the availability of uniform and high-quality event sequences and accurate camera poses, and mainly focus on the object level reconstruction, thus limiting their practical applications. In this work, we propose AE-NeRF to address the challenges of learning event-based NeRF from non-ideal conditions, including non-uniform event sequences, noisy poses, and various scales of scenes. Our method exploits the density of event streams and jointly learn a pose correction module with an event-based NeRF (e-NeRF) framework for robust 3D reconstruction from inaccurate camera poses. To generalize to larger scenes, we propose hierarchical event distillation with a proposal e-NeRF network and a vanilla e-NeRF network to resample and refine the reconstruction process. We further propose an event reconstruction loss and a temporal loss to improve the view consistency of the reconstructed scene. We established a comprehensive benchmark that includes large-scale scenes to simulate practical non-ideal conditions, incorporating both synthetic and challenging real-world event datasets. The experimental results show that our method achieves a new state-of-the-art in event-based 3D reconstruction.",
        "tags": [
            "3D",
            "NeRF"
        ]
    },
    {
        "id": "105",
        "title": "DarkFarseer: Inductive Spatio-temporal Kriging via Hidden Style Enhancement and Sparsity-Noise Mitigation",
        "author": [
            "Zhuoxuan Liang",
            "Wei Li",
            "Dalin Zhang",
            "Yidan Chen",
            "Zhihong Wang",
            "Xiangping Zheng",
            "Moustafa Youssef"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02808",
        "abstract": "With the rapid growth of the Internet of Things and Cyber-Physical Systems, widespread sensor deployment has become essential. However, the high costs of building sensor networks limit their scale and coverage, making fine-grained deployment challenging. Inductive Spatio-Temporal Kriging (ISK) addresses this issue by introducing virtual sensors. Based on graph neural networks (GNNs) extracting the relationships between physical and virtual sensors, ISK can infer the measurements of virtual sensors from physical sensors. However, current ISK methods rely on conventional message-passing mechanisms and network architectures, without effectively extracting spatio-temporal features of physical sensors and focusing on representing virtual sensors. Additionally, existing graph construction methods face issues of sparse and noisy connections, destroying ISK performance. To address these issues, we propose DarkFarseer, a novel ISK framework with three key components. First, we propose the Neighbor Hidden Style Enhancement module with a style transfer strategy to enhance the representation of virtual nodes in a temporal-then-spatial manner to better extract the spatial relationships between physical and virtual nodes. Second, we propose Virtual-Component Contrastive Learning, which aims to enrich the node representation by establishing the association between the patterns of virtual nodes and the regional patterns within graph components. Lastly, we design a Similarity-Based Graph Denoising Strategy, which reduces the connectivity strength of noisy connections around virtual nodes and their neighbors based on their temporal information and regional spatial patterns. Extensive experiments demonstrate that DarkFarseer significantly outperforms existing ISK methods.",
        "tags": [
            "Style Transfer"
        ]
    },
    {
        "id": "106",
        "title": "First-place Solution for Streetscape Shop Sign Recognition Competition",
        "author": [
            "Bin Wang",
            "Li Jing"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02811",
        "abstract": "Text recognition technology applied to street-view storefront signs is increasingly utilized across various practical domains, including map navigation, smart city planning analysis, and business value assessments in commercial districts. This technology holds significant research and commercial potential. Nevertheless, it faces numerous challenges. Street view images often contain signboards with complex designs and diverse text styles, complicating the text recognition process. A notable advancement in this field was introduced by our team in a recent competition. We developed a novel multistage approach that integrates multimodal feature fusion, extensive self-supervised training, and a Transformer-based large model. Furthermore, innovative techniques such as BoxDQN, which relies on reinforcement learning, and text rectification methods were employed, leading to impressive outcomes. Comprehensive experiments have validated the effectiveness of these methods, showcasing our potential to enhance text recognition capabilities in complex urban environments.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "107",
        "title": "InpDiffusion: Image Inpainting Localization via Conditional Diffusion Models",
        "author": [
            "Kai Wang",
            "Shaozhang Niu",
            "Qixian Hao",
            "Jiwei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02816",
        "abstract": "As artificial intelligence advances rapidly, particularly with the advent of GANs and diffusion models, the accuracy of Image Inpainting Localization (IIL) has become increasingly challenging. Current IIL methods face two main challenges: a tendency towards overconfidence, leading to incorrect predictions; and difficulty in detecting subtle tampering boundaries in inpainted images. In response, we propose a new paradigm that treats IIL as a conditional mask generation task utilizing diffusion models. Our method, InpDiffusion, utilizes the denoising process enhanced by the integration of image semantic conditions to progressively refine predictions. During denoising, we employ edge conditions and introduce a novel edge supervision strategy to enhance the model's perception of edge details in inpainted objects. Balancing the diffusion model's stochastic sampling with edge supervision of tampered image regions mitigates the risk of incorrect predictions from overconfidence and prevents the loss of subtle boundaries that can result from overly stochastic processes. Furthermore, we propose an innovative Dual-stream Multi-scale Feature Extractor (DMFE) for extracting multi-scale features, enhancing feature representation by considering both semantic and edge conditions of the inpainted images. Extensive experiments across challenging datasets demonstrate that the InpDiffusion significantly outperforms existing state-of-the-art methods in IIL tasks, while also showcasing excellent generalization capabilities and robustness.",
        "tags": [
            "Diffusion",
            "Inpainting"
        ]
    },
    {
        "id": "108",
        "title": "RDD4D: 4D Attention-Guided Road Damage Detection And Classification",
        "author": [
            "Asma Alkalbani",
            "Muhammad Saqib",
            "Ahmed Salim Alrawahi",
            "Abbas Anwar",
            "Chandarnath Adak",
            "Saeed Anwar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02822",
        "abstract": "Road damage detection and assessment are crucial components of infrastructure maintenance. However, current methods often struggle with detecting multiple types of road damage in a single image, particularly at varying scales. This is due to the lack of road datasets with various damage types having varying scales. To overcome this deficiency, first, we present a novel dataset called Diverse Road Damage Dataset (DRDD) for road damage detection that captures the diverse road damage types in individual images, addressing a crucial gap in existing datasets. Then, we provide our model, RDD4D, that exploits Attention4D blocks, enabling better feature refinement across multiple scales. The Attention4D module processes feature maps through an attention mechanism combining positional encoding and \"Talking Head\" components to capture local and global contextual information. In our comprehensive experimental analysis comparing various state-of-the-art models on our proposed, our enhanced model demonstrated superior performance in detecting large-sized road cracks with an Average Precision (AP) of 0.458 and maintained competitive performance with an overall AP of 0.445. Moreover, we also provide results on the CrackTinyNet dataset; our model achieved around a 0.21 increase in performance. The code, model weights, dataset, and our results are available on \\href{https://github.com/msaqib17/Road_Damage_Detection}{https://github.com/msaqib17/Road\\_Damage\\_Detection}.",
        "tags": [
            "Detection",
            "Talking Head"
        ]
    },
    {
        "id": "109",
        "title": "Randomly Sampled Language Reasoning Problems Reveal Limits of LLMs",
        "author": [
            "Kavi Gupta",
            "Kate Sanders",
            "Armando Solar-Lezama"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02825",
        "abstract": "Can LLMs pick up language structure from examples? Evidence in prior work seems to indicate yes, as pretrained models repeatedly demonstrate the ability to adapt to new language structures and vocabularies. However, this line of research typically considers languages that are present within common pretraining datasets, or otherwise share notable similarities with these seen languages. In contrast, in this work we attempt to measure models' language understanding capacity while circumventing the risk of dataset recall. We parameterize large families of language tasks recognized by deterministic finite automata (DFAs), and can thus sample novel language reasoning problems to fairly evaulate LLMs regardless of training data. We find that, even in the strikingly simple setting of 3-state DFAs, LLMs underperform unparameterized ngram models on both language recognition and synthesis tasks. These results suggest that LLMs struggle to match the ability of basic language models in recognizing and reasoning over languages that are sufficiently distinct from the ones they see at training time, underscoring the distinction between learning individual languages and possessing a general theory of language.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "110",
        "title": "Samba-asr state-of-the-art speech recognition leveraging structured state-space models",
        "author": [
            "Syed Abdul Gaffar Shakhadri",
            "Kruthika KR",
            "Kartik Basavaraj Angadi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02832",
        "abstract": "We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture dependencies, Samba ASR effectively models both local and global temporal dependencies using efficient state-space dynamics, achieving remarkable performance gains. By addressing the limitations of transformers, such as quadratic scaling with input length and difficulty in handling long-range dependencies, Samba ASR achieves superior accuracy and efficiency.\nExperimental results demonstrate that Samba ASR surpasses existing open-source transformer-based ASR models across various standard benchmarks, establishing it as the new state of the art in ASR. Extensive evaluations on benchmark datasets show significant improvements in Word Error Rate (WER), with competitive performance even in low-resource scenarios. Furthermore, the computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.\nOur contributions include:\nA new Samba ASR architecture demonstrating the superiority of SSMs over transformer-based models for speech sequence processing. A comprehensive evaluation on public benchmarks showcasing state-of-the-art performance. An analysis of computational efficiency, robustness to noise, and sequence generalization. This work highlights the viability of Mamba SSMs as a transformer-free alternative for efficient and accurate ASR. By leveraging state-space modeling advancements, Samba ASR sets a new benchmark for ASR performance and future research.",
        "tags": [
            "Mamba",
            "SSMs",
            "State Space Models",
            "Transformer"
        ]
    },
    {
        "id": "111",
        "title": "Integrating Language-Image Prior into EEG Decoding for Cross-Task Zero-Calibration RSVP-BCI",
        "author": [
            "Xujin Li",
            "Wei Wei",
            "Shuang Qiu",
            "Xinyi Zhang",
            "Fu Li",
            "Huiguang He"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02841",
        "abstract": "Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI) is an effective technology used for information detection by detecting Event-Related Potentials (ERPs). The current RSVP decoding methods can perform well in decoding EEG signals within a single RSVP task, but their decoding performance significantly decreases when directly applied to different RSVP tasks without calibration data from the new tasks. This limits the rapid and efficient deployment of RSVP-BCI systems for detecting different categories of targets in various scenarios. To overcome this limitation, this study aims to enhance the cross-task zero-calibration RSVP decoding performance. First, we design three distinct RSVP tasks for target image retrieval and build an open-source dataset containing EEG signals and corresponding stimulus images. Then we propose an EEG with Language-Image Prior fusion Transformer (ELIPformer) for cross-task zero-calibration RSVP decoding. Specifically, we propose a prompt encoder based on the language-image pre-trained model to extract language-image features from task-specific prompts and stimulus images as prior knowledge for enhancing EEG decoding. A cross bidirectional attention mechanism is also adopted to facilitate the effective feature fusion and alignment between the EEG and language-image features. Extensive experiments demonstrate that the proposed model achieves superior performance in cross-task zero-calibration RSVP decoding, which promotes the RSVP-BCI system from research to practical application.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "112",
        "title": "Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text Classification",
        "author": [
            "Yubo Wang",
            "Haoyang Li",
            "Fei Teng",
            "Lei Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02844",
        "abstract": "Text classification is a fundamental task in natural language processing, pivotal to various applications such as query optimization, data integration, and schema matching. While neural network-based models, such as CNN and BERT, have demonstrated remarkable performance in text classification, their effectiveness heavily relies on abundant labeled training data. This dependency makes these models less effective in dynamic few-shot text classification, where labeled data is scarce, and target labels frequently evolve based on application needs. Recently, large language models (LLMs) have shown promise due to their extensive pretraining and contextual understanding. Current approaches provide LLMs with text inputs, candidate labels, and additional side information (e.g., descriptions) to predict text labels. However, their effectiveness is hindered by the increased input size and the noise introduced through side information processing. To address these limitations, we propose a graph-based online retrieval-augmented generation framework, namely GORAG, for dynamic few-shot text classification. GORAG constructs and maintains an adaptive information graph by extracting side information across all target texts, rather than treating each input independently. It employs a weighted edge mechanism to prioritize the importance and reliability of extracted information and dynamically retrieves relevant context using a minimum-cost spanning tree tailored for each text input. Empirical evaluations demonstrate that GORAG outperforms existing approaches by providing more comprehensive and accurate contextual information.",
        "tags": [
            "BERT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "113",
        "title": "HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian Splatting Based Data Augmentation",
        "author": [
            "Wentian Qu",
            "Jiahe Li",
            "Jian Cheng",
            "Jian Shi",
            "Chenyu Meng",
            "Cuixia Ma",
            "Hongan Wang",
            "Xiaoming Deng",
            "Yinda Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02845",
        "abstract": "Understanding of bimanual hand-object interaction plays an important role in robotics and virtual reality. However, due to significant occlusions between hands and object as well as the high degree-of-freedom motions, it is challenging to collect and annotate a high-quality, large-scale dataset, which prevents further improvement of bimanual hand-object interaction-related baselines. In this work, we propose a new 3D Gaussian Splatting based data augmentation framework for bimanual hand-object interaction, which is capable of augmenting existing dataset to large-scale photorealistic data with various hand-object pose and viewpoints. First, we use mesh-based 3DGS to model objects and hands, and to deal with the rendering blur problem due to multi-resolution input images used, we design a super-resolution module. Second, we extend the single hand grasping pose optimization module for the bimanual hand object to generate various poses of bimanual hand-object interaction, which can significantly expand the pose distribution of the dataset. Third, we conduct an analysis for the impact of different aspects of the proposed data augmentation on the understanding of the bimanual hand-object interaction. We perform our data augmentation on two benchmarks, H2O and Arctic, and verify that our method can improve the performance of the baselines.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Robotics",
            "Super Resolution"
        ]
    },
    {
        "id": "114",
        "title": "Large Language Models for Video Surveillance Applications",
        "author": [
            "Ulindu De Silva",
            "Leon Fernando",
            "Billy Lau Pik Lik",
            "Zann Koh",
            "Sam Conrad Joyce",
            "Belinda Yuen",
            "Chau Yuen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02850",
        "abstract": "The rapid increase in video content production has resulted in enormous data volumes, creating significant challenges for efficient analysis and resource management. To address this, robust video analysis tools are essential. This paper presents an innovative proof of concept using Generative Artificial Intelligence (GenAI) in the form of Vision Language Models to enhance the downstream video analysis process. Our tool generates customized textual summaries based on user-defined queries, providing focused insights within extensive video datasets. Unlike traditional methods that offer generic summaries or limited action recognition, our approach utilizes Vision Language Models to extract relevant information, improving analysis precision and efficiency. The proposed method produces textual summaries from extensive CCTV footage, which can then be stored for an indefinite time in a very small storage space compared to videos, allowing users to quickly navigate and verify significant events without exhaustive manual review. Qualitative evaluations result in 80% and 70% accuracy in temporal and spatial quality and consistency of the pipeline respectively.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "115",
        "title": "A Novel Vision Transformer for Camera-LiDAR Fusion based Traffic Object Segmentation",
        "author": [
            "Toomas Tahves",
            "Junyi Gu",
            "Mauro Bellone",
            "Raivo Sell"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02858",
        "abstract": "This paper presents Camera-LiDAR Fusion Transformer (CLFT) models for traffic object segmentation, which leverage the fusion of camera and LiDAR data using vision transformers. Building on the methodology of visual transformers that exploit the self-attention mechanism, we extend segmentation capabilities with additional classification options to a diverse class of objects including cyclists, traffic signs, and pedestrians across diverse weather conditions. Despite good performance, the models face challenges under adverse conditions which underscores the need for further optimization to enhance performance in darkness and rain. In summary, the CLFT models offer a compelling solution for autonomous driving perception, advancing the state-of-the-art in multimodal fusion and object segmentation, with ongoing efforts required to address existing limitations and fully harness their potential in practical deployments.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "116",
        "title": "Towards HRTF Personalization using Denoising Diffusion Models",
        "author": [
            "Juan Camilo Albarrac√≠n S√°nchez",
            "Luca Comanducci",
            "Mirco Pezzoli",
            "Fabio Antonacci"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02871",
        "abstract": "Head-Related Transfer Functions (HRTFs) have fundamental applications for realistic rendering in immersive audio scenarios. However, they are strongly subject-dependent as they vary considerably depending on the shape of the ears, head and torso. Thus, personalization procedures are required for accurate binaural rendering. Recently, Denoising Diffusion Probabilistic Models (DDPMs), a class of generative learning techniques, have been applied to solve a variety of signal processing-related problems. In this paper, we propose a first approach for using DDPM conditioned on anthropometric measurements to generate personalized Head-Related Impulse Response (HRIR), the time-domain representation of HRTF. The results show the feasibility of DDPMs for HRTF personalization obtaining performance in line with state-of-the-art models.",
        "tags": [
            "DDPM",
            "Diffusion"
        ]
    },
    {
        "id": "117",
        "title": "Conditional Mutual Information Based Diffusion Posterior Sampling for Solving Inverse Problems",
        "author": [
            "Shayan Mohajer Hamidi",
            "En-Hui Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02880",
        "abstract": "Inverse problems are prevalent across various disciplines in science and engineering. In the field of computer vision, tasks such as inpainting, deblurring, and super-resolution are commonly formulated as inverse problems. Recently, diffusion models (DMs) have emerged as a promising approach for addressing noisy linear inverse problems, offering effective solutions without requiring additional task-specific training. Specifically, with the prior provided by DMs, one can sample from the posterior by finding the likelihood. Since the likelihood is intractable, it is often approximated in the literature. However, this approximation compromises the quality of the generated images. To overcome this limitation and improve the effectiveness of DMs in solving inverse problems, we propose an information-theoretic approach. Specifically, we maximize the conditional mutual information $\\mathrm{I}(\\boldsymbol{x}_0; \\boldsymbol{y} | \\boldsymbol{x}_t)$, where $\\boldsymbol{x}_0$ represents the reconstructed signal, $\\boldsymbol{y}$ is the measurement, and $\\boldsymbol{x}_t$ is the intermediate signal at stage $t$. This ensures that the intermediate signals $\\boldsymbol{x}_t$ are generated in a way that the final reconstructed signal $\\boldsymbol{x}_0$ retains as much information as possible about the measurement $\\boldsymbol{y}$. We demonstrate that this method can be seamlessly integrated with recent approaches and, once incorporated, enhances their performance both qualitatively and quantitatively.",
        "tags": [
            "Deblurring",
            "Diffusion",
            "Inpainting",
            "Super Resolution"
        ]
    },
    {
        "id": "118",
        "title": "MDP3: A Training-free Approach for List-wise Frame Selection in Video-LLMs",
        "author": [
            "Hui Sun",
            "Shiyin Lu",
            "Huanyu Wang",
            "Qing-Guo Chen",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang",
            "Ming Li"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02885",
        "abstract": "Video large language models (Video-LLMs) have made significant progress in understanding videos. However, processing multiple frames leads to lengthy visual token sequences, presenting challenges such as the limited context length cannot accommodate the entire video, and the inclusion of irrelevant frames hinders visual perception. Hence, effective frame selection is crucial. This paper emphasizes that frame selection should follow three key principles: query relevance, list-wise diversity, and sequentiality. Existing methods, such as uniform frame sampling and query-frame matching, do not capture all of these principles. Thus, we propose Markov decision determinantal point process with dynamic programming (MDP3) for frame selection, a training-free and model-agnostic method that can be seamlessly integrated into existing Video-LLMs. Our method first estimates frame similarities conditioned on the query using a conditional Gaussian kernel within the reproducing kernel Hilbert space~(RKHS). We then apply the determinantal point process~(DPP) to the similarity matrix to capture both query relevance and list-wise diversity. To incorporate sequentiality, we segment the video and apply DPP within each segment, conditioned on the preceding segment selection, modeled as a Markov decision process~(MDP) for allocating selection sizes across segments. Theoretically, MDP3 provides a \\((1 - 1/e)\\)-approximate solution to the NP-hard list-wise frame selection problem with pseudo-polynomial time complexity, demonstrating its efficiency. Empirically, MDP3 significantly outperforms existing methods, verifying its effectiveness and robustness.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "119",
        "title": "FoundPAD: Foundation Models Reloaded for Face Presentation Attack Detection",
        "author": [
            "Guray Ozgur",
            "Eduarda Caldeira",
            "Tahar Chettaoui",
            "Fadi Boutros",
            "Raghavendra Ramachandra",
            "Naser Damer"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02892",
        "abstract": "Although face recognition systems have seen a massive performance enhancement in recent years, they are still targeted by threats such as presentation attacks, leading to the need for generalizable presentation attack detection (PAD) algorithms. Current PAD solutions suffer from two main problems: low generalization to unknown cenarios and large training data requirements. Foundation models (FM) are pre-trained on extensive datasets, achieving remarkable results when generalizing to unseen domains and allowing for efficient task-specific adaption even when little training data are available. In this work, we recognize the potential of FMs to address common PAD problems and tackle the PAD task with an adapted FM for the first time. The FM under consideration is adapted with LoRA weights while simultaneously training a classification header. The resultant architecture, FoundPAD, is highly generalizable to unseen domains, achieving competitive results in several settings under different data availability scenarios and even when using synthetic training data. To encourage reproducibility and facilitate further research in PAD, we publicly release the implementation of FoundPAD at https://github.com/gurayozgur/FoundPAD .",
        "tags": [
            "Detection",
            "LoRA"
        ]
    },
    {
        "id": "120",
        "title": "DeCon: Detecting Incorrect Assertions via Postconditions Generated by a Large Language Model",
        "author": [
            "Hao Yu",
            "Tianyu Chen",
            "Jiaming Huang",
            "Zongyang Li",
            "Dezhi Ran",
            "Xinyu Wang",
            "Ying Li",
            "Assaf Marron",
            "David Harel",
            "Yuan Xie",
            "Tao Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02901",
        "abstract": "Recently, given the docstring for the target problem and the target function signature, large language models (LLMs) have been used not only to generate source code, but also to generate test cases, consisting of test inputs and assertions (e.g., in the form of checking an actual output against the expected output). However, as shown by our empirical study on assertions generated by four LLMs for the HumanEval benchmark, over 62% of the generated assertions are incorrect (i.e., failed on the ground-truth problem solution). To detect incorrect assertions (given the docstring and the target function signature along with a sample of example inputs and outputs), in this paper, we propose a new approach named DeCon to effectively detect incorrect assertions via LLM-generated postconditions for the target problem (a postcondition is a predicate that must always be true just after the execution of the ground-truth problem solution). Our approach requires a small set of I/O examples (i.e., a sample of example inputs and outputs) for the target problem (e.g., the I/O examples included in the docstring for a target problem in HumanEval). We use the given I/O examples to filter out those LLM-generated postconditions that are violated by at least one given I/O example. We then use the remaining postconditions to detect incorrect assertions as those assertions that violate at least one remaining postcondition. Experimental results show that DeCon can detect averagely more than 64% (63% and 65.5% detected by GPT-3.5 and GPT-4, respectively) incorrect assertions generated by four state-of-the-art LLMs, and DeCon can also improve the effectiveness of these LLMs in code generation by 4% in terms of Pass@1. In addition, although DeCon might filter out correct assertions, the fault-finding ability of the remaining correct assertions decreases only slightly.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "121",
        "title": "Skillful High-Resolution Ensemble Precipitation Forecasting with an Integrated Deep Learning Framework",
        "author": [
            "Shuangshuang He",
            "Hongli Liang",
            "Yuanting Zhang",
            "Xingyuan Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02905",
        "abstract": "High-resolution precipitation forecasts are crucial for providing accurate weather prediction and supporting effective responses to extreme weather events. Traditional numerical models struggle with stochastic subgrid-scale processes, while recent deep learning models often produce blurry results. To address these challenges, we propose a physics-inspired deep learning framework for high-resolution (0.05\\textdegree{} $\\times$ 0.05\\textdegree{}) ensemble precipitation forecasting. Trained on ERA5 and CMPA high-resolution precipitation datasets, the framework integrates deterministic and probabilistic components. The deterministic model, based on a 3D SwinTransformer, captures average precipitation at mesoscale resolution and incorporates strategies to enhance performance, particularly for moderate to heavy rainfall. The probabilistic model employs conditional diffusion in latent space to account for uncertainties in residual precipitation at convective scales. During inference, ensemble members are generated by repeatedly sampling latent variables, enabling the model to represent precipitation uncertainty. Our model significantly enhances spatial resolution and forecast accuracy. Rank histogram shows that the ensemble system is reliable and unbiased. In a case study of heavy precipitation in southern China, the model outputs align more closely with observed precipitation distributions than ERA5, demonstrating superior capability in capturing extreme precipitation events. Additionally, 5-day real-time forecasts show good performance in terms of CSI scores.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "122",
        "title": "Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis",
        "author": [
            "Thang-Anh-Quan Nguyen",
            "Nathan Piasco",
            "Luis Rold√£o",
            "Moussab Bennehar",
            "Dzmitry Tsishkou",
            "Laurent Caraffa",
            "Jean-Philippe Tarel",
            "Roland Br√©mond"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02913",
        "abstract": "In this paper, we present PointmapDiffusion, a novel framework for single-image novel view synthesis (NVS) that utilizes pre-trained 2D diffusion models. Our method is the first to leverage pointmaps (i.e. rasterized 3D scene coordinates) as a conditioning signal, capturing geometric prior from the reference images to guide the diffusion process. By embedding reference attention blocks and a ControlNet for pointmap features, our model balances between generative capability and geometric consistency, enabling accurate view synthesis across varying viewpoints. Extensive experiments on diverse real-world datasets demonstrate that PointmapDiffusion achieves high-quality, multi-view consistent results with significantly fewer trainable parameters compared to other baselines for single-image NVS tasks.",
        "tags": [
            "3D",
            "ControlNet",
            "Diffusion"
        ]
    },
    {
        "id": "123",
        "title": "Deep Generative Model-Aided Power System Dynamic State Estimation and Reconstruction with Unknown Control Inputs or Data Distributions",
        "author": [
            "Jianhua Pei",
            "Ping Wang",
            "Jingyu Wang",
            "Dongyuan Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02928",
        "abstract": "Fast and robust dynamic state estimation (DSE) is essential for accurately capturing the internal dynamic processes of power systems, and it serves as the foundation for reliably implementing real-time dynamic modeling, monitoring, and control applications. Nonetheless, on one hand, traditional DSE methods based on Kalman filtering or particle filtering have high accuracy requirements for system parameters, control inputs, phasor measurement unit (PMU) data, and centralized DSE communication. Consequently, these methods often face accuracy bottlenecks when dealing with structural or system process errors, unknown control vectors, PMU anomalies, and communication contingencies. On the other hand, deep learning-aided DSE, while parameter-free, often suffers from generalization issues under unforeseen operating conditions. To address these challenges, this paper proposes an effective approach that leverages deep generative models from AI-generated content (AIGC) to assist DSE. The proposed approach employs an encoder-decoder architecture to estimate unknown control input variables, a robust encoder to mitigate the impact of bad PMU data, and latent diffusion model to address communication issues in centralized DSE. Additionally, a lightweight adaptor is designed to quickly adjust the latent vector distribution. Extensive experimental results on the IEEE 39-bus system and the NPCC 140-bus system demonstrate the effectiveness and superiority of the proposed method in addressing DSE modeling imperfection, measurement uncertainties, communication contingencies, and unknown distribution challenges, while also proving its ability to reduce data storage and communication resource requirements.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "124",
        "title": "Self-Attention as a Parametric Endofunctor: A Categorical Framework for Transformer Architectures",
        "author": [
            "Charles O'Neill"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02931",
        "abstract": "Self-attention mechanisms have revolutionised deep learning architectures, but their mathematical foundations remain incompletely understood. We establish that these mechanisms can be formalised through categorical algebra, presenting a framework that focuses on the linear components of self-attention. We prove that the query, key, and value maps in self-attention naturally form a parametric endofunctor in the 2-category $\\mathbf{Para}(\\mathbf{Vect})$ of parametric morphisms. We show that stacking multiple self-attention layers corresponds to constructing the free monad on this endofunctor. For positional encodings, we demonstrate that strictly additive position embeddings constitute monoid actions on the embedding space, while standard sinusoidal encodings, though not additive, possess a universal property among faithful position-preserving functors. We establish that the linear portions of self-attention exhibit natural equivariance properties with respect to permutations of input tokens. Finally, we prove that the ``circuits'' identified in mechanistic interpretability correspond precisely to compositions of parametric morphisms in our framework. This categorical perspective unifies geometric, algebraic, and interpretability-based approaches to transformer analysis, while making explicit the mathematical structures underlying attention mechanisms. Our treatment focuses exclusively on linear maps, setting aside nonlinearities like softmax and layer normalisation, which require more sophisticated categorical structures. Our results extend recent work on categorical foundations for deep learning while providing insights into the algebraic structure of attention mechanisms.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "125",
        "title": "Efficient Langevin sampling with position-dependent diffusion",
        "author": [
            "Eugen Bronasco",
            "Benedict Leimkuhler",
            "Dominic Phillips",
            "Gilles Vilmart"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02943",
        "abstract": "We introduce a numerical method for Brownian dynamics with position dependent diffusion tensor which is second order accurate while requiring only one force evaluation per timestep. Analysis of the sampling bias is performed using the algebraic framework of exotic aromatic Butcher-series. Numerical experiments confirm the theoretical order of convergence and illustrate the efficiency of the new method.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "126",
        "title": "SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild",
        "author": [
            "Jiawei Liu",
            "Yuanzhi Zhu",
            "Feiyu Gao",
            "Zhibo Yang",
            "Peng Wang",
            "Junyang Lin",
            "Xinggang Wang",
            "Wenyu Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02962",
        "abstract": "Generating visual text in natural scene images is a challenging task with many unsolved problems. Different from generating text on artificially designed images (such as posters, covers, cartoons, etc.), the text in natural scene images needs to meet the following four key criteria: (1) Fidelity: the generated text should appear as realistic as a photograph and be completely accurate, with no errors in any of the strokes. (2) Reasonability: the text should be generated on reasonable carrier areas (such as boards, signs, walls, etc.), and the generated text content should also be relevant to the scene. (3) Utility: the generated text can facilitate to the training of natural scene OCR (Optical Character Recognition) tasks. (4) Controllability: The attribute of the text (such as font and color) should be controllable as http://needed.In this paper, we propose a two stage method, SceneVTG++, which simultaneously satisfies the four aspects mentioned above. SceneVTG++ consists of a Text Layout and Content Generator (TLCG) and a Controllable Local Text Diffusion (CLTD). The former utilizes the world knowledge of multi modal large language models to find reasonable text areas and recommend text content according to the nature scene background images, while the latter generates controllable multilingual text based on the diffusion model. Through extensive experiments, we respectively verified the effectiveness of TLCG and CLTD, and demonstrated the state-of-the-art text generation performance of SceneVTG++. In addition, the generated images have superior utility in OCR tasks like text detection and text recognition. Codes and datasets will be available.",
        "tags": [
            "Detection",
            "Diffusion",
            "Large Language Models"
        ]
    },
    {
        "id": "127",
        "title": "Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild",
        "author": [
            "Wanpeng Hu",
            "Haodi Liu",
            "Lin Chen",
            "Feng Zhou",
            "Changming Xiao",
            "Qi Yang",
            "Changshui Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02964",
        "abstract": "Complex visual reasoning remains a key challenge today. Typically, the challenge is tackled using methodologies such as Chain of Thought (COT) and visual instruction tuning. However, how to organically combine these two methodologies for greater success remains unexplored. Also, issues like hallucinations and high training cost still need to be addressed. In this work, we devise an innovative multi-round training and reasoning framework suitable for lightweight Multimodal Large Language Models (MLLMs). Our self-questioning approach heuristically guides MLLMs to focus on visual clues relevant to the target problem, reducing hallucinations and enhancing the model's ability to describe fine-grained image details. This ultimately enables the model to perform well in complex visual reasoning and question-answering tasks. We have named this framework Socratic Questioning(SQ). To facilitate future research, we create a multimodal mini-dataset named CapQA, which includes 1k images of fine-grained activities, for visual instruction tuning and evaluation, our proposed SQ method leads to a 31.2% improvement in the hallucination score. Our extensive experiments on various benchmarks demonstrate SQ's remarkable capabilities in heuristic self-questioning, zero-shot visual reasoning and hallucination mitigation. Our model and code will be publicly available.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "128",
        "title": "FlipedRAG: Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models",
        "author": [
            "Zhuo Chen",
            "Yuyang Gong",
            "Miaokun Chen",
            "Haotan Liu",
            "Qikai Cheng",
            "Fan Zhang",
            "Wei Lu",
            "Xiaozhong Liu",
            "Jiawei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02968",
        "abstract": "Retrieval-Augmented Generation (RAG) addresses hallucination and real-time constraints by dynamically retrieving relevant information from a knowledge database to supplement the LLMs' input. When presented with a query, RAG selects the most semantically similar texts from its knowledge bases and uses them as context for the LLMs to generate more accurate responses. RAG also creates a new attack surface, especially since RAG databases are frequently sourced from public domains. While existing studies have predominantly focused on optimizing RAG's performance and efficiency, emerging research has begun addressing the security concerns associated with RAG. However, these works have some limitations, typically focusing on either white-box methodologies or heuristic-based black-box attacks. Furthermore, prior research has mainly targeted simple factoid question answering, which is neither practically challenging nor resistant to correction. In this paper, we unveil a more realistic and threatening scenario: opinion manipulation for controversial topics against RAG. Particularly, we propose a novel RAG black-box attack method, termed FlipedRAG, which is transfer-based. By leveraging instruction engineering, we obtain partial retrieval model outputs from black-box RAG system, facilitating the training of surrogate models to enhance the effectiveness of opinion manipulation attack. Extensive experimental results confirms that our approach significantly enhances the average success rate of opinion manipulation by 16.7%. It achieves an average of a 50% directional change in the opinion polarity of RAG responses across four themes. Additionally, it induces a 20% shift in user cognition. Furthermore, we discuss the efficacy of potential defense mechanisms and conclude that they are insufficient in mitigating this type of attack, highlighting the urgent need to develop novel defensive strategies.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "129",
        "title": "HaWoR: World-Space Hand Motion Reconstruction from Egocentric Videos",
        "author": [
            "Jinglei Zhang",
            "Jiankang Deng",
            "Chao Ma",
            "Rolandos Alexandros Potamias"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02973",
        "abstract": "Despite the advent in 3D hand pose estimation, current methods predominantly focus on single-image 3D hand reconstruction in the camera frame, overlooking the world-space motion of the hands. Such limitation prohibits their direct use in egocentric video settings, where hands and camera are continuously in motion. In this work, we propose HaWoR, a high-fidelity method for hand motion reconstruction in world coordinates from egocentric videos. We propose to decouple the task by reconstructing the hand motion in the camera space and estimating the camera trajectory in the world coordinate system. To achieve precise camera trajectory estimation, we propose an adaptive egocentric SLAM framework that addresses the shortcomings of traditional SLAM methods, providing robust performance under challenging camera dynamics. To ensure robust hand motion trajectories, even when the hands move out of view frustum, we devise a novel motion infiller network that effectively completes the missing frames of the sequence. Through extensive quantitative and qualitative evaluations, we demonstrate that HaWoR achieves state-of-the-art performance on both hand motion reconstruction and world-frame camera trajectory estimation under different egocentric benchmark datasets. Code and models are available on https://hawor-project.github.io/ .",
        "tags": [
            "3D",
            "Pose Estimation",
            "SLAM"
        ]
    },
    {
        "id": "130",
        "title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution",
        "author": [
            "Rui Xie",
            "Yinhong Liu",
            "Penghao Zhou",
            "Chen Zhao",
            "Jun Zhou",
            "Kai Zhang",
            "Zhenyu Zhang",
            "Jian Yang",
            "Zhenheng Yang",
            "Ying Tai"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02976",
        "abstract": "Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (\\textit{e.g.}, CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce\\textbf{~\\name} (\\textbf{S}patial-\\textbf{T}emporal \\textbf{A}ugmentation with T2V models for \\textbf{R}eal-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate\\textbf{~\\name}~outperforms state-of-the-art methods on both synthetic and real-world datasets.",
        "tags": [
            "Diffusion",
            "GAN",
            "Super Resolution",
            "Text-to-Video"
        ]
    },
    {
        "id": "131",
        "title": "Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation",
        "author": [
            "Zhi Qu",
            "Yiran Wang",
            "Jiannan Mao",
            "Chenchen Ding",
            "Hideki Tanaka",
            "Masao Utiyama",
            "Taro Watanabe"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02979",
        "abstract": "The multilingual neural machine translation (MNMT) enables arbitrary translations across multiple languages by training a model with limited parameters using parallel data only. However, the performance of such MNMT models still lags behind that of large language models (LLMs), limiting their practicality. In this work, we address this limitation by introducing registering to achieve the new state-of-the-art of decoder-only MNMT models. Specifically, we insert a set of artificial tokens specifying the target language, called registers, into the input sequence between the source and target tokens. By modifying the attention mask, the target token generation only pays attention to the activation of registers, representing the source tokens in the target language space. Experiments on EC-40, a large-scale benchmark, show that our method outperforms related methods driven by optimizing multilingual representations. We further scale up and collect 9.3 billion sentence pairs across 24 languages from public datasets to pre-train two models, namely MITRE (multilingual translation with registers). One of them, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with commercial LLMs, and shows strong adaptability in fine-tuning. Finally, we open-source our models to facilitate further research and development in MNMT: https://github.com/zhiqu22/mitre.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "132",
        "title": "CALM: Curiosity-Driven Auditing for Large Language Models",
        "author": [
            "Xiang Zheng",
            "Longxiang Wang",
            "Yi Liu",
            "Xingjun Ma",
            "Chao Shen",
            "Cong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02997",
        "abstract": "Auditing Large Language Models (LLMs) is a crucial and challenging task. In this study, we focus on auditing black-box LLMs without access to their parameters, only to the provided service. We treat this type of auditing as a black-box optimization problem where the goal is to automatically uncover input-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe behaviors. For instance, we may seek a non-toxic input that the target LLM responds to with a toxic output or an input that induces the hallucinative response from the target LLM containing politically sensitive individuals. This black-box optimization is challenging due to the scarcity of feasible points, the discrete nature of the prompt space, and the large search space. To address these challenges, we propose Curiosity-Driven Auditing for Large Language Models (CALM), which uses intrinsically motivated reinforcement learning to finetune an LLM as the auditor agent to uncover potential harmful and biased input-output pairs of the target LLM. CALM successfully identifies derogatory completions involving celebrities and uncovers inputs that elicit specific names under the black-box setting. This work offers a promising direction for auditing black-box LLMs. Our code is available at https://github.com/x-zheng16/CALM.git.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "133",
        "title": "TransPixar: Advancing Text-to-Video Generation with Transparency",
        "author": [
            "Luozhou Wang",
            "Yijun Li",
            "Zhifei Chen",
            "Jui-Hsien Wang",
            "Zhifei Zhang",
            "He Zhang",
            "Zhe Lin",
            "Yingcong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03006",
        "abstract": "Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixar, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.",
        "tags": [
            "DiT",
            "Diffusion",
            "Diffusion Transformer",
            "LoRA",
            "Text-to-Video",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "134",
        "title": "Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment",
        "author": [
            "Pegah Khayatan",
            "Mustafa Shukor",
            "Jayneel Parekh",
            "Matthieu Cord"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03012",
        "abstract": "Multimodal LLMs have reached remarkable levels of proficiency in understanding multimodal inputs, driving extensive research to develop increasingly powerful models. However, much less attention has been paid to understanding and explaining the underlying mechanisms of these models. Most existing explainability research examines these models only in their final states, overlooking the dynamic representational shifts that occur during training. In this work, we systematically analyze the evolution of hidden state representations to reveal how fine-tuning alters the internal structure of a model to specialize in new multimodal tasks. Using a concept-based approach, we map hidden states to interpretable visual and textual concepts, enabling us to trace changes in encoded concepts across modalities as training progresses. We also demonstrate the use of shift vectors to capture these concepts changes. These shift vectors allow us to recover fine-tuned concepts by shifting those in the original model. Finally, we explore the practical impact of our findings on model steering, showing that we can adjust multimodal LLMs behaviors without any training, such as modifying answer types, captions style, or biasing the model toward specific responses. Our work sheds light on how multimodal representations evolve through fine-tuning and offers a new perspective for interpreting model adaptation in multimodal tasks. The code for this project is publicly available at https://github.com/mshukor/xl-vlms.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "135",
        "title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization Degradation for Mathematical Reasoning",
        "author": [
            "Zhen Li",
            "Yupeng Su",
            "Runming Yang",
            "Zhongwei Xie",
            "Ngai Wong",
            "Hongxia Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03035",
        "abstract": "Large language models have achieved significant advancements in complex mathematical reasoning benchmarks, such as MATH. However, their substantial computational requirements present challenges for practical deployment. Model quantization has emerged as an effective strategy to reduce memory usage and computational costs by employing lower precision and bit-width representations. In this study, we systematically evaluate the impact of quantization on mathematical reasoning tasks. We introduce a multidimensional evaluation framework that qualitatively assesses specific capability dimensions and conduct quantitative analyses on the step-by-step outputs of various quantization methods. Our results demonstrate that quantization differentially affects numerical computation and reasoning planning abilities, identifying key areas where quantized models experience performance degradation.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "136",
        "title": "ChronoSense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events",
        "author": [
            "Duygu Sezen Islakoglu",
            "Jan-Christoph Kalo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03040",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success in various NLP tasks, yet they still face significant challenges in reasoning and arithmetic. Temporal reasoning, a critical component of natural language understanding, has raised increasing research attention. However, comprehensive testing of Allen's interval relations (e.g., before, after, during) -- a fundamental framework for temporal relationships -- remains underexplored. To fill this gap, we present ChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It includes 16 tasks, focusing on identifying the Allen relation between two temporal events and temporal arithmetic, using both abstract events and real-world data from Wikidata. We assess the performance of seven recent LLMs using this benchmark and the results indicate that models handle Allen relations, even symmetrical ones, quite differently. Moreover, the findings suggest that the models may rely on memorization to answer time-related questions. Overall, the models' low performance highlights the need for improved temporal understanding in LLMs and ChronoSense offers a robust framework for future research in this area. Our dataset and the source code are available at https://github.com/duyguislakoglu/chronosense.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "137",
        "title": "Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation",
        "author": [
            "Guy Yariv",
            "Yuval Kirstain",
            "Amit Zohar",
            "Shelly Sheynin",
            "Yaniv Taigman",
            "Yossi Adi",
            "Sagie Benaim",
            "Adam Polyak"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03059",
        "abstract": "We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object motion, especially in multi-object scenarios. To address these limitations, we propose a two-stage compositional framework that decomposes I2V generation into: (i) An explicit intermediate representation generation stage, followed by (ii) A video generation stage that is conditioned on this representation. Our key innovation is the introduction of a mask-based motion trajectory as an intermediate representation, that captures both semantic object information and motion, enabling an expressive but compact representation of motion and semantics. To incorporate the learned representation in the second stage, we utilize object-level attention objectives. Specifically, we consider a spatial, per-object, masked-cross attention objective, integrating object-specific prompts into corresponding latent space regions and a masked spatio-temporal self-attention objective, ensuring frame-to-frame consistency for each object. We evaluate our method on challenging benchmarks with multi-object and high-motion scenarios and empirically demonstrate that the proposed method achieves state-of-the-art results in temporal coherence, motion realism, and text-prompt faithfulness. Additionally, we introduce \\benchmark, a new challenging benchmark for single-object and multi-object I2V generation, and demonstrate our method's superiority on this benchmark. Project page is available at https://guyyariv.github.io/TTM/.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "138",
        "title": "Retrieval-Augmented TLAPS Proof Generation with Large Language Models",
        "author": [
            "Yuhao Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03073",
        "abstract": "We present a novel approach to automated proof generation for the TLA+ Proof System (TLAPS) using Large Language Models (LLMs). Our method combines two key components: a sub-proof obligation generation phase that breaks down complex proof obligations into simpler sub-obligations, and a proof generation phase that leverages Retrieval-Augmented Generation with verified proof examples. We evaluate our approach using proof obligations from varying complexity levels of proof obligations, spanning from fundamental arithmetic properties to the properties of algorithms. Our experiments demonstrate that while the method successfully generates valid proofs for intermediate-complexity obligations, it faces limitations with more complex theorems. These results indicate that our approach can effectively assist in proof development for certain classes of properties, contributing to the broader goal of integrating LLMs into formal verification workflows.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "139",
        "title": "Qinco2: Vector Compression and Search with Improved Implicit Neural Codebooks",
        "author": [
            "Th√©ophane Vallaeys",
            "Matthew Muckley",
            "Jakob Verbeek",
            "Matthijs Douze"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03078",
        "abstract": "Vector quantization is a fundamental technique for compression and large-scale nearest neighbor search. For high-accuracy operating points, multi-codebook quantization associates data vectors with one element from each of multiple codebooks. An example is residual quantization (RQ), which iteratively quantizes the residual error of previous steps. Dependencies between the different parts of the code are, however, ignored in RQ, which leads to suboptimal rate-distortion performance. QINCo recently addressed this inefficiency by using a neural network to determine the quantization codebook in RQ based on the vector reconstruction from previous steps. In this paper we introduce QINCo2 which extends and improves QINCo with (i) improved vector encoding using codeword pre-selection and beam-search, (ii) a fast approximate decoder leveraging codeword pairs to establish accurate short-lists for search, and (iii) an optimized training procedure and network architecture. We conduct experiments on four datasets to evaluate QINCo2 for vector compression and billion-scale nearest neighbor search. We obtain outstanding results in both settings, improving the state-of-the-art reconstruction MSE by 34% for 16-byte vector compression on BigANN, and search accuracy by 24% with 8-byte encodings on Deep1M.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "140",
        "title": "LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases",
        "author": [
            "Dylan Bouchard",
            "Mohit Singh Chauhan",
            "David Skarbrevik",
            "Viren Bajaj",
            "Zeya Ahmad"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03112",
        "abstract": "Large Language Models (LLMs) have been observed to exhibit bias in numerous ways, potentially creating or worsening outcomes for specific groups identified by protected attributes such as sex, race, sexual orientation, or age. To help address this gap, we introduce LangFair, an open-source Python package that aims to equip LLM practitioners with the tools to evaluate bias and fairness risks relevant to their specific use cases. The package offers functionality to easily generate evaluation datasets, comprised of LLM responses to use-case-specific prompts, and subsequently calculate applicable metrics for the practitioner's use case. To guide in metric selection, LangFair offers an actionable decision framework.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "141",
        "title": "CAT: Content-Adaptive Image Tokenization",
        "author": [
            "Junhong Shen",
            "Kushal Tirumala",
            "Michihiro Yasunaga",
            "Ishan Misra",
            "Luke Zettlemoyer",
            "Lili Yu",
            "Chunting Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03120",
        "abstract": "Most existing image tokenizers encode images into a fixed number of tokens or patches, overlooking the inherent variability in image complexity. To address this, we introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts representation capacity based on the image content and encodes simpler images into fewer tokens. We design a caption-based evaluation system that leverages large language models (LLMs) to predict content complexity and determine the optimal compression ratio for a given image, taking into account factors critical to human perception. Trained on images with diverse compression ratios, CAT demonstrates robust performance in image reconstruction. We also utilize its variable-length latent representations to train Diffusion Transformers (DiTs) for ImageNet generation. By optimizing token allocation, CAT improves the FID score over fixed-ratio baselines trained with the same flops and boosts the inference throughput by 18.5%.",
        "tags": [
            "Diffusion",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "142",
        "title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models",
        "author": [
            "Mingyang Song",
            "Zhaochen Su",
            "Xiaoye Qu",
            "Jiawei Zhou",
            "Yu Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03124",
        "abstract": "Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development.",
        "tags": [
            "Detection",
            "Large Language Models"
        ]
    },
    {
        "id": "143",
        "title": "VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity",
        "author": [
            "Yerong Li",
            "Yiren Liu",
            "Yun Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03139",
        "abstract": "Scenario-based training has been widely adopted in many public service sectors. Recent advancements in Large Language Models (LLMs) have shown promise in simulating diverse personas to create these training scenarios. However, little is known about how LLMs can be developed to simulate victims for scenario-based training purposes. In this paper, we introduce VicSim (victim simulator), a novel model that addresses three key dimensions of user simulation: informational faithfulness, emotional dynamics, and language style (e.g., grammar usage). We pioneer the integration of scenario-based victim modeling with GAN-based training workflow and key-information-based prompting, aiming to enhance the realism of simulated victims. Our adversarial training approach teaches the discriminator to recognize grammar and emotional cues as reliable indicators of synthetic content. According to evaluations by human raters, the VicSim model outperforms GPT-4 in terms of human-likeness.",
        "tags": [
            "GAN",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "144",
        "title": "Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches",
        "author": [
            "Alhassan Mumuni",
            "Fuseini Mumuni"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03151",
        "abstract": "Generative artificial intelligence (AI) systems based on large-scale pretrained foundation models (PFMs) such as vision-language models, large language models (LLMs), diffusion models and vision-language-action (VLA) models have demonstrated the ability to solve complex and truly non-trivial AI problems in a wide variety of domains and contexts. Multimodal large language models (MLLMs), in particular, learn from vast and diverse data sources, allowing rich and nuanced representations of the world and, thereby, providing extensive capabilities, including the ability to reason, engage in meaningful dialog; collaborate with humans and other agents to jointly solve complex problems; and understand social and emotional aspects of humans. Despite this impressive feat, the cognitive abilities of state-of-the-art LLMs trained on large-scale datasets are still superficial and brittle. Consequently, generic LLMs are severely limited in their generalist capabilities. A number of foundational problems -- embodiment, symbol grounding, causality and memory -- are required to be addressed for LLMs to attain human-level general intelligence. These concepts are more aligned with human cognition and provide LLMs with inherent human-like cognitive properties that support the realization of physically-plausible, semantically meaningful, flexible and more generalizable knowledge and intelligence. In this work, we discuss the aforementioned foundational issues and survey state-of-the art approaches for implementing these concepts in LLMs. Specifically, we discuss how the principles of embodiment, symbol grounding, causality and memory can be leveraged toward the attainment of artificial general intelligence (AGI) in an organic manner.",
        "tags": [
            "Diffusion",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "145",
        "title": "The Scaling Law for LoRA Base on Mutual Information Upper Bound",
        "author": [
            "Jing Zhang",
            "Hui Gao",
            "Peng Zhang",
            "Shuzhen Sun",
            "Chang Yang",
            "Yuexian Hou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03152",
        "abstract": "LoRA (Low-Rank Adaptation) is a widely used model fine-tuning method. In fine-tuning, the law among model performance, model parameters, and data complexity has been a focal issue in the field. Existing methods often leverage external metrics (such as cross-entropy or perplexity) to evaluate model performance. In the fine-tuning process for large models, two types of knowledge are typically involved: the frozen, general knowledge acquired by the model during pre-training and the new knowledge learned through the LoRA module from the current data. Generally, the less LoRA's learned knowledge relies on the large model, the more it captures the specific knowledge of new data, thereby enhancing its adaptability to new tasks. However, external metrics do not readily capture the dependency relationship between these two types of knowledge. Therefore, we designed an internal metric based on the Mutual Information Upper Bound (MIUB) theory to investigate the scaling law of large-model LoRA fine-tuning. In our experiments, we validated this approach on benchmark datasets, using the Llama3-8B and Phi3-3B models. The results show that the proposed MIUB metric aligns more accurately and stably with the scaling law of LoRA fine-tuning compared to cross-entropy and perplexity.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "146",
        "title": "Segment Anything Model for Zero-shot Single Particle Tracking in Liquid Phase Transmission Electron Microscopy",
        "author": [
            "Risha Goel",
            "Zain Shabeeb",
            "Isabel Panicker",
            "Vida Jamali"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03153",
        "abstract": "Liquid phase transmission electron microscopy (LPTEM) offers an unparalleled combination of spatial and temporal resolution, making it a promising tool for single particle tracking at the nanoscale. However, the absence of a standardized framework for identifying and tracking nanoparticles in noisy LPTEM videos has impeded progress in the field to develop this technique as a single particle tracking tool. To address this, we leveraged Segment Anything Model 2 (SAM 2), released by Meta, which is a foundation model developed for segmenting videos and images. Here, we demonstrate that SAM 2 can successfully segment LPTEM videos in a zero-shot manner and without requiring fine-tuning. Building on this capability, we introduce SAM4EM, a comprehensive framework that integrates promptable video segmentation with particle tracking and statistical analysis, providing an end-to-end LPTEM analysis framework for single particle tracking. SAM4EM achieves nearly 50-fold higher accuracy in segmenting and analyzing LPTEM videos compared to state-of-the-art methods, paving the way for broader applications of LPTEM in nanoscale imaging.",
        "tags": [
            "SAM",
            "Segment Anything",
            "Segmentation"
        ]
    },
    {
        "id": "147",
        "title": "Deep-Relative-Trust-Based Diffusion for Decentralized Deep Learning",
        "author": [
            "Muyun Li",
            "Aaron Fainman",
            "Stefan Vlaski"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03162",
        "abstract": "Decentralized learning strategies allow a collection of agents to learn efficiently from local data sets without the need for central aggregation or orchestration. Current decentralized learning paradigms typically rely on an averaging mechanism to encourage agreement in the parameter space. We argue that in the context of deep neural networks, which are often over-parameterized, encouraging consensus of the neural network outputs, as opposed to their parameters can be more appropriate. This motivates the development of a new decentralized learning algorithm, termed DRT diffusion, based on deep relative trust (DRT), a recently introduced similarity measure for neural networks. We provide convergence analysis for the proposed strategy, and numerically establish its benefit to generalization, especially with sparse topologies, in an image classification task.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "148",
        "title": "Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for SQL2Text",
        "author": [
            "Ali Al-Lawati",
            "Jason Lucas",
            "Prasenjit Mitra"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03166",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in various NLP tasks, including semantic parsing, which trans lates natural language into formal code representations. However, the reverse process, translating code into natural language, termed semantic captioning, has received less attention. This task is becoming increasingly important as LLMs are integrated into platforms for code generation, security analysis, and educational purposes. In this paper, we focus on the captioning of SQL query (SQL2Text) to address the critical need for understanding and explaining SQL queries in an era where LLM-generated code poses potential security risks. We repurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt using GPT-4o to generate multiple additional utterances, which enhances the robustness of the datasets for the reverse task. We conduct our experiments using in-context learning (ICL) based on different sample selection methods, emphasizing smaller, more computationally efficient LLMs. Our findings demonstrate that leveraging the inherent graph properties of SQL for ICL sample selection significantly outperforms random selection by up to 39% on BLEU score and provides better results than alternative methods. Dataset and codes are published: \\url{https://github.com/aliwister/ast-icl}.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "149",
        "title": "MObI: Multimodal Object Inpainting Using Diffusion Models",
        "author": [
            "Alexandru Buburuzan",
            "Anuj Sharma",
            "John Redford",
            "Puneet K. Dokania",
            "Romain Mueller"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03173",
        "abstract": "Safety-critical applications, such as autonomous driving, require extensive multimodal data for rigorous testing. Methods based on synthetic data are gaining prominence due to the cost and complexity of gathering real-world data but require a high degree of realism and controllability in order to be useful. This paper introduces MObI, a novel framework for Multimodal Object Inpainting that leverages a diffusion model to create realistic and controllable object inpaintings across perceptual modalities, demonstrated for both camera and lidar simultaneously. Using a single reference RGB image, MObI enables objects to be seamlessly inserted into existing multimodal scenes at a 3D location specified by a bounding box, while maintaining semantic consistency and multimodal coherence. Unlike traditional inpainting methods that rely solely on edit masks, our 3D bounding box conditioning gives objects accurate spatial positioning and realistic scaling. As a result, our approach can be used to insert novel objects flexibly into multimodal scenes, providing significant advantages for testing perception models.",
        "tags": [
            "3D",
            "Diffusion",
            "Inpainting"
        ]
    },
    {
        "id": "150",
        "title": "Classifier-Guided Captioning Across Modalities",
        "author": [
            "Ariel Shaulov",
            "Tal Shaharabany",
            "Eitan Shaar",
            "Gal Chechik",
            "Lior Wolf"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03183",
        "abstract": "Most current captioning systems use language models trained on data from specific settings, such as image-based captioning via Amazon Mechanical Turk, limiting their ability to generalize to other modality distributions and contexts. This limitation hinders performance in tasks like audio or video captioning, where different semantic cues are needed. Addressing this challenge is crucial for creating more adaptable and versatile captioning frameworks applicable across diverse real-world contexts. In this work, we introduce a method to adapt captioning networks to the semantics of alternative settings, such as capturing audibility in audio captioning, where it is crucial to describe sounds and their sources. Our framework consists of two main components: (i) a frozen captioning system incorporating a language model (LM), and (ii) a text classifier that guides the captioning system. The classifier is trained on a dataset automatically generated by GPT-4, using tailored prompts specifically designed to enhance key aspects of the generated captions. Importantly, the framework operates solely during inference, eliminating the need for further training of the underlying captioning model. We evaluate the framework on various models and modalities, with a focus on audio captioning, and report promising results. Notably, when combined with an existing zero-shot audio captioning system, our framework improves its quality and sets state-of-the-art performance in zero-shot audio captioning.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "151",
        "title": "CLIX: Cross-Lingual Explanations of Idiomatic Expressions",
        "author": [
            "Aaron Gluck",
            "Katharina von der Wense",
            "Maria Pacheco"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03191",
        "abstract": "Automated definition generation systems have been proposed to support vocabulary expansion for language learners. The main barrier to the success of these systems is that learners often struggle to understand definitions due to the presence of potentially unfamiliar words and grammar, particularly when non-standard language is involved. To address these challenges, we propose CLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We explore the capabilities of current NLP models for this task, and observe that while it remains challenging, large language models show promise. Finally, we perform a detailed error analysis to highlight the key challenges that need to be addressed before we can reliably incorporate these systems into educational tools.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "152",
        "title": "The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input",
        "author": [
            "Alon Jacovi",
            "Andrew Wang",
            "Chris Alberti",
            "Connie Tao",
            "Jon Lipovetz",
            "Kate Olszewska",
            "Lukas Haas",
            "Michelle Liu",
            "Nate Keating",
            "Adam Bloniarz",
            "Carl Saroufim",
            "Corey Fry",
            "Dror Marcus",
            "Doron Kukliansky",
            "Gaurav Singh Tomar",
            "James Swirhun",
            "Jinwei Xing",
            "Lily Wang",
            "Madhu Gurumurthy",
            "Michael Aaron",
            "Moran Ambar",
            "Rachana Fellinger",
            "Rui Wang",
            "Zizhao Zhang",
            "Sasha Goldshtein",
            "Dipanjan Das"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03200",
        "abstract": "We introduce FACTS Grounding, an online leaderboard and associated benchmark that evaluates language models' ability to generate text that is factually accurate with respect to given context in the user prompt. In our benchmark, each prompt includes a user request and a full document, with a maximum length of 32k tokens, requiring long-form responses. The long-form responses are required to be fully grounded in the provided context document while fulfilling the user request. Models are evaluated using automated judge models in two phases: (1) responses are disqualified if they do not fulfill the user request; (2) they are judged as accurate if the response is fully grounded in the provided document. The automated judge models were comprehensively evaluated against a held-out test-set to pick the best prompt template, and the final factuality score is an aggregate of multiple judge models to mitigate evaluation bias. The FACTS Grounding leaderboard will be actively maintained over time, and contains both public and private splits to allow for external participation while guarding the integrity of the leaderboard. It can be found at https://www.kaggle.com/facts-leaderboard.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "153",
        "title": "Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity",
        "author": [
            "Ayat A. Najjar",
            "Huthaifa I. Ashqar",
            "Omar A. Darwish",
            "Eman Hammad"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03203",
        "abstract": "This study seeks to enhance academic integrity by providing tools to detect AI-generated content in student work using advanced technologies. The findings promote transparency and accountability, helping educators maintain ethical standards and supporting the responsible integration of AI in education. A key contribution of this work is the generation of the CyberHumanAI dataset, which has 1000 observations, 500 of which are written by humans and the other 500 produced by ChatGPT. We evaluate various machine learning (ML) and deep learning (DL) algorithms on the CyberHumanAI dataset comparing human-written and AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT). Results demonstrate that traditional ML algorithms, specifically XGBoost and Random Forest, achieve high performance (83% and 81% accuracies respectively). Results also show that classifying shorter content seems to be more challenging than classifying longer content. Further, using Explainable Artificial Intelligence (XAI) we identify discriminative features influencing the ML model's predictions, where human-written content tends to use a practical language (e.g., use and allow). Meanwhile AI-generated text is characterized by more abstract and formal terms (e.g., realm and employ). Finally, a comparative analysis with GPTZero show that our narrowly focused, simple, and fine-tuned model can outperform generalized systems like GPTZero. The proposed model achieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when tasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a tendency to classify challenging and small-content cases as either mixed or unrecognized while our proposed model showed a more balanced performance across the three classes.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "154",
        "title": "Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text",
        "author": [
            "Ayat Najjar",
            "Huthaifa I. Ashqar",
            "Omar Darwish",
            "Eman Hammad"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03212",
        "abstract": "The development of Generative AI Large Language Models (LLMs) raised the alarm regarding identifying content produced through generative AI or humans. In one case, issues arise when students heavily rely on such tools in a manner that can affect the development of their writing or coding skills. Other issues of plagiarism also apply. This study aims to support efforts to detect and identify textual content generated using LLM tools. We hypothesize that LLMs-generated text is detectable by machine learning (ML), and investigate ML models that can recognize and differentiate texts generated by multiple LLMs tools. We leverage several ML and Deep Learning (DL) algorithms such as Random Forest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable Artificial Intelligence (XAI) to understand the important features in attribution. Our method is divided into 1) binary classification to differentiate between human-written and AI-text, and 2) multi classification, to differentiate between human-written text and the text generated by the five different LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity). Results show high accuracy in the multi and binary classification. Our model outperformed GPTZero with 98.5\\% accuracy to 78.3\\%. Notably, GPTZero was unable to recognize about 4.2\\% of the observations, but our model was able to recognize the complete test dataset. XAI results showed that understanding feature importance across different classes enables detailed author/source profiles. Further, aiding in attribution and supporting plagiarism detection by highlighting unique stylistic and structural elements ensuring robust content originality verification.",
        "tags": [
            "ChatGPT",
            "Detection",
            "LLMs",
            "LLaMA",
            "Large Language Models",
            "RNN"
        ]
    },
    {
        "id": "155",
        "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
        "author": [
            "Rui Qian",
            "Shuangrui Ding",
            "Xiaoyi Dong",
            "Pan Zhang",
            "Yuhang Zang",
            "Yuhang Cao",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03218",
        "abstract": "Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing. 2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users. However, inherent conflicts exist among the desired capabilities. The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction. To unify the conflicted capabilities within a harmonious system, we present Dispider, a system that disentangles Perception, Decision, and Reaction. Dispider features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction. Once the interaction is triggered, an asynchronous interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime. Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams. Experiments show that Dispider not only maintains strong performance in conventional video QA tasks, but also significantly surpasses previous online models in streaming scenario responses, thereby validating the effectiveness of our architecture. The code and model are released at \\url{https://github.com/Mark12Ding/Dispider}.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "156",
        "title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning",
        "author": [
            "Beichen Zhang",
            "Yuhong Liu",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Pan Zhang",
            "Haodong Duan",
            "Yuhang Cao",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03226",
        "abstract": "Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical benchmarks, and 7.5\\% gain combined with MCTS.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "157",
        "title": "Gaussian Masked Autoencoders",
        "author": [
            "Jathushan Rajasegaran",
            "Xinlei Chen",
            "Rulilong Li",
            "Christoph Feichtenhofer",
            "Jitendra Malik",
            "Shiry Ginosar"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03229",
        "abstract": "This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While reconstructive self-supervised learning frameworks such as MAE learns good semantic abstractions, it is not trained for explicit spatial awareness. Our approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic abstractions and spatial understanding jointly. Like MAE, it reconstructs the image end-to-end in the pixel space, but beyond MAE, it also introduces an intermediate, 3D Gaussian-based representation and renders images via splatting. We show that GMAE can enable various zero-shot learning capabilities of spatial understanding (e.g., figure-ground segmentation, image layering, edge detection, etc.) while preserving the high-level semantics of self-supervised representation quality from MAE. To our knowledge, we are the first to employ Gaussian primitives in an image representation learning framework beyond optimization-based single-scene reconstructions. We believe GMAE will inspire further research in this direction and contribute to developing next-generation techniques for modeling high-fidelity visual data. More details at https://brjathu.github.io/gmae",
        "tags": [
            "3D",
            "Detection",
            "Gaussian Splatting",
            "Segmentation"
        ]
    },
    {
        "id": "158",
        "title": "METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring",
        "author": [
            "Ollie Liu",
            "Sami Jaghouar",
            "Johannes Hagemann",
            "Shangshang Wang",
            "Jason Wiemels",
            "Jeff Kaufman",
            "Willie Neiswanger"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02045",
        "abstract": "We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "159",
        "title": "Establishing baselines for generative discovery of inorganic crystals",
        "author": [
            "Nathan J. Szymanski",
            "Christopher J. Bartel"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02144",
        "abstract": "Generative artificial intelligence offers a promising avenue for materials discovery, yet its advantages over traditional methods remain unclear. In this work, we introduce and benchmark two baseline approaches - random enumeration of charge-balanced prototypes and data-driven ion exchange of known compounds - against three generative models: a variational autoencoder, a large language model, and a diffusion model. Our results show that established methods such as ion exchange perform comparably well in generating stable materials, although many of these materials tend to closely resemble known compounds. In contrast, generative models excel at proposing novel structural frameworks and, when sufficient training data is available, can more effectively target properties such as electronic band gap and bulk modulus while maintaining a high stability rate. To enhance the performance of both the baseline and generative approaches, we implement a post-generation screening step in which all proposed structures are passed through stability and property filters from pre-trained machine learning models including universal interatomic potentials. This low-cost filtering step leads to substantial improvement in the success rates of all methods, remains computationally efficient, and ultimately provides a practical pathway toward more effective generative strategies for materials discovery.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "160",
        "title": "Beyond Log-Concavity and Score Regularity: Improved Convergence Bounds for Score-Based Generative Models in W2-distance",
        "author": [
            "Marta Gentiloni-Silveri",
            "Antonio Ocello"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02298",
        "abstract": "Score-based Generative Models (SGMs) aim to sample from a target distribution by learning score functions using samples perturbed by Gaussian noise. Existing convergence bounds for SGMs in the $\\mathcal{W}_2$-distance rely on stringent assumptions about the data distribution. In this work, we present a novel framework for analyzing $\\mathcal{W}_2$-convergence in SGMs, significantly relaxing traditional assumptions such as log-concavity and score regularity. Leveraging the regularization properties of the Ornstein-Uhlenbeck (OU) process, we show that weak log-concavity of the data distribution evolves into log-concavity over time. This transition is rigorously quantified through a PDE-based analysis of the Hamilton-Jacobi-Bellman equation governing the log-density of the forward process. Moreover, we establish that the drift of the time-reversed OU process alternates between contractive and non-contractive regimes, reflecting the dynamics of concavity. Our approach circumvents the need for stringent regularity conditions on the score function and its estimators, relying instead on milder, more practical assumptions. We demonstrate the wide applicability of this framework through explicit computations on Gaussian mixture models, illustrating its versatility and potential for broader classes of data distributions.",
        "tags": [
            "Score-Based Generative"
        ]
    },
    {
        "id": "161",
        "title": "Who Wrote This? Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities",
        "author": [
            "Tara Radvand",
            "Mojtaba Abdolmaleki",
            "Mohamed Mostagir",
            "Ambuj Tewari"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02406",
        "abstract": "Verifying the provenance of content is crucial to the function of many organizations, e.g., educational institutions, social media platforms, firms, etc. This problem is becoming increasingly difficult as text generated by Large Language Models (LLMs) becomes almost indistinguishable from human-generated content. In addition, many institutions utilize in-house LLMs and want to ensure that external, non-sanctioned LLMs do not produce content within the institution. In this paper, we answer the following question: Given a piece of text, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can be a human)? We model LLM-generated text as a sequential stochastic process with complete dependence on history and design zero-shot statistical tests to distinguish between (i) the text generated by two different sets of LLMs $A$ (in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and human-generated texts. We prove that the type I and type II errors for our tests decrease exponentially in the text length. In designing our tests, we derive concentration inequalities on the difference between log-perplexity and the average entropy of the string under $A$. Specifically, for a given string, we demonstrate that if the string is generated by $A$, the log-perplexity of the string under $A$ converges to the average entropy of the string under $A$, except with an exponentially small probability in string length. We also show that if $B$ generates the text, except with an exponentially small probability in string length, the log-perplexity of the string under $A$ converges to the average cross-entropy of $B$ and $A$. Lastly, we present preliminary experimental results to support our theoretical results. By enabling guaranteed (with high probability) finding of the origin of harmful LLM-generated text with arbitrary size, we can help fight misinformation.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "162",
        "title": "A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models",
        "author": [
            "Yinpeng Cai",
            "Lexin Li",
            "Linjun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02441",
        "abstract": "Large Language Models (LLMs) are rapidly gaining enormous popularity in recent years. However, the training of LLMs has raised significant privacy and legal concerns, particularly regarding the inclusion of copyrighted materials in their training data without proper attribution or licensing, which falls under the broader issue of data misappropriation. In this article, we focus on a specific problem of data misappropriation detection, namely, to determine whether a given LLM has incorporated data generated by another LLM. To address this issue, we propose embedding watermarks into the copyrighted training data and formulating the detection of data misappropriation as a hypothesis testing problem. We develop a general statistical testing framework, construct a pivotal statistic, determine the optimal rejection threshold, and explicitly control the type I and type II errors. Furthermore, we establish the asymptotic optimality properties of the proposed tests, and demonstrate its empirical effectiveness through intensive numerical experiments.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "163",
        "title": "Remote Inference over Dynamic Links via Adaptive Rate Deep Task-Oriented Vector Quantization",
        "author": [
            "Eyal Fishel",
            "May Malka",
            "Shai Ginzach",
            "Nir Shlezinger"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02521",
        "abstract": "A broad range of technologies rely on remote inference, wherein data acquired is conveyed over a communication channel for inference in a remote server. Communication between the participating entities is often carried out over rate-limited channels, necessitating data compression for reducing latency. While deep learning facilitates joint design of the compression mapping along with encoding and inference rules, existing learned compression mechanisms are static, and struggle in adapting their resolution to changes in channel conditions and to dynamic links. To address this, we propose Adaptive Rate Task-Oriented Vector Quantization (ARTOVeQ), a learned compression mechanism that is tailored for remote inference over dynamic links. ARTOVeQ is based on designing nested codebooks along with a learning algorithm employing progressive learning. We show that ARTOVeQ extends to support low-latency inference that is gradually refined via successive refinement principles, and that it enables the simultaneous usage of multiple resolutions when conveying high-dimensional data. Numerical results demonstrate that the proposed scheme yields remote deep inference that operates with multiple rates, supports a broad range of bit budgets, and facilitates rapid inference that gradually improves with more bits exchanged, while approaching the performance of single-rate deep quantization methods.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "164",
        "title": "Transformers Simulate MLE for Sequence Generation in Bayesian Networks",
        "author": [
            "Yuan Cao",
            "Yihan He",
            "Dennis Wu",
            "Hong-Yu Chen",
            "Jianqing Fan",
            "Han Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02547",
        "abstract": "Transformers have achieved significant success in various fields, notably excelling in tasks involving sequential data like natural language processing. Despite these achievements, the theoretical understanding of transformers' capabilities remains limited. In this paper, we investigate the theoretical capabilities of transformers to autoregressively generate sequences in Bayesian networks based on in-context maximum likelihood estimation (MLE). Specifically, we consider a setting where a context is formed by a set of independent sequences generated according to a Bayesian network. We demonstrate that there exists a simple transformer model that can (i) estimate the conditional probabilities of the Bayesian network according to the context, and (ii) autoregressively generate a new sample according to the Bayesian network with estimated conditional probabilities. We further demonstrate in extensive experiments that such a transformer does not only exist in theory, but can also be effectively obtained through training. Our analysis highlights the potential of transformers to learn complex probabilistic models and contributes to a better understanding of large language models as a powerful class of sequence generators.",
        "tags": [
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "165",
        "title": "LWFNet: Coherent Doppler Wind Lidar-Based Network for Wind Field Retrieval",
        "author": [
            "Ran Tao",
            "Chong Wang",
            "Hao Chen",
            "Mingjiao Jia",
            "Xiang Shang",
            "Luoyuan Qu",
            "Guoliang Shentu",
            "Yanyu Lu",
            "Yanfeng Huo",
            "Lei Bai",
            "Xianghui Xue",
            "Xiankang Dou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02613",
        "abstract": "Accurate detection of wind fields within the troposphere is essential for atmospheric dynamics research and plays a crucial role in extreme weather forecasting. Coherent Doppler wind lidar (CDWL) is widely regarded as the most suitable technique for high spatial and temporal resolution wind field detection. However, since coherent detection relies heavily on the concentration of aerosol particles, which cause Mie scattering, the received backscattering lidar signal exhibits significantly low intensity at high altitudes. As a result, conventional methods, such as spectral centroid estimation, often fail to produce credible and accurate wind retrieval results in these regions. To address this issue, we propose LWFNet, the first Lidar-based Wind Field (WF) retrieval neural Network, built upon Transformer and the Kolmogorov-Arnold network. Our model is trained solely on targets derived from the traditional wind retrieval algorithm and utilizes radiosonde measurements as the ground truth for test results evaluation. Experimental results demonstrate that LWFNet not only extends the maximum wind field detection range but also produces more accurate results, exhibiting a level of precision that surpasses the labeled targets. This phenomenon, which we refer to as super-accuracy, is explored by investigating the potential underlying factors that contribute to this intriguing occurrence. In addition, we compare the performance of LWFNet with other state-of-the-art (SOTA) models, highlighting its superior effectiveness and capability in high-resolution wind retrieval. LWFNet demonstrates remarkable performance in lidar-based wind field retrieval, setting a benchmark for future research and advancing the development of deep learning models in this domain.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "166",
        "title": "Diff-Lung: Diffusion-Based Texture Synthesis for Enhanced Pathological Tissue Segmentation in Lung CT Scans",
        "author": [
            "Rezkellah Noureddine Khiati",
            "Pierre-Yves Brillet",
            "Radu Ispas",
            "Catalin Fetita"
        ],
        "pdf": "https://arxiv.org/pdf/2501.02867",
        "abstract": "Accurate quantification of the extent of lung pathological patterns (fibrosis, ground-glass opacity, emphysema, consolidation) is prerequisite for diagnosis and follow-up of interstitial lung diseases. However, segmentation is challenging due to the significant class imbalance between healthy and pathological tissues. This paper addresses this issue by leveraging a diffusion model for data augmentation applied during training an AI model. Our approach generates synthetic pathological tissue patches while preserving essential shape characteristics and intricate details specific to each tissue type. This method enhances the segmentation process by increasing the occurence of underrepresented classes in the training data. We demonstrate that our diffusion-based augmentation technique improves segmentation accuracy across all pathological tissue types, particularly for the less common patterns. This advancement contributes to more reliable automated analysis of lung CT scans, potentially improving clinical decision-making and patient outcomes",
        "tags": [
            "Diffusion",
            "Segmentation"
        ]
    },
    {
        "id": "167",
        "title": "DDRM-PR: Fourier Phase Retrieval using Denoising Diffusion Restoration Models",
        "author": [
            "Mehmet Onurcan Kaya",
            "Figen S. Oktem"
        ],
        "pdf": "https://arxiv.org/pdf/2501.03030",
        "abstract": "Diffusion models have demonstrated their utility as learned priors for solving various inverse problems. However, most existing approaches are limited to linear inverse problems. This paper exploits the efficient and unsupervised posterior sampling framework of Denoising Diffusion Restoration Models (DDRM) for the solution of nonlinear phase retrieval problem, which requires reconstructing an image from its noisy intensity-only measurements such as Fourier intensity. The approach combines the model-based alternating-projection methods with the DDRM to utilize pretrained unconditional diffusion priors for phase retrieval. The performance is demonstrated through both simulations and experimental data. Results demonstrate the potential of this approach for improving the alternating-projection methods as well as its limitations.",
        "tags": [
            "Diffusion"
        ]
    }
]