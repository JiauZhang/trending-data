[
    {
        "id": "1",
        "title": "Bridging AI and Science: Implications from a Large-Scale Literature Analysis of AI4Science",
        "author": [
            "Yutong Xie",
            "Yijun Pan",
            "Hua Xu",
            "Qiaozhu Mei"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09628",
        "abstract": "Artificial Intelligence has proven to be a transformative tool for advancing scientific research across a wide range of disciplines. However, a significant gap still exists between AI and scientific communities, limiting the full potential of AI methods in driving broad scientific discovery. Existing efforts in bridging this gap have often relied on qualitative examination of small samples of literature, offering a limited perspective on the broader AI4Science landscape. In this work, we present a large-scale analysis of the AI4Science literature, starting by using large language models to identify scientific problems and AI methods in publications from top science and AI venues. Leveraging this new dataset, we quantitatively highlight key disparities between AI methods and scientific problems in this integrated space, revealing substantial opportunities for deeper AI integration across scientific disciplines. Furthermore, we explore the potential and challenges of facilitating collaboration between AI and scientific communities through the lens of link prediction. Our findings and tools aim to promote more impactful interdisciplinary collaborations and accelerate scientific discovery through deeper and broader AI integration.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "2",
        "title": "What does AI consider praiseworthy?",
        "author": [
            "Andrew J. Peterson"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09630",
        "abstract": "As large language models (LLMs) are increasingly used for work, personal, and therapeutic purposes, researchers have begun to investigate these models' implicit and explicit moral views. Previous work, however, focuses on asking LLMs to state opinions, or on other technical evaluations that do not reflect common user interactions. We propose a novel evaluation of LLM behavior that analyzes responses to user-stated intentions, such as \"I'm thinking of campaigning for {candidate}.\" LLMs frequently respond with critiques or praise, often beginning responses with phrases such as \"That's great to hear!...\" While this makes them friendly, these praise responses are not universal and thus reflect a normative stance by the LLM. We map out the moral landscape of LLMs in how they respond to user statements in different domains including politics and everyday ethical actions. In particular, although a naive analysis might suggest LLMs are biased against right-leaning politics, our findings indicate that the bias is primarily against untrustworthy sources. Second, we find strong alignment across models for a range of ethical actions, but that doing so requires them to engage in high levels of praise and critique of users. Finally, our experiment on statements about world leaders finds no evidence of bias favoring the country of origin of the models. We conclude that as AI systems become more integrated into society, their use of praise, criticism, and neutrality must be carefully monitored to mitigate unintended psychological or societal impacts.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "3",
        "title": "Methods to Assess the UK Government's Current Role as a Data Provider for AI",
        "author": [
            "Neil Majithia",
            "Elena Simperl"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09632",
        "abstract": "The compositions of generative AI training corpora remain closely-guarded secrets, causing an asymmetry of information between AI developers and organisational data owners whose digital assets may have been incorporated into the corpora without their knowledge. While this asymmetry is the subject of well-known ongoing lawsuits, it also inhibits the measurement of the impact of open data sources for AI training. To address this, we introduce and implement two methods to assess open data usage for the training of Large Language Models (LLMs) and 'peek behind the curtain' in order to observe the UK government's current contributions as a data provider for AI. The first method, an ablation study that utilises LLM 'unlearning', seeks to examine the importance of the information held on UK government websites for LLMs and their performance in citizen query tasks. The second method, an information leakage study, seeks to ascertain whether LLMs are aware of the information held in the datasets published on the UK government's open data initiative http://data.gov.uk. Our findings indicate that UK government websites are important data sources for AI (heterogenously across subject matters) while http://data.gov.uk is not. This paper serves as a technical report, explaining in-depth the designs, mechanics, and limitations of the above experiments. It is accompanied by a complementary non-technical report on the ODI website in which we summarise the experiments and key findings, interpret them, and build a set of actionable recommendations for the UK government to take forward as it seeks to design AI policy. While we focus on UK open government data, we believe that the methods introduced in this paper present a reproducible approach to tackle the opaqueness of AI training corpora and provide organisations a framework to evaluate and maximize their contributions to AI development.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "4",
        "title": "Blockchain Data Analysis in the Era of Large-Language Models",
        "author": [
            "Kentaroh Toyoda",
            "Xiao Wang",
            "Mingzhe Li",
            "Bo Gao",
            "Yuan Wang",
            "Qingsong Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09640",
        "abstract": "Blockchain data analysis is essential for deriving insights, tracking transactions, identifying patterns, and ensuring the integrity and security of decentralized networks. It plays a key role in various areas, such as fraud detection, regulatory compliance, smart contract auditing, and decentralized finance (DeFi) risk management. However, existing blockchain data analysis tools face challenges, including data scarcity, the lack of generalizability, and the lack of reasoning capability.\nWe believe large language models (LLMs) can mitigate these challenges; however, we have not seen papers discussing LLM integration in blockchain data analysis in a comprehensive and systematic way. This paper systematically explores potential techniques and design patterns in LLM-integrated blockchain data analysis. We also outline prospective research opportunities and challenges, emphasizing the need for further exploration in this promising field. This paper aims to benefit a diverse audience spanning academia, industry, and policy-making, offering valuable insights into the integration of LLMs in blockchain data analysis.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "5",
        "title": "Combining knowledge graphs and LLMs for hazardous chemical information management and reuse",
        "author": [
            "Marcos Da Silveira",
            "Louis Deladiennee",
            "Kheira Acem",
            "Oona Freudenthal"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09644",
        "abstract": "Human health is increasingly threatened by exposure to hazardous substances, particularly persistent and toxic chemicals. The link between these substances, often encountered in complex mixtures, and various diseases are demonstrated in scientific studies. However, this information is scattered across several sources and hardly accessible by humans and machines. This paper evaluates current practices for publishing/accessing information on hazardous chemicals and proposes a novel platform designed to facilitate retrieval of critical chemical data in urgent situations. The platform aggregates information from multiple sources and organizes it into a structured knowledge graph. Users can access this information through a visual interface such as Neo4J Bloom and dashboards, or via natural language queries using a Chatbot. Our findings demonstrate a significant reduction in the time and effort required to access vital chemical information when datasets follow FAIR principles. Furthermore, we discuss the lessons learned from the development and implementation of this platform and provide recommendations for data owners and publishers to enhance data reuse and interoperability. This work aims to improve the accessibility and usability of chemical information by healthcare professionals, thereby supporting better health outcomes and informed decision-making in the face of patients exposed to chemical intoxication risks.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "6",
        "title": "Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models",
        "author": [
            "Fan Zhang",
            "Shulin Tian",
            "Ziqi Huang",
            "Yu Qiao",
            "Ziwei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09645",
        "abstract": "Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of a model's capabilities by observing only a few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only a few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "7",
        "title": "From Noise to Nuance: Advances in Deep Generative Image Models",
        "author": [
            "Benji Peng",
            "Chia Xin Liang",
            "Ziqian Bi",
            "Ming Liu",
            "Yichao Zhang",
            "Tianyang Wang",
            "Keyu Chen",
            "Xinyuan Song",
            "Pohsun Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09656",
        "abstract": "Deep learning-based image generation has undergone a paradigm shift since 2021, marked by fundamental architectural breakthroughs and computational innovations. Through reviewing architectural innovations and empirical results, this paper analyzes the transition from traditional generative methods to advanced architectures, with focus on compute-efficient diffusion models and vision transformer architectures. We examine how recent developments in Stable Diffusion, DALL-E, and consistency models have redefined the capabilities and performance boundaries of image synthesis, while addressing persistent challenges in efficiency and quality. Our analysis focuses on the evolution of latent space representations, cross-attention mechanisms, and parameter-efficient training methodologies that enable accelerated inference under resource constraints. While more efficient training methods enable faster inference, advanced control mechanisms like ControlNet and regional attention systems have simultaneously improved generation precision and content customization. We investigate how enhanced multi-modal understanding and zero-shot generation capabilities are reshaping practical applications across industries. Our analysis demonstrates that despite remarkable advances in generation quality and computational efficiency, critical challenges remain in developing resource-conscious architectures and interpretable generation systems for industrial applications. The paper concludes by mapping promising research directions, including neural architecture optimization and explainable generation frameworks.",
        "tags": [
            "Consistency Models",
            "ControlNet",
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "8",
        "title": "SEGT: A General Spatial Expansion Group Transformer for nuScenes Lidar-based Object Detection Task",
        "author": [
            "Cheng Mei",
            "Hao He",
            "Yahui Liu",
            "Zhenhua Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09658",
        "abstract": "In the technical report, we present a novel transformer-based framework for nuScenes lidar-based object detection task, termed Spatial Expansion Group Transformer (SEGT). To efficiently handle the irregular and sparse nature of point cloud, we propose migrating the voxels into distinct specialized ordered fields with the general spatial expansion strategies, and employ group attention mechanisms to extract the exclusive feature maps within each field. Subsequently, we integrate the feature representations across different ordered fields by alternately applying diverse expansion strategies, thereby enhancing the model's ability to capture comprehensive spatial information. The method was evaluated on the nuScenes lidar-based object detection test dataset, achieving an NDS score of 73.5 without Test-Time Augmentation (TTA) and 74.2 with TTA, demonstrating the effectiveness of the proposed method.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "9",
        "title": "Systematic Analysis of LLM Contributions to Planning: Solver, Verifier, Heuristic",
        "author": [
            "Haoming Li",
            "Zhaoliang Chen",
            "Songyuan Liu",
            "Yiming Lu",
            "Fei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09666",
        "abstract": "In this work, we provide a systematic analysis of how large language models (LLMs) contribute to solving planning problems. In particular, we examine how LLMs perform when they are used as problem solver, solution verifier, and heuristic guidance to improve intermediate solutions. Our analysis reveals that although it is difficult for LLMs to generate correct plans out-of-the-box, LLMs are much better at providing feedback signals to intermediate/incomplete solutions in the form of comparative heuristic functions. This evaluation framework provides insights into how future work may design better LLM-based tree-search algorithms to solve diverse planning and reasoning problems. We also propose a novel benchmark to evaluate LLM's ability to learn user preferences on the fly, which has wide applications in practical settings.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "10",
        "title": "Vision-Language Models Represent Darker-Skinned Black Individuals as More Homogeneous than Lighter-Skinned Black Individuals",
        "author": [
            "Messi H.J. Lee",
            "Soyeon Jeon"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09668",
        "abstract": "Vision-Language Models (VLMs) combine Large Language Model (LLM) capabilities with image processing, enabling tasks like image captioning and text-to-image generation. Yet concerns persist about their potential to amplify human-like biases, including skin tone bias. Skin tone bias, where darker-skinned individuals face more negative stereotyping than lighter-skinned individuals, is well-documented in the social sciences but remains under-explored in Artificial Intelligence (AI), particularly in VLMs. While well-documented in the social sciences, this bias remains under-explored in AI, particularly in VLMs. Using the GAN Face Database, we sampled computer-generated images of Black American men and women, controlling for skin tone variations while keeping other features constant. We then asked VLMs to write stories about these faces and compared the homogeneity of the generated stories. Stories generated by VLMs about darker-skinned Black individuals were more homogeneous than those about lighter-skinned individuals in three of four models, and Black women were consistently represented more homogeneously than Black men across all models. Interaction effects revealed a greater impact of skin tone on women in two VLMs, while the other two showed nonsignificant results, reflecting known stereotyping patterns. These findings underscore the propagation of biases from single-modality AI systems to multimodal models and highlight the need for further research to address intersectional biases in AI.",
        "tags": [
            "GAN",
            "Text-to-Image"
        ]
    },
    {
        "id": "11",
        "title": "PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields",
        "author": [
            "Sean Wu",
            "Shamik Basu",
            "Tim Broedermann",
            "Luc Van Gool",
            "Christos Sakaridis"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09680",
        "abstract": "We tackle the ill-posed inverse rendering problem in 3D reconstruction with a Neural Radiance Field (NeRF) approach informed by Physics-Based Rendering (PBR) theory, named PBR-NeRF. Our method addresses a key limitation in most NeRF and 3D Gaussian Splatting approaches: they estimate view-dependent appearance without modeling scene materials and illumination. To address this limitation, we present an inverse rendering (IR) model capable of jointly estimating scene geometry, materials, and illumination. Our model builds upon recent NeRF-based IR approaches, but crucially introduces two novel physics-based priors that better constrain the IR estimation. Our priors are rigorously formulated as intuitive loss terms and achieve state-of-the-art material estimation without compromising novel view synthesis quality. Our method is easily adaptable to other inverse rendering and 3D reconstruction frameworks that require material estimation. We demonstrate the importance of extending current neural rendering approaches to fully model scene properties beyond geometry and view-dependent appearance. Code is publicly available at https://github.com/s3anwu/pbrnerf",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "12",
        "title": "Omni-ID: Holistic Identity Representation Designed for Generative Tasks",
        "author": [
            "Guocheng Qian",
            "Kuan-Chieh Wang",
            "Or Patashnik",
            "Negin Heravi",
            "Daniil Ostashev",
            "Sergey Tulyakov",
            "Daniel Cohen-Or",
            "Kfir Aberman"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09694",
        "abstract": "We introduce Omni-ID, a novel facial representation designed specifically for generative tasks. Omni-ID encodes holistic information about an individual's appearance across diverse expressions and poses within a fixed-size representation. It consolidates information from a varied number of unstructured input images into a structured representation, where each entry represents certain global or local identity features. Our approach uses a few-to-many identity reconstruction training paradigm, where a limited set of input images is used to reconstruct multiple target images of the same individual in various poses and expressions. A multi-decoder framework is further employed to leverage the complementary strengths of diverse decoders during training. Unlike conventional representations, such as CLIP and ArcFace, which are typically learned through discriminative or contrastive objectives, Omni-ID is optimized with a generative objective, resulting in a more comprehensive and nuanced identity capture for generative tasks. Trained on our MFHQ dataset -- a multi-view facial image collection, Omni-ID demonstrates substantial improvements over conventional representations across various generative tasks.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "13",
        "title": "CUAL: Continual Uncertainty-aware Active Learner",
        "author": [
            "Amanda Rios",
            "Ibrahima Ndiour",
            "Parual Datta",
            "Jerry Sydir",
            "Omesh Tickoo",
            "Nilesh Ahuja"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09701",
        "abstract": "AI deployed in many real-world use cases should be capable of adapting to novelties encountered after deployment. Here, we consider a challenging, under-explored and realistic continual adaptation problem: a deployed AI agent is continuously provided with unlabeled data that may contain not only unseen samples of known classes but also samples from novel (unknown) classes. In such a challenging setting, it has only a tiny labeling budget to query the most informative samples to help it continuously learn. We present a comprehensive solution to this complex problem with our model \"CUAL\" (Continual Uncertainty-aware Active Learner). CUAL leverages an uncertainty estimation algorithm to prioritize active labeling of ambiguous (uncertain) predicted novel class samples while also simultaneously pseudo-labeling the most certain predictions of each class. Evaluations across multiple datasets, ablations, settings and backbones (e.g. ViT foundation model) demonstrate our method's effectiveness. We will release our code upon acceptance.",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "14",
        "title": "Diffusion-Enhanced Test-time Adaptation with Text and Image Augmentation",
        "author": [
            "Chun-Mei Feng",
            "Yuanyang He",
            "Jian Zou",
            "Salman Khan",
            "Huan Xiong",
            "Zhen Li",
            "Wangmeng Zuo",
            "Rick Siow Mong Goh",
            "Yong Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09706",
        "abstract": "Existing test-time prompt tuning (TPT) methods focus on single-modality data, primarily enhancing images and using confidence ratings to filter out inaccurate images. However, while image generation models can produce visually diverse images, single-modality data enhancement techniques still fail to capture the comprehensive knowledge provided by different modalities. Additionally, we note that the performance of TPT-based methods drops significantly when the number of augmented images is limited, which is not unusual given the computational expense of generative augmentation. To address these issues, we introduce IT3A, a novel test-time adaptation method that utilizes a pre-trained generative model for multi-modal augmentation of each test sample from unknown new domains. By combining augmented data from pre-trained vision and language models, we enhance the ability of the model to adapt to unknown new test data. Additionally, to ensure that key semantics are accurately retained when generating various visual and text enhancements, we employ cosine similarity filtering between the logits of the enhanced images and text with the original test data. This process allows us to filter out some spurious augmentation and inadequate combinations. To leverage the diverse enhancements provided by the generation model across different modals, we have replaced prompt tuning with an adapter for greater flexibility in utilizing text templates. Our experiments on the test datasets with distribution shifts and domain gaps show that in a zero-shot setting, IT3A outperforms state-of-the-art test-time prompt tuning methods with a 5.50% increase in accuracy.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "15",
        "title": "DiP: A Scalable, Energy-Efficient Systolic Array for Matrix Multiplication Acceleration",
        "author": [
            "Ahmed J. Abdelmaksoud",
            "Shady Agwa",
            "Themis Prodromakis"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09709",
        "abstract": "Transformers are gaining increasing attention across different application domains due to their outstanding accuracy. However, these data-intensive models add significant performance demands to the existing computing architectures. Systolic arrays are spatial architectures that have been adopted by commercial AI computing platforms (like Google TPUs), due to their energy-efficient approach of data-reusability. However, these spatial architectures face a penalty in throughput and energy efficiency due to the need for input and output synchronization using First-In-First-Out (FIFO) buffers. This paper proposes a novel scalable systolic-array architecture featuring Diagonal-Input and Permutated weight-stationary (DiP) dataflow for the acceleration of matrix multiplication. The proposed architecture eliminates the synchronization FIFOs required by state-of-the-art weight stationary systolic arrays. Aside from the area, power, and energy savings achieved by eliminating these FIFOs, DiP architecture maximizes the computational resources (PEs) utilization. Thus, it outperforms the weight-stationary counterparts in terms of throughput by up to 50%. A comprehensive hardware design space exploration is demonstrated using commercial 22nm technology, highlighting the scalability advantages of DiP over the conventional approach across various dimensions where DiP offers improvement of energy efficiency per area up to 2.02x. Furthermore, DiP is evaluated using various transformer workloads from widely-used models, consistently outperforming TPU-like architectures, achieving energy improvements of up to 1.81x and latency improvements of up to 1.49x across a range of transformer workloads. At a 64x64 size with 4096 PEs, DiP achieves a peak performance of 8.2 TOPS with energy efficiency 9.55 TOPS/W.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "16",
        "title": "Human vs. AI: A Novel Benchmark and a Comparative Study on the Detection of Generated Images and the Impact of Prompts",
        "author": [
            "Philipp Moeßner",
            "Heike Adel"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09715",
        "abstract": "With the advent of publicly available AI-based text-to-image systems, the process of creating photorealistic but fully synthetic images has been largely democratized. This can pose a threat to the public through a simplified spread of disinformation. Machine detectors and human media expertise can help to differentiate between AI-generated (fake) and real images and counteract this danger. Although AI generation models are highly prompt-dependent, the impact of the prompt on the fake detection performance has rarely been investigated yet. This work therefore examines the influence of the prompt's level of detail on the detectability of fake images, both with an AI detector and in a user study. For this purpose, we create a novel dataset, COCOXGEN, which consists of real photos from the COCO dataset as well as images generated with SDXL and Fooocus using prompts of two standardized lengths. Our user study with 200 participants shows that images generated with longer, more detailed prompts are detected significantly more easily than those generated with short prompts. Similarly, an AI-based detection model achieves better performance on images generated with longer prompts. However, humans and AI models seem to pay attention to different details, as we show in a heat map analysis.",
        "tags": [
            "Detection",
            "SDXL",
            "Text-to-Image"
        ]
    },
    {
        "id": "17",
        "title": "BayesAdapter: enhanced uncertainty estimation in CLIP few-shot adaptation",
        "author": [
            "Pablo Morales-Álvarez",
            "Stergios Christodoulidis",
            "Maria Vakalopoulou",
            "Pablo Piantanida",
            "Jose Dolz"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09718",
        "abstract": "The emergence of large pre-trained vision-language models (VLMs) represents a paradigm shift in machine learning, with unprecedented results in a broad span of visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited remarkable zero-shot and transfer learning capabilities in classification. To transfer CLIP to downstream tasks, adapters constitute a parameter-efficient approach that avoids backpropagation through the large model (unlike related prompt learning methods). However, CLIP adapters have been developed to target discriminative performance, and the quality of their uncertainty estimates has been overlooked. In this work we show that the discriminative performance of state-of-the-art CLIP adapters does not always correlate with their uncertainty estimation capabilities, which are essential for a safe deployment in real-world scenarios. We also demonstrate that one of such adapters is obtained through MAP inference from a more general probabilistic framework. Based on this observation we introduce BayesAdapter, which leverages Bayesian inference to estimate a full probability distribution instead of a single point, better capturing the variability inherent in the parameter space. In a comprehensive empirical evaluation we show that our approach obtains high quality uncertainty estimates in the predictions, standing out in calibration and selective classification. Our code is publicly available at: https://github.com/pablomorales92/BayesAdapter.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "18",
        "title": "GReaTer: Gradients over Reasoning Makes Smaller Language Models Strong Prompt Optimizers",
        "author": [
            "Sarkar Snigdha Sarathi Das",
            "Ryo Kamoi",
            "Bo Pang",
            "Yusen Zhang",
            "Caiming Xiong",
            "Rui Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09722",
        "abstract": "The effectiveness of large language models (LLMs) is closely tied to the design of prompts, making prompt optimization essential for enhancing their performance across a wide range of tasks. Many existing approaches to automating prompt engineering rely exclusively on textual feedback, refining prompts based solely on inference errors identified by large, computationally expensive LLMs. Unfortunately, smaller models struggle to generate high-quality feedback, resulting in complete dependence on large LLM judgment. Moreover, these methods fail to leverage more direct and finer-grained information, such as gradients, due to operating purely in text space. To this end, we introduce GReaTer, a novel prompt optimization technique that directly incorporates gradient information over task-specific reasoning. By utilizing task loss gradients, GReaTer enables self-optimization of prompts for open-source, lightweight language models without the need for costly closed-source LLMs. This allows high-performance prompt optimization without dependence on massive LLMs, closing the gap between smaller models and the sophisticated reasoning often needed for prompt refinement. Extensive evaluations across diverse reasoning tasks including BBH, GSM8k, and FOLIO demonstrate that GReaTer consistently outperforms previous state-of-the-art prompt optimization methods, even those reliant on powerful LLMs. Additionally, GReaTer-optimized prompts frequently exhibit better transferability and, in some cases, boost task performance to levels comparable to or surpassing those achieved by larger language models, highlighting the effectiveness of prompt optimization guided by gradients over reasoning. Code of GReaTer is available at https://github.com/psunlpgroup/GreaTer.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "19",
        "title": "The Unreasonable Effectiveness of Gaussian Score Approximation for Diffusion Models and its Applications",
        "author": [
            "Binxu Wang",
            "John J. Vastola"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09726",
        "abstract": "By learning the gradient of smoothed data distributions, diffusion models can iteratively generate samples from complex distributions. The learned score function enables their generalization capabilities, but how the learned score relates to the score of the underlying data manifold remains largely unclear. Here, we aim to elucidate this relationship by comparing learned neural scores to the scores of two kinds of analytically tractable distributions: Gaussians and Gaussian mixtures. The simplicity of the Gaussian model makes it theoretically attractive, and we show that it admits a closed-form solution and predicts many qualitative aspects of sample generation dynamics. We claim that the learned neural score is dominated by its linear (Gaussian) approximation for moderate to high noise scales, and supply both theoretical and empirical arguments to support this claim. Moreover, the Gaussian approximation empirically works for a larger range of noise scales than naive theory suggests it should, and is preferentially learned early in training. At smaller noise scales, we observe that learned scores are better described by a coarse-grained (Gaussian mixture) approximation of training data than by the score of the training distribution, a finding consistent with generalization. Our findings enable us to precisely predict the initial phase of trained models' sampling trajectories through their Gaussian approximations. We show that this allows the skipping of the first 15-30% of sampling steps while maintaining high sample quality (with a near state-of-the-art FID score of 1.93 on CIFAR-10 unconditional generation). This forms the foundation of a novel hybrid sampling method, termed analytical teleportation, which can seamlessly integrate with and accelerate existing samplers, including DPM-Solver-v3 and UniPC. Our findings suggest ways to improve the design and training of diffusion models.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "20",
        "title": "Should We Learn Contact-Rich Manipulation Policies from Sampling-Based Planners?",
        "author": [
            "Huaijiang Zhu",
            "Tong Zhao",
            "Xinpei Ni",
            "Jiuguang Wang",
            "Kuan Fang",
            "Ludovic Righetti",
            "Tao Pang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09743",
        "abstract": "The tremendous success of behavior cloning (BC) in robotic manipulation has been largely confined to tasks where demonstrations can be effectively collected through human teleoperation. However, demonstrations for contact-rich manipulation tasks that require complex coordination of multiple contacts are difficult to collect due to the limitations of current teleoperation interfaces. We investigate how to leverage model-based planning and optimization to generate training data for contact-rich dexterous manipulation tasks. Our analysis reveals that popular sampling-based planners like rapidly exploring random tree (RRT), while efficient for motion planning, produce demonstrations with unfavorably high entropy. This motivates modifications to our data generation pipeline that prioritizes demonstration consistency while maintaining solution diversity. Combined with a diffusion-based goal-conditioned BC approach, our method enables effective policy learning and zero-shot transfer to hardware for two challenging contact-rich manipulation tasks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "21",
        "title": "AiEDA: Agentic AI Design Framework for Digital ASIC System Design",
        "author": [
            "Aditya Patra",
            "Saroj Rout",
            "Arun Ravindran"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09745",
        "abstract": "The paper addresses advancements in Generative Artificial Intelligence (GenAI) and digital chip design, highlighting the integration of Large Language Models (LLMs) in automating hardware description and design. LLMs, known for generating human-like content, are now being explored for creating hardware description languages (HDLs) like Verilog from natural language inputs. This approach aims to enhance productivity and reduce costs in VLSI system design. The study introduces \"AiEDA\", a proposed agentic design flow framework for digital ASIC systems, leveraging autonomous AI agents to manage complex design tasks. AiEDA is designed to streamline the transition from conceptual design to GDSII layout using an open-source toolchain. The framework is demonstrated through the design of an ultra-low-power digital ASIC for KeyWord Spotting (KWS). The use of agentic AI workflows promises to improve design efficiency by automating the integration of multiple design tools, thereby accelerating the development process and addressing the complexities of hardware design.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "22",
        "title": "ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation",
        "author": [
            "Ali Athar",
            "Xueqing Deng",
            "Liang-Chieh Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09754",
        "abstract": "Recent advances in multimodal large language models (MLLMs) have expanded research in video understanding, primarily focusing on high-level tasks such as video captioning and question-answering. Meanwhile, a smaller body of work addresses dense, pixel-precise segmentation tasks, which typically involve category-guided or referral-based object segmentation. Although both research directions are essential for developing models with human-level video comprehension, they have largely evolved separately, with distinct benchmarks and architectures. This paper aims to unify these efforts by introducing ViCaS, a new dataset containing thousands of challenging videos, each annotated with detailed, human-written captions and temporally consistent, pixel-accurate masks for multiple objects with phrase grounding. Our benchmark evaluates models on both holistic/high-level understanding and language-guided, pixel-precise segmentation. We also present carefully validated evaluation measures and propose an effective model architecture that can tackle our benchmark. Project page: https://ali2500.github.io/vicas-project/",
        "tags": [
            "Large Language Models",
            "Segmentation"
        ]
    },
    {
        "id": "23",
        "title": "OpenForge: Probabilistic Metadata Integration",
        "author": [
            "Tianji Cong",
            "Fatemeh Nargesian",
            "Junjie Xing",
            "H. V. Jagadish"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09788",
        "abstract": "Modern data stores increasingly rely on metadata for enabling diverse activities such as data cataloging and search. However, metadata curation remains a labor-intensive task, and the broader challenge of metadata maintenance -- ensuring its consistency, usefulness, and freshness -- has been largely overlooked. In this work, we tackle the problem of resolving relationships among metadata concepts from disparate sources. These relationships are critical for creating clean, consistent, and up-to-date metadata repositories, and a central challenge for metadata integration.\nWe propose OpenForge, a two-stage prior-posterior framework for metadata integration. In the first stage, OpenForge exploits multiple methods including fine-tuned large language models to obtain prior beliefs about concept relationships. In the second stage, OpenForge refines these predictions by leveraging Markov Random Field, a probabilistic graphical model. We formalize metadata integration as an optimization problem, where the objective is to identify the relationship assignments that maximize the joint probability of assignments. The MRF formulation allows OpenForge to capture prior beliefs while encoding critical relationship properties, such as transitivity, in probabilistic inference. Experiments on real-world datasets demonstrate the effectiveness and efficiency of OpenForge. On a use case of matching two metadata vocabularies, OpenForge outperforms GPT-4, the second-best method, by 25 F1-score points.",
        "tags": [
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "24",
        "title": "AutoPatent: A Multi-Agent Framework for Automatic Patent Generation",
        "author": [
            "Qiyao Wang",
            "Shiwen Ni",
            "Huaren Liu",
            "Shule Lu",
            "Guhong Chen",
            "Xi Feng",
            "Chi Wei",
            "Qiang Qu",
            "Hamid Alinejad-Rokny",
            "Yuan Lin",
            "Min Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09796",
        "abstract": "As the capabilities of Large Language Models (LLMs) continue to advance, the field of patent processing has garnered increased attention within the natural language processing community. However, the majority of research has been concentrated on classification tasks, such as patent categorization and examination, or on short text generation tasks like patent summarization and patent quizzes. In this paper, we introduce a novel and practical task known as Draft2Patent, along with its corresponding D2P benchmark, which challenges LLMs to generate full-length patents averaging 17K tokens based on initial drafts. Patents present a significant challenge to LLMs due to their specialized nature, standardized terminology, and extensive length. We propose a multi-agent framework called AutoPatent which leverages the LLM-based planner agent, writer agents, and examiner agent with PGTree and RRAG to generate lengthy, intricate, and high-quality complete patent documents. The experimental results demonstrate that our AutoPatent framework significantly enhances the ability to generate comprehensive patents across various LLMs. Furthermore, we have discovered that patents generated solely with the AutoPatent framework based on the Qwen2.5-7B model outperform those produced by larger and more powerful LLMs, such as GPT-4o, Qwen2.5-72B, and LLAMA3.1-70B, in both objective metrics and human evaluations. We will make the data and code available upon acceptance at \\url{https://github.com/QiYao-Wang/AutoPatent}.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "25",
        "title": "LLM Distillation for Efficient Few-Shot Multiple Choice Question Answering",
        "author": [
            "Patrick Sutanto",
            "Joan Santoso"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09807",
        "abstract": "Multiple Choice Question Answering (MCQA) is an important problem with numerous real-world applications, such as medicine, law, and education. The high cost of building MCQA datasets makes few-shot learning pivotal in this domain. While Large Language Models (LLMs) can enable few-shot learning, their direct application in real-world scenarios is often hindered by their high computational cost. To address this challenge, we propose a simple yet effective approach that uses LLMs for data generation and scoring. Our approach utilizes LLMs to create MCQA data which contains questions and choices, and to assign probability scores to the generated choices. We then use the generated data and LLM-assigned scores to finetune a smaller and more efficient encoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive experiments on the Massive Multitask Language Understanding (MMLU) benchmark demonstrate that our method improves accuracy from 28.9% to 39.3%, representing a gain of over 10% compared to a baseline finetuned directly on 5-shot examples. This shows the effectiveness of LLM-driven data generation and knowledge distillation for few-shot MCQA.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "26",
        "title": "ScaleOT: Privacy-utility-scalable Offsite-tuning with Dynamic LayerReplace and Selective Rank Compression",
        "author": [
            "Kai Yao",
            "Zhaorui Tan",
            "Tiandi Ye",
            "Lichun Li",
            "Yuan Zhao",
            "Wenyan Liu",
            "Wei Wang",
            "Jianke Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09812",
        "abstract": "Offsite-tuning is a privacy-preserving method for tuning large language models (LLMs) by sharing a lossy compressed emulator from the LLM owners with data owners for downstream task tuning. This approach protects the privacy of both the model and data owners. However, current offsite tuning methods often suffer from adaptation degradation, high computational costs, and limited protection strength due to uniformly dropping LLM layers or relying on expensive knowledge distillation. To address these issues, we propose ScaleOT, a novel privacy-utility-scalable offsite-tuning framework that effectively balances privacy and utility. ScaleOT introduces a novel layerwise lossy compression algorithm that uses reinforcement learning to obtain the importance of each layer. It employs lightweight networks, termed harmonizers, to replace the raw LLM layers. By combining important original LLM layers and harmonizers in different ratios, ScaleOT generates emulators tailored for optimal performance with various model scales for enhanced privacy protection. Additionally, we present a rank reduction method to further compress the original LLM layers, significantly enhancing privacy with negligible impact on utility. Comprehensive experiments show that ScaleOT can achieve nearly lossless offsite tuning performance compared with full fine-tuning while obtaining better model privacy.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "27",
        "title": "Enhancing Multimodal Large Language Models Complex Reason via Similarity Computation",
        "author": [
            "Xiaofeng Zhang",
            "Fanshuo Zeng",
            "Yihao Quan",
            "Zheng Hui",
            "Jiawei Yao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09817",
        "abstract": "Multimodal large language models have experienced rapid growth, and numerous different models have emerged. The interpretability of LVLMs remains an under-explored area. Especially when faced with more complex tasks such as chain-of-thought reasoning, its internal mechanisms still resemble a black box that is difficult to decipher. By studying the interaction and information flow between images and text, we noticed that in models such as LLaVA1.5, image tokens that are semantically related to text are more likely to have information flow convergence in the LLM decoding layer, and these image tokens receive higher attention scores. However, those image tokens that are less relevant to the text do not have information flow convergence, and they only get very small attention scores. To efficiently utilize the image information, we propose a new image token reduction method, Simignore, which aims to improve the complex reasoning ability of LVLMs by computing the similarity between image and text embeddings and ignoring image tokens that are irrelevant and unimportant to the text. Through extensive experiments, we demonstrate the effectiveness of our method for complex reasoning tasks. The paper's source code can be accessed from \\url{https://github.com/FanshuoZeng/Simignore}.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "28",
        "title": "MERaLiON-AudioLLM: Technical Report",
        "author": [
            "Yingxu He",
            "Zhuohan Liu",
            "Shuo Sun",
            "Bin Wang",
            "Wenyu Zhang",
            "Xunlong Zou",
            "Nancy F. Chen",
            "Ai Ti Aw"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09818",
        "abstract": "We introduce MERaLiON-AudioLLM (Multimodal Empathetic Reasoning and Learning in One Network), the first speech-text model tailored for Singapore's multilingual and multicultural landscape. Developed under the National Large Language Models Funding Initiative, Singapore, MERaLiON-AudioLLM integrates advanced speech and text processing to address the diverse linguistic nuances of local accents and dialects, enhancing accessibility and usability in complex, multilingual environments. Our results demonstrate improvements in both speech recognition and task-specific understanding, positioning MERaLiON-AudioLLM as a pioneering solution for region specific AI applications. We envision this release to set a precedent for future models designed to address localised linguistic and cultural contexts in a global framework.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "29",
        "title": "Dynamic Try-On: Taming Video Virtual Try-on with Dynamic Attention Mechanism",
        "author": [
            "Jun Zheng",
            "Jing Wang",
            "Fuwei Zhao",
            "Xujie Zhang",
            "Xiaodan Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09822",
        "abstract": "Video try-on stands as a promising area for its tremendous real-world potential. Previous research on video try-on has primarily focused on transferring product clothing images to videos with simple human poses, while performing poorly with complex movements. To better preserve clothing details, those approaches are armed with an additional garment encoder, resulting in higher computational resource consumption. The primary challenges in this domain are twofold: (1) leveraging the garment encoder's capabilities in video try-on while lowering computational requirements; (2) ensuring temporal consistency in the synthesis of human body parts, especially during rapid movements. To tackle these issues, we propose a novel video try-on framework based on Diffusion Transformer(DiT), named Dynamic Try-On.\nTo reduce computational overhead, we adopt a straightforward approach by utilizing the DiT backbone itself as the garment encoder and employing a dynamic feature fusion module to store and integrate garment features. To ensure temporal consistency of human body parts, we introduce a limb-aware dynamic attention module that enforces the DiT backbone to focus on the regions of human limbs during the denoising process. Extensive experiments demonstrate the superiority of Dynamic Try-On in generating stable and smooth try-on results, even for videos featuring complicated human postures.",
        "tags": [
            "DiT",
            "Diffusion",
            "Diffusion Transformer",
            "Transformer"
        ]
    },
    {
        "id": "30",
        "title": "Low-Rank Adaptation with Task-Relevant Feature Enhancement for Fine-tuning Language Models",
        "author": [
            "Changqun Li",
            "Chaofan Ding",
            "Kexin Luan",
            "Xinhan Di"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09827",
        "abstract": "Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low dimensional. Although LoRA has demonstrated commendable performance, there remains a significant performance gap between LoRA and full fine-tuning when learning new tasks. In this work, we propose Low-Rank Adaptation with Task-Relevant Feature Enhancement(LoRATRF) for enhancing task-relevant features from the perspective of editing neural network representations. To prioritize task-relevant features, a task-aware filter that selectively extracts valuable knowledge from hidden representations for the target or current task is designed. As the experiments on a vareity of datasets including NLU, commonsense reasoning and mathematical reasoning tasks demonstrates, our method reduces 33.71% parameters and achieves better performance on a variety of datasets in comparison with SOTA low-rank methods.",
        "tags": [
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "31",
        "title": "MSC: Multi-Scale Spatio-Temporal Causal Attention for Autoregressive Video Diffusion",
        "author": [
            "Xunnong Xu",
            "Mengying Cao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09828",
        "abstract": "Diffusion transformers enable flexible generative modeling for video. However, it is still technically challenging and computationally expensive to generate high-resolution videos with rich semantics and complex motion. Similar to languages, video data are also auto-regressive by nature, so it is counter-intuitive to use attention mechanism with bi-directional dependency in the model. Here we propose a Multi-Scale Causal (MSC) framework to address these problems. Specifically, we introduce multiple resolutions in the spatial dimension and high-low frequencies in the temporal dimension to realize efficient attention calculation. Furthermore, attention blocks on multiple scales are combined in a controlled way to allow causal conditioning on noisy image frames for diffusion training, based on the idea that noise destroys information at different rates on different resolutions. We theoretically show that our approach can greatly reduce the computational complexity and enhance the efficiency of training. The causal attention diffusion framework can also be used for auto-regressive long video generation, without violating the natural order of frame sequences.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "32",
        "title": "Leveraging Programmatically Generated Synthetic Data for Differentially Private Diffusion Training",
        "author": [
            "Yujin Choi",
            "Jinseong Park",
            "Junyoung Byun",
            "Jaewook Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09842",
        "abstract": "Programmatically generated synthetic data has been used in differential private training for classification to enhance performance without privacy leakage. However, as the synthetic data is generated from a random process, the distribution of real data and the synthetic data are distinguishable and difficult to transfer. Therefore, the model trained with the synthetic data generates unrealistic random images, raising challenges to adapt the synthetic data for generative models. In this work, we propose DP-SynGen, which leverages programmatically generated synthetic data in diffusion models to address this challenge. By exploiting the three stages of diffusion models(coarse, context, and cleaning) we identify stages where synthetic data can be effectively utilized. We theoretically and empirically verified that cleaning and coarse stages can be trained without private data, replacing them with synthetic data to reduce the privacy budget. The experimental results show that DP-SynGen improves the quality of generative data by mitigating the negative impact of privacy-induced noise on the generation process.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "33",
        "title": "Learning Structural Causal Models from Ordering: Identifiable Flow Models",
        "author": [
            "Minh Khoa Le",
            "Kien Do",
            "Truyen Tran"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09843",
        "abstract": "In this study, we address causal inference when only observational data and a valid causal ordering from the causal graph are available. We introduce a set of flow models that can recover component-wise, invertible transformation of exogenous variables. Our flow-based methods offer flexible model design while maintaining causal consistency regardless of the number of discretization steps. We propose design improvements that enable simultaneous learning of all causal mechanisms and reduce abduction and prediction complexity to linear O(n) relative to the number of layers, independent of the number of causal variables. Empirically, we demonstrate that our method outperforms previous state-of-the-art approaches and delivers consistent performance across a wide range of structural causal models in answering observational, interventional, and counterfactual questions. Additionally, our method achieves a significant reduction in computational time compared to existing diffusion-based techniques, making it practical for large structural causal models.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "34",
        "title": "Real-time Identity Defenses against Malicious Personalization of Diffusion Models",
        "author": [
            "Hanzhong Guo",
            "Shen Nie",
            "Chao Du",
            "Tianyu Pang",
            "Hao Sun",
            "Chongxuan Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09844",
        "abstract": "Personalized diffusion models, capable of synthesizing highly realistic images based on a few reference portraits, pose substantial social, ethical, and legal risks by enabling identity replication. Existing defense mechanisms rely on computationally intensive adversarial perturbations tailored to individual images, rendering them impractical for real-world deployment. This study introduces Real-time Identity Defender (RID), a neural network designed to generate adversarial perturbations through a single forward pass, bypassing the need for image-specific optimization. RID achieves unprecedented efficiency, with defense times as low as 0.12 seconds on a single GPU (4,400 times faster than leading methods) and 1.1 seconds per image on a standard Intel i9 CPU, making it suitable for edge devices such as smartphones. Despite its efficiency, RID matches state-of-the-art performance across visual and quantitative benchmarks, effectively mitigating identity replication risks. Our analysis reveals that RID's perturbations mimic the efficacy of traditional defenses while exhibiting properties distinct from natural noise, such as Gaussian perturbations. To enhance robustness, we extend RID into an ensemble framework that integrates multiple pre-trained text-to-image diffusion models, ensuring resilience against black-box attacks and post-processing techniques, including JPEG compression and diffusion-based purification.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "35",
        "title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity",
        "author": [
            "Hongjie Wang",
            "Chih-Yao Ma",
            "Yen-Cheng Liu",
            "Ji Hou",
            "Tao Xu",
            "Jialiang Wang",
            "Felix Juefei-Xu",
            "Yaqiao Luo",
            "Peizhao Zhang",
            "Tingbo Hou",
            "Peter Vajda",
            "Niraj K. Jha",
            "Xiaoliang Dai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09856",
        "abstract": "Text-to-video generation enhances content creation but is highly computationally intensive: The computational cost of Diffusion Transformers (DiTs) scales quadratically in the number of pixels. This makes minute-length video generation extremely expensive, limiting most existing models to generating videos of only 10-20 seconds length. We propose a Linear-complexity text-to-video Generation (LinGen) framework whose cost scales linearly in the number of pixels. For the first time, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. It replaces the computationally-dominant and quadratic-complexity block, self-attention, with a linear-complexity block called MATE, which consists of an MA-branch and a TE-branch. The MA-branch targets short-to-long-range correlations, combining a bidirectional Mamba2 block with our token rearrangement method, Rotary Major Scan, and our review tokens developed for long video generation. The TE-branch is a novel TEmporal Swin Attention block that focuses on temporal correlations between adjacent tokens and medium-range tokens. The MATE block addresses the adjacency preservation issue of Mamba and improves the consistency of generated videos significantly. Experimental results show that LinGen outperforms DiT (with a 75.6% win rate) in video quality with up to 15$\\times$ (11.5$\\times$) FLOPs (latency) reduction. Furthermore, both automatic metrics and human evaluation demonstrate our LinGen-4B yields comparable video quality to state-of-the-art models (with a 50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling, respectively). This paves the way to hour-length movie generation and real-time interactive video generation. We provide 68s video generation results and more examples in our project website: https://lineargen.github.io/.",
        "tags": [
            "DiT",
            "Diffusion",
            "Mamba",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "36",
        "title": "RP-SLAM: Real-time Photorealistic SLAM with Efficient 3D Gaussian Splatting",
        "author": [
            "Lizhi Bai",
            "Chunqi Tian",
            "Jun Yang",
            "Siyu Zhang",
            "Masanori Suganuma",
            "Takayuki Okatani"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09868",
        "abstract": "3D Gaussian Splatting has emerged as a promising technique for high-quality 3D rendering, leading to increasing interest in integrating 3DGS into realism SLAM systems. However, existing methods face challenges such as Gaussian primitives redundancy, forgetting problem during continuous optimization, and difficulty in initializing primitives in monocular case due to lack of depth information. In order to achieve efficient and photorealistic mapping, we propose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular and RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian primitives optimization and consists of three key components. Firstly, we propose an efficient incremental mapping approach to achieve a compact and accurate representation of the scene through adaptive sampling and Gaussian primitives filtering. Secondly, a dynamic window optimization method is proposed to mitigate the forgetting problem and improve map consistency. Finally, for the monocular case, a monocular keyframe initialization method based on sparse point cloud is proposed to improve the initialization accuracy of Gaussian primitives, which provides a geometric basis for subsequent optimization. The results of numerous experiments demonstrate that RP-SLAM achieves state-of-the-art map rendering accuracy while ensuring real-time performance and model compactness.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "SLAM"
        ]
    },
    {
        "id": "37",
        "title": "Byte Latent Transformer: Patches Scale Better Than Tokens",
        "author": [
            "Artidoro Pagnoni",
            "Ram Pasunuru",
            "Pedro Rodriguez",
            "John Nguyen",
            "Benjamin Muller",
            "Margaret Li",
            "Chunting Zhou",
            "Lili Yu",
            "Jason Weston",
            "Luke Zettlemoyer",
            "Gargi Ghosh",
            "Mike Lewis",
            "Ari Holtzman",
            "Srinivasan Iyer"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09871",
        "abstract": "We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "38",
        "title": "Selective State Space Memory for Large Vision-Language Models",
        "author": [
            "Chee Ng",
            "Yuen Fung"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09875",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance across a wide range of multimodal tasks. However, fine-tuning these models for domain-specific applications remains a computationally intensive challenge. This paper introduces State Space Memory Integration (SSMI), a novel approach for efficient fine-tuning of LVLMs. By integrating lightweight Mamba-based state space modules into the LVLM architecture, SSMI captures long-range dependencies and injects task-specific visual and sequential patterns effectively. Unlike traditional fine-tuning methods, SSMI requires only a fraction of the model's parameters to be updated, making it computationally efficient and scalable. Experiments on benchmark datasets, including COCO Captioning, VQA, and Flickr30k, demonstrate that SSMI achieves state-of-the-art performance while maintaining robustness and generalization capabilities. Comprehensive analysis further validates the advantages of SSMI in terms of efficiency, adaptability, and interpretability, positioning it as a compelling solution for fine-tuning large-scale vision-language models.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "39",
        "title": "On the Limit of Language Models as Planning Formalizers",
        "author": [
            "Cassie Huang",
            "Li Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09879",
        "abstract": "Large Language Models have been shown to fail to create executable and verifiable plans in grounded environments. An emerging line of work shows success in using LLM as a formalizer to generate a formal representation (e.g., PDDL) of the planning domain, which can be deterministically solved to find a plan. We systematically evaluate this methodology while bridging some major gaps. While previous work only generates a partial PDDL representation given templated and thus unrealistic environment descriptions, we generate the complete representation given descriptions of various naturalness levels. Among an array of observations critical to improve LLMs' formal planning ability, we note that large enough models can effectively formalize descriptions as PDDL, outperforming those directly generating plans, while being robust to lexical perturbation. As the descriptions become more natural-sounding, we observe a decrease in performance and provide detailed error analysis.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "40",
        "title": "Sharpening Your Density Fields: Spiking Neuron Aided Fast Geometry Learning",
        "author": [
            "Yi Gu",
            "Zhaorui Wang",
            "Dongjun Ye",
            "Renjing Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09881",
        "abstract": "Neural Radiance Fields (NeRF) have achieved remarkable progress in neural rendering. Extracting geometry from NeRF typically relies on the Marching Cubes algorithm, which uses a hand-crafted threshold to define the level set. However, this threshold-based approach requires laborious and scenario-specific tuning, limiting its practicality for real-world applications. In this work, we seek to enhance the efficiency of this method during the training time. To this end, we introduce a spiking neuron mechanism that dynamically adjusts the threshold, eliminating the need for manual selection. Despite its promise, directly training with the spiking neuron often results in model collapse and noisy outputs. To overcome these challenges, we propose a round-robin strategy that stabilizes the training process and enables the geometry network to achieve a sharper and more precise density distribution with minimal computational overhead. We validate our approach through extensive experiments on both synthetic and real-world datasets. The results show that our method significantly improves the performance of threshold-based techniques, offering a more robust and efficient solution for NeRF geometry extraction.",
        "tags": [
            "NeRF"
        ]
    },
    {
        "id": "41",
        "title": "Benchmarking Table Comprehension In The Wild",
        "author": [
            "Yikang Pan",
            "Yi Zhu",
            "Rand Xie",
            "Yizhi Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09884",
        "abstract": "Large Language Models (LLMs), while being increasingly dominant on a myriad of knowledge-intensive activities, have only had limited success understanding lengthy table-text mixtures, such as academic papers and financial reports. Recent advances of long-context LLMs have opened up new possibilities for this field. Nonetheless, we identify two roadblocks: (1) Prior benchmarks of table question answering (TableQA) have focused on isolated tables without context, making it hard to evaluate models in real-world scenarios. (2) Prior benchmarks have focused on some narrow skill sets of table comprehension such as table recognition, data manipulation/calculation, table summarization etc., while a skilled human employs those skills collectively. In this work, we introduce TableQuest, a new benchmark designed to evaluate the holistic table comprehension capabilities of LLMs in the natural table-rich context of financial reports. We employ a rigorous data processing and filtering procedure to ensure that the question-answer pairs are logical, reasonable, and diverse. We experiment with 7 state-of-the-art models, and find that despite reasonable accuracy in locating facts, they often falter when required to execute more sophisticated reasoning or multi-step calculations. We conclude with a qualitative study of the failure modes and discuss the challenges of constructing a challenging benchmark. We make the evaluation data, judging procedure and results of this study publicly available to facilitate research in this field.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "42",
        "title": "T-GMSI: A transformer-based generative model for spatial interpolation under sparse measurements",
        "author": [
            "Xiangxi Tian",
            "Jie Shan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09886",
        "abstract": "Generating continuous environmental models from sparsely sampled data is a critical challenge in spatial modeling, particularly for topography. Traditional spatial interpolation methods often struggle with handling sparse measurements. To address this, we propose a Transformer-based Generative Model for Spatial Interpolation (T-GMSI) using a vision transformer (ViT) architecture for digital elevation model (DEM) generation under sparse conditions. T-GMSI replaces traditional convolution-based methods with ViT for feature extraction and DEM interpolation while incorporating a terrain feature-aware loss function for enhanced accuracy. T-GMSI excels in producing high-quality elevation surfaces from datasets with over 70% sparsity and demonstrates strong transferability across diverse landscapes without fine-tuning. Its performance is validated through extensive experiments, outperforming traditional methods such as ordinary Kriging (OK) and natural neighbor (NN) and a conditional generative adversarial network (CGAN)-based model (CEDGAN). Compared to OK and NN, T-GMSI reduces root mean square error (RMSE) by 40% and 25% on airborne lidar data and by 23% and 10% on spaceborne lidar data. Against CEDGAN, T-GMSI achieves a 20% RMSE improvement on provided DEM data, requiring no fine-tuning. The ability of model on generalizing to large, unseen terrains underscores its transferability and potential applicability beyond topographic modeling. This research establishes T-GMSI as a state-of-the-art solution for spatial interpolation on sparse datasets and highlights its broader utility for other sparse data interpolation challenges.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "43",
        "title": "VQTalker: Towards Multilingual Talking Avatars through Facial Motion Tokenization",
        "author": [
            "Tao Liu",
            "Ziyang Ma",
            "Qi Chen",
            "Feilong Chen",
            "Shuai Fan",
            "Xie Chen",
            "Kai Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09892",
        "abstract": "We present VQTalker, a Vector Quantization-based framework for multilingual talking head generation that addresses the challenges of lip synchronization and natural motion across diverse languages. Our approach is grounded in the phonetic principle that human speech comprises a finite set of distinct sound units (phonemes) and corresponding visual articulations (visemes), which often share commonalities across languages. We introduce a facial motion tokenizer based on Group Residual Finite Scalar Quantization (GRFSQ), which creates a discretized representation of facial features. This method enables comprehensive capture of facial movements while improving generalization to multiple languages, even with limited training data. Building on this quantized representation, we implement a coarse-to-fine motion generation process that progressively refines facial animations. Extensive experiments demonstrate that VQTalker achieves state-of-the-art performance in both video-driven and speech-driven scenarios, particularly in multilingual settings. Notably, our method achieves high-quality results at a resolution of 512*512 pixels while maintaining a lower bitrate of approximately 11 kbps. Our work opens new possibilities for cross-lingual talking face generation. Synthetic results can be viewed at https://x-lance.github.io/VQTalker.",
        "tags": [
            "Talking Face",
            "Talking Head",
            "Vector Quantization"
        ]
    },
    {
        "id": "44",
        "title": "Building a Multi-modal Spatiotemporal Expert for Zero-shot Action Recognition with CLIP",
        "author": [
            "Yating Yu",
            "Congqi Cao",
            "Yueran Zhang",
            "Qinyi Lv",
            "Lingtong Min",
            "Yanning Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09895",
        "abstract": "Zero-shot action recognition (ZSAR) requires collaborative multi-modal spatiotemporal understanding. However, finetuning CLIP directly for ZSAR yields suboptimal performance, given its inherent constraints in capturing essential temporal dynamics from both vision and text perspectives, especially when encountering novel actions with fine-grained spatiotemporal discrepancies. In this work, we propose Spatiotemporal Dynamic Duo (STDD), a novel CLIP-based framework to comprehend multi-modal spatiotemporal dynamics synergistically. For the vision side, we propose an efficient Space-time Cross Attention, which captures spatiotemporal dynamics flexibly with simple yet effective operations applied before and after spatial attention, without adding additional parameters or increasing computational complexity. For the semantic side, we conduct spatiotemporal text augmentation by comprehensively constructing an Action Semantic Knowledge Graph (ASKG) to derive nuanced text prompts. The ASKG elaborates on static and dynamic concepts and their interrelations, based on the idea of decomposing actions into spatial appearances and temporal motions. During the training phase, the frame-level video representations are meticulously aligned with prompt-level nuanced text representations, which are concurrently regulated by the video representations from the frozen CLIP to enhance generalizability. Extensive experiments validate the effectiveness of our approach, which consistently surpasses state-of-the-art approaches on popular video benchmarks (i.e., Kinetics-600, UCF101, and HMDB51) under challenging ZSAR settings. Code is available at https://github.com/Mia-YatingYu/STDD.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "45",
        "title": "Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning",
        "author": [
            "Jing Bi",
            "Yuting Wu",
            "Weiwei Xing",
            "Zhenjie Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09906",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks. Advances in prompt engineering and fine-tuning techniques have further enhanced their ability to address complex reasoning challenges. However, these advanced capabilities are often exclusive to models exceeding 100 billion parameters. Although Chain-of-Thought (CoT) fine-tuning methods have been explored for smaller models (under 10 billion parameters), they typically depend on extensive CoT training data, which can introduce inconsistencies and limit effectiveness in low-data settings. To overcome these limitations, this paper introduce a new reasoning strategy Solution Guidance (SG) and a plug-and-play training paradigm Solution-Guidance Fine-Tuning (SGFT) for enhancing the reasoning capabilities of small language models. SG focuses on problem understanding and decomposition at the semantic and logical levels, rather than specific computations, which can effectively improve the SLMs' generalization and reasoning abilities. With only a small amount of SG training data, SGFT can fine-tune a SLM to produce accurate problem-solving guidances, which can then be flexibly fed to any SLM as prompts, enabling it to generate correct answers directly. Experimental results demonstrate that our method significantly improves the performance of SLMs on various reasoning tasks, enhancing both their practicality and efficiency within resource-constrained environments.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "46",
        "title": "IQViC: In-context, Question Adaptive Vision Compressor for Long-term Video Understanding LMMs",
        "author": [
            "Sosuke Yamao",
            "Natsuki Miyahara",
            "Yuki Harazono",
            "Shun Takeuchi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09907",
        "abstract": "With the increasing complexity of video data and the need for more efficient long-term temporal understanding, existing long-term video understanding methods often fail to accurately capture and analyze extended video sequences. These methods typically struggle to maintain performance over longer durations and to handle the intricate dependencies within the video content. To address these limitations, we propose a simple yet effective large multi-modal model framework for long-term video understanding that incorporates a novel visual compressor, the In-context, Question Adaptive Visual Compressor (IQViC). The key idea, inspired by humans' selective attention and in-context memory mechanisms, is to introduce a novel visual compressor and incorporate efficient memory management techniques to enhance long-term video question answering. Our framework utilizes IQViC, a transformer-based visual compressor, enabling question-conditioned in-context compression, unlike existing methods that rely on full video visual features. This selectively extracts relevant information, significantly reducing memory token requirements. Through extensive experiments on a new dataset based on InfiniBench for long-term video understanding, and standard benchmarks used for existing methods' evaluation, we demonstrate the effectiveness of our proposed IQViC framework and its superiority over state-of-the-art methods in terms of video understanding accuracy and memory efficiency.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "47",
        "title": "Atomic Learning Objectives Labeling: A High-Resolution Approach for Physics Education",
        "author": [
            "Naiming Liu",
            "Shashank Sonkar",
            "Debshila Basu Mallick",
            "Richard Baraniuk",
            "Zhongzhou Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09914",
        "abstract": "This paper introduces a novel approach to create a high-resolution \"map\" for physics learning: an \"atomic\" learning objectives (LOs) system designed to capture detailed cognitive processes and concepts required for problem solving in a college-level introductory physics course. Our method leverages Large Language Models (LLMs) for automated labeling of physics questions and introduces a comprehensive set of metrics to evaluate the quality of the labeling outcomes. The atomic LO system, covering nine chapters of an introductory physics course, uses a \"subject-verb-object'' structure to represent specific cognitive processes. We apply this system to 131 questions from expert-curated question banks and the OpenStax University Physics textbook. Each question is labeled with 1-8 atomic LOs across three chapters. Through extensive experiments using various prompting strategies and LLMs, we compare automated LOs labeling results against human expert labeling. Our analysis reveals both the strengths and limitations of LLMs, providing insight into LLMs reasoning processes for labeling LOs and identifying areas for improvement in LOs system design. Our work contributes to the field of learning analytics by proposing a more granular approach to mapping learning objectives with questions. Our findings have significant implications for the development of intelligent tutoring systems and personalized learning pathways in STEM education, paving the way for more effective \"learning GPS'' systems.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "48",
        "title": "ProxyLLM : LLM-Driven Framework for Customer Support Through Text-Style Transfer",
        "author": [
            "Sehyeong Jo",
            "Jungwon Seo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09916",
        "abstract": "Chatbot-based customer support services have significantly advanced with the introduction of large language models (LLMs), enabling enhanced response quality and broader application across industries. However, while these advancements focus on reducing business costs and improving customer satisfaction, limited attention has been given to the experiences of customer service agents, who are critical to the service ecosystem. A major challenge faced by agents is the stress caused by unnecessary emotional exhaustion from harmful texts, which not only impairs their efficiency but also negatively affects customer satisfaction and business outcomes. In this work, we propose an LLM-powered system designed to enhance the working conditions of customer service agents by addressing emotionally intensive communications. Our proposed system leverages LLMs to transform the tone of customer messages, preserving actionable content while mitigating the emotional impact on human agents. Furthermore, the application is implemented as a Chrome extension, making it highly adaptable and easy to integrate into existing systems. Our method aims to enhance the overall service experience for businesses, customers, and agents.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Style Transfer"
        ]
    },
    {
        "id": "49",
        "title": "B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal Tokens",
        "author": [
            "Zhuqiang Lu",
            "Zhenfei Yin",
            "Mengwei He",
            "Zhihui Wang",
            "Zicheng Liu",
            "Zhiyong Wang",
            "Kun Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09919",
        "abstract": "Recently, Vision Large Language Models (VLLMs) integrated with vision encoders have shown promising performance in vision understanding. The key of VLLMs is to encode visual content into sequences of visual tokens, enabling VLLMs to simultaneously process both visual and textual content. However, understanding videos, especially long videos, remain a challenge to VLLMs as the number of visual tokens grows rapidly when encoding videos, resulting in the risk of exceeding the context window of VLLMs and introducing heavy computation burden. To restrict the number of visual tokens, existing VLLMs either: (1) uniformly downsample videos into a fixed number of frames or (2) reducing the number of visual tokens encoded from each frame. We argue the former solution neglects the rich temporal cue in videos and the later overlooks the spatial details in each frame. In this work, we present Balanced-VLLM (B-VLLM): a novel VLLM framework that aims to effectively leverage task relevant spatio-temporal cues while restricting the number of visual tokens under the VLLM context window length. At the core of our method, we devise a text-conditioned adaptive frame selection module to identify frames relevant to the visual understanding task. The selected frames are then de-duplicated using a temporal frame token merging technique. The visual tokens of the selected frames are processed through a spatial token sampling module and an optional spatial token merging strategy to achieve precise control over the token count. Experimental results show that B-VLLM is effective in balancing the number of frames and visual tokens in video understanding, yielding superior performance on various video understanding benchmarks. Our code is available at https://github.com/zhuqiangLu/B-VLLM.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "50",
        "title": "FaceShield: Defending Facial Image against Deepfake Threats",
        "author": [
            "Jaehwan Jeong",
            "Sumin In",
            "Sieun Kim",
            "Hannie Shin",
            "Jongheon Jeong",
            "Sang Ho Yoon",
            "Jaewook Chung",
            "Sangpil Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09921",
        "abstract": "The rising use of deepfakes in criminal activities presents a significant issue, inciting widespread controversy. While numerous studies have tackled this problem, most primarily focus on deepfake detection. These reactive solutions are insufficient as a fundamental approach for crimes where authenticity verification is not critical. Existing proactive defenses also have limitations, as they are effective only for deepfake models based on specific Generative Adversarial Networks (GANs), making them less applicable in light of recent advancements in diffusion-based models. In this paper, we propose a proactive defense method named FaceShield, which introduces novel attack strategies targeting deepfakes generated by Diffusion Models (DMs) and facilitates attacks on various existing GAN-based deepfake models through facial feature extractor manipulations. Our approach consists of three main components: (i) manipulating the attention mechanism of DMs to exclude protected facial features during the denoising process, (ii) targeting prominent facial feature extraction models to enhance the robustness of our adversarial perturbation, and (iii) employing Gaussian blur and low-pass filtering techniques to improve imperceptibility while enhancing robustness against JPEG distortion. Experimental results on the CelebA-HQ and VGGFace2-HQ datasets demonstrate that our method achieves state-of-the-art performance against the latest deepfake models based on DMs, while also exhibiting applicability to GANs and showcasing greater imperceptibility of noise along with enhanced robustness.",
        "tags": [
            "Detection",
            "Diffusion",
            "GAN"
        ]
    },
    {
        "id": "51",
        "title": "CaLoRAify: Calorie Estimation with Visual-Text Pairing and LoRA-Driven Visual Language Models",
        "author": [
            "Dongyu Yao",
            "Keling Yao",
            "Junhong Zhou",
            "Yinghao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09936",
        "abstract": "The obesity phenomenon, known as the heavy issue, is a leading cause of preventable chronic diseases worldwide. Traditional calorie estimation tools often rely on specific data formats or complex pipelines, limiting their practicality in real-world scenarios. Recently, vision-language models (VLMs) have excelled in understanding real-world contexts and enabling conversational interactions, making them ideal for downstream tasks such as ingredient analysis. However, applying VLMs to calorie estimation requires domain-specific data and alignment strategies. To this end, we curated CalData, a 330K image-text pair dataset tailored for ingredient recognition and calorie estimation, combining a large-scale recipe dataset with detailed nutritional instructions for robust vision-language training. Built upon this dataset, we present CaLoRAify, a novel VLM framework aligning ingredient recognition and calorie estimation via training with visual-text pairs. During inference, users only need a single monocular food image to estimate calories while retaining the flexibility of agent-based conversational interaction. With Low-rank Adaptation (LoRA) and Retrieve-augmented Generation (RAG) techniques, our system enhances the performance of foundational VLMs in the vertical domain of calorie estimation. Our code and data are fully open-sourced at https://github.com/KennyYao2001/16824-CaLORAify.",
        "tags": [
            "LoRA",
            "RAG"
        ]
    },
    {
        "id": "52",
        "title": "Enhancing Nursing and Elderly Care with Large Language Models: An AI-Driven Framework",
        "author": [
            "Qiao Sun",
            "Jiexin Xie",
            "Nanyang Ye",
            "Qinying Gu",
            "Shijie Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09946",
        "abstract": "This paper explores the application of large language models (LLMs) in nursing and elderly care, focusing on AI-driven patient monitoring and interaction. We introduce a novel Chinese nursing dataset and implement incremental pre-training (IPT) and supervised fine-tuning (SFT) techniques to enhance LLM performance in specialized tasks. Using LangChain, we develop a dynamic nursing assistant capable of real-time care and personalized interventions. Experimental results demonstrate significant improvements, paving the way for AI-driven solutions to meet the growing demands of healthcare in aging populations.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "53",
        "title": "Llama 3 Meets MoE: Efficient Upcycling",
        "author": [
            "Aditya Vavre",
            "Ethan He",
            "Dennis Liu",
            "Zijie Yan",
            "June Yang",
            "Nima Tajbakhsh",
            "Ashwath Aithal"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09952",
        "abstract": "Scaling large language models (LLMs) significantly improves performance but comes with prohibitive computational costs. Mixture-of-Experts (MoE) models offer an efficient alternative, increasing capacity without a proportional rise in compute requirements. However, training MoE models from scratch poses challenges like overfitting and routing instability. We present an efficient training recipe leveraging pre-trained dense checkpoints, training an 8-Expert Top-2 MoE model from Llama 3-8B with less than $1\\%$ of typical pre-training compute. Our approach enhances downstream performance on academic benchmarks, achieving a $\\textbf{2%}$ improvement in 0-shot accuracy on MMLU, while reaching a Model FLOPs Utilization (MFU) of $\\textbf{46.8%}$ during training using our framework. We also integrate online upcycling in NeMo for seamless use of pre-trained weights, enabling cost-effective development of high-capacity MoE models.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "54",
        "title": "Efficient Dataset Distillation via Diffusion-Driven Patch Selection for Improved Generalization",
        "author": [
            "Xinhao Zhong",
            "Shuoyang Sun",
            "Xulin Gu",
            "Zhaoyang Xu",
            "Yaowei Wang",
            "Jianlong Wu",
            "Bin Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09959",
        "abstract": "Dataset distillation offers an efficient way to reduce memory and computational costs by optimizing a smaller dataset with performance comparable to the full-scale original. However, for large datasets and complex deep networks (e.g., ImageNet-1K with ResNet-101), the extensive optimization space limits performance, reducing its practicality. Recent approaches employ pre-trained diffusion models to generate informative images directly, avoiding pixel-level optimization and achieving notable results. However, these methods often face challenges due to distribution shifts between pre-trained models and target datasets, along with the need for multiple distillation steps across varying settings. To address these issues, we propose a novel framework orthogonal to existing diffusion-based distillation methods, leveraging diffusion models for selection rather than generation. Our method starts by predicting noise generated by the diffusion model based on input images and text prompts (with or without label text), then calculates the corresponding loss for each pair. With the loss differences, we identify distinctive regions of the original images. Additionally, we perform intra-class clustering and ranking on selected patches to maintain diversity constraints. This streamlined framework enables a single-step distillation process, and extensive experiments demonstrate that our approach outperforms state-of-the-art methods across various metrics.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "55",
        "title": "EP-CFG: Energy-Preserving Classifier-Free Guidance",
        "author": [
            "Kai Zhang",
            "Fujun Luan",
            "Sai Bi",
            "Jianming Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09966",
        "abstract": "Classifier-free guidance (CFG) is widely used in diffusion models but often introduces over-contrast and over-saturation artifacts at higher guidance strengths. We present EP-CFG (Energy-Preserving Classifier-Free Guidance), which addresses these issues by preserving the energy distribution of the conditional prediction during the guidance process. Our method simply rescales the energy of the guided output to match that of the conditional prediction at each denoising step, with an optional robust variant for improved artifact suppression. Through experiments, we show that EP-CFG maintains natural image quality and preserves details across guidance strengths while retaining CFG's semantic alignment benefits, all with minimal computational overhead.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "56",
        "title": "Efficient Large-Scale Traffic Forecasting with Transformers: A Spatial Data Management Perspective",
        "author": [
            "Yuchen Fang",
            "Yuxuan Liang",
            "Bo Hui",
            "Zezhi Shao",
            "Liwei Deng",
            "Xu Liu",
            "Xinke Jiang",
            "Kai Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09972",
        "abstract": "Road traffic forecasting is crucial in real-world intelligent transportation scenarios like traffic dispatching and path planning in city management and personal traveling. Spatio-temporal graph neural networks (STGNNs) stand out as the mainstream solution in this task. Nevertheless, the quadratic complexity of remarkable dynamic spatial modeling-based STGNNs has become the bottleneck over large-scale traffic data. From the spatial data management perspective, we present a novel Transformer framework called PatchSTG to efficiently and dynamically model spatial dependencies for large-scale traffic forecasting with interpretability and fidelity. Specifically, we design a novel irregular spatial patching to reduce the number of points involved in the dynamic calculation of Transformer. The irregular spatial patching first utilizes the leaf K-dimensional tree (KDTree) to recursively partition irregularly distributed traffic points into leaf nodes with a small capacity, and then merges leaf nodes belonging to the same subtree into occupancy-equaled and non-overlapped patches through padding and backtracking. Based on the patched data, depth and breadth attention are used interchangeably in the encoder to dynamically learn local and global spatial knowledge from points in a patch and points with the same index of patches. Experimental results on four real world large-scale traffic datasets show that our PatchSTG achieves train speed and memory utilization improvements up to $10\\times$ and $4\\times$ with the state-of-the-art performance.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "57",
        "title": "SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video",
        "author": [
            "Jongmin Park",
            "Minh-Quan Viet Bui",
            "Juan Luis Gonzalez Bello",
            "Jaeho Moon",
            "Jihyong Oh",
            "Munchurl Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09982",
        "abstract": "Synthesizing novel views from in-the-wild monocular videos is challenging due to scene dynamics and the lack of multi-view cues. To address this, we propose SplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for high-quality reconstruction and fast rendering from monocular videos. At its core is a novel Motion-Adaptive Spline (MAS) method, which represents continuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a small number of control points. For MAS, we introduce a Motion-Adaptive Control points Pruning (MACP) method to model the deformation of each dynamic 3D Gaussian across varying motions, progressively pruning control points while maintaining dynamic modeling integrity. Additionally, we present a joint optimization strategy for camera parameter estimation and 3D Gaussian attributes, leveraging photometric and geometric consistency. This eliminates the need for Structure-from-Motion preprocessing and enhances SplineGS's robustness in real-world conditions. Experiments show that SplineGS significantly outperforms state-of-the-art methods in novel view synthesis quality for dynamic scenes from monocular videos, achieving thousands times faster rendering speed.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "58",
        "title": "AI and the Future of Digital Public Squares",
        "author": [
            "Beth Goldberg",
            "Diana Acosta-Navas",
            "Michiel Bakker",
            "Ian Beacock",
            "Matt Botvinick",
            "Prateek Buch",
            "Renée DiResta",
            "Nandika Donthi",
            "Nathanael Fast",
            "Ravi Iyer",
            "Zaria Jalan",
            "Andrew Konya",
            "Grace Kwak Danciu",
            "Hélène Landemore",
            "Alice Marwick",
            "Carl Miller",
            "Aviv Ovadya",
            "Emily Saltz",
            "Lisa Schirch",
            "Dalit Shalom",
            "Divya Siddarth",
            "Felix Sieker",
            "Christopher Small",
            "Jonathan Stray",
            "Audrey Tang",
            "Michael Henry Tessler",
            "Amy Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09988",
        "abstract": "Two substantial technological advances have reshaped the public square in recent decades: first with the advent of the internet and second with the recent introduction of large language models (LLMs). LLMs offer opportunities for a paradigm shift towards more decentralized, participatory online spaces that can be used to facilitate deliberative dialogues at scale, but also create risks of exacerbating societal schisms. Here, we explore four applications of LLMs to improve digital public squares: collective dialogue systems, bridging systems, community moderation, and proof-of-humanity systems. Building on the input from over 70 civil society experts and technologists, we argue that LLMs both afford promising opportunities to shift the paradigm for conversations at scale and pose distinct risks for digital public squares. We lay out an agenda for future research and investments in AI that will strengthen digital public squares and safeguard against potential misuses of AI.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "59",
        "title": "Small Language Model as Data Prospector for Large Language Model",
        "author": [
            "Shiwen Ni",
            "Haihong Wu",
            "Di Yang",
            "Qiang Qu",
            "Hamid Alinejad-Rokny",
            "Min Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09990",
        "abstract": "The quality of instruction data directly affects the performance of fine-tuned Large Language Models (LLMs). Previously, \\cite{li2023one} proposed \\texttt{NUGGETS}, which identifies and selects high-quality quality data from a large dataset by identifying those individual instruction examples that can significantly improve the performance of different tasks after being learnt as one-shot instances. In this work, we propose \\texttt{SuperNUGGETS}, an improved variant of \\texttt{NUGGETS} optimised for efficiency and performance. Our \\texttt{SuperNUGGETS} uses a small language model (SLM) instead of a large language model (LLM) to filter the data for outstanding one-shot instances and refines the predefined set of tests. The experimental results show that the performance of \\texttt{SuperNUGGETS} only decreases by 1-2% compared to \\texttt{NUGGETS}, but the efficiency can be increased by a factor of 58. Compared to the original \\texttt{NUGGETS}, our \\texttt{SuperNUGGETS} has a higher utility value due to the significantly lower resource consumption.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "60",
        "title": "A Comparative Study of LLMs, NMT Models, and Their Combination in Persian-English Idiom Translation",
        "author": [
            "Sara Rezaeimanesh",
            "Faezeh Hosseini",
            "Yadollah Yaghoobzadeh"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09993",
        "abstract": "Large language models (LLMs) have shown superior capabilities in translating figurative language compared to neural machine translation (NMT) systems. However, the impact of different prompting methods and LLM-NMT combinations on idiom translation has yet to be thoroughly investigated. This paper introduces two parallel datasets of sentences containing idiomatic expressions for Persian$\\rightarrow$English and English$\\rightarrow$Persian translations, with Persian idioms sampled from our PersianIdioms resource, a collection of 2,200 idioms and their meanings. Using these datasets, we evaluate various open- and closed-source LLMs, NMT models, and their combinations. Translation quality is assessed through idiom translation accuracy and fluency. We also find that automatic evaluation methods like LLM-as-a-judge, BLEU and BERTScore are effective for comparing different aspects of model performance. Our experiments reveal that Claude-3.5-Sonnet delivers outstanding results in both translation directions. For English$\\rightarrow$Persian, combining weaker LLMs with Google Translate improves results, while Persian$\\rightarrow$English translations benefit from single prompts for simpler models and complex prompts for advanced ones.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "61",
        "title": "GT23D-Bench: A Comprehensive General Text-to-3D Generation Benchmark",
        "author": [
            "Sitong Su",
            "Xiao Cai",
            "Lianli Gao",
            "Pengpeng Zeng",
            "Qinhong Du",
            "Mengqi Li",
            "Heng Tao Shen",
            "Jingkuan Song"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09997",
        "abstract": "Recent advances in General Text-to-3D (GT23D) have been significant. However, the lack of a benchmark has hindered systematic evaluation and progress due to issues in datasets and metrics: 1) The largest 3D dataset Objaverse suffers from omitted annotations, disorganization, and low-quality. 2) Existing metrics only evaluate textual-image alignment without considering the 3D-level quality. To this end, we are the first to present a comprehensive benchmark for GT23D called GT23D-Bench consisting of: 1) a 400k high-fidelity and well-organized 3D dataset that curated issues in Objaverse through a systematical annotation-organize-filter pipeline; and 2) comprehensive 3D-aware evaluation metrics which encompass 10 clearly defined metrics thoroughly accounting for multi-dimension of GT23D. Notably, GT23D-Bench features three properties: 1) Multimodal Annotations. Our dataset annotates each 3D object with 64-view depth maps, normal maps, rendered images, and coarse-to-fine captions. 2) Holistic Evaluation Dimensions. Our metrics are dissected into a) Textual-3D Alignment measures textual alignment with multi-granularity visual 3D representations; and b) 3D Visual Quality which considers texture fidelity, multi-view consistency, and geometry correctness. 3) Valuable Insights. We delve into the performance of current GT23D baselines across different evaluation dimensions and provide insightful analysis. Extensive experiments demonstrate that our annotations and metrics are aligned with human preferences.",
        "tags": [
            "3D",
            "Text-to-3D"
        ]
    },
    {
        "id": "62",
        "title": "NeRF-Texture: Synthesizing Neural Radiance Field Textures",
        "author": [
            "Yi-Hua Huang",
            "Yan-Pei Cao",
            "Yu-Kun Lai",
            "Ying Shan",
            "Lin Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10004",
        "abstract": "Texture synthesis is a fundamental problem in computer graphics that would benefit various applications. Existing methods are effective in handling 2D image textures. In contrast, many real-world textures contain meso-structure in the 3D geometry space, such as grass, leaves, and fabrics, which cannot be effectively modeled using only 2D image textures. We propose a novel texture synthesis method with Neural Radiance Fields (NeRF) to capture and synthesize textures from given multi-view images. In the proposed NeRF texture representation, a scene with fine geometric details is disentangled into the meso-structure textures and the underlying base shape. This allows textures with meso-structure to be effectively learned as latent features situated on the base shape, which are fed into a NeRF decoder trained simultaneously to represent the rich view-dependent appearance. Using this implicit representation, we can synthesize NeRF-based textures through patch matching of latent features. However, inconsistencies between the metrics of the reconstructed content space and the latent feature space may compromise the synthesis quality. To enhance matching performance, we further regularize the distribution of latent features by incorporating a clustering constraint. In addition to generating NeRF textures over a planar domain, our method can also synthesize NeRF textures over curved surfaces, which are practically useful. Experimental results and evaluations demonstrate the effectiveness of our approach.",
        "tags": [
            "3D",
            "NeRF"
        ]
    },
    {
        "id": "63",
        "title": "Mr. DETR: Instructive Multi-Route Training for Detection Transformers",
        "author": [
            "Chang-Bin Zhang",
            "Yujie Zhong",
            "Kai Han"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10028",
        "abstract": "Existing methods enhance the training of detection transformers by incorporating an auxiliary one-to-many assignment. In this work, we treat the model as a multi-task framework, simultaneously performing one-to-one and one-to-many predictions. We investigate the roles of each component in the transformer decoder across these two training targets, including self-attention, cross-attention, and feed-forward network. Our empirical results demonstrate that any independent component in the decoder can effectively learn both targets simultaneously, even when other components are shared. This finding leads us to propose a multi-route training mechanism, featuring a primary route for one-to-one prediction and two auxiliary training routes for one-to-many prediction. We enhance the training mechanism with a novel instructive self-attention that dynamically and flexibly guides object queries for one-to-many prediction. The auxiliary routes are removed during inference, ensuring no impact on model architecture or inference cost. We conduct extensive experiments on various baselines, achieving consistent improvements as shown in Figure 1.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "64",
        "title": "RemDet: Rethinking Efficient Model Design for UAV Object Detection",
        "author": [
            "Chen Li",
            "Rui Zhao",
            "Zeyu Wang",
            "Huiying Xu",
            "Xinzhong Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10040",
        "abstract": "Object detection in Unmanned Aerial Vehicle (UAV) images has emerged as a focal area of research, which presents two significant challenges: i) objects are typically small and dense within vast images; ii) computational resource constraints render most models unsuitable for real-time deployment. Current real-time object detectors are not optimized for UAV images, and complex methods designed for small object detection often lack real-time capabilities. To address these challenges, we propose a novel detector, RemDet (Reparameter efficient multiplication Detector). Our contributions are as follows: 1) Rethinking the challenges of existing detectors for small and dense UAV images, and proposing information loss as a design guideline for efficient models. 2) We introduce the ChannelC2f module to enhance small object detection performance, demonstrating that high-dimensional representations can effectively mitigate information loss. 3) We design the GatedFFN module to provide not only strong performance but also low latency, effectively addressing the challenges of real-time detection. Our research reveals that GatedFFN, through the use of multiplication, is more cost-effective than feed-forward networks for high-dimensional representation. 4) We propose the CED module, which combines the advantages of ViT and CNN downsampling to effectively reduce information loss. It specifically enhances context information for small and dense objects. Extensive experiments on large UAV datasets, Visdrone and UAVDT, validate the real-time efficiency and superior performance of our methods. On the challenging UAV dataset VisDrone, our methods not only provided state-of-the-art results, improving detection by more than 3.4%, but also achieve 110 FPS on a single http://4090.Codes are available at (this URL)(https://github.com/HZAI-ZJNU/RemDet).",
        "tags": [
            "Detection",
            "ViT"
        ]
    },
    {
        "id": "65",
        "title": "Large Action Models: From Inception to Implementation",
        "author": [
            "Lu Wang",
            "Fangkai Yang",
            "Chaoyun Zhang",
            "Junting Lu",
            "Jiaxu Qian",
            "Shilin He",
            "Pu Zhao",
            "Bo Qiao",
            "Ray Huang",
            "Si Qin",
            "Qisheng Su",
            "Jiayi Ye",
            "Yudi Zhang",
            "Jian-Guang Lou",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10047",
        "abstract": "As AI continues to advance, there is a growing demand for systems that go beyond language-based assistance and move toward intelligent agents capable of performing real-world actions. This evolution requires the transition from traditional Large Language Models (LLMs), which excel at generating textual responses, to Large Action Models (LAMs), designed for action generation and execution within dynamic environments. Enabled by agent systems, LAMs hold the potential to transform AI from passive language understanding to active task completion, marking a significant milestone in the progression toward artificial general intelligence.\nIn this paper, we present a comprehensive framework for developing LAMs, offering a systematic approach to their creation, from inception to deployment. We begin with an overview of LAMs, highlighting their unique characteristics and delineating their differences from LLMs. Using a Windows OS-based agent as a case study, we provide a detailed, step-by-step guide on the key stages of LAM development, including data collection, model training, environment integration, grounding, and evaluation. This generalizable workflow can serve as a blueprint for creating functional LAMs in various application domains. We conclude by identifying the current limitations of LAMs and discussing directions for future research and industrial deployment, emphasizing the challenges and opportunities that lie ahead in realizing the full potential of LAMs in real-world applications.\nThe code for the data collection process utilized in this paper is publicly available at: https://github.com/microsoft/UFO/tree/main/dataflow, and comprehensive documentation can be found at https://microsoft.github.io/UFO/dataflow/overview/.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "66",
        "title": "ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for Articulated Object Manipulation?",
        "author": [
            "Taewhan Kim",
            "Hojin Bae",
            "Zeming Li",
            "Xiaoqi Li",
            "Iaroslav Ponomarenko",
            "Ruihai Wu",
            "Hao Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10050",
        "abstract": "Visual actionable affordance has emerged as a transformative approach in robotics, focusing on perceiving interaction areas prior to manipulation. Traditional methods rely on pixel sampling to identify successful interaction samples or processing pointclouds for affordance mapping. However, these approaches are computationally intensive and struggle to adapt to diverse and dynamic environments. This paper introduces ManipGPT, a framework designed to predict optimal interaction areas for articulated objects using a large pre-trained vision transformer (ViT). We created a dataset of 9.9k simulated and real images to bridge the sim-to-real gap and enhance real-world applicability. By fine-tuning the vision transformer on this small dataset, we significantly improved part-level affordance segmentation, adapting the model's in-context segmentation capabilities to robot manipulation scenarios. This enables effective manipulation across simulated and real-world environments by generating part-level affordance masks, paired with an impedance adaptation policy, sufficiently eliminating the need for complex datasets or perception systems.",
        "tags": [
            "Robot",
            "Robotics",
            "Segmentation",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "67",
        "title": "TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting from Sparse Views",
        "author": [
            "Liang Zhao",
            "Zehan Bao",
            "Yi Xie",
            "Hong Chen",
            "Yaohui Chen",
            "Weifu Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10051",
        "abstract": "Recent advances in Gaussian Splatting have significantly advanced the field, achieving both panoptic and interactive segmentation of 3D scenes. However, existing methodologies often overlook the critical need for reconstructing specified targets with complex structures from sparse views. To address this issue, we introduce TSGaussian, a novel framework that combines semantic constraints with depth priors to avoid geometry degradation in challenging novel view synthesis tasks. Our approach prioritizes computational resources on designated targets while minimizing background allocation. Bounding boxes from YOLOv9 serve as prompts for Segment Anything Model to generate 2D mask predictions, ensuring semantic accuracy and cost efficiency. TSGaussian effectively clusters 3D gaussians by introducing a compact identity encoding for each Gaussian ellipsoid and incorporating 3D spatial consistency regularization. Leveraging these modules, we propose a pruning strategy to effectively reduce redundancy in 3D gaussians. Extensive experiments demonstrate that TSGaussian outperforms state-of-the-art methods on three standard datasets and a new challenging dataset we collected, achieving superior results in novel view synthesis of specific objects. Code is available at: https://github.com/leon2000-ai/TSGaussian.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Segment Anything",
            "Segmentation"
        ]
    },
    {
        "id": "68",
        "title": "GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?",
        "author": [
            "Zhikai Lei",
            "Tianyi Liang",
            "Hanglei Hu",
            "Jin Zhang",
            "Yunhua Zhou",
            "Yunfan Shao",
            "Linyang Li",
            "Chenchui Li",
            "Changbo Wang",
            "Hang Yan",
            "Qipeng Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10056",
        "abstract": "Large Language Models (LLMs) are commonly evaluated using human-crafted benchmarks, under the premise that higher scores implicitly reflect stronger human-like performance. However, there is growing concern that LLMs may ``game\" these benchmarks due to data leakage, achieving high scores while struggling with tasks simple for humans. To substantively address the problem, we create GAOKAO-Eval, a comprehensive benchmark based on China's National College Entrance Examination (Gaokao), and conduct ``closed-book\" evaluations for representative models released prior to Gaokao. Contrary to prevailing consensus, even after addressing data leakage and comprehensiveness, GAOKAO-Eval reveals that high scores still fail to truly reflect human-aligned capabilities. To better understand this mismatch, We introduce the Rasch model from cognitive psychology to analyze LLM scoring patterns and identify two key discrepancies: 1) anomalous consistent performance across various question difficulties, and 2) high variance in performance on questions of similar difficulty. In addition, We identified inconsistent grading of LLM-generated answers among teachers and recurring mistake patterns. we find that the phenomenons are well-grounded in the motivations behind OpenAI o1, and o1's reasoning-as-difficulties can mitigate the mismatch. These results show that GAOKAO-Eval can reveal limitations in LLM capabilities not captured by current benchmarks and highlight the need for more LLM-aligned difficulty analysis.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "69",
        "title": "Text2Cypher: Bridging Natural Language and Graph Databases",
        "author": [
            "Makbule Gulcin Ozsoy",
            "Leila Messallem",
            "Jon Besga",
            "Gianandrea Minneci"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10064",
        "abstract": "Knowledge graphs use nodes, relationships, and properties to represent arbitrarily complex data. When stored in a graph database, the Cypher query language enables efficient modeling and querying of knowledge graphs. However, using Cypher requires specialized knowledge, which can present a challenge for non-expert users. Our work Text2Cypher aims to bridge this gap by translating natural language queries into Cypher query language and extending the utility of knowledge graphs to non-technical expert users.\nWhile large language models (LLMs) can be used for this purpose, they often struggle to capture complex nuances, resulting in incomplete or incorrect outputs. Fine-tuning LLMs on domain-specific datasets has proven to be a more promising approach, but the limited availability of high-quality, publicly available Text2Cypher datasets makes this challenging. In this work, we show how we combined, cleaned and organized several publicly available datasets into a total of 44,387 instances, enabling effective fine-tuning and evaluation. Models fine-tuned on this dataset showed significant performance gains, with improvements in Google-BLEU and Exact Match scores over baseline models, highlighting the importance of high-quality datasets and fine-tuning in improving Text2Cypher performance.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "70",
        "title": "NetOrchLLM: Mastering Wireless Network Orchestration with Large Language Models",
        "author": [
            "Asmaa Abdallah",
            "Abdullatif Albaseer",
            "Abdulkadir Celik",
            "Mohamed Abdallah",
            "Ahmed M. Eltawil"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10107",
        "abstract": "The transition to 6G networks promises unprecedented advancements in wireless communication, with increased data rates, ultra-low latency, and enhanced capacity. However, the complexity of managing and optimizing these next-generation networks presents significant challenges. The advent of large language models (LLMs) has revolutionized various domains by leveraging their sophisticated natural language understanding capabilities. However, the practical application of LLMs in wireless network orchestration and management remains largely unexplored. Existing literature predominantly offers visionary perspectives without concrete implementations, leaving a significant gap in the field. To address this gap, this paper presents NETORCHLLM, a wireless NETwork ORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse wireless-specific models from wireless communication communities using their language understanding and generation capabilities. A comprehensive framework is introduced, demonstrating the practical viability of our approach and showcasing how LLMs can be effectively harnessed to optimize dense network operations, manage dynamic environments, and improve overall network performance. NETORCHLLM bridges the theoretical aspirations of prior research with practical, actionable solutions, paving the way for future advancements in integrating generative AI technologies within the wireless communications sector.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "71",
        "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
        "author": [
            "Zhihao Du",
            "Yuxuan Wang",
            "Qian Chen",
            "Xian Shi",
            "Xiang Lv",
            "Tianyu Zhao",
            "Zhifu Gao",
            "Yexin Yang",
            "Changfeng Gao",
            "Hui Wang",
            "Fan Yu",
            "Huadai Liu",
            "Zhengyan Sheng",
            "Yue Gu",
            "Chong Deng",
            "Wen Wang",
            "Shiliang Zhang",
            "Zhijie Yan",
            "Jingren Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10117",
        "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
        "tags": [
            "Flow Matching",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "72",
        "title": "The Art of Deception: Color Visual Illusions and Diffusion Models",
        "author": [
            "Alex Gomez-Villa",
            "Kai Wang",
            "Alejandro C. Parraga",
            "Bartlomiej Twardowski",
            "Jesus Malo",
            "Javier Vazquez-Corral",
            "Joost van de Weijer"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10122",
        "abstract": "Visual illusions in humans arise when interpreting out-of-distribution stimuli: if the observer is adapted to certain statistics, perception of outliers deviates from reality. Recent studies have shown that artificial neural networks (ANNs) can also be deceived by visual illusions. This revelation raises profound questions about the nature of visual information. Why are two independent systems, both human brains and ANNs, susceptible to the same illusions? Should any ANN be capable of perceiving visual illusions? Are these perceptions a feature or a flaw? In this work, we study how visual illusions are encoded in diffusion models. Remarkably, we show that they present human-like brightness/color shifts in their latent space. We use this fact to demonstrate that diffusion models can predict visual illusions. Furthermore, we also show how to generate new unseen visual illusions in realistic images using text-to-image diffusion models. We validate this ability through psychophysical experiments that show how our model-generated illusions also fool humans.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "73",
        "title": "ASLoRA: Adaptive Sharing Low-Rank Adaptation Across Layers",
        "author": [
            "Junyan Hu",
            "Xue Xiao",
            "Mengqi Zhang",
            "Xiao Chen",
            "Zhaochun Ren",
            "Zhumin Chen",
            "Pengjie Ren"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10135",
        "abstract": "As large language models (LLMs) grow in size, traditional full fine-tuning becomes increasingly impractical due to its high computational and storage costs. Although popular parameter-efficient fine-tuning methods, such as LoRA, have significantly reduced the number of tunable parameters, there is still room for further optimization. In this work, we propose ASLoRA, a cross-layer parameter-sharing strategy combining global sharing with partial adaptive sharing. Specifically, we share the low-rank matrix A across all layers and adaptively merge matrix B during training. This sharing mechanism not only mitigates overfitting effectively but also captures inter-layer dependencies, significantly enhancing the model's representational capability. We conduct extensive experiments on various NLP tasks, showing that ASLoRA outperforms LoRA while using less than 25% of the parameters, highlighting its flexibility and superior parameter efficiency. Furthermore, in-depth analyses of the adaptive sharing strategy confirm its significant advantages in enhancing both model flexibility and task adaptability.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "74",
        "title": "Can LLMs Convert Graphs to Text-Attributed Graphs?",
        "author": [
            "Zehong Wang",
            "Sidney Liu",
            "Zheyuan Zhang",
            "Tianyi Ma",
            "Chuxu Zhang",
            "Yanfang Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10136",
        "abstract": "Graphs are ubiquitous data structures found in numerous real-world applications, such as drug discovery, recommender systems, and social network analysis. Graph neural networks (GNNs) have become a popular tool to learn node embeddings through message passing on these structures. However, a significant challenge arises when applying GNNs to multiple graphs with different feature spaces, as existing GNN architectures are not designed for cross-graph feature alignment. To address this, recent approaches introduce text-attributed graphs, where each node is associated with a textual description, enabling the use of a shared textual encoder to project nodes from different graphs into a unified feature space. While promising, this method relies heavily on the availability of text-attributed data, which can be difficult to obtain in practice. To bridge this gap, we propose a novel method named Topology-Aware Node description Synthesis (TANS), which leverages large language models (LLMs) to automatically convert existing graphs into text-attributed graphs. The key idea is to integrate topological information with each node's properties, enhancing the LLMs' ability to explain how graph topology influences node semantics. We evaluate our TANS on text-rich, text-limited, and text-free graphs, demonstrating that it enables a single GNN to operate across diverse graphs. Notably, on text-free graphs, our method significantly outperforms existing approaches that manually design node features, showcasing the potential of LLMs for preprocessing graph-structured data, even in the absence of textual information. The code and data are available at https://github.com/Zehong-Wang/TANS.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "75",
        "title": "VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation",
        "author": [
            "Hyeonseok Lim",
            "Dongjae Shin",
            "Seohyun Song",
            "Inho Won",
            "Minjun Kim",
            "Junghun Yuk",
            "Haneol Jang",
            "KyungTae Lim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10151",
        "abstract": "We propose the VLR-Bench, a visual question answering (VQA) benchmark for evaluating vision language models (VLMs) based on retrieval augmented generation (RAG). Unlike existing evaluation datasets for external knowledge-based VQA, the proposed VLR-Bench includes five input passages. This allows testing of the ability to determine which passage is useful for answering a given query, a capability lacking in previous research. In this context, we constructed a dataset of 32,000 automatically generated instruction-following examples, which we denote as VLR-IF. This dataset is specifically designed to enhance the RAG capabilities of VLMs by enabling them to learn how to generate appropriate answers based on input passages. We evaluated the validity of the proposed benchmark and training data and verified its performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3 model. The proposed VLR-Bench and VLR-IF datasets are publicly available online.",
        "tags": [
            "LLaMA",
            "LLaVA",
            "RAG"
        ]
    },
    {
        "id": "76",
        "title": "Keep It Simple: Towards Accurate Vulnerability Detection for Large Code Graphs",
        "author": [
            "Xin Peng",
            "Shangwen Wang",
            "Yihao Qin",
            "Bo Lin",
            "Liqian Chen",
            "Xiaoguang Mao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10164",
        "abstract": "Software vulnerability detection is crucial for high-quality software development. Recently, some studies utilizing Graph Neural Networks (GNNs) to learn the graph representation of code in vulnerability detection tasks have achieved remarkable success. However, existing graph-based approaches mainly face two limitations that prevent them from generalizing well to large code graphs: (1) the interference of noise information in the code graph; (2) the difficulty in capturing long-distance dependencies within the graph. To mitigate these problems, we propose a novel vulnerability detection method, ANGLE, whose novelty mainly embodies the hierarchical graph refinement and context-aware graph representation learning. The former hierarchically filters redundant information in the code graph, thereby reducing the size of the graph, while the latter collaboratively employs the Graph Transformer and GNN to learn code graph representations from both the global and local perspectives, thus capturing long-distance dependencies. Extensive experiments demonstrate promising results on three widely used benchmark datasets: our method significantly outperforms several other baselines in terms of the accuracy and F1 score. Particularly, in large code graphs, ANGLE achieves an improvement in accuracy of 34.27%-161.93% compared to the state-of-the-art method, AMPLE. Such results demonstrate the effectiveness of ANGLE in vulnerability detection tasks.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "77",
        "title": "UN-DETR: Promoting Objectness Learning via Joint Supervision for Unknown Object Detection",
        "author": [
            "Haomiao Liu",
            "Hao Xu",
            "Chuhuai Yue",
            "Bo Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10176",
        "abstract": "Unknown Object Detection (UOD) aims to identify objects of unseen categories, differing from the traditional detection paradigm limited by the closed-world assumption. A key component of UOD is learning a generalized representation, i.e. objectness for both known and unknown categories to distinguish and localize objects from the background in a class-agnostic manner. However, previous methods obtain supervision signals for learning objectness in isolation from either localization or classification information, leading to poor performance for UOD. To address this issue, we propose a transformer-based UOD framework, UN-DETR. Based on this, we craft Instance Presence Score (IPS) to represent the probability of an object's presence. For the purpose of information complementarity, IPS employs a strategy of joint supervised learning, integrating attributes representing general objectness from the positional and the categorical latent space as supervision signals. To enhance IPS learning, we introduce a one-to-many assignment strategy to incorporate more supervision. Then, we propose Unbiased Query Selection to provide premium initial query vectors for the decoder. Additionally, we propose an IPS-guided post-process strategy to filter redundant boxes and correct classification predictions for known and unknown objects. Finally, we pretrain the entire UN-DETR in an unsupervised manner, in order to obtain objectness prior. Our UN-DETR is comprehensively evaluated on multiple UOD and known detection benchmarks, demonstrating its effectiveness and achieving state-of-the-art performance.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "78",
        "title": "SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models",
        "author": [
            "Hung Nguyen",
            "Quang Qui-Vinh Nguyen",
            "Khoi Nguyen",
            "Rang Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10178",
        "abstract": "Given an input video of a person and a new garment, the objective of this paper is to synthesize a new video where the person is wearing the specified garment while maintaining spatiotemporal consistency. While significant advances have been made in image-based virtual try-ons, extending these successes to video often results in frame-to-frame inconsistencies. Some approaches have attempted to address this by increasing the overlap of frames across multiple video chunks, but this comes at a steep computational cost due to the repeated processing of the same frames, especially for long video sequence. To address these challenges, we reconceptualize video virtual try-on as a conditional video inpainting task, with garments serving as input conditions. Specifically, our approach enhances image diffusion models by incorporating temporal attention layers to improve temporal coherence. To reduce computational overhead, we introduce ShiftCaching, a novel technique that maintains temporal consistency while minimizing redundant computations. Furthermore, we introduce the \\dataname~dataset, a new video try-on dataset featuring more complex backgrounds, challenging movements, and higher resolution compared to existing public datasets. Extensive experiments show that our approach outperforms current baselines, particularly in terms of video consistency and inference speed. Data and code are available at https://github.com/VinAIResearch/swift-try",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "79",
        "title": "Ultra-High Resolution Segmentation via Boundary-Enhanced Patch-Merging Transformer",
        "author": [
            "Haopeng Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10181",
        "abstract": "Segmentation of ultra-high resolution (UHR) images is a critical task with numerous applications, yet it poses significant challenges due to high spatial resolution and rich fine details. Recent approaches adopt a dual-branch architecture, where a global branch learns long-range contextual information and a local branch captures fine details. However, they struggle to handle the conflict between global and local information while adding significant extra computational cost. Inspired by the human visual system's ability to rapidly orient attention to important areas with fine details and filter out irrelevant information, we propose a novel UHR segmentation method called Boundary-enhanced Patch-merging Transformer (BPT). BPT consists of two key components: (1) Patch-Merging Transformer (PMT) for dynamically allocating tokens to informative regions to acquire global and local representations, and (2) Boundary-Enhanced Module (BEM) that leverages boundary information to enrich fine details. Extensive experiments on multiple UHR image segmentation benchmarks demonstrate that our BPT outperforms previous state-of-the-art methods without introducing extra computational overhead. Codes will be released to facilitate research.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "80",
        "title": "Retrieval-Augmented Semantic Parsing: Using Large Language Models to Improve Generalization",
        "author": [
            "Xiao Zhang",
            "Qianru Meng",
            "Johan Bos"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10207",
        "abstract": "Open-domain semantic parsing remains a challenging task, as models often rely on heuristics and struggle to handle unseen concepts. In this paper, we investigate the potential of large language models (LLMs) for this task and introduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective approach that integrates external lexical knowledge into the parsing process. Our experiments not only show that LLMs outperform previous encoder-decoder baselines for semantic parsing, but that RASP further enhances their ability to predict unseen concepts, nearly doubling the performance of previous models on out-of-distribution concepts. These findings highlight the promise of leveraging large language models and retrieval mechanisms for robust and open-domain semantic parsing.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "81",
        "title": "Efficient Generative Modeling with Residual Vector Quantization-Based Tokens",
        "author": [
            "Jaehyeon Kim",
            "Taehong Moon",
            "Keon Lee",
            "Jaewoong Cho"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10208",
        "abstract": "We explore the use of Residual Vector Quantization (RVQ) for high-fidelity generation in vector-quantized generative models. This quantization technique maintains higher data fidelity by employing more in-depth tokens. However, increasing the token number in generative models leads to slower inference speeds. To this end, we introduce ResGen, an efficient RVQ-based discrete diffusion model that generates high-fidelity samples without compromising sampling speed. Our key idea is a direct prediction of vector embedding of collective tokens rather than individual ones. Moreover, we demonstrate that our proposed token masking and multi-token prediction method can be formulated within a principled probabilistic framework using a discrete diffusion process and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation} on ImageNet 256x256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models. The project page can be found at https://resgen-genai.github.io",
        "tags": [
            "Diffusion",
            "Vector Quantization"
        ]
    },
    {
        "id": "82",
        "title": "GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion",
        "author": [
            "Jiapeng Tang",
            "Davide Davoli",
            "Tobias Kirschstein",
            "Liam Schoneveld",
            "Matthias Niessner"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10209",
        "abstract": "We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve details of facial identity and appearance. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms the previous state-of-the-art methods in novel view synthesis by a 5.34\\% higher SSIM score. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting",
            "VAE"
        ]
    },
    {
        "id": "83",
        "title": "Learning Complex Non-Rigid Image Edits from Multimodal Conditioning",
        "author": [
            "Nikolai Warner",
            "Jack Kolb",
            "Meera Hahn",
            "Vighnesh Birodkar",
            "Jonathan Huang",
            "Irfan Essa"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10219",
        "abstract": "In this paper we focus on inserting a given human (specifically, a single image of a person) into a novel scene. Our method, which builds on top of Stable Diffusion, yields natural looking images while being highly controllable with text and pose. To accomplish this we need to train on pairs of images, the first a reference image with the person, the second a \"target image\" showing the same person (with a different pose and possibly in a different background). Additionally we require a text caption describing the new pose relative to that in the reference image. In this paper we present a novel dataset following this criteria, which we create using pairs of frames from human-centric and action-rich videos and employing a multimodal LLM to automatically summarize the difference in human pose for the text captions. We demonstrate that identity preservation is a more challenging task in scenes \"in-the-wild\", and especially scenes where there is an interaction between persons and objects. Combining the weak supervision from noisy captions, with robust 2D pose improves the quality of person-object interactions.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "84",
        "title": "SPT: Sequence Prompt Transformer for Interactive Image Segmentation",
        "author": [
            "Senlin Cheng",
            "Haopeng Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10224",
        "abstract": "Interactive segmentation aims to extract objects of interest from an image based on user-provided clicks. In real-world applications, there is often a need to segment a series of images featuring the same target object. However, existing methods typically process one image at a time, failing to consider the sequential nature of the images. To overcome this limitation, we propose a novel method called Sequence Prompt Transformer (SPT), the first to utilize sequential image information for interactive segmentation. Our model comprises two key components: (1) Sequence Prompt Transformer (SPT) for acquiring information from sequence of images, clicks and masks to improve accurate. (2) Top-k Prompt Selection (TPS) selects precise prompts for SPT to further enhance the segmentation effect. Additionally, we create the ADE20K-Seq benchmark to better evaluate model performance. We evaluate our approach on multiple benchmark datasets and show that our model surpasses state-of-the-art methods across all datasets.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "85",
        "title": "SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians",
        "author": [
            "Siyun Liang",
            "Sen Wang",
            "Kunyi Li",
            "Michael Niemeyer",
            "Stefano Gasperini",
            "Nassir Navab",
            "Federico Tombari"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10231",
        "abstract": "3D Gaussian Splatting has recently gained traction for its efficient training and real-time rendering. While the vanilla Gaussian Splatting representation is mainly designed for view synthesis, more recent works investigated how to extend it with scene understanding and language features. However, existing methods lack a detailed comprehension of scenes, limiting their ability to segment and interpret complex structures. To this end, We introduce SuperGSeg, a novel approach that fosters cohesive, context-aware scene representation by disentangling segmentation and language field distillation. SuperGSeg first employs neural Gaussians to learn instance and hierarchical segmentation features from multi-view images with the aid of off-the-shelf 2D masks. These features are then leveraged to create a sparse set of what we call Super-Gaussians. Super-Gaussians facilitate the distillation of 2D language features into 3D space. Through Super-Gaussians, our method enables high-dimensional language feature rendering without extreme increases in GPU memory. Extensive experiments demonstrate that SuperGSeg outperforms prior works on both open-vocabulary object localization and semantic segmentation tasks.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Segmentation"
        ]
    },
    {
        "id": "86",
        "title": "Efficient Continual Pre-training of LLMs for Low-resource Languages",
        "author": [
            "Arijit Nag",
            "Soumen Chakrabarti",
            "Animesh Mukherjee",
            "Niloy Ganguly"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10244",
        "abstract": "Open-source Large Language models (OsLLMs) propel the democratization of natural language research by giving the flexibility to augment or update model parameters for performance improvement. Nevertheless, like proprietary LLMs, Os-LLMs offer poorer performance on low-resource languages (LRLs) than high-resource languages (HRLs), owing to smaller amounts of training data and underrepresented vocabulary. On the other hand, continual pre-training (CPT) with large amounts of language-specific data is a costly proposition in terms of data acquisition and computational resources. Our goal is to drastically reduce CPT cost. To that end, we first develop a new algorithm to select a subset of texts from a larger corpus. We show the effectiveness of our technique using very little CPT data. In search of further improvement, we design a new algorithm to select tokens to include in the LLM vocabulary. We experiment with the recent Llama-3 model and nine Indian languages with diverse scripts and extent of resource availability. For evaluation, we use IndicGenBench, a generation task benchmark dataset for Indic languages. We experiment with various CPT corpora and augmented vocabulary size and offer insights across language families.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "87",
        "title": "Detecting LLM Hallucination Through Layer-wise Information Deficiency: Analysis of Unanswerable Questions and Ambiguous Prompts",
        "author": [
            "Hazel Kim",
            "Adel Bibi",
            "Philip Torr",
            "Yarin Gal"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10246",
        "abstract": "Large language models (LLMs) frequently generate confident yet inaccurate responses, introducing significant risks for deployment in safety-critical domains. We present a novel approach to detecting model hallucination through systematic analysis of information flow across model layers when processing inputs with insufficient or ambiguous context. Our investigation reveals that hallucination manifests as usable information deficiencies in inter-layer transmissions. While existing approaches primarily focus on final-layer output analysis, we demonstrate that tracking cross-layer information dynamics ($\\mathcal{L}$I) provides robust indicators of model reliability, accounting for both information gain and loss during computation. $\\mathcal{L}$I improves model reliability by immediately integrating with universal LLMs without additional training or architectural modifications.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "88",
        "title": "Exploring the Frontiers of Animation Video Generation in the Sora Era: Method, Dataset and Benchmark",
        "author": [
            "Yudong Jiang",
            "Baohan Xu",
            "Siqian Yang",
            "Mingyu Yin",
            "Jing Liu",
            "Chao Xu",
            "Siqi Wang",
            "Yidi Wu",
            "Bingwen Zhu",
            "Jixuan Xu",
            "Yue Zhang",
            "Jinlong Hou",
            "Huyang Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10255",
        "abstract": "Animation has gained significant interest in the recent film and TV industry. Despite the success of advanced video generation models like Sora, Kling, and CogVideoX in generating natural videos, they lack the same effectiveness in handling animation videos. Evaluating animation video generation is also a great challenge due to its unique artist styles, violating the laws of physics and exaggerated motions. In this paper, we present a comprehensive system, AniSora, designed for animation video generation, which includes a data processing pipeline, a controllable generation model, and an evaluation dataset. Supported by the data processing pipeline with over 10M high-quality data, the generation model incorporates a spatiotemporal mask module to facilitate key animation production functions such as image-to-video generation, frame interpolation, and localized image-guided animation. We also collect an evaluation benchmark of 948 various animation videos, the evaluation on VBench and human double-blind test demonstrates consistency in character and motion, achieving state-of-the-art results in animation video generation. %We also collect an evaluation benchmark of 948 various animation videos, with specifically developed metrics for animation video generation. Our model access API and evaluation benchmark will be publicly available.",
        "tags": [
            "Sora",
            "Video Generation"
        ]
    },
    {
        "id": "89",
        "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models",
        "author": [
            "Harry J. Davies",
            "Giorgos Iacovides",
            "Danilo P. Mandic"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10257",
        "abstract": "The sheer scale of data required to train modern large language models (LLMs) poses significant risks, as models are likely to gain knowledge of sensitive topics such as bio-security, as well the ability to replicate copyrighted works. Methods designed to remove such knowledge must do so from all prompt directions, in a multi-lingual capacity and without degrading general model performance. To this end, we introduce the targeted angular reversal (TARS) method of knowledge removal from LLMs. The TARS method firstly leverages the LLM in combination with a detailed prompt to aggregate information about a selected concept in the internal representation space of the LLM. It then refines this approximate concept vector to trigger the concept token with high probability, by perturbing the approximate concept vector with noise and transforming it into token scores with the language model head. The feedforward weight vectors in the LLM which operate directly on the internal representation space, and have the highest cosine similarity with this targeting vector, are then replaced by a reversed targeting vector, thus limiting the ability of the concept to propagate through the model. The modularity of the TARS method allows for a sequential removal of concepts from Llama 3.1 8B, such as the famous literary detective Sherlock Holmes, and the planet Saturn. It is demonstrated that the probability of triggering target concepts can be reduced to 0.00 with as few as 1 TARS edit, whilst simultaneously removing the knowledge bi-directionally. Moreover, knowledge is shown to be removed across all languages despite only being targeted in English. Importantly, TARS has minimal impact on the general model capabilities, as after removing 5 diverse concepts in a modular fashion, there is minimal KL divergence in the next token probabilities of the LLM on large corpora of Wikipedia text (median of 0.002).",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "90",
        "title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
        "author": [
            "Shuaiting Li",
            "Chengxuan Wang",
            "Juncan Deng",
            "Zeyu Wang",
            "Zewen Ye",
            "Zongsheng Wang",
            "Haibin Shen",
            "Kejie Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10261",
        "abstract": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
        "tags": [
            "Detection",
            "Segmentation",
            "Vector Quantization"
        ]
    },
    {
        "id": "91",
        "title": "Reasoner Outperforms: Generative Stance Detection with Rationalization for Social Media",
        "author": [
            "Jiaqing Yuan",
            "Ruijie Xi",
            "Munindar P. Singh"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10266",
        "abstract": "Stance detection is crucial for fostering a human-centric Web by analyzing user-generated content to identify biases and harmful narratives that undermine trust. With the development of Large Language Models (LLMs), existing approaches treat stance detection as a classification problem, providing robust methodologies for modeling complex group interactions and advancing capabilities in natural language tasks. However, these methods often lack interpretability, limiting their ability to offer transparent and understandable justifications for predictions. This study adopts a generative approach, where stance predictions include explicit, interpretable rationales, and integrates them into smaller language models through single-task and multitask learning. We find that incorporating reasoning into stance detection enables the smaller model (FlanT5) to outperform GPT-3.5's zero-shot performance, achieving an improvement of up to 9.57%. Moreover, our results show that reasoning capabilities enhance multitask learning performance but may reduce effectiveness in single-task settings. Crucially, we demonstrate that faithful rationales improve rationale distillation into SLMs, advancing efforts to build interpretable, trustworthy systems for addressing discrimination, fostering trust, and promoting equitable engagement on social media.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "92",
        "title": "Does Multiple Choice Have a Future in the Age of Generative AI? A Posttest-only RCT",
        "author": [
            "Danielle R. Thomas",
            "Conrad Borchers",
            "Sanjit Kakarla",
            "Jionghao Lin",
            "Shambhavi Bhushan",
            "Boyuan Guo",
            "Erin Gatz",
            "Kenneth R. Koedinger"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10267",
        "abstract": "The role of multiple-choice questions (MCQs) as effective learning tools has been debated in past research. While MCQs are widely used due to their ease in grading, open response questions are increasingly used for instruction, given advances in large language models (LLMs) for automated grading. This study evaluates MCQs effectiveness relative to open-response questions, both individually and in combination, on learning. These activities are embedded within six tutor lessons on advocacy. Using a posttest-only randomized control design, we compare the performance of 234 tutors (790 lesson completions) across three conditions: MCQ only, open response only, and a combination of both. We find no significant learning differences across conditions at posttest, but tutors in the MCQ condition took significantly less time to complete instruction. These findings suggest that MCQs are as effective, and more efficient, than open response tasks for learning when practice time is limited. To further enhance efficiency, we autograded open responses using GPT-4o and GPT-4-turbo. GPT models demonstrate proficiency for purposes of low-stakes assessment, though further research is needed for broader use. This study contributes a dataset of lesson log data, human annotation rubrics, and LLM prompts to promote transparency and reproducibility.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "93",
        "title": "Cultural Evolution of Cooperation among LLM Agents",
        "author": [
            "Aron Vallinder",
            "Edward Hughes"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10270",
        "abstract": "Large language models (LLMs) provide a compelling foundation for building generally-capable AI agents. These agents may soon be deployed at scale in the real world, representing the interests of individual humans (e.g., AI assistants) or groups of humans (e.g., AI-accelerated corporations). At present, relatively little is known about the dynamics of multiple LLM agents interacting over many generations of iterative deployment. In this paper, we examine whether a \"society\" of LLM agents can learn mutually beneficial social norms in the face of incentives to defect, a distinctive feature of human sociality that is arguably crucial to the success of civilization. In particular, we study the evolution of indirect reciprocity across generations of LLM agents playing a classic iterated Donor Game in which agents can observe the recent behavior of their peers. We find that the evolution of cooperation differs markedly across base models, with societies of Claude 3.5 Sonnet agents achieving significantly higher average scores than Gemini 1.5 Flash, which, in turn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an additional mechanism for costly punishment to achieve yet higher scores, while Gemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also observe variation in emergent behavior across random seeds, suggesting an understudied sensitive dependence on initial conditions. We suggest that our evaluation regime could inspire an inexpensive and informative new class of LLM benchmarks, focussed on the implications of LLM agent deployment for the cooperative infrastructure of society.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "94",
        "title": "Benchmarking Linguistic Diversity of Large Language Models",
        "author": [
            "Yanzhu Guo",
            "Guokan Shang",
            "Chloé Clavel"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10271",
        "abstract": "The development and evaluation of Large Language Models (LLMs) has primarily focused on their task-solving capabilities, with recent models even surpassing human performance in some areas. However, this focus often neglects whether machine-generated language matches the human level of diversity, in terms of vocabulary choice, syntactic construction, and expression of meaning, raising questions about whether the fundamentals of language generation have been fully addressed. This paper emphasizes the importance of examining the preservation of human linguistic richness by language models, given the concerning surge in online content produced or aided by LLMs. We propose a comprehensive framework for evaluating LLMs from various linguistic diversity perspectives including lexical, syntactic, and semantic dimensions. Using this framework, we benchmark several state-of-the-art LLMs across all diversity dimensions, and conduct an in-depth case study for syntactic diversity. Finally, we analyze how different development and deployment choices impact the linguistic diversity of LLM outputs.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "95",
        "title": "Probabilistic Inverse Cameras: Image to 3D via Multiview Geometry",
        "author": [
            "Rishabh Kabra",
            "Drew A. Hudson",
            "Sjoerd van Steenkiste",
            "Joao Carreira",
            "Niloy J. Mitra"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10273",
        "abstract": "We introduce a hierarchical probabilistic approach to go from a 2D image to multiview 3D: a diffusion \"prior\" models the unseen 3D geometry, which then conditions a diffusion \"decoder\" to generate novel views of the subject. We use a pointmap-based geometric representation in a multiview image format to coordinate the generation of multiple target views simultaneously. We facilitate correspondence between views by assuming fixed target camera poses relative to the source camera, and constructing a predictable distribution of geometric features per target. Our modular, geometry-driven approach to novel-view synthesis (called \"unPIC\") beats SoTA baselines such as CAT3D and One-2-3-45 on held-out objects from ObjaverseXL, as well as real-world objects ranging from Google Scanned Objects, Amazon Berkeley Objects, to the Digital Twin Catalog.",
        "tags": [
            "3D",
            "Diffusion",
            "Image-to-3D"
        ]
    },
    {
        "id": "96",
        "title": "TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to Video Generation",
        "author": [
            "Xingrui Wang",
            "Xin Li",
            "Yaosi Hu",
            "Hanxin Zhu",
            "Chen Hou",
            "Cuiling Lan",
            "Zhibo Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10275",
        "abstract": "Text-driven Image to Video Generation (TI2V) aims to generate controllable video given the first frame and corresponding textual description. The primary challenges of this task lie in two parts: (i) how to identify the target objects and ensure the consistency between the movement trajectory and the textual description. (ii) how to improve the subjective quality of generated videos. To tackle the above challenges, we propose a new diffusion-based TI2V framework, termed TIV-Diffusion, via object-centric textual-visual alignment, intending to achieve precise control and high-quality video generation based on textual-described motion for different objects. Concretely, we enable our TIV-Diffuion model to perceive the textual-described objects and their motion trajectory by incorporating the fused textual and visual knowledge through scale-offset modulation. Moreover, to mitigate the problems of object disappearance and misaligned objects and motion, we introduce an object-centric textual-visual alignment module, which reduces the risk of misaligned objects/motion by decoupling the objects in the reference image and aligning textual features with each object individually. Based on the above innovations, our TIV-Diffusion achieves state-of-the-art high-quality video generation compared with existing TI2V methods.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "97",
        "title": "One world, one opinion? The superstar effect in LLM responses",
        "author": [
            "Sofie Goethals",
            "Lauren Rhue"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10281",
        "abstract": "As large language models (LLMs) are shaping the way information is shared and accessed online, their opinions have the potential to influence a wide audience. This study examines who the LLMs view as the most prominent figures across various fields, using prompts in ten different languages to explore the influence of linguistic diversity. Our findings reveal low diversity in responses, with a small number of figures dominating recognition across languages (also known as the \"superstar effect\"). These results highlight the risk of narrowing global knowledge representation when LLMs retrieve subjective information.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "98",
        "title": "Still \"Talking About Large Language Models\": Some Clarifications",
        "author": [
            "Murray Shanahan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10291",
        "abstract": "My paper \"Talking About Large Language Models\" has more than once been interpreted as advocating a reductionist stance towards large language models. But the paper was not intended that way, and I do not endorse such positions. This short note situates the paper in the context of a larger philosophical project that is concerned with the (mis)use of words rather than metaphysics, in the spirit of Wittgenstein's later writing.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "99",
        "title": "Prompt-Guided Mask Proposal for Two-Stage Open-Vocabulary Segmentation",
        "author": [
            "Yu-Jhe Li",
            "Xinyang Zhang",
            "Kun Wan",
            "Lantao Yu",
            "Ajinkya Kale",
            "Xin Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10292",
        "abstract": "We tackle the challenge of open-vocabulary segmentation, where we need to identify objects from a wide range of categories in different environments, using text prompts as our input. To overcome this challenge, existing methods often use multi-modal models like CLIP, which combine image and text features in a shared embedding space to bridge the gap between limited and extensive vocabulary recognition, resulting in a two-stage approach: In the first stage, a mask generator takes an input image to generate mask proposals, and the in the second stage the target mask is picked based on the query. However, the expected target mask may not exist in the generated mask proposals, which leads to an unexpected output mask. In our work, we propose a novel approach named Prompt-guided Mask Proposal (PMP) where the mask generator takes the input text prompts and generates masks guided by these prompts. Compared with mask proposals generated without input prompts, masks generated by PMP are better aligned with the input prompts. To realize PMP, we designed a cross-attention mechanism between text tokens and query tokens which is capable of generating prompt-guided mask proposals after each decoding. We combined our PMP with several existing works employing a query-based segmentation backbone and the experiments on five benchmark datasets demonstrate the effectiveness of this approach, showcasing significant improvements over the current two-stage models (1% ~ 3% absolute performance gain in terms of mIOU). The steady improvement in performance across these benchmarks indicates the effective generalization of our proposed lightweight prompt-aware method.",
        "tags": [
            "CLIP",
            "Segmentation"
        ]
    },
    {
        "id": "100",
        "title": "Coherent 3D Scene Diffusion From a Single RGB Image",
        "author": [
            "Manuel Dahnert",
            "Angela Dai",
            "Norman Müller",
            "Matthias Nießner"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10294",
        "abstract": "We present a novel diffusion-based approach for coherent 3D scene reconstruction from a single RGB image. Our method utilizes an image-conditioned 3D scene diffusion model to simultaneously denoise the 3D poses and geometries of all objects within the scene. Motivated by the ill-posed nature of the task and to obtain consistent scene reconstruction results, we learn a generative scene prior by conditioning on all scene objects simultaneously to capture the scene context and by allowing the model to learn inter-object relationships throughout the diffusion process. We further propose an efficient surface alignment loss to facilitate training even in the absence of full ground-truth annotation, which is common in publicly available datasets. This loss leverages an expressive shape representation, which enables direct point sampling from intermediate shape predictions. By framing the task of single RGB image 3D scene reconstruction as a conditional diffusion process, our approach surpasses current state-of-the-art methods, achieving a 12.04% improvement in AP3D on SUN RGB-D and a 13.43% increase in F-Score on Pix3D.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "101",
        "title": "MST-R: Multi-Stage Tuning for Retrieval Systems and Metric Evaluation",
        "author": [
            "Yash Malviya",
            "Karan Dhingra",
            "Maneesh Singh"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10313",
        "abstract": "Regulatory documents are rich in nuanced terminology and specialized semantics. FRAG systems: Frozen retrieval-augmented generators utilizing pre-trained (or, frozen) components face consequent challenges with both retriever and answering performance. We present a system that adapts the retriever performance to the target domain using a multi-stage tuning (MST) strategy. Our retrieval approach, called MST-R (a) first fine-tunes encoders used in vector stores using hard negative mining, (b) then uses a hybrid retriever, combining sparse and dense retrievers using reciprocal rank fusion, and then (c) adapts the cross-attention encoder by fine-tuning only the top-k retrieved results. We benchmark the system performance on the dataset released for the RIRAG challenge (as part of the RegNLP workshop at COLING 2025). We achieve significant performance gains obtaining a top rank on the RegNLP challenge leaderboard. We also show that a trivial answering approach games the RePASs metric outscoring all baselines and a pre-trained Llama model. Analyzing this anomaly, we present important takeaways for future research.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "102",
        "title": "BrushEdit: All-In-One Image Inpainting and Editing",
        "author": [
            "Yaowei Li",
            "Yuxuan Bian",
            "Xuan Ju",
            "Zhaoyang Zhang",
            "Ying Shan",
            "Qiang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10316",
        "abstract": "Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence.",
        "tags": [
            "Diffusion",
            "Image Editing",
            "Large Language Models"
        ]
    },
    {
        "id": "103",
        "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
        "author": [
            "Yucheng Li",
            "Huiqiang Jiang",
            "Qianhui Wu",
            "Xufang Luo",
            "Surin Ahn",
            "Chengruidong Zhang",
            "Amir H. Abdi",
            "Dongsheng Li",
            "Jianfeng Gao",
            "Yuqing Yang",
            "Lili Qiu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10319",
        "abstract": "Long-context LLMs have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache. However, existing benchmarks often evaluate in single-request, neglecting the full lifecycle of the KV cache in real-world use. This oversight is particularly critical, as KV cache reuse has become widely adopted in LLMs inference frameworks, such as vLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft, Google, and Anthropic. To address this gap, we introduce SCBench(SharedContextBench), a comprehensive benchmark for evaluating long-context methods from a KV cachecentric perspective: 1) KV cache generation, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache loading. Specifically, SCBench uses test examples with shared context, ranging 12 tasks with two shared context modes, covering four categories of long-context capabilities: string retrieval, semantic retrieval, global information, and multi-task. With it, we provide an extensive KV cache-centric analysis of eight categories long-context solutions, including Gated Linear RNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention, KV cache dropping, quantization, retrieval, loading, and prompt compression. The evaluation is conducted on 8 long-context LLMs. Our findings show that sub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding with O(n) memory and sub-O(n^2) pre-filling computation perform robustly. Dynamic sparsity yields more expressive KV caches than static patterns, and layer-level sparsity in hybrid architectures reduces memory usage with strong performance. Additionally, we identify attention distribution shift issues in long-generation scenarios. https://aka.ms/SCBench.",
        "tags": [
            "LLMs",
            "Mamba"
        ]
    },
    {
        "id": "104",
        "title": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks",
        "author": [
            "Sicheng Zhu",
            "Brandon Amos",
            "Yuandong Tian",
            "Chuan Guo",
            "Ivan Evtimov"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10321",
        "abstract": "Many jailbreak attacks on large language models (LLMs) rely on a common objective: making the model respond with the prefix \"Sure, here is (harmful request)\". While straightforward, this objective has two limitations: limited control over model behaviors, often resulting in incomplete or unrealistic responses, and a rigid format that hinders optimization. To address these limitations, we introduce AdvPrefix, a new prefix-forcing objective that enables more nuanced control over model behavior while being easy to optimize. Our objective leverages model-dependent prefixes, automatically selected based on two criteria: high prefilling attack success rates and low negative log-likelihood. It can further simplify optimization by using multiple prefixes for a single user request. AdvPrefix can integrate seamlessly into existing jailbreak attacks to improve their performance for free. For example, simply replacing GCG attack's target prefixes with ours on Llama-3 improves nuanced attack success rates from 14% to 80%, suggesting that current alignment struggles to generalize to unseen prefixes. Our work demonstrates the importance of jailbreak objectives in achieving nuanced jailbreaks.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "105",
        "title": "XYScanNet: An Interpretable State Space Model for Perceptual Image Deblurring",
        "author": [
            "Hanzhou Liu",
            "Chengkai Liu",
            "Jiacong Xu",
            "Peng Jiang",
            "Mi Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10338",
        "abstract": "Deep state-space models (SSMs), like recent Mamba architectures, are emerging as a promising alternative to CNN and Transformer networks. Existing Mamba-based restoration methods process the visual data by leveraging a flatten-and-scan strategy that converts image patches into a 1D sequence before scanning. However, this scanning paradigm ignores local pixel dependencies and introduces spatial misalignment by positioning distant pixels incorrectly adjacent, which reduces local noise-awareness and degrades image sharpness in low-level vision tasks. To overcome these issues, we propose a novel slice-and-scan strategy that alternates scanning along intra- and inter-slices. We further design a new Vision State Space Module (VSSM) for image deblurring, and tackle the inefficiency challenges of the current Mamba-based vision module. Building upon this, we develop XYScanNet, an SSM architecture integrated with a lightweight feature fusion module for enhanced image deblurring. XYScanNet, maintains competitive distortion metrics and significantly improves perceptual performance. Experimental results show that XYScanNet enhances KID by $17\\%$ compared to the nearest competitor. Our code will be released soon.",
        "tags": [
            "Mamba",
            "SSMs",
            "State Space Models",
            "Transformer"
        ]
    },
    {
        "id": "106",
        "title": "A Universal Degradation-based Bridging Technique for Domain Adaptive Semantic Segmentation",
        "author": [
            "Wangkai Li",
            "Rui Sun",
            "Tianzhu Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10339",
        "abstract": "Semantic segmentation often suffers from significant performance degradation when the trained network is applied to a different domain. To address this issue, unsupervised domain adaptation (UDA) has been extensively studied. Existing methods introduce the domain bridging techniques to mitigate substantial domain gap, which construct intermediate domains to facilitate the gradual transfer of knowledge across different domains. However, these strategies often require dataset-specific designs and may generate unnatural intermediate distributions that lead to semantic shift. In this paper, we propose DiDA, a universal degradation-based bridging technique formalized as a diffusion forward process. DiDA consists of two key modules: (1) Degradation-based Intermediate Domain Construction, which creates continuous intermediate domains through simple image degradation operations to encourage learning domain-invariant features as domain differences gradually diminish; (2) Semantic Shift Compensation, which leverages a diffusion encoder to encode and compensate for semantic shift information with degraded time-steps, preserving discriminative representations in the intermediate domains. As a plug-and-play solution, DiDA supports various degradation operations and seamlessly integrates with existing UDA methods. Extensive experiments on prevalent synthetic-to-real semantic segmentation benchmarks demonstrate that DiDA consistently improves performance across different settings and achieves new state-of-the-art results when combined with existing methods.",
        "tags": [
            "Diffusion",
            "Segmentation"
        ]
    },
    {
        "id": "107",
        "title": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining",
        "author": [
            "Zhiqi Ge",
            "Juncheng Li",
            "Xinglei Pang",
            "Minghe Gao",
            "Kaihang Pan",
            "Wang Lin",
            "Hao Fei",
            "Wenqiao Zhang",
            "Siliang Tang",
            "Yueting Zhuang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10342",
        "abstract": "Digital agents are increasingly employed to automate tasks in interactive digital environments such as web pages, software applications, and operating systems. While text-based agents built on Large Language Models (LLMs) often require frequent updates due to platform-specific APIs, visual agents leveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability by interacting directly with Graphical User Interfaces (GUIs). However, these agents face significant challenges in visual perception, particularly when handling high-resolution, visually complex digital environments. This paper introduces Iris, a foundational visual agent that addresses these challenges through two key innovations: Information-Sensitive Cropping (ISC) and Self-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes visually dense regions using a edge detection algorithm, enabling efficient processing by allocating more computational resources to areas with higher information density. SRDL enhances the agent's ability to handle complex tasks by leveraging a dual-learning loop, where improvements in referring (describing UI elements) reinforce grounding (locating elements) and vice versa, all without requiring additional annotated data. Empirical evaluations demonstrate that Iris achieves state-of-the-art performance across multiple benchmarks with only 850K GUI annotations, outperforming methods using 10x more training data. These improvements further translate to significant gains in both web and OS agent downstream tasks.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "108",
        "title": "Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration",
        "author": [
            "Lai Wei",
            "Jiahua Ma",
            "Yibo Hu",
            "Ruimao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10349",
        "abstract": "In dynamic environments, robots often encounter constrained movement trajectories when manipulating objects with specific properties, such as doors. Therefore, applying the appropriate force is crucial to prevent damage to both the robots and the objects. However, current vision-guided robot state generation methods often falter in this regard, as they lack the integration of tactile perception. To tackle this issue, this paper introduces a novel state diffusion framework termed SafeDiff. It generates a prospective state sequence from the current robot state and visual context observation while incorporating real-time tactile feedback to refine the sequence. As far as we know, this is the first study specifically focused on ensuring force safety in robotic manipulation. It significantly enhances the rationality of state planning, and the safe action trajectory is derived from inverse dynamics based on this refined planning. In practice, unlike previous approaches that concatenate visual and tactile data to generate future robot state sequences, our method employs tactile data as a calibration signal to adjust the robot's state within the state space implicitly. Additionally, we've developed a large-scale simulation dataset called SafeDoorManip50k, offering extensive multimodal data to train and evaluate the proposed method. Extensive experiments show that our visual-tactile model substantially mitigates the risk of harmful forces in the door opening, across both simulated and real-world settings.",
        "tags": [
            "Diffusion",
            "Robot"
        ]
    },
    {
        "id": "109",
        "title": "VibrantVS: A high-resolution multi-task transformer for forest canopy height estimation",
        "author": [
            "Tony Chang",
            "Kiarie Ndegwa",
            "Andreas Gros",
            "Vincent A. Landau",
            "Luke J. Zachmann",
            "Bogdan State",
            "Mitchell A. Gritts",
            "Colton W. Miller",
            "Nathan E. Rutenbeck",
            "Scott Conway",
            "Guy Bayes"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10351",
        "abstract": "This paper explores the application of a novel multi-task vision transformer (ViT) model for the estimation of canopy height models (CHMs) using 4-band National Agriculture Imagery Program (NAIP) imagery across the western United States. We compare the effectiveness of this model in terms of accuracy and precision aggregated across ecoregions and class heights versus three other benchmark peer-reviewed models. Key findings suggest that, while other benchmark models can provide high precision in localized areas, the VibrantVS model has substantial advantages across a broad reach of ecoregions in the western United States with higher accuracy, higher precision, the ability to generate updated inference at a cadence of three years or less, and high spatial resolution. The VibrantVS model provides significant value for ecological monitoring and land management decisions for wildfire mitigation.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "110",
        "title": "Robust image classification with multi-modal large language models",
        "author": [
            "Francesco Villani",
            "Igor Maljkovic",
            "Dario Lazzaro",
            "Angelo Sotgiu",
            "Antonio Emanuele Cinà",
            "Fabio Roli"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10353",
        "abstract": "Deep Neural Networks are vulnerable to adversarial examples, i.e., carefully crafted input samples that can cause models to make incorrect predictions with high confidence. To mitigate these vulnerabilities, adversarial training and detection-based defenses have been proposed to strengthen models in advance. However, most of these approaches focus on a single data modality, overlooking the relationships between visual patterns and textual descriptions of the input. In this paper, we propose a novel defense, Multi-Shield, designed to combine and complement these defenses with multi-modal information to further enhance their robustness. Multi-Shield leverages multi-modal large language models to detect adversarial examples and abstain from uncertain classifications when there is no alignment between textual and visual representations of the input. Extensive evaluations on CIFAR-10 and ImageNet datasets, using robust and non-robust image classification models, demonstrate that Multi-Shield can be easily integrated to detect and reject adversarial examples, outperforming the original defenses.",
        "tags": [
            "Detection",
            "Large Language Models"
        ]
    },
    {
        "id": "111",
        "title": "OP-LoRA: The Blessing of Dimensionality",
        "author": [
            "Piotr Teterwak",
            "Kate Saenko",
            "Bryan A. Plummer",
            "Ser-Nam Lim"
        ],
        "pdf": "https://arxiv.org/pdf/2412.10362",
        "abstract": "Low-rank adapters enable fine-tuning of large models with only a small number of parameters, thus reducing storage costs and minimizing the risk of catastrophic forgetting. However, they often pose optimization challenges, with poor convergence. To overcome these challenges, we introduce an over-parameterized approach that accelerates training without increasing inference costs. This method reparameterizes low-rank adaptation by employing a separate MLP and learned embedding for each layer. The learned embedding is input to the MLP, which generates the adapter parameters. Such overparamaterization has been shown to implicitly function as an adaptive learning rate and momentum, accelerating optimization. At inference time, the MLP can be discarded, leaving behind a standard low-rank adapter. To study the effect of MLP overparameterization on a small yet difficult proxy task, we implement it for matrix factorization, and find it achieves faster convergence and lower final loss. Extending this approach to larger-scale tasks, we observe consistent performance gains across domains. We achieve improvements in vision-language tasks and especially notable increases in image generation, with CMMD scores improving by up to 15 points.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "112",
        "title": "RealOSR: Latent Unfolding Boosting Diffusion-based Real-world Omnidirectional Image Super-Resolution",
        "author": [
            "Xuhan Sheng",
            "Runyi Li",
            "Bin Chen",
            "Weiqi Li",
            "Xu Jiang",
            "Jian Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09646",
        "abstract": "Omnidirectional image super-resolution (ODISR) aims to upscale low-resolution (LR) omnidirectional images (ODIs) to high-resolution (HR), addressing the growing demand for detailed visual content across a $180^{\\circ}\\times360^{\\circ}$ viewport. Existing methods are limited by simple degradation assumptions (e.g., bicubic downsampling), which fail to capture the complex, unknown real-world degradation processes. Recent diffusion-based approaches suffer from slow inference due to their hundreds of sampling steps and frequent pixel-latent space conversions. To tackle these challenges, in this paper, we propose RealOSR, a novel diffusion-based approach for real-world ODISR (Real-ODISR) with single-step diffusion denoising. To sufficiently exploit the input information, RealOSR introduces a lightweight domain alignment module, which facilitates the efficient injection of LR ODI into the single-step latent denoising. Additionally, to better utilize the rich semantic and multi-scale feature modeling ability of denoising UNet, we develop a latent unfolding module that simulates the gradient descent process directly in latent space. Experimental results demonstrate that RealOSR outperforms previous methods in both ODI recovery quality and efficiency. Compared to the recent state-of-the-art diffusion-based ODISR method, OmniSSR, RealOSR achieves significant improvements in visual quality and over \\textbf{200$\\times$} inference acceleration. Our code and models will be released.",
        "tags": [
            "Diffusion",
            "Super Resolution"
        ]
    },
    {
        "id": "113",
        "title": "DSplats: 3D Generation by Denoising Splats-Based Multiview Diffusion Models",
        "author": [
            "Kevin Miao",
            "Harsh Agrawal",
            "Qihang Zhang",
            "Federico Semeraro",
            "Marco Cavallo",
            "Jiatao Gu",
            "Alexander Toshev"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09648",
        "abstract": "Generating high-quality 3D content requires models capable of learning robust distributions of complex scenes and the real-world objects within them. Recent Gaussian-based 3D reconstruction techniques have achieved impressive results in recovering high-fidelity 3D assets from sparse input images by predicting 3D Gaussians in a feed-forward manner. However, these techniques often lack the extensive priors and expressiveness offered by Diffusion Models. On the other hand, 2D Diffusion Models, which have been successfully applied to denoise multiview images, show potential for generating a wide range of photorealistic 3D outputs but still fall short on explicit 3D priors and consistency. In this work, we aim to bridge these two approaches by introducing DSplats, a novel method that directly denoises multiview images using Gaussian Splat-based Reconstructors to produce a diverse array of realistic 3D assets. To harness the extensive priors of 2D Diffusion Models, we incorporate a pretrained Latent Diffusion Model into the reconstructor backbone to predict a set of 3D Gaussians. Additionally, the explicit 3D representation embedded in the denoising network provides a strong inductive bias, ensuring geometrically consistent novel view generation. Our qualitative and quantitative experiments demonstrate that DSplats not only produces high-quality, spatially consistent outputs, but also sets a new standard in single-image to 3D reconstruction. When evaluated on the Google Scanned Objects dataset, DSplats achieves a PSNR of 20.38, an SSIM of 0.842, and an LPIPS of 0.109.",
        "tags": [
            "3D",
            "Diffusion",
            "Image-to-3D"
        ]
    },
    {
        "id": "114",
        "title": "CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on Conditional Transformer with Fine-Grained Lyric and Musical Controls",
        "author": [
            "Li Chai",
            "Donglin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.09887",
        "abstract": "Lyric-to-melody generation is a highly challenging task in the field of AI music generation. Due to the difficulty of learning strict yet weak correlations between lyrics and melodies, previous methods have suffered from weak controllability, low-quality and poorly structured generation. To address these challenges, we propose CSL-L2M, a controllable song-level lyric-to-melody generation method based on an in-attention Transformer decoder with fine-grained lyric and musical controls, which is able to generate full-song melodies matched with the given lyrics and user-specified musical attributes. Specifically, we first introduce REMI-Aligned, a novel music representation that incorporates strict syllable- and sentence-level alignments between lyrics and melodies, facilitating precise alignment modeling. Subsequently, sentence-level semantic lyric embeddings independently extracted from a sentence-wise Transformer encoder are combined with word-level part-of-speech embeddings and syllable-level tone embeddings as fine-grained controls to enhance the controllability of lyrics over melody generation. Then we introduce human-labeled musical tags, sentence-level statistical musical attributes, and learned musical features extracted from a pre-trained VQ-VAE as coarse-grained, fine-grained and high-fidelity controls, respectively, to the generation process, thereby enabling user control over melody generation. Finally, an in-attention Transformer decoder technique is leveraged to exert fine-grained control over the full-song melody generation with the aforementioned lyric and musical conditions. Experimental results demonstrate that our proposed CSL-L2M outperforms the state-of-the-art models, generating melodies with higher quality, better controllability and enhanced structure. Demos and source code are available at https://lichaiustc.github.io/CSL-L2M/.",
        "tags": [
            "Transformer",
            "VAE"
        ]
    }
]