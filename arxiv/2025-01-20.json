[
    {
        "id": "1",
        "title": "Enhancing the De-identification of Personally Identifiable Information in Educational Data",
        "author": [
            "Y. Shen",
            "Z. Ji",
            "J. Lin",
            "K. R. Koedginer"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09765",
        "abstract": "Protecting Personally Identifiable Information (PII), such as names, is a critical requirement in learning technologies to safeguard student and teacher privacy and maintain trust. Accurate PII detection is an essential step toward anonymizing sensitive information while preserving the utility of educational data. Motivated by recent advancements in artificial intelligence, our study investigates the GPT-4o-mini model as a cost-effective and efficient solution for PII detection tasks. We explore both prompting and fine-tuning approaches and compare GPT-4o-mini's performance against established frameworks, including Microsoft Presidio and Azure AI Language. Our evaluation on two public datasets, CRAPII and TSCC, demonstrates that the fine-tuned GPT-4o-mini model achieves superior performance, with a recall of 0.9589 on CRAPII. Additionally, fine-tuned GPT-4o-mini significantly improves precision scores (a threefold increase) while reducing computational costs to nearly one-tenth of those associated with Azure AI Language. Furthermore, our bias analysis reveals that the fine-tuned GPT-4o-mini model consistently delivers accurate results across diverse cultural backgrounds and genders. The generalizability analysis using the TSCC dataset further highlights its robustness, achieving a recall of 0.9895 with minimal additional training data from TSCC. These results emphasize the potential of fine-tuned GPT-4o-mini as an accurate and cost-effective tool for PII detection in educational data. It offers robust privacy protection while preserving the data's utility for research and pedagogical analysis. Our code is available on GitHub: https://github.com/AnonJD/PrivacyAI",
        "tags": [
            "Detection",
            "GPT"
        ]
    },
    {
        "id": "2",
        "title": "Boosting Tool Use of Large Language Models via Iterative Reinforced Fine-Tuning",
        "author": [
            "Yirong Zeng",
            "Xiao Ding",
            "Yuxian Wang",
            "Weiwen Liu",
            "Wu Ning",
            "Yutai Hou",
            "Xu Huang",
            "Bing Qin",
            "Ting Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09766",
        "abstract": "Augmenting large language models (LLMs) with external tools is a promising approach to enhance their capabilities. Effectively leveraging this potential for complex tasks hinges crucially on improving their ability to use tools. Synthesizing tool use data by simulating the real world is an effective approach. Nevertheless, our investigation reveals that training gains significantly decay as the scale of these data increases. The primary factor is the model's poor performance (a.k.a deficiency) in complex scenarios, which hinders learning from data using SFT. Driven by this objective, we propose an iterative reinforced fine-tuning strategy to continually guide the model to alleviate it. Specifically, we first identify deficiency-related data based on feedback from the policy model, then perform a Monte Carlo Tree Search to collect fine-grained preference pairs to pinpoint deficiencies. Subsequently, we update the policy model using preference optimization to align with ground truth and misalign with deficiencies. This process can be iterated. Moreover, before the iteration, we propose an easy-to-hard warm-up SFT strategy to facilitate learning from challenging data. The experiments demonstrate our models go beyond the same parametric models, outperforming many larger open-source and closed-source models. Additionally, it has achieved notable training gains in complex tool use scenarios.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "3",
        "title": "Can Large Language Models Predict the Outcome of Judicial Decisions?",
        "author": [
            "Mohamed Bayan Kmainasi",
            "Ali Ezzat Shahroor",
            "Amani Al-Ghraibah"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09768",
        "abstract": "Large Language Models (LLMs) have shown exceptional capabilities in Natural Language Processing (NLP) across diverse domains. However, their application in specialized tasks such as Legal Judgment Prediction (LJP) for low-resource languages like Arabic remains underexplored. In this work, we address this gap by developing an Arabic LJP dataset, collected and preprocessed from Saudi commercial court judgments. We benchmark state-of-the-art open-source LLMs, including LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as zero-shot, one-shot, and fine-tuning using QLoRA. Additionally, we used a comprehensive evaluation framework combining quantitative metrics (BLEU and ROUGE) and qualitative assessments (Coherence, legal language, clarity). Our results demonstrate that fine-tuned smaller models achieve comparable performance to larger models in task-specific contexts while offering significant resource efficiency. Furthermore, we investigate the effects of prompt engineering and fine-tuning on model outputs, providing insights into performance variability and instruction sensitivity. By making the dataset, implementation code, and models publicly available, we establish a robust foundation for future research in Arabic legal NLP.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "4",
        "title": "Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong",
        "author": [
            "Tairan Fu",
            "Javier Conde",
            "Gonzalo MartÃ­nez",
            "MarÃ­a Grandury",
            "Pedro Reviriego"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09775",
        "abstract": "One of the most widely used methods to evaluate LLMs are Multiple Choice Question (MCQ) tests. MCQ benchmarks enable the testing of LLM knowledge on almost any topic at scale as the results can be processed automatically. To help the LLM answer, a few examples called few shots can be included in the prompt. Moreover, the LLM can be asked to answer the question directly with the selected option or to first provide the reasoning and then the selected answer, which is known as chain of thought. In addition to checking whether the selected answer is correct, the evaluation can look at the LLM-estimated probability of its response as an indication of the confidence of the LLM in the response. In this paper, we study how the LLM confidence in its answer depends on whether the model has been asked to answer directly or to provide the reasoning before answering. The results of the evaluation of questions on a wide range of topics in seven different models show that LLMs are more confident in their answers when they provide reasoning before the answer. This occurs regardless of whether the selected answer is correct. Our hypothesis is that this behavior is due to the reasoning that modifies the probability of the selected answer, as the LLM predicts the answer based on the input question and the reasoning that supports the selection made. Therefore, LLM estimated probabilities seem to have intrinsic limitations that should be understood in order to use them in evaluation procedures. Interestingly, the same behavior has been observed in humans, for whom explaining an answer increases confidence in its correctness.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "5",
        "title": "Sentiment Analysis in Twitter Social Network Centered on Cryptocurrencies Using Machine Learning",
        "author": [
            "Vahid Amiri",
            "Mahmood Ahmadi"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09777",
        "abstract": "Cryptocurrency is a digital currency that uses blockchain technology with secure encryption. Due to the decentralization of these currencies, traditional monetary systems and the capital market of each they, can influence a society. Therefore, due to the importance of the issue, the need to understand public opinion and analyze people's opinions in this regard increases. To understand the opinions and views of people about different topics, you can take help from social networks because they are a rich source of opinions. The Twitter social network is one of the main platforms where users discuss various topics, therefore, in the shortest time and with the lowest cost, the opinion of the community can be measured on this social network. Twitter Sentiment Analysis (TSA) is a field that analyzes the sentiment expressed in tweets. Considering that most of TSA's research efforts on cryptocurrencies are focused on English language, the purpose of this paper is to investigate the opinions of Iranian users on the Twitter social network about cryptocurrencies and provide the best model for classifying tweets based on sentiment. In the case of automatic analysis of tweets, managers and officials in the field of economy can gain knowledge from the general public's point of view about this issue and use the information obtained in order to properly manage this phenomenon. For this purpose, in this paper, in order to build emotion classification models, natural language processing techniques such as bag of words (BOW) and FastText for text vectorization and classical machine learning algorithms including KNN, SVM and Adaboost learning methods Deep including LSTM and BERT model were used for classification, and finally BERT linguistic model had the best accuracy with 83.50%.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "6",
        "title": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos",
        "author": [
            "Zhongwei Ren",
            "Yunchao Wei",
            "Xun Guo",
            "Yao Zhao",
            "Bingyi Kang",
            "Jiashi Feng",
            "Xiaojie Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09781",
        "abstract": "This work explores whether a deep generative model can learn complex knowledge solely from visual input, in contrast to the prevalent focus on text-based models like large language models (LLMs). We develop VideoWorld, an auto-regressive video generation model trained on unlabeled video data, and test its knowledge acquisition abilities in video-based Go and robotic control tasks. Our experiments reveal two key findings: (1) video-only training provides sufficient information for learning knowledge, including rules, reasoning and planning capabilities, and (2) the representation of visual change is crucial for knowledge acquisition. To improve both the efficiency and efficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key component of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional level in the Video-GoBench with just a 300-million-parameter model, without relying on search algorithms or reward mechanisms typical in reinforcement learning. In robotic tasks, VideoWorld effectively learns diverse control operations and generalizes across environments, approaching the performance of oracle models in CALVIN and RLBench. This study opens new avenues for knowledge acquisition from visual data, with all code, data, and models open-sourced for further research.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Video Generation"
        ]
    },
    {
        "id": "7",
        "title": "SMPLest-X: Ultimate Scaling for Expressive Human Pose and Shape Estimation",
        "author": [
            "Wanqi Yin",
            "Zhongang Cai",
            "Ruisi Wang",
            "Ailing Zeng",
            "Chen Wei",
            "Qingping Sun",
            "Haiyi Mei",
            "Yanjun Wang",
            "Hui En Pang",
            "Mingyuan Zhang",
            "Lei Zhang",
            "Chen Change Loy",
            "Atsushi Yamashita",
            "Lei Yang",
            "Ziwei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09782",
        "abstract": "Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods focus on training innovative architectural designs on confined datasets. In this work, we investigate the impact of scaling up EHPS towards a family of generalist foundation models. 1) For data scaling, we perform a systematic investigation on 40 EHPS datasets, encompassing a wide range of scenarios that a model trained on any single dataset cannot handle. More importantly, capitalizing on insights obtained from the extensive benchmarking process, we optimize our training scheme and select datasets that lead to a significant leap in EHPS capabilities. Ultimately, we achieve diminishing returns at 10M training instances from diverse data sources. 2) For model scaling, we take advantage of vision transformers (up to ViT-Huge as the backbone) to study the scaling law of model sizes in EHPS. To exclude the influence of algorithmic design, we base our experiments on two minimalist architectures: SMPLer-X, which consists of an intermediate step for hand and face localization, and SMPLest-X, an even simpler version that reduces the network to its bare essentials and highlights significant advances in the capture of articulated hands. With big data and the large model, the foundation models exhibit strong performance across diverse test benchmarks and excellent transferability to even unseen environments. Moreover, our finetuning strategy turns the generalist into specialist models, allowing them to achieve further performance boosts. Notably, our foundation models consistently deliver state-of-the-art results on seven benchmarks such as AGORA, UBody, EgoBody, and our proposed SynHand dataset for comprehensive hand evaluation. (Code is available at: https://github.com/wqyin/SMPLest-X).",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "8",
        "title": "Computing Optimization-Based Prompt Injections Against Closed-Weights Models By Misusing a Fine-Tuning API",
        "author": [
            "Andrey Labunets",
            "Nishit V. Pandya",
            "Ashish Hooda",
            "Xiaohan Fu",
            "Earlence Fernandes"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09798",
        "abstract": "We surface a new threat to closed-weight Large Language Models (LLMs) that enables an attacker to compute optimization-based prompt injections. Specifically, we characterize how an attacker can leverage the loss-like information returned from the remote fine-tuning interface to guide the search for adversarial prompts. The fine-tuning interface is hosted by an LLM vendor and allows developers to fine-tune LLMs for their tasks, thus providing utility, but also exposes enough information for an attacker to compute adversarial prompts. Through an experimental analysis, we characterize the loss-like values returned by the Gemini fine-tuning API and demonstrate that they provide a useful signal for discrete optimization of adversarial prompts using a greedy search algorithm. Using the PurpleLlama prompt injection benchmark, we demonstrate attack success rates between 65% and 82% on Google's Gemini family of LLMs. These attacks exploit the classic utility-security tradeoff - the fine-tuning interface provides a useful feature for developers but also exposes the LLMs to powerful attacks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "9",
        "title": "Conversational Text Extraction with Large Language Models Using Retrieval-Augmented Systems",
        "author": [
            "Soham Roy",
            "Mitul Goswami",
            "Nisharg Nargund",
            "Suneeta Mohanty",
            "Prasant Kumar Pattnaik"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09801",
        "abstract": "This study introduces a system leveraging Large Language Models (LLMs) to extract text and enhance user interaction with PDF documents via a conversational interface. Utilizing Retrieval-Augmented Generation (RAG), the system provides informative responses to user inquiries while highlighting relevant passages within the PDF. Upon user upload, the system processes the PDF, employing sentence embeddings to create a document-specific vector store. This vector store enables efficient retrieval of pertinent sections in response to user queries. The LLM then engages in a conversational exchange, using the retrieved information to extract text and generate comprehensive, contextually aware answers. While our approach demonstrates competitive ROUGE values compared to existing state-of-the-art techniques for text extraction and summarization, we acknowledge that further qualitative evaluation is necessary to fully assess its effectiveness in real-world applications. The proposed system gives competitive ROUGE values as compared to existing state-of-the-art techniques for text extraction and summarization, thus offering a valuable tool for researchers, students, and anyone seeking to efficiently extract knowledge and gain insights from documents through an intuitive question-answering interface.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "10",
        "title": "Enhancing Generalization in Chain of Thought Reasoning for Smaller Models",
        "author": [
            "Maxwell J. Yin",
            "Dingyi Jiang",
            "Yongbing Chen",
            "Boyu Wang",
            "Charles Ling"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09804",
        "abstract": "Chain-of-Thought (CoT) reasoning in smaller language models is a challenging natural language process problem yet highly desirable in many real-life applications. Existing CoT knowledge distillation methods often suffer from overly conservative memorization in smaller LLMs, leading to low generalization confidence. As fully preserving the CoT ability of teacher model is impossible, we hypothesize that adversarial CoT fine-tuning is crucial for developing smaller LLM with robust CoT generalization. To this end, we propose \\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled fine-tuning framework that integrates diverse CoT domains. Specifically, PRADA pioneers two CoT improvements in smaller LLM: (1) Recovering the domain-invariant feature insight which typically lost during distillation with domain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT prompt engineering by employing domain-adversarial approaches. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks. Moreover, our empirical findings reveal that the smaller LLM, when leveraging PRADA, aligns closely with domain knowledge, thereby improving the explainability of our approach.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "11",
        "title": "Lossy Compression with Pretrained Diffusion Models",
        "author": [
            "Jeremy Vonderfecht",
            "Feng Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09815",
        "abstract": "We apply the DiffC algorithm (Theis et al. 2022) to Stable Diffusion 1.5, 2.1, XL, and Flux-dev, and demonstrate that these pretrained models are remarkably capable lossy image compressors. A principled algorithm for lossy compression using pretrained diffusion models has been understood since at least Ho et al. 2020, but challenges in reverse-channel coding have prevented such algorithms from ever being fully implemented. We introduce simple workarounds that lead to the first complete implementation of DiffC, which is capable of compressing and decompressing images using Stable Diffusion in under 10 seconds. Despite requiring no additional training, our method is competitive with other state-of-the-art generative compression methods at low ultra-low bitrates.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "12",
        "title": "Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer",
        "author": [
            "Haoyu Zhang",
            "Raghavendra Ramachandra",
            "Kiran Raja",
            "Christoph Busch"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09817",
        "abstract": "Face morphing attacks have posed severe threats to Face Recognition Systems (FRS), which are operated in border control and passport issuance use cases. Correspondingly, morphing attack detection algorithms (MAD) are needed to defend against such attacks. MAD approaches must be robust enough to handle unknown attacks in an open-set scenario where attacks can originate from various morphing generation algorithms, post-processing and the diversity of printers/scanners. The problem of generalization is further pronounced when the detection has to be made on a single suspected image. In this paper, we propose a generalized single-image-based MAD (S-MAD) algorithm by learning the encoding from Vision Transformer (ViT) architecture. Compared to CNN-based architectures, ViT model has the advantage on integrating local and global information and hence can be suitable to detect the morphing traces widely distributed among the face region. Extensive experiments are carried out on face morphing datasets generated using publicly available FRGC face datasets. Several state-of-the-art (SOTA) MAD algorithms, including representative ones that have been publicly evaluated, have been selected and benchmarked with our ViT-based approach. Obtained results demonstrate the improved detection performance of the proposed S-MAD method on inter-dataset testing (when different data is used for training and testing) and comparable performance on intra-dataset testing (when the same data is used for training and testing) experimental protocol.",
        "tags": [
            "Detection",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "13",
        "title": "Improving Automated Feedback Systems for Tutor Training in Low-Resource Scenarios through Data Augmentation",
        "author": [
            "Chentianye Xu",
            "Jionghao Lin",
            "Tongshuang Wu",
            "Vincent Aleven",
            "Kenneth R. Koedinger"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09824",
        "abstract": "Tutoring is an effective instructional method for enhancing student learning, yet its success relies on the skill and experience of the tutors. This reliance presents challenges for the widespread implementation of tutoring, particularly in training novice tutors. To support tutor training programs, real-time automated feedback systems are essential for efficiently training large numbers of tutors. Lin et al.'s previous study employed Generative Pre-Trained Transformers (GPT) for sequence labeling to identify desirable and undesirable praise components in a tutor training dataset, providing explanatory feedback. However, this approach requires a significant amount of labeled data for fine-tuning, which is both labor-intensive and dependent on expert input. To address the challenges associated with extensive data labeling, the current study explores the use of prompting more advanced GPT models like GPT-4o to generate synthetic datasets for augmenting labeled response data, followed by fine-tuning a GPT-3.5 model. Our results demonstrate that our data augmentation approach generalizes effectively to identify other types of praise, compared to the same model fine-tuned without augmentation. These findings suggest that for data-intensive tasks, synthetic data generated through GPT model prompting can substantially enhance fine-tuned model performance in low-resource scenarios.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "14",
        "title": "PIXELS: Progressive Image Xemplar-based Editing with Latent Surgery",
        "author": [
            "Shristi Das Biswas",
            "Matthew Shreve",
            "Xuelu Li",
            "Prateek Singhal",
            "Kaushik Roy"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09826",
        "abstract": "Recent advancements in language-guided diffusion models for image editing are often bottle-necked by cumbersome prompt engineering to precisely articulate desired changes. An intuitive alternative calls on guidance from in-the-wild image exemplars to help users bring their imagined edits to life. Contemporary exemplar-based editing methods shy away from leveraging the rich latent space learnt by pre-existing large text-to-image (TTI) models and fall back on training with curated objective functions to achieve the task. Though somewhat effective, this demands significant computational resources and lacks compatibility with diverse base models and arbitrary exemplar count. On further investigation, we also find that these techniques restrict user control to only applying uniform global changes over the entire edited region. In this paper, we introduce a novel framework for progressive exemplar-driven editing with off-the-shelf diffusion models, dubbed PIXELS, to enable customization by providing granular control over edits, allowing adjustments at the pixel or region level. Our method operates solely during inference to facilitate imitative editing, enabling users to draw inspiration from a dynamic number of reference images, or multimodal prompts, and progressively incorporate all the desired changes without retraining or fine-tuning existing TTI models. This capability of fine-grained control opens up a range of new possibilities, including selective modification of individual objects and specifying gradual spatial changes. We demonstrate that PIXELS delivers high-quality edits efficiently, leading to a notable improvement in quantitative metrics as well as human evaluation. By making high-quality image editing more accessible, PIXELS has the potential to enable professional-grade edits to a wider audience with the ease of using any open-source image generation model.",
        "tags": [
            "Diffusion",
            "Image Editing",
            "Text-to-Image"
        ]
    },
    {
        "id": "15",
        "title": "EraseBench: Understanding The Ripple Effects of Concept Erasure Techniques",
        "author": [
            "Ibtihel Amara",
            "Ahmed Imtiaz Humayun",
            "Ivana Kajic",
            "Zarana Parekh",
            "Natalie Harris",
            "Sarah Young",
            "Chirag Nagpal",
            "Najoung Kim",
            "Junfeng He",
            "Cristina Nader Vasconcelos",
            "Deepak Ramachandran",
            "Goolnoosh Farnadi",
            "Katherine Heller",
            "Mohammad Havaei",
            "Negar Rostamzadeh"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09833",
        "abstract": "Concept erasure techniques have recently gained significant attention for their potential to remove unwanted concepts from text-to-image models. While these methods often demonstrate success in controlled scenarios, their robustness in real-world applications and readiness for deployment remain uncertain. In this work, we identify a critical gap in evaluating sanitized models, particularly in terms of their performance across various concept dimensions. We systematically investigate the failure modes of current concept erasure techniques, with a focus on visually similar, binomial, and semantically related concepts. We propose that these interconnected relationships give rise to a phenomenon of concept entanglement resulting in ripple effects and degradation in image quality. To facilitate more comprehensive evaluation, we introduce EraseBENCH, a multi-dimensional benchmark designed to assess concept erasure methods with greater depth. Our dataset includes over 100 diverse concepts and more than 1,000 tailored prompts, paired with a comprehensive suite of metrics that together offer a holistic view of erasure efficacy. Our findings reveal that even state-of-the-art techniques struggle with maintaining quality post-erasure, indicating that these approaches are not yet ready for real-world deployment. This highlights the gap in reliability of the concept erasure techniques.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "16",
        "title": "CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified Intermediate Representation",
        "author": [
            "Alex Berian",
            "Daniel Brignac",
            "JhihYang Wu",
            "Natnael Daba",
            "Abhijit Mahalanobis"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09838",
        "abstract": "Geospatial imaging leverages data from diverse sensing modalities-such as EO, SAR, and LiDAR, ranging from ground-level drones to satellite views. These heterogeneous inputs offer significant opportunities for scene understanding but present challenges in interpreting geometry accurately, particularly in the absence of precise ground truth data. To address this, we propose CrossModalityDiffusion, a modular framework designed to generate images across different modalities and viewpoints without prior knowledge of scene geometry. CrossModalityDiffusion employs modality-specific encoders that take multiple input images and produce geometry-aware feature volumes that encode scene structure relative to their input camera positions. The space where the feature volumes are placed acts as a common ground for unifying input modalities. These feature volumes are overlapped and rendered into feature images from novel perspectives using volumetric rendering techniques. The rendered feature images are used as conditioning inputs for a modality-specific diffusion model, enabling the synthesis of novel images for the desired output modality. In this paper, we show that jointly training different modules ensures consistent geometric understanding across all modalities within the framework. We validate CrossModalityDiffusion's capabilities on the synthetic ShapeNet cars dataset, demonstrating its effectiveness in generating accurate and consistent novel views across multiple imaging modalities and perspectives.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "17",
        "title": "Optimization is Better than Generation: Optimizing Commit Message Leveraging Human-written Commit Message",
        "author": [
            "Jiawei Li",
            "David FaragÃ³",
            "Christian Petrov",
            "Iftekhar Ahmed"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09861",
        "abstract": "Commit messages are crucial in software development, supporting maintenance tasks and communication among developers. While Large Language Models (LLMs) have advanced Commit Message Generation (CMG) using various software contexts, some contexts developers consider are often missed by CMG techniques and can't be easily retrieved or even retrieved at all by automated tools. To address this, we propose Commit Message Optimization (CMO), which enhances human-written messages by leveraging LLMs and search-based optimization. CMO starts with human-written messages and iteratively improves them by integrating key contexts and feedback from external evaluators. Our extensive evaluation shows CMO generates commit messages that are significantly more Rational, Comprehensive, and Expressive while outperforming state-of-the-art CMG methods and human messages 88.2%-95.4% of the time.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "18",
        "title": "Fine-grained Testing for Autonomous Driving Software: a Study on Autoware with LLM-driven Unit Testing",
        "author": [
            "Wenhan Wang",
            "Xuan Xie",
            "Yuheng Huang",
            "Renzhi Wang",
            "An Ran Chen",
            "Lei Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09866",
        "abstract": "Testing autonomous driving systems (ADS) is critical to ensuring their reliability and safety. Existing ADS testing works focuses on designing scenarios to evaluate system-level behaviors, while fine-grained testing of ADS source code has received comparatively little attention. To address this gap, we present the first study on testing, specifically unit testing, for ADS source code. Our study focuses on an industrial ADS framework, Autoware. We analyze both human-written test cases and those generated by large language models (LLMs). Our findings reveal that human-written test cases in Autoware exhibit limited test coverage, and significant challenges remain in applying LLM-generated tests for Autoware unit testing. To overcome these challenges, we propose AwTest-LLM, a novel approach to enhance test coverage and improve test case pass rates across Autoware packages.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "19",
        "title": "An LLM-Guided Tutoring System for Social Skills Training",
        "author": [
            "Michael Guevarra",
            "Indronil Bhattacharjee",
            "Srijita Das",
            "Christabel Wayllace",
            "Carrie Demmans Epp",
            "Matthew E. Taylor",
            "Alan Tay"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09870",
        "abstract": "Social skills training targets behaviors necessary for success in social interactions. However, traditional classroom training for such skills is often insufficient to teach effective communication -- one-to-one interaction in real-world scenarios is preferred to lecture-style information delivery. This paper introduces a framework that allows instructors to collaborate with large language models to dynamically design realistic scenarios for students to communicate. Our framework uses these scenarios to enable student rehearsal, provide immediate feedback, and visualize performance for both students and instructors. Unlike traditional intelligent tutoring systems, instructors can easily co-create scenarios with a large language model without technical skills. Additionally, the system generates new scenario branches in real time when existing options do not fit the student's response.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "20",
        "title": "Geometry-Preserving Encoder/Decoder in Latent Generative Models",
        "author": [
            "Wonjun Lee",
            "Riley C.W. O'Neill",
            "Dongmian Zou",
            "Jeff Calder",
            "Gilad Lerman"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09876",
        "abstract": "Generative modeling aims to generate new data samples that resemble a given dataset, with diffusion models recently becoming the most popular generative model. One of the main challenges of diffusion models is solving the problem in the input space, which tends to be very high-dimensional. Recently, solving diffusion models in the latent space through an encoder that maps from the data space to a lower-dimensional latent space has been considered to make the training process more efficient and has shown state-of-the-art results. The variational autoencoder (VAE) is the most commonly used encoder/decoder framework in this domain, known for its ability to learn latent representations and generate data samples. In this paper, we introduce a novel encoder/decoder framework with theoretical properties distinct from those of the VAE, specifically designed to preserve the geometric structure of the data distribution. We demonstrate the significant advantages of this geometry-preserving encoder in the training process of both the encoder and decoder. Additionally, we provide theoretical results proving convergence of the training process, including convergence guarantees for encoder training, and results showing faster convergence of decoder training when using the geometry-preserving encoder.",
        "tags": [
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": "21",
        "title": "ASTRA: A Scene-aware TRAnsformer-based model for trajectory prediction",
        "author": [
            "Izzeddin Teeti",
            "Aniket Thomas",
            "Munish Monga",
            "Sachin Kumar",
            "Uddeshya Singh",
            "Andrew Bradley",
            "Biplab Banerjee",
            "Fabio Cuzzolin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09878",
        "abstract": "We present ASTRA (A} Scene-aware TRAnsformer-based model for trajectory prediction), a light-weight pedestrian trajectory forecasting model that integrates the scene context, spatial dynamics, social inter-agent interactions and temporal progressions for precise forecasting. We utilised a U-Net-based feature extractor, via its latent vector representation, to capture scene representations and a graph-aware transformer encoder for capturing social interactions. These components are integrated to learn an agent-scene aware embedding, enabling the model to learn spatial dynamics and forecast the future trajectory of pedestrians. The model is designed to produce both deterministic and stochastic outcomes, with the stochastic predictions being generated by incorporating a Conditional Variational Auto-Encoder (CVAE). ASTRA also proposes a simple yet effective weighted penalty loss function, which helps to yield predictions that outperform a wide array of state-of-the-art deterministic and generative models. ASTRA demonstrates an average improvement of 27%/10% in deterministic/stochastic settings on the ETH-UCY dataset, and 26% improvement on the PIE dataset, respectively, along with seven times fewer parameters than the existing state-of-the-art model (see Figure 1). Additionally, the model's versatility allows it to generalize across different perspectives, such as Bird's Eye View (BEV) and Ego-Vehicle View (EVV).",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "22",
        "title": "FLORA: Formal Language Model Enables Robust Training-free Zero-shot Object Referring Analysis",
        "author": [
            "Zhe Chen",
            "Zijing Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09887",
        "abstract": "Object Referring Analysis (ORA), commonly known as referring expression comprehension, requires the identification and localization of specific objects in an image based on natural descriptions. Unlike generic object detection, ORA requires both accurate language understanding and precise visual localization, making it inherently more complex. Although recent pre-trained large visual grounding detectors have achieved significant progress, they heavily rely on extensively labeled data and time-consuming learning. To address these, we introduce a novel, training-free framework for zero-shot ORA, termed FLORA (Formal Language for Object Referring and Analysis). FLORA harnesses the inherent reasoning capabilities of large language models (LLMs) and integrates a formal language model - a logical framework that regulates language within structured, rule-based descriptions - to provide effective zero-shot ORA. More specifically, our formal language model (FLM) enables an effective, logic-driven interpretation of object descriptions without necessitating any training processes. Built upon FLM-regulated LLM outputs, we further devise a Bayesian inference framework and employ appropriate off-the-shelf interpretive models to finalize the reasoning, delivering favorable robustness against LLM hallucinations and compelling ORA performance in a training-free manner. In practice, our FLORA boosts the zero-shot performance of existing pretrained grounding detectors by up to around 45%. Our comprehensive evaluation across different challenging datasets also confirms that FLORA consistently surpasses current state-of-the-art zero-shot methods in both detection and segmentation tasks associated with zero-shot ORA. We believe our probabilistic parsing and reasoning of the LLM outputs elevate the reliability and interpretability of zero-shot ORA. We shall release codes upon publication.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models",
            "Segmentation"
        ]
    },
    {
        "id": "23",
        "title": "Understanding the Effectiveness of LLMs in Automated Self-Admitted Technical Debt Repayment",
        "author": [
            "Mohammad Sadegh Sheikhaei",
            "Yuan Tian",
            "Shaowei Wang",
            "Bowen Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09888",
        "abstract": "Self-Admitted Technical Debt (SATD), cases where developers intentionally acknowledge suboptimal solutions in code through comments, poses a significant challenge to software maintainability. Left unresolved, SATD can degrade code quality and increase maintenance costs. While Large Language Models (LLMs) have shown promise in tasks like code generation and program repair, their potential in automated SATD repayment remains underexplored.\nIn this paper, we identify three key challenges in training and evaluating LLMs for SATD repayment: (1) dataset representativeness and scalability, (2) removal of irrelevant SATD repayments, and (3) limitations of existing evaluation metrics. To address the first two dataset-related challenges, we adopt a language-independent SATD tracing tool and design a 10-step filtering pipeline to extract SATD repayments from repositories, resulting two large-scale datasets: 58,722 items for Python and 97,347 items for Java. To improve evaluation, we introduce two diff-based metrics, BLEU-diff and CrystalBLEU-diff, which measure code changes rather than whole code. Additionally, we propose another new metric, LEMOD, which is both interpretable and informative. Using our new benchmarks and evaluation metrics, we evaluate two types of automated SATD repayment methods: fine-tuning smaller models, and prompt engineering with five large-scale models. Our results reveal that fine-tuned small models achieve comparable Exact Match (EM) scores to prompt-based approaches but underperform on BLEU-based metrics and LEMOD. Notably, Gemma-2-9B leads in EM, addressing 10.1% of Python and 8.1% of Java SATDs, while Llama-3.1-70B-Instruct and GPT-4o-mini excel on BLEU-diff, CrystalBLEU-diff, and LEMOD metrics. Our work contributes a robust benchmark, improved evaluation metrics, and a comprehensive evaluation of LLMs, advancing research on automated SATD repayment.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "24",
        "title": "Evolving Deeper LLM Thinking",
        "author": [
            "Kuang-Huei Lee",
            "Ian Fischer",
            "Yueh-Hua Wu",
            "Dave Marwood",
            "Shumeet Baluja",
            "Dale Schuurmans",
            "Xinyun Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09891",
        "abstract": "We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "25",
        "title": "Sparse Binary Representation Learning for Knowledge Tracing",
        "author": [
            "Yahya Badran",
            "Christine Preisach"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09893",
        "abstract": "Knowledge tracing (KT) models aim to predict students' future performance based on their historical interactions. Most existing KT models rely exclusively on human-defined knowledge concepts (KCs) associated with exercises. As a result, the effectiveness of these models is highly dependent on the quality and completeness of the predefined KCs. Human errors in labeling and the cost of covering all potential underlying KCs can limit model performance.\nIn this paper, we propose a KT model, Sparse Binary Representation KT (SBRKT), that generates new KC labels, referred to as auxiliary KCs, which can augment the predefined KCs to address the limitations of relying solely on human-defined KCs. These are learned through a binary vector representation, where each bit indicates the presence (one) or absence (zero) of an auxiliary KC. The resulting discrete representation allows these auxiliary KCs to be utilized in training any KT model that incorporates KCs. Unlike pre-trained dense embeddings, which are limited to models designed to accept such vectors, our discrete representations are compatible with both classical models, such as Bayesian Knowledge Tracing (BKT), and modern deep learning approaches.\nTo generate this discrete representation, SBRKT employs a binarization method that learns a sparse representation, fully trainable via stochastic gradient descent. Additionally, SBRKT incorporates a recurrent neural network (RNN) to capture temporal dynamics and predict future student responses by effectively combining the auxiliary and predefined KCs. Experimental results demonstrate that SBRKT outperforms the tested baselines on several datasets and achieves competitive performance on others. Furthermore, incorporating the learned auxiliary KCs consistently enhances the performance of BKT across all tested datasets.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "26",
        "title": "Towards A Litmus Test for Common Sense",
        "author": [
            "Hugo Latapie"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09913",
        "abstract": "This paper is the second in a planned series aimed at envisioning a path to safe and beneficial artificial intelligence. Building on the conceptual insights of \"Common Sense Is All You Need,\" we propose a more formal litmus test for common sense, adopting an axiomatic approach that combines minimal prior knowledge (MPK) constraints with diagonal or Godel-style arguments to create tasks beyond the agent's known concept set. We discuss how this approach applies to the Abstraction and Reasoning Corpus (ARC), acknowledging training/test data constraints, physical or virtual embodiment, and large language models (LLMs). We also integrate observations regarding emergent deceptive hallucinations, in which more capable AI systems may intentionally fabricate plausible yet misleading outputs to disguise knowledge gaps. The overarching theme is that scaling AI without ensuring common sense risks intensifying such deceptive tendencies, thereby undermining safety and trust. Aligning with the broader goal of developing beneficial AI without causing harm, our axiomatic litmus test not only diagnoses whether an AI can handle truly novel concepts but also provides a stepping stone toward an ethical, reliable foundation for future safe, beneficial, and aligned artificial intelligence.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "27",
        "title": "IE-Bench: Advancing the Measurement of Text-Driven Image Editing for Human Perception Alignment",
        "author": [
            "Shangkun Sun",
            "Bowen Qu",
            "Xiaoyu Liang",
            "Songlin Fan",
            "Wei Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09927",
        "abstract": "Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding results different editing methods, and total 3,010 Mean Opinion Scores (MOS) provided by 25 human subjects. Furthermore, we introduce IE-QA, a multi-modality source-aware quality assessment method for text-driven image editing. To the best of our knowledge, IE-Bench offers the first IQA dataset and model tailored for text-driven image editing. Extensive experiments demonstrate IE-QA's superior subjective-alignments on the text-driven image editing task compared with previous metrics. We will make all related data and code available to the public.",
        "tags": [
            "Image Editing"
        ]
    },
    {
        "id": "28",
        "title": "Dialogue Benchmark Generation from Knowledge Graphs with Cost-Effective Retrieval-Augmented LLMs",
        "author": [
            "Reham Omar",
            "Omij Mangukiya",
            "Essam Mansour"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09928",
        "abstract": "Dialogue benchmarks are crucial in training and evaluating chatbots engaging in domain-specific conversations. Knowledge graphs (KGs) represent semantically rich and well-organized data spanning various domains, such as DBLP, DBpedia, and YAGO. Traditionally, dialogue benchmarks have been manually created from documents, neglecting the potential of KGs in automating this process. Some question-answering benchmarks are automatically generated using extensive preprocessing from KGs, but they do not support dialogue generation. This paper introduces Chatty-Gen, a novel multi-stage retrieval-augmented generation platform for automatically generating high-quality dialogue benchmarks tailored to a specific domain using a KG. Chatty-Gen decomposes the generation process into manageable stages and uses assertion rules for automatic validation between stages. Our approach enables control over intermediate results to prevent time-consuming restarts due to hallucinations. It also reduces reliance on costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront processing of the entire KG using efficient query-based retrieval to find representative subgraphs based on the dialogue context. Our experiments with several real and large KGs demonstrate that Chatty-Gen significantly outperforms state-of-the-art systems and ensures consistent model and system performance across multiple LLMs of diverse capabilities, such as GPT-4o, Gemini 1.5, Llama 3, and Mistral.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA"
        ]
    },
    {
        "id": "29",
        "title": "Steering Large Language Models with Feature Guided Activation Additions",
        "author": [
            "Samuel Soo",
            "Wesley Teng",
            "Chandrasekaran Balaganesh"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09929",
        "abstract": "Effective and reliable control over large language model (LLM) behavior is a significant challenge. While activation steering methods, which add steering vectors to a model's hidden states, are a promising approach, existing techniques often lack precision and interpretability in how they influence model outputs. We introduce Feature Guided Activation Additions (FGAA), a novel activation steering method that leverages insights from Contrastive Activation Addition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating in the latent space of a Sparse Autoencoder (SAE) and employing optimization techniques to select desired SAE features, FGAA constructs precise steering vectors that provide better steering effects while maintaining coherence of steered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B models across various steering tasks demonstrate that FGAA outperforms existing steering methods of CAA, SAE decoder steering, and SAE-TS. Our results also highlight important trade-offs between steering scale and general model capabilities that are consistent across all tested steering methods.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "30",
        "title": "Surface-SOS: Self-Supervised Object Segmentation via Neural Surface Representation",
        "author": [
            "Xiaoyun Zheng",
            "Liwei Liao",
            "Jianbo Jiao",
            "Feng Gao",
            "Ronggang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09947",
        "abstract": "Self-supervised Object Segmentation (SOS) aims to segment objects without any annotations. Under conditions of multi-camera inputs, the structural, textural and geometrical consistency among each view can be leveraged to achieve fine-grained object segmentation. To make better use of the above information, we propose Surface representation based Self-supervised Object Segmentation (Surface-SOS), a new framework to segment objects for each view by 3D surface representation from multi-view images of a scene. To model high-quality geometry surfaces for complex scenes, we design a novel scene representation scheme, which decomposes the scene into two complementary neural representation modules respectively with a Signed Distance Function (SDF). Moreover, Surface-SOS is able to refine single-view segmentation with multi-view unlabeled images, by introducing coarse segmentation masks as additional input. To the best of our knowledge, Surface-SOS is the first self-supervised approach that leverages neural surface representation to break the dependence on large amounts of annotated data and strong constraints. These constraints typically involve observing target objects against a static background or relying on temporal supervision in videos. Extensive experiments on standard benchmarks including LLFF, CO3D, BlendedMVS, TUM and several real-world scenes show that Surface-SOS always yields finer object masks than its NeRF-based counterparts and surpasses supervised single-view baselines remarkably. Code is available at: https://github.com/zhengxyun/Surface-SOS.",
        "tags": [
            "3D",
            "NeRF",
            "Segmentation"
        ]
    },
    {
        "id": "31",
        "title": "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation based on Knowledge Graphs",
        "author": [
            "Zengyi Gao",
            "Yukun Cao",
            "Hairu Wang",
            "Ao Ke",
            "Yuan Feng",
            "Xike Xie",
            "S Kevin Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09957",
        "abstract": "To mitigate the hallucination and knowledge deficiency in large language models (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) has shown promising potential by utilizing KGs as external resource to enhance LLMs http://reasoning.However, existing KG-RAG approaches struggle with a trade-off between flexibility and retrieval http://quality.Modular methods prioritize flexibility by avoiding the use of KG-fine-tuned models during retrieval, leading to fixed retrieval strategies and suboptimal retrieval http://quality.Conversely, coupled methods embed KG information within models to improve retrieval quality, but at the expense of http://flexibility.In this paper, we propose a novel flexible modular KG-RAG framework, termed FRAG, which synergizes the advantages of both http://approaches.FRAG estimates the hop range of reasoning paths based solely on the query and classify it as either simple or http://complex.To match the complexity of the query, tailored pipelines are applied to ensure efficient and accurate reasoning path retrieval, thus fostering the final reasoning http://process.By using the query text instead of the KG to infer the structural information of reasoning paths and employing adaptable retrieval strategies, FRAG improves retrieval quality while maintaining http://flexibility.Moreover, FRAG does not require extra LLMs fine-tuning or calls, significantly boosting efficiency and conserving http://resources.Extensive experiments show that FRAG achieves state-of-the-art performance with high efficiency and low resource consumption.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "32",
        "title": "Discrete Prior-based Temporal-coherent Content Prediction for Blind Face Video Restoration",
        "author": [
            "Lianxin Xie",
            "Bingbing Zheng",
            "Wen Xue",
            "Yunfei Zhang",
            "Le Jiang",
            "Ruotao Xu",
            "Si Wu",
            "Hau-San Wong"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09960",
        "abstract": "Blind face video restoration aims to restore high-fidelity details from videos subjected to complex and unknown degradations. This task poses a significant challenge of managing temporal heterogeneity while at the same time maintaining stable face attributes. In this paper, we introduce a Discrete Prior-based Temporal-Coherent content prediction transformer to address the challenge, and our model is referred to as DP-TempCoh. Specifically, we incorporate a spatial-temporal-aware content prediction module to synthesize high-quality content from discrete visual priors, conditioned on degraded video tokens. To further enhance the temporal coherence of the predicted content, a motion statistics modulation module is designed to adjust the content, based on discrete motion priors in terms of cross-frame mean and variance. As a result, the statistics of the predicted content can match with that of real videos over time. By performing extensive experiments, we verify the effectiveness of the design elements and demonstrate the superior performance of our DP-TempCoh in both synthetically and naturally degraded video restoration.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "33",
        "title": "Explainable artificial intelligence (XAI): from inherent explainability to large language models",
        "author": [
            "Fuseini Mumuni",
            "Alhassan Mumuni"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09967",
        "abstract": "Artificial Intelligence (AI) has continued to achieve tremendous success in recent times. However, the decision logic of these frameworks is often not transparent, making it difficult for stakeholders to understand, interpret or explain their behavior. This limitation hinders trust in machine learning systems and causes a general reluctance towards their adoption in practical applications, particularly in mission-critical domains like healthcare and autonomous driving. Explainable AI (XAI) techniques facilitate the explainability or interpretability of machine learning models, enabling users to discern the basis of the decision and possibly avert undesirable behavior. This comprehensive survey details the advancements of explainable AI methods, from inherently interpretable models to modern approaches for achieving interpretability of various black box models, including large language models (LLMs). Additionally, we review explainable AI techniques that leverage LLM and vision-language model (VLM) frameworks to automate or improve the explainability of other machine learning models. The use of LLM and VLM as interpretability methods particularly enables high-level, semantically meaningful explanations of model decisions and behavior. Throughout the paper, we highlight the scientific principles, strengths and weaknesses of state-of-the-art methods and outline different areas of improvement. Where appropriate, we also present qualitative and quantitative comparison results of various methods to show how they compare. Finally, we discuss the key challenges of XAI and directions for future research.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "34",
        "title": "RichSpace: Enriching Text-to-Video Prompt Space via Text Embedding Interpolation",
        "author": [
            "Yuefan Cao",
            "Chengyue Gong",
            "Xiaoyu Li",
            "Yingyu Liang",
            "Zhizhou Sha",
            "Zhenmei Shi",
            "Zhao Song"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09982",
        "abstract": "Text-to-video generation models have made impressive progress, but they still struggle with generating videos with complex features. This limitation often arises from the inability of the text encoder to produce accurate embeddings, which hinders the video generation model. In this work, we propose a novel approach to overcome this challenge by selecting the optimal text embedding through interpolation in the embedding space. We demonstrate that this method enables the video generation model to produce the desired videos. Additionally, we introduce a simple algorithm using perpendicular foot embeddings and cosine similarity to identify the optimal interpolation embedding. Our findings highlight the importance of accurate text embeddings and offer a pathway for improving text-to-video generation performance.",
        "tags": [
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "35",
        "title": "Agent-as-Judge for Factual Summarization of Long Narratives",
        "author": [
            "Yeonseok Jeong",
            "Minsoo Kim",
            "Seung-won Hwang",
            "Byung-Hak Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09993",
        "abstract": "Large Language Models (LLMs) have demonstrated near-human performance in summarization tasks based on traditional metrics such as ROUGE and BERTScore. However, these metrics do not adequately capture critical aspects of summarization quality, such as factual accuracy, particularly for long narratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the limitations of metrics based on lexical similarity but still exhibit factual inconsistencies, especially in understanding character relationships and states. In this work, we introduce NarrativeFactScore, a novel \"Agent-as-a-Judge\" framework for evaluating and refining summaries. By leveraging a Character Knowledge Graph (CKG) extracted from input and generated summaries, NarrativeFactScore assesses the factual consistency and provides actionable guidance for refinement, such as identifying missing or erroneous facts. We demonstrate the effectiveness of NarrativeFactScore through a detailed workflow illustration and extensive validation on widely adopted benchmarks, achieving superior performance compared to competitive methods. Our results highlight the potential of agent-driven evaluation systems to improve the factual reliability of LLM-generated summaries.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "36",
        "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models",
        "author": [
            "Qiang Liu",
            "Xinlong Chen",
            "Yue Ding",
            "Shizhen Xu",
            "Shu Wu",
            "Liang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09997",
        "abstract": "Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational complexity, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "37",
        "title": "Adaptive Spatiotemporal Augmentation for Improving Dynamic Graph Learning",
        "author": [
            "Xu Chu",
            "Hanlin Xue",
            "Bingce Wang",
            "Xiaoyang Liu",
            "Weiping Li",
            "Tong Mo",
            "Tuoyu Feng",
            "Zhijie Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10010",
        "abstract": "Dynamic graph augmentation is used to improve the performance of dynamic GNNs. Most methods assume temporal locality, meaning that recent edges are more influential than earlier edges. However, for temporal changes in edges caused by random noise, overemphasizing recent edges while neglecting earlier ones may lead to the model capturing noise. To address this issue, we propose STAA (SpatioTemporal Activity-Aware Random Walk Diffusion). STAA identifies nodes likely to have noisy edges in spatiotemporal dimensions. Spatially, it analyzes critical topological positions through graph wavelet coefficients. Temporally, it analyzes edge evolution through graph wavelet coefficient change rates. Then, random walks are used to reduce the weights of noisy edges, deriving a diffusion matrix containing spatiotemporal information as an augmented adjacency matrix for dynamic GNN learning. Experiments on multiple datasets show that STAA outperforms other dynamic graph augmentation methods in node classification and link prediction tasks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "38",
        "title": "Mitigating Hallucinations on Object Attributes using Multiview Images and Negative Instructions",
        "author": [
            "Zhijie Tan",
            "Yuzhi Li",
            "Shengwei Meng",
            "Xiang Yuan",
            "Weiping Li",
            "Tong Mo",
            "Bingce Wang",
            "Xu Chu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10011",
        "abstract": "Current popular Large Vision-Language Models (LVLMs) are suffering from Hallucinations on Object Attributes (HoOA), leading to incorrect determination of fine-grained attributes in the input images. Leveraging significant advancements in 3D generation from a single image, this paper proposes a novel method to mitigate HoOA in LVLMs. This method utilizes multiview images sampled from generated 3D representations as visual prompts for LVLMs, thereby providing more visual information from other viewpoints. Furthermore, we observe the input order of multiple multiview images significantly affects the performance of LVLMs. Consequently, we have devised Multiview Image Augmented VLM (MIAVLM), incorporating a Multiview Attributes Perceiver (MAP) submodule capable of simultaneously eliminating the influence of input image order and aligning visual information from multiview images with Large Language Models (LLMs). Besides, we designed and employed negative instructions to mitigate LVLMs' bias towards ``Yes\" responses. Comprehensive experiments demonstrate the effectiveness of our method.",
        "tags": [
            "3D",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "39",
        "title": "DiffuEraser: A Diffusion Model for Video Inpainting",
        "author": [
            "Xiaowen Li",
            "Haolan Xue",
            "Peiran Ren",
            "Liefeng Bo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10018",
        "abstract": "Recent video inpainting algorithms integrate flow-based pixel propagation with transformer-based generation to leverage optical flow for restoring textures and objects using information from neighboring frames, while completing masked regions through visual Transformers. However, these approaches often encounter blurring and temporal inconsistencies when dealing with large masks, highlighting the need for models with enhanced generative capabilities. Recently, diffusion models have emerged as a prominent technique in image and video generation due to their impressive performance. In this paper, we introduce DiffuEraser, a video inpainting model based on stable diffusion, designed to fill masked regions with greater details and more coherent structures. We incorporate prior information to provide initialization and weak conditioning,which helps mitigate noisy artifacts and suppress hallucinations. Additionally, to improve temporal consistency during long-sequence inference, we expand the temporal receptive fields of both the prior model and DiffuEraser, and further enhance consistency by leveraging the temporal smoothing property of Video Diffusion Models. Experimental results demonstrate that our proposed method outperforms state-of-the-art techniques in both content completeness and temporal consistency while maintaining acceptable efficiency.",
        "tags": [
            "Diffusion",
            "Inpainting",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "40",
        "title": "X-Dyna: Expressive Dynamic Human Image Animation",
        "author": [
            "Di Chang",
            "Hongyi Xu",
            "You Xie",
            "Yipeng Gao",
            "Zhengfei Kuang",
            "Shengqu Cai",
            "Chenxu Zhang",
            "Guoxian Song",
            "Chao Wang",
            "Yichun Shi",
            "Zeyuan Chen",
            "Shijie Zhou",
            "Linjie Luo",
            "Gordon Wetzstein",
            "Mohammad Soleymani"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10021",
        "abstract": "We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for animating a single human image using facial expressions and body movements derived from a driving video, that generates realistic, context-aware dynamics for both the subject and the surrounding environment. Building on prior approaches centered on human pose control, X-Dyna addresses key shortcomings causing the loss of dynamic details, enhancing the lifelike qualities of human video animations. At the core of our approach is the Dynamics-Adapter, a lightweight module that effectively integrates reference appearance context into the spatial attentions of the diffusion backbone while preserving the capacity of motion modules in synthesizing fluid and intricate dynamic details. Beyond body pose control, we connect a local control module with our model to capture identity-disentangled facial expressions, facilitating accurate expression transfer for enhanced realism in animated scenes. Together, these components form a unified framework capable of learning physical human motion and natural scene dynamics from a diverse blend of human and scene videos. Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna outperforms state-of-the-art methods, creating highly lifelike and expressive animations. The code is available at https://github.com/bytedance/X-Dyna.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "41",
        "title": "Exploring Code Comprehension in Scientific Programming: Preliminary Insights from Research Scientists",
        "author": [
            "Alyssia Chen",
            "Carol Wong",
            "Bonita Sharif",
            "Anthony Peruma"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10037",
        "abstract": "Scientific software-defined as computer programs, scripts, or code used in scientific research, data analysis, modeling, or simulation-has become central to modern research. However, there is limited research on the readability and understandability of scientific code, both of which are vital for effective collaboration and reproducibility in scientific research. This study surveys 57 research scientists from various disciplines to explore their programming backgrounds, practices, and the challenges they face regarding code readability. Our findings reveal that most participants learn programming through self-study or on the-job training, with 57.9% lacking formal instruction in writing readable code. Scientists mainly use Python and R, relying on comments and documentation for readability. While most consider code readability essential for scientific reproducibility, they often face issues with inadequate documentation and poor naming conventions, with challenges including cryptic names and inconsistent conventions. Our findings also show low adoption of code quality tools and a trend towards utilizing large language models to improve code quality. These findings offer practical insights into enhancing coding practices and supporting sustainable development in scientific software.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "42",
        "title": "Spatiotemporal Prediction of Secondary Crashes by Rebalancing Dynamic and Static Data with Generative Adversarial Networks",
        "author": [
            "Junlan Chen",
            "Yiqun Li",
            "Chenyu Ling",
            "Ziyuan Pu",
            "Xiucheng Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10041",
        "abstract": "Data imbalance is a common issue in analyzing and predicting sudden traffic events. Secondary crashes constitute only a small proportion of all crashes. These secondary crashes, triggered by primary crashes, significantly exacerbate traffic congestion and increase the severity of incidents. However, the severe imbalance of secondary crash data poses significant challenges for prediction models, affecting their generalization ability and prediction accuracy. Existing methods fail to fully address the complexity of traffic crash data, particularly the coexistence of dynamic and static features, and often struggle to effectively handle data samples of varying lengths. Furthermore, most current studies predict the occurrence probability and spatiotemporal distribution of secondary crashes separately, lacking an integrated solution. To address these challenges, this study proposes a hybrid model named VarFusiGAN-Transformer, aimed at improving the fidelity of secondary crash data generation and jointly predicting the occurrence and spatiotemporal distribution of secondary crashes. The VarFusiGAN-Transformer model employs Long Short-Term Memory (LSTM) networks to enhance the generation of multivariate long-time series data, incorporating a static data generator and an auxiliary discriminator to model the joint distribution of dynamic and static features. In addition, the model's prediction module achieves simultaneous prediction of both the occurrence and spatiotemporal distribution of secondary crashes. Compared to existing methods, the proposed model demonstrates superior performance in generating high-fidelity data and improving prediction accuracy.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "43",
        "title": "HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial Network for High-Fidelity Speech Super-Resolution",
        "author": [
            "Shengkui Zhao",
            "Kun Zhou",
            "Zexu Pan",
            "Yukun Ma",
            "Chong Zhang",
            "Bin Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10045",
        "abstract": "The application of generative adversarial networks (GANs) has recently advanced speech super-resolution (SR) based on intermediate representations like mel-spectrograms. However, existing SR methods that typically rely on independently trained and concatenated networks may lead to inconsistent representations and poor speech quality, especially in out-of-domain scenarios. In this work, we propose HiFi-SR, a unified network that leverages end-to-end adversarial training to achieve high-fidelity speech super-resolution. Our model features a unified transformer-convolutional generator designed to seamlessly handle both the prediction of latent representations and their conversion into time-domain waveforms. The transformer network serves as a powerful encoder, converting low-resolution mel-spectrograms into latent space representations, while the convolutional network upscales these representations into high-resolution waveforms. To enhance high-frequency fidelity, we incorporate a multi-band, multi-scale time-frequency discriminator, along with a multi-scale mel-reconstruction loss in the adversarial training process. HiFi-SR is versatile, capable of upscaling any input speech signal between 4 kHz and 32 kHz to a 48 kHz sampling rate. Experimental results demonstrate that HiFi-SR significantly outperforms existing speech SR methods across both objective metrics and ABX preference tests, for both in-domain and out-of-domain scenarios (https://github.com/modelscope/ClearerVoice-Studio).",
        "tags": [
            "Super Resolution",
            "Transformer"
        ]
    },
    {
        "id": "44",
        "title": "Conditional Latent Diffusion-Based Speech Enhancement Via Dual Context Learning",
        "author": [
            "Shengkui Zhao",
            "Zexu Pan",
            "Kun Zhou",
            "Yukun Ma",
            "Chong Zhang",
            "Bin Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10052",
        "abstract": "Recently, the application of diffusion probabilistic models has advanced speech enhancement through generative approaches. However, existing diffusion-based methods have focused on the generation process in high-dimensional waveform or spectral domains, leading to increased generation complexity and slower inference speeds. Additionally, these methods have primarily modelled clean speech distributions, with limited exploration of noise distributions, thereby constraining the discriminative capability of diffusion models for speech enhancement. To address these issues, we propose a novel approach that integrates a conditional latent diffusion model (cLDM) with dual-context learning (DCL). Our method utilizes a variational autoencoder (VAE) to compress mel-spectrograms into a low-dimensional latent space. We then apply cLDM to transform the latent representations of both clean speech and background noise into Gaussian noise by the DCL process, and a parameterized model is trained to reverse this process, conditioned on noisy latent representations and text embeddings. By operating in a lower-dimensional space, the latent representations reduce the complexity of the generation process, while the DCL process enhances the model's ability to handle diverse and unseen noise environments. Our experiments demonstrate the strong performance of the proposed approach compared to existing diffusion-based methods, even with fewer iterative steps, and highlight the superior generalization capability of our models to out-of-domain noise datasets (https://github.com/modelscope/ClearerVoice-Studio).",
        "tags": [
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": "45",
        "title": "AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented Generation via Tree-based Search",
        "author": [
            "Wenfeng Feng",
            "Chuzhan Hao",
            "Yuewei Zhang",
            "Jingyi Song",
            "Hao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10053",
        "abstract": "Leveraging the autonomous decision-making capabilities of large language models (LLMs) demonstrates superior performance in reasoning tasks. Despite the successes of iterative or recursive retrieval-augmented generation (RAG), they often are trapped in a single solution space when confronted with complex tasks. In this paper, we propose a novel thinking pattern in RAG which integrates system analysis with efficient reasoning actions, significantly activating intrinsic reasoning capabilities and expanding the solution space of specific tasks via Monte Carlo Tree Search (MCTS), dubbed AirRAG. Specifically, our approach designs five fundamental reasoning actions that are expanded to a wide tree-based reasoning spaces using MCTS. The extension also uses self-consistency verification to explore potential reasoning paths and implement inference scaling. In addition, computationally optimal strategies are used to apply more inference computation to key actions to achieve further performance improvements. Experimental results demonstrate the effectiveness of AirRAG through considerable performance gains over complex QA datasets. Furthermore, AirRAG is flexible and lightweight, making it easy to integrate with other advanced technologies.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "46",
        "title": "Accelerating Large Language Models through Partially Linear Feed-Forward Network",
        "author": [
            "Gansen Hu",
            "Zhaoguo Wang",
            "Jinglin Wei",
            "Wei Huang",
            "Haibo Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10054",
        "abstract": "Large language models (LLMs) demonstrate remarkable capabilities but face deployment challenges due to their massive parameter counts. While existing compression techniques like pruning can reduce model size, it leads to significant accuracy degradation under high compression ratios. We present a novel perspective inspired by constant folding in compiler optimization. Our approach enables parameter reduction by treating activation functions in LLMs as linear functions.\nHowever, recent LLMs use complex non-linear activations like GELU that prevent direct application of this technique. We propose TARDIS, which enables optimization of LLMs with non-linear activations by partially approximating them with linear functions in frequently occurring input ranges. For outlier inputs, TARDIS employs an online predictor to dynamically fall back to original computations.\nOur experiments demonstrate that TARDIS achieves 80% parameter reduction in feed-forward networks, while significantly outperforming state-of-the-art pruning methods Wanda and RIA with up to 65% higher accuracy. In practical deployments for a 7B model, TARDIS achieves 1.6x end-to-end inference speedup when integrated with the vLLM serving system, and 1.4x speedup with the widely adopted HuggingFace implementation, while incurring only a 10.9% accuracy trade-off.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "47",
        "title": "OMoE: Diversifying Mixture of Low-Rank Adaptation by Orthogonal Finetuning",
        "author": [
            "Jinyuan Feng",
            "Zhiqiang Pu",
            "Tianyi Hu",
            "Dongmin Li",
            "Xiaolin Ai",
            "Huimu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10062",
        "abstract": "Building mixture-of-experts (MoE) architecture for Low-rank adaptation (LoRA) is emerging as a potential direction in parameter-efficient fine-tuning (PEFT) for its modular design and remarkable performance. However, simply stacking the number of experts cannot guarantee significant improvement. In this work, we first conduct qualitative analysis to indicate that experts collapse to similar representations in vanilla MoE, limiting the capacity of modular design and computational efficiency. Ulteriorly, Our analysis reveals that the performance of previous MoE variants maybe limited by a lack of diversity among experts. Motivated by these findings, we propose Orthogonal Mixture-of-Experts (OMoE), a resource-efficient MoE variant that trains experts in an orthogonal manner to promote diversity. In OMoE, a Gram-Schmidt process is leveraged to enforce that the experts' representations lie within the Stiefel manifold. By applying orthogonal constraints directly to the architecture, OMoE keeps the learning objective unchanged, without compromising optimality. Our method is simple and alleviates memory bottlenecks, as it incurs minimal experts compared to vanilla MoE models. Experiments on diverse commonsense reasoning benchmarks demonstrate that OMoE can consistently achieve stable and efficient performance improvement when compared with the state-of-the-art methods while significantly reducing the number of required experts.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "48",
        "title": "CLIP-PCQA: Exploring Subjective-Aligned Vision-Language Modeling for Point Cloud Quality Assessment",
        "author": [
            "Yating Liu",
            "Yujie Zhang",
            "Ziyu Shan",
            "Yiling Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10071",
        "abstract": "In recent years, No-Reference Point Cloud Quality Assessment (NR-PCQA) research has achieved significant progress. However, existing methods mostly seek a direct mapping function from visual data to the Mean Opinion Score (MOS), which is contradictory to the mechanism of practical subjective evaluation. To address this, we propose a novel language-driven PCQA method named CLIP-PCQA. Considering that human beings prefer to describe visual quality using discrete quality descriptions (e.g., \"excellent\" and \"poor\") rather than specific scores, we adopt a retrieval-based mapping strategy to simulate the process of subjective assessment. More specifically, based on the philosophy of CLIP, we calculate the cosine similarity between the visual features and multiple textual features corresponding to different quality descriptions, in which process an effective contrastive loss and learnable prompts are introduced to enhance the feature extraction. Meanwhile, given the personal limitations and bias in subjective experiments, we further covert the feature similarities into probabilities and consider the Opinion Score Distribution (OSD) rather than a single MOS as the final target. Experimental results show that our CLIP-PCQA outperforms other State-Of-The-Art (SOTA) approaches.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "49",
        "title": "Few-shot Structure-Informed Machinery Part Segmentation with Foundation Models and Graph Neural Networks",
        "author": [
            "Michael Schwingshackl",
            "Fabio Francisco Oberweger",
            "Markus Murschitz"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10080",
        "abstract": "This paper proposes a novel approach to few-shot semantic segmentation for machinery with multiple parts that exhibit spatial and hierarchical relationships. Our method integrates the foundation models CLIPSeg and Segment Anything Model (SAM) with the interest point detector SuperPoint and a graph convolutional network (GCN) to accurately segment machinery parts. By providing 1 to 25 annotated samples, our model, evaluated on a purely synthetic dataset depicting a truck-mounted loading crane, achieves effective segmentation across various levels of detail. Training times are kept under five minutes on consumer GPUs. The model demonstrates robust generalization to real data, achieving a qualitative synthetic-to-real generalization with a $J\\&F$ score of 92.2 on real data using 10 synthetic support samples. When benchmarked on the DAVIS 2017 dataset, it achieves a $J\\&F$ score of 71.5 in semi-supervised video segmentation with three support samples. This method's fast training times and effective generalization to real data make it a valuable tool for autonomous systems interacting with machinery and infrastructure, and illustrate the potential of combined and orchestrated foundation models for few-shot segmentation tasks.",
        "tags": [
            "SAM",
            "Segment Anything",
            "Segmentation"
        ]
    },
    {
        "id": "50",
        "title": "How Do Programming Students Use Generative AI?",
        "author": [
            "Christian Rahe",
            "Walid Maalej"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10091",
        "abstract": "Programming students have a widespread access to powerful Generative AI tools like ChatGPT. While this can help understand the learning material and assist with exercises, educators are voicing more and more concerns about an over-reliance on generated outputs and lack of critical thinking skills. It is thus important to understand how students actually use generative AI and what impact this could have on their learning behavior. To this end, we conducted a study including an exploratory experiment with 37 programming students, giving them monitored access to ChatGPT while solving a code understanding and improving exercise. While only 23 of the students actually opted to use the chatbot, the majority of those eventually prompted it to simply generate a full solution. We observed two prevalent usage strategies: to seek knowledge about general concepts and to directly generate solutions. Instead of using the bot to comprehend the code and their own mistakes, students often got trapped in a vicious cycle of submitting wrong generated code and then asking the bot for a fix. Those who self-reported using generative AI regularly were more likely to prompt the bot to generate a solution. Our findings indicate that concerns about potential decrease in programmers' agency and productivity with Generative AI are justified. We discuss how researchers and educators can respond to the potential risk of students uncritically over-relying on generative AI. We also discuss potential modifications to our study design for large-scale replications.",
        "tags": [
            "ChatGPT"
        ]
    },
    {
        "id": "51",
        "title": "LLM Reasoner and Automated Planner: A new NPC approach",
        "author": [
            "Israel Puerta-Merino",
            "Jordi Sabater-Mir"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10106",
        "abstract": "In domains requiring intelligent agents to emulate plausible human-like behaviour, such as formative simulations, traditional techniques like behaviour trees encounter significant challenges. Large Language Models (LLMs), despite not always yielding optimal solutions, usually offer plausible and human-like responses to a given problem. In this paper, we exploit this capability and propose a novel architecture that integrates an LLM for decision-making with a classical automated planner that can generate sound plans for that decision. The combination aims to equip an agent with the ability to make decisions in various situations, even if they were not anticipated during the design phase.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "52",
        "title": "BBPOS: BERT-based Part-of-Speech Tagging for Uzbek",
        "author": [
            "Latofat Bobojonova",
            "Arofat Akhundjanova",
            "Phil Ostheimer",
            "Sophie Fellenz"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10107",
        "abstract": "This paper advances NLP research for the low-resource Uzbek language by evaluating two previously untested monolingual Uzbek BERT models on the part-of-speech (POS) tagging task and introducing the first publicly available UPOS-tagged benchmark dataset for Uzbek. Our fine-tuned models achieve 91% average accuracy, outperforming the baseline multi-lingual BERT as well as the rule-based tagger. Notably, these models capture intermediate POS changes through affixes and demonstrate context sensitivity, unlike existing rule-based taggers.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "53",
        "title": "DiffVSR: Enhancing Real-World Video Super-Resolution with Diffusion Models for Advanced Visual Quality and Temporal Consistency",
        "author": [
            "Xiaohui Li",
            "Yihao Liu",
            "Shuo Cao",
            "Ziyan Chen",
            "Shaobin Zhuang",
            "Xiangyu Chen",
            "Yinan He",
            "Yi Wang",
            "Yu Qiao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10110",
        "abstract": "Diffusion models have demonstrated exceptional capabilities in image generation and restoration, yet their application to video super-resolution faces significant challenges in maintaining both high fidelity and temporal consistency. We present DiffVSR, a diffusion-based framework for real-world video super-resolution that effectively addresses these challenges through key innovations. For intra-sequence coherence, we develop a multi-scale temporal attention module and temporal-enhanced VAE decoder that capture fine-grained motion details. To ensure inter-sequence stability, we introduce a noise rescheduling mechanism with an interweaved latent transition approach, which enhances temporal consistency without additional training overhead. We propose a progressive learning strategy that transitions from simple to complex degradations, enabling robust optimization despite limited high-quality video data. Extensive experiments demonstrate that DiffVSR delivers superior results in both visual quality and temporal consistency, setting a new performance standard in real-world video super-resolution.",
        "tags": [
            "Diffusion",
            "Super Resolution",
            "VAE"
        ]
    },
    {
        "id": "54",
        "title": "GAWM: Global-Aware World Model for Multi-Agent Reinforcement Learning",
        "author": [
            "Zifeng Shi",
            "Meiqin Liu",
            "Senlin Zhang",
            "Ronghao Zheng",
            "Shanling Dong",
            "Ping Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10116",
        "abstract": "In recent years, Model-based Multi-Agent Reinforcement Learning (MARL) has demonstrated significant advantages over model-free methods in terms of sample efficiency by using independent environment dynamics world models for data sample augmentation. However, without considering the limited sample size, these methods still lag behind model-free methods in terms of final convergence performance and stability. This is primarily due to the world model's insufficient and unstable representation of global states in partially observable environments. This limitation hampers the ability to ensure global consistency in the data samples and results in a time-varying and unstable distribution mismatch between the pseudo data samples generated by the world model and the real samples. This issue becomes particularly pronounced in more complex multi-agent environments. To address this challenge, we propose a model-based MARL method called GAWM, which enhances the centralized world model's ability to achieve globally unified and accurate representation of state information while adhering to the CTDE paradigm. GAWM uniquely leverages an additional Transformer architecture to fuse local observation information from different agents, thereby improving its ability to extract and represent global state information. This enhancement not only improves sample efficiency but also enhances training stability, leading to superior convergence performance, particularly in complex and challenging multi-agent environments. This advancement enables model-based methods to be effectively applied to more complex multi-agent environments. Experimental results demonstrate that GAWM outperforms various model-free and model-based approaches, achieving exceptional performance in the challenging domains of SMAC.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "55",
        "title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
        "author": [
            "Yichen He",
            "Guanhua Huang",
            "Peiyuan Feng",
            "Yuan Lin",
            "Yuchen Zhang",
            "Hang Li",
            "Weinan E"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10120",
        "abstract": "We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholarly queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.",
        "tags": [
            "ChatGPT",
            "GPT",
            "Large Language Models"
        ]
    },
    {
        "id": "56",
        "title": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario",
        "author": [
            "Lucen Zhong",
            "Zhengxiao Du",
            "Xiaohan Zhang",
            "Haiyi Hu",
            "Jie Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10132",
        "abstract": "Enhancing large language models (LLMs) with real-time APIs can help generate more accurate and up-to-date responses. However, evaluating the function calling abilities of LLMs in real-world scenarios remains under-explored due to the complexity of data collection and evaluation. In this work, we introduce ComplexFuncBench, a benchmark for complex function calling across five real-world scenarios. Compared to existing benchmarks, ComplexFuncBench encompasses multi-step and constrained function calling, which requires long-parameter filing, parameter value reasoning, and 128k long context. Additionally, we propose an automatic framework, ComplexEval, for quantitatively evaluating complex function calling tasks. Through comprehensive experiments, we demonstrate the deficiencies of state-of-the-art LLMs in function calling and suggest future directions for optimizing these capabilities. The data and code are available at \\url{https://github.com/THUDM/ComplexFuncBench}.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "57",
        "title": "Exploring the Impact of Generative Artificial Intelligence in Education: A Thematic Analysis",
        "author": [
            "Abhishek Kaushik",
            "Sargam Yadav",
            "Andrew Browne",
            "David Lillis",
            "David Williams",
            "Jack Mc Donnell",
            "Peadar Grant",
            "Siobhan Connolly Kernan",
            "Shubham Sharma",
            "Mansi Arora"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10134",
        "abstract": "The recent advancements in Generative Artificial intelligence (GenAI) technology have been transformative for the field of education. Large Language Models (LLMs) such as ChatGPT and Bard can be leveraged to automate boilerplate tasks, create content for personalised teaching, and handle repetitive tasks to allow more time for creative thinking. However, it is important to develop guidelines, policies, and assessment methods in the education sector to ensure the responsible integration of these tools. In this article, thematic analysis has been performed on seven essays obtained from professionals in the education sector to understand the advantages and pitfalls of using GenAI models such as ChatGPT and Bard in education. Exploratory Data Analysis (EDA) has been performed on the essays to extract further insights from the text. The study found several themes which highlight benefits and drawbacks of GenAI tools, as well as suggestions to overcome these limitations and ensure that students are using these tools in a responsible and ethical manner.",
        "tags": [
            "ChatGPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "58",
        "title": "CSHNet: A Novel Information Asymmetric Image Translation Method",
        "author": [
            "Xi Yang",
            "Haoyuan Shi",
            "Zihan Wang",
            "Nannan Wang",
            "Xinbo Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10197",
        "abstract": "Despite advancements in cross-domain image translation, challenges persist in asymmetric tasks such as SAR-to-Optical and Sketch-to-Instance conversions, which involve transforming data from a less detailed domain into one with richer content. Traditional CNN-based methods are effective at capturing fine details but struggle with global structure, leading to unwanted merging of image regions. To address this, we propose the CNN-Swin Hybrid Network (CSHNet), which combines two key modules: Swin Embedded CNN (SEC) and CNN Embedded Swin (CES), forming the SEC-CES-Bottleneck (SCB). SEC leverages CNN's detailed feature extraction while integrating the Swin Transformer's structural bias. CES, in turn, preserves the Swin Transformer's global integrity, compensating for CNN's lack of focus on structure. Additionally, CSHNet includes two components designed to enhance cross-domain information retention: the Interactive Guided Connection (IGC), which enables dynamic information exchange between SEC and CES, and Adaptive Edge Perception Loss (AEPL), which maintains structural boundaries during translation. Experimental results show that CSHNet outperforms existing methods in both visual quality and performance metrics across scene-level and instance-level datasets. Our code is available at: https://github.com/XduShi/CSHNet.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "59",
        "title": "The Relevance of AWS Chronos: An Evaluation of Standard Methods for Time Series Forecasting with Limited Tuning",
        "author": [
            "Matthew Baron",
            "Alex Karpinski"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10216",
        "abstract": "A systematic comparison of Chronos, a transformer-based time series forecasting framework, against traditional approaches including ARIMA and Prophet. We evaluate these models across multiple time horizons and user categories, with a focus on the impact of historical context length. Our analysis reveals that while Chronos demonstrates superior performance for longer-term predictions and maintains accuracy with increased context, traditional models show significant degradation as context length increases. We find that prediction quality varies systematically between user classes, suggesting that underlying behavior patterns always influence model performance. This study provides a case for deploying Chronos in real-world applications where limited model tuning is feasible, especially in scenarios requiring longer prediction.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "60",
        "title": "Towards An Integrated Approach for Expressive Piano Performance Synthesis from Music Scores",
        "author": [
            "Jingjing Tang",
            "Erica Cooper",
            "Xin Wang",
            "Junichi Yamagishi",
            "George Fazekas"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10222",
        "abstract": "This paper presents an integrated system that transforms symbolic music scores into expressive piano performance audio. By combining a Transformer-based Expressive Performance Rendering (EPR) model with a fine-tuned neural MIDI synthesiser, our approach directly generates expressive audio performances from score inputs. To the best of our knowledge, this is the first system to offer a streamlined method for converting score MIDI files lacking expression control into rich, expressive piano performances. We conducted experiments using subsets of the ATEPP dataset, evaluating the system with both objective metrics and subjective listening tests. Our system not only accurately reconstructs human-like expressiveness, but also captures the acoustic ambience of environments such as concert halls and recording studios. Additionally, the proposed system demonstrates its ability to achieve musical expressiveness while ensuring good audio quality in its outputs.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "61",
        "title": "GSTAR: Gaussian Surface Tracking and Reconstruction",
        "author": [
            "Chengwei Zheng",
            "Lixin Xue",
            "Juan Zarate",
            "Jie Song"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10283",
        "abstract": "3D Gaussian Splatting techniques have enabled efficient photo-realistic rendering of static scenes. Recent works have extended these approaches to support surface reconstruction and tracking. However, tracking dynamic surfaces with 3D Gaussians remains challenging due to complex topology changes, such as surfaces appearing, disappearing, or splitting. To address these challenges, we propose GSTAR, a novel method that achieves photo-realistic rendering, accurate surface reconstruction, and reliable 3D tracking for general dynamic scenes with changing topology. Given multi-view captures as input, GSTAR binds Gaussians to mesh faces to represent dynamic objects. For surfaces with consistent topology, GSTAR maintains the mesh topology and tracks the meshes using Gaussians. In regions where topology changes, GSTAR adaptively unbinds Gaussians from the mesh, enabling accurate registration and the generation of new surfaces based on these optimized Gaussians. Additionally, we introduce a surface-based scene flow method that provides robust initialization for tracking between frames. Experiments demonstrate that our method effectively tracks and reconstructs dynamic surfaces, enabling a range of applications. Our project page with the code release is available at https://chengwei-zheng.github.io/GSTAR/.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "62",
        "title": "Addressing Popularity Bias in Third-Party Library Recommendations Using LLMs",
        "author": [
            "Claudio Di Sipio",
            "Juri Di Rocco",
            "Davide Di Ruscio",
            "Vladyslav Bulhakov"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10313",
        "abstract": "Recommender systems for software engineering (RSSE) play a crucial role in automating development tasks by providing relevant suggestions according to the developer's context. However, they suffer from the so-called popularity bias, i.e., the phenomenon of recommending popular items that might be irrelevant to the current task. In particular, the long-tail effect can hamper the system's performance in terms of accuracy, thus leading to false positives in the provided recommendations. Foundation models are the most advanced generative AI-based models that achieve relevant results in several SE tasks.\nThis paper aims to investigate the capability of large language models (LLMs) to address the popularity bias in recommender systems of third-party libraries (TPLs). We conduct an ablation study experimenting with state-of-the-art techniques to mitigate the popularity bias, including fine-tuning and popularity penalty mechanisms. Our findings reveal that the considered LLMs cannot address the popularity bias in TPL recommenders, even though fine-tuning and post-processing penalty mechanism contributes to increasing the overall diversity of the provided recommendations. In addition, we discuss the limitations of LLMs in this context and suggest potential improvements to address the popularity bias in TPL recommenders, thus paving the way for additional experiments in this direction.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "63",
        "title": "Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling",
        "author": [
            "Suvodip Dey",
            "Yi-Jyun Sun",
            "Gokhan Tur",
            "Dilek Hakkani-Tur"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10316",
        "abstract": "Recent LLMs have enabled significant advancements for conversational agents. However, they are also well-known to hallucinate, i.e., they often produce responses that seem plausible but are not factually correct. On the other hand, users tend to over-rely on LLM-based AI agents; they accept the AI's suggestion even when it is wrong. Adding good friction, such as explanations or getting user confirmations, has been proposed as a mitigation in AI-supported decision-making systems. In this paper, we propose an accountability model for LLM-based task-oriented dialogue agents to address user overreliance via friction turns in cases of model uncertainty and errors associated with dialogue state tracking (DST). The accountability model is an augmented LLM with an additional accountability head, which functions as a binary classifier to predict the slots of the dialogue states. We perform our experiments with three backbone LLMs (Llama, Mistral, Gemma) on two established task-oriented datasets (MultiWOZ and Snips). Our empirical findings demonstrate that this approach not only enables reliable estimation of AI agent errors but also guides the LLM decoder in generating more accurate actions. We observe around 3% absolute improvement in joint goal accuracy by incorporating accountability heads in modern LLMs for the MultiWOZ dataset. We also show that this method enables the agent to self-correct its actions, further boosting its performance by 3%. Finally, we discuss the application of accountability modeling to prevent user overreliance by introducing friction.",
        "tags": [
            "LLMs",
            "LLaMA"
        ]
    },
    {
        "id": "64",
        "title": "HiMix: Reducing Computational Complexity in Large Vision-Language Models",
        "author": [
            "Xuange Zhang",
            "Dengjie Li",
            "Bo Liu",
            "Zenghao Bao",
            "Yao Zhou",
            "Baisong Yang",
            "Zhongying Liu",
            "Yujie Zhong",
            "Zheng Zhao",
            "Tongtong Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10318",
        "abstract": "Benefiting from recent advancements in large language models and modality alignment techniques, existing Large Vision-Language Models(LVLMs) have achieved prominent performance across a wide range of scenarios. However, the excessive computational complexity limits the widespread use of these models in practical applications. We argue that one main bottleneck in computational complexity is caused by the involvement of redundant vision sequences in model computation. This is inspired by a reassessment of the efficiency of vision and language information transmission in the language decoder of LVLMs. Then, we propose a novel hierarchical vision-language interaction mechanism called Hierarchical Vision injection for Mixture Attention (HiMix). In HiMix, only the language sequence undergoes full forward propagation, while the vision sequence interacts with the language at specific stages within each language decoder layer. It is striking that our approach significantly reduces computational complexity with minimal performance loss. Specifically, HiMix achieves a 10x reduction in the computational cost of the language decoder across multiple LVLM models while maintaining comparable performance. This highlights the advantages of our method, and we hope our research brings new perspectives to the field of vision-language understanding. Project Page: https://xuange923.github.io/HiMix",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "65",
        "title": "DiffStereo: High-Frequency Aware Diffusion Model for Stereo Image Restoration",
        "author": [
            "Huiyun Cao",
            "Yuan Shi",
            "Bin Xia",
            "Xiaoyu Jin",
            "Wenming Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10325",
        "abstract": "Diffusion models (DMs) have achieved promising performance in image restoration but haven't been explored for stereo images. The application of DM in stereo image restoration is confronted with a series of challenges. The need to reconstruct two images exacerbates DM's computational cost. Additionally, existing latent DMs usually focus on semantic information and remove high-frequency details as redundancy during latent compression, which is precisely what matters for image restoration. To address the above problems, we propose a high-frequency aware diffusion model, DiffStereo for stereo image restoration as the first attempt at DM in this domain. Specifically, DiffStereo first learns latent high-frequency representations (LHFR) of HQ images. DM is then trained in the learned space to estimate LHFR for stereo images, which are fused into a transformer-based stereo image restoration network providing beneficial high-frequency information of corresponding HQ images. The resolution of LHFR is kept the same as input images, which preserves the inherent texture from distortion. And the compression in channels alleviates the computational burden of DM. Furthermore, we devise a position encoding scheme when integrating the LHFR into the restoration network, enabling distinctive guidance in different depths of the restoration network. Comprehensive experiments verify that by combining generative DM and transformer, DiffStereo achieves both higher reconstruction accuracy and better perceptual quality on stereo super-resolution, deblurring, and low-light enhancement compared with state-of-the-art methods.",
        "tags": [
            "Deblurring",
            "Diffusion",
            "Super Resolution",
            "Transformer"
        ]
    },
    {
        "id": "66",
        "title": "Large language models for automated scholarly paper review: A survey",
        "author": [
            "Zhenzhen Zhuang",
            "Jiandong Chen",
            "Hongfeng Xu",
            "Yuwen Jiang",
            "Jialiang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10326",
        "abstract": "Large language models (LLMs) have significantly impacted human society, influencing various domains. Among them, academia is not simply a domain affected by LLMs, but it is also the pivotal force in the development of LLMs. In academic publications, this phenomenon is represented during the incorporation of LLMs into the peer review mechanism for reviewing manuscripts. We proposed the concept of automated scholarly paper review (ASPR) in our previous paper. As the incorporation grows, it now enters the coexistence phase of ASPR and peer review, which is described in that paper. LLMs hold transformative potential for the full-scale implementation of ASPR, but they also pose new issues and challenges that need to be addressed. In this survey paper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin with a survey to find out which LLMs are used to conduct ASPR. Then, we review what ASPR-related technological bottlenecks have been solved with the incorporation of LLM technology. After that, we move on to explore new methods, new datasets, new source code, and new online systems that come with LLMs for ASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and investigate the attitudes and reactions of publishers and academia to ASPR. Lastly, we discuss the challenges associated with the development of LLMs for ASPR. We hope this survey can serve as an inspirational reference for the researchers and promote the progress of ASPR for its actual implementation.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "67",
        "title": "Agent4Edu: Generating Learner Response Data by Generative Agents for Intelligent Education Systems",
        "author": [
            "Weibo Gao",
            "Qi Liu",
            "Linan Yue",
            "Fangzhou Yao",
            "Rui Lv",
            "Zheng Zhang",
            "Hao Wang",
            "Zhenya Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10332",
        "abstract": "Personalized learning represents a promising educational strategy within intelligent educational systems, aiming to enhance learners' practice efficiency. However, the discrepancy between offline metrics and online performance significantly impedes their progress. To address this challenge, we introduce Agent4Edu, a novel personalized learning simulator leveraging recent advancements in human intelligence through large language models (LLMs). Agent4Edu features LLM-powered generative agents equipped with learner profile, memory, and action modules tailored to personalized learning algorithms. The learner profiles are initialized using real-world response data, capturing practice styles and cognitive factors. Inspired by human psychology theory, the memory module records practice facts and high-level summaries, integrating reflection mechanisms. The action module supports various behaviors, including exercise understanding, analysis, and response generation. Each agent can interact with personalized learning algorithms, such as computerized adaptive testing, enabling a multifaceted evaluation and enhancement of customized services. Through a comprehensive assessment, we explore the strengths and weaknesses of Agent4Edu, emphasizing the consistency and discrepancies in responses between agents and human learners. The code, data, and appendix are publicly available at https://github.com/bigdata-ustc/Agent4Edu.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "68",
        "title": "Credit Risk Identification in Supply Chains Using Generative Adversarial Networks",
        "author": [
            "Zizhou Zhang",
            "Xinshi Li",
            "Yu Cheng",
            "Zhenrui Chen",
            "Qianying Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10348",
        "abstract": "Credit risk management within supply chains has emerged as a critical research area due to its significant implications for operational stability and financial sustainability. The intricate interdependencies among supply chain participants mean that credit risks can propagate across networks, with impacts varying by industry. This study explores the application of Generative Adversarial Networks (GANs) to enhance credit risk identification in supply chains. GANs enable the generation of synthetic credit risk scenarios, addressing challenges related to data scarcity and imbalanced datasets. By leveraging GAN-generated data, the model improves predictive accuracy while effectively capturing dynamic and temporal dependencies in supply chain data. The research focuses on three representative industries-manufacturing (steel), distribution (pharmaceuticals), and services (e-commerce) to assess industry-specific credit risk contagion. Experimental results demonstrate that the GAN-based model outperforms traditional methods, including logistic regression, decision trees, and neural networks, achieving superior accuracy, recall, and F1 scores. The findings underscore the potential of GANs in proactive risk management, offering robust tools for mitigating financial disruptions in supply chains. Future research could expand the model by incorporating external market factors and supplier relationships to further enhance predictive capabilities. Keywords- Generative Adversarial Networks (GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data Augmentation",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "69",
        "title": "Convergent Sixth-order Compact Finite Difference Method for Variable-Coefficient Elliptic PDEs in Curved Domains",
        "author": [
            "Bin Han",
            "Jiwoon Sim"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10358",
        "abstract": "Finite difference methods (FDMs) are widely used for solving partial differential equations (PDEs) due to their relatively simple implementation. However, they face significant challenges when applied to non-rectangular domains and in establishing theoretical convergence, particularly for high-order schemes. In this paper, we focus on solving the elliptic equation $-\\nabla \\cdot (a\\nabla u)=f$ in a two-dimensional curved domain $\\Omega$, where the diffusion coefficient $a$ is variable and smooth. We propose a sixth-order $9$-point compact FDM that only utilizes the grid points in $(h \\mathbb{Z}^2)\\cap \\Omega$ for any mesh size $h>0$, without relying on ghost points or information outside $\\overline{\\Omega}$. All the boundary stencils near $\\partial \\Omega$ have at most $6$ different configurations and use at most $8$ grid points inside $\\Omega$. We rigorously establish the sixth-order convergence of the numerically approximated solution $u_h$ in the $\\infty$-norm. Additionally, we derive a gradient approximation $\\nabla u$ directly from $u_h$ without solving auxiliary equations. This gradient approximation achieves proven accuracy of order $5+\\frac{1}{q}$ in the $q$-norm for all $1\\le q\\le \\infty$ (with a logarithmic factor $\\log h$ for $1\\le q<2$). To validate our proposed sixth-order compact finite different method, we provide several numerical examples that illustrate the sixth-order accuracy and computational efficiency of both the numerical solution and the gradient approximation for solving elliptic PDEs in curved domains.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "70",
        "title": "FaceXBench: Evaluating Multimodal LLMs on Face Understanding",
        "author": [
            "Kartik Narayan",
            "Vibashan VS",
            "Vishal M. Patel"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10360",
        "abstract": "Multimodal Large Language Models (MLLMs) demonstrate impressive problem-solving abilities across a wide range of tasks and domains. However, their capacity for face understanding has not been systematically studied. To address this gap, we introduce FaceXBench, a comprehensive benchmark designed to evaluate MLLMs on complex face understanding tasks. FaceXBench includes 5,000 multimodal multiple-choice questions derived from 25 public datasets and a newly created dataset, FaceXAPI. These questions cover 14 tasks across 6 broad categories, assessing MLLMs' face understanding abilities in bias and fairness, face authentication, recognition, analysis, localization and tool retrieval. Using FaceXBench, we conduct an extensive evaluation of 26 open-source MLLMs alongside 2 proprietary models, revealing the unique challenges in complex face understanding tasks. We analyze the models across three evaluation settings: zero-shot, in-context task description, and chain-of-thought prompting. Our detailed analysis reveals that current MLLMs, including advanced models like GPT-4o, and GeminiPro 1.5, show significant room for improvement. We believe FaceXBench will be a crucial resource for developing MLLMs equipped to perform sophisticated face understanding. Code: https://github.com/Kartik-3004/facexbench",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "71",
        "title": "Boosting the Accuracy of Stock Market Prediction via Multi-Layer Hybrid MTL Structure",
        "author": [
            "Yuxi Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2501.09760",
        "abstract": "Accurate stock market prediction provides great opportunities for informed decision-making, yet existing methods struggle with financial data's non-linear, high-dimensional, and volatile characteristics. Advanced predictive models are needed to effectively address these complexities. This paper proposes a novel multi-layer hybrid multi-task learning (MTL) framework aimed at achieving more efficient stock market predictions. It involves a Transformer encoder to extract complex correspondences between various input features, a Bidirectional Gated Recurrent Unit (BiGRU) to capture long-term temporal relationships, and a Kolmogorov-Arnold Network (KAN) to enhance the learning process. Experimental evaluations indicate that the proposed learning structure achieves great performance, with an MAE as low as 1.078, a MAPE as low as 0.012, and an R^2 as high as 0.98, when compared with other competitive networks.",
        "tags": [
            "KAN",
            "Transformer"
        ]
    },
    {
        "id": "72",
        "title": "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an Extreme Value Theory Approach",
        "author": [
            "Nicolas Atienza",
            "Christophe Labreuche",
            "Johanne Cohen",
            "Michele Sebag"
        ],
        "pdf": "https://arxiv.org/pdf/2501.10202",
        "abstract": "This paper introduces a novel method, Sample-efficient Probabilistic Detection using Extreme Value Theory (SPADE), which transforms a classifier into an abstaining classifier, offering provable protection against out-of-distribution and adversarial samples. The approach is based on a Generalized Extreme Value (GEV) model of the training distribution in the classifier's latent space, enabling the formal characterization of OOD samples. Interestingly, under mild assumptions, the GEV model also allows for formally characterizing adversarial samples. The abstaining classifier, which rejects samples based on their assessment by the GEV model, provably avoids OOD and adversarial samples. The empirical validation of the approach, conducted on various neural architectures (ResNet, VGG, and Vision Transformer) and medium and large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its frugality, stability, and efficiency compared to the state of the art.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    }
]