[
    {
        "id": "1",
        "title": "Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing",
        "author": [
            "Hao Fei",
            "Shengqiong Wu",
            "Hanwang Zhang",
            "Tat-Seng Chua",
            "Shuicheng Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19806",
        "abstract": "Recent developments of vision large language models (LLMs) have seen remarkable progress, yet still encounter challenges towards multimodal generalists, such as coarse-grained instance-level understanding, lack of unified support for both images and videos, and insufficient coverage across various vision tasks. In this paper, we present VITRON, a universal pixel-level vision LLM designed for comprehensive understanding, generating, segmenting, and editing of both static images and dynamic videos. Building on top of an LLM backbone, VITRON incorporates encoders for images, videos, and pixel-level regional visuals within its frontend modules, while employing state-of-the-art visual specialists as its backend, via which VITRON supports a spectrum of vision end tasks, spanning visual comprehension to visual generation, from low level to high level. To ensure an effective and precise message passing from LLM to backend modules for function invocation, we propose a novel hybrid method by simultaneously integrating discrete textual instructions and continuous signal embeddings. Further, we design various pixel-level spatiotemporal vision-language alignment learning for VITRON to reach the best fine-grained visual capability. Finally, a cross-task synergy module is advised to learn to maximize the task-invariant fine-grained visual features, enhancing the synergy between different visual tasks. Demonstrated over 12 visual tasks and evaluated across 22 datasets, VITRON showcases its extensive capabilities in the four main vision task clusters. Overall, this work illuminates the great potential of developing a more unified multimodal generalist. Project homepage: https://vitron-llm.github.io/",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "2",
        "title": "LINKs: Large Language Model Integrated Management for 6G Empowered Digital Twin NetworKs",
        "author": [
            "Shufan Jiang",
            "Bangyan Lin",
            "Yue Wu",
            "Yuan Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19811",
        "abstract": "In the rapidly evolving landscape of digital twins (DT) and 6G networks, the integration of large language models (LLMs) presents a novel approach to network management. This paper explores the application of LLMs in managing 6G-empowered DT networks, with a focus on optimizing data retrieval and communication efficiency in smart city scenarios. The proposed framework leverages LLMs for intelligent DT problem analysis and radio resource management (RRM) in fully autonomous way without any manual intervention. Our proposed framework -- LINKs, builds up a lazy loading strategy which can minimize transmission delay by selectively retrieving the relevant data. Based on the data retrieval plan, LLMs transform the retrieval task into an numerical optimization problem and utilizing solvers to build an optimal RRM, ensuring efficient communication across the network. Simulation results demonstrate the performance improvements in data planning and network management, highlighting the potential of LLMs to enhance the integration of DT and 6G technologies.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "3",
        "title": "ChipAlign: Instruction Alignment in Large Language Models for Chip Design via Geodesic Interpolation",
        "author": [
            "Chenhui Deng",
            "Yunsheng Bai",
            "Haoxing Ren"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19819",
        "abstract": "Recent advancements in large language models (LLMs) have expanded their application across various domains, including chip design, where domain-adapted chip models like ChipNeMo have emerged. However, these models often struggle with instruction alignment, a crucial capability for LLMs that involves following explicit human directives. This limitation impedes the practical application of chip LLMs, including serving as assistant chatbots for hardware design engineers. In this work, we introduce ChipAlign, a novel approach that utilizes a training-free model merging strategy, combining the strengths of a general instruction-aligned LLM with a chip-specific LLM. By considering the underlying manifold in the weight space, ChipAlign employs geodesic interpolation to effectively fuse the weights of input LLMs, producing a merged model that inherits strong instruction alignment and chip expertise from the respective instruction and chip LLMs. Our results demonstrate that ChipAlign significantly enhances instruction-following capabilities of existing chip LLMs, achieving up to a 26.6% improvement on the IFEval benchmark, while maintaining comparable expertise in the chip domain. This improvement in instruction alignment also translates to notable gains in instruction-involved QA tasks, delivering performance enhancements of 3.9% on the OpenROAD QA benchmark and 8.25% on production-level chip QA benchmarks, surpassing state-of-the-art baselines.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "4",
        "title": "GaLore$+$: Boosting Low-Rank Adaptation for LLMs with Cross-Head Projection",
        "author": [
            "Xutao Liao",
            "Shaohui Li",
            "Yuhui Xu",
            "Zhi Li",
            "Yu Liu",
            "You He"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19820",
        "abstract": "Recent low-rank training methods, such as GaLore, have significantly reduced the memory required to optimize large language models (LLMs). However, these methods often suffer from time-consuming low-rank projection estimations. In particular, the singular value decomposition (SVD) in GaLore can consume more than 80\\% of the total training time. To address this issue, we propose GaLore$+$, which uses cross-head low-rank projection to reduce the substantial time consumption in estimating low-rank projections for multi-head attention. In addition, we employ randomized subspace iteration to achieve fast SVD. To further enhance performance, we propose sparsely coded residuals to reduce the errors caused by low-rank approximation on the first- and second-order moments of the optimizers and weight updates. We evaluate GaLore$+$ on arithmetic reasoning and natural language generation datasets. Our experiments demonstrate that GaLore$+$ delivers superior performance while achieving approximately $4\\times$ fine-tuning speed compared to vanilla GaLore.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "5",
        "title": "Nanoscaling Floating-Point (NxFP): NanoMantissa, Adaptive Microexponents, and Code Recycling for Direct-Cast Compression of Large Language Models",
        "author": [
            "Yun-Chen Lo",
            "Gu-Yeon Wei",
            "David Brooks"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19821",
        "abstract": "As cutting-edge large language models (LLMs) continue to transform various industries, their fast-growing model size and sequence length have led to memory traffic and capacity challenges. Recently, AMD, Arm, Intel, Meta, Microsoft, NVIDIA, and Qualcomm have proposed a Microscaling standard (Mx), which augments block floating-point with microexponents to achieve promising perplexity-to-footprint trade-offs. However, the Microscaling suffers from significant perplexity degradation on modern LLMs with less than six bits. This paper profiles modern LLMs and identifies three main challenges of low-bit Microscaling format, i.e., inaccurate tracking of outliers, vacant quantization levels, and wasted binary code. In response, Nanoscaling (NxFP) proposes three techniques, i.e., NanoMantissa, Adaptive Microexponent, and Code Recycling to enable better accuracy and smaller memory footprint than state-of-the-art MxFP. Experimental results on direct-cast inference across various modern LLMs demonstrate that our proposed methods outperform state-of-the-art MxFP by up to 0.64 in perplexity and by up to 30% in accuracy on MMLU benchmarks. Furthermore, NxFP reduces memory footprint by up to 16% while achieving comparable perplexity as MxFP.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "6",
        "title": "A Survey on Large Language Models for Communication, Network, and Service Management: Application Insights, Challenges, and Future Directions",
        "author": [
            "Gordon Owusu Boateng",
            "Hani Sami",
            "Ahmed Alagha",
            "Hanae Elmekki",
            "Ahmad Hammoud",
            "Rabeb Mizouni",
            "Azzam Mourad",
            "Hadi Otrok",
            "Jamal Bentahar",
            "Sami Muhaidat",
            "Chamseddine Talhi",
            "Zbigniew Dziong",
            "Mohsen Guizani"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19823",
        "abstract": "The rapid evolution of communication networks in recent decades has intensified the need for advanced Network and Service Management (NSM) strategies to address the growing demands for efficiency, scalability, enhanced performance, and reliability of these networks. Large Language Models (LLMs) have received tremendous attention due to their unparalleled capabilities in various Natural Language Processing (NLP) tasks and generating context-aware insights, offering transformative potential for automating diverse communication NSM tasks. Contrasting existing surveys that consider a single network domain, this survey investigates the integration of LLMs across different communication network domains, including mobile networks and related technologies, vehicular networks, cloud-based networks, and fog/edge-based networks. First, the survey provides foundational knowledge of LLMs, explicitly detailing the generic transformer architecture, general-purpose and domain-specific LLMs, LLM model pre-training and fine-tuning, and their relation to communication NSM. Under a novel taxonomy of network monitoring and reporting, AI-powered network planning, network deployment and distribution, and continuous network support, we extensively categorize LLM applications for NSM tasks in each of the different network domains, exploring existing literature and their contributions thus far. Then, we identify existing challenges and open issues, as well as future research directions for LLM-driven communication NSM, emphasizing the need for scalable, adaptable, and resource-efficient solutions that align with the dynamic landscape of communication networks. We envision that this survey serves as a holistic roadmap, providing critical insights for leveraging LLMs to enhance NSM.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "7",
        "title": "AnalogXpert: Automating Analog Topology Synthesis by Incorporating Circuit Design Expertise into Large Language Models",
        "author": [
            "Haoyi Zhang",
            "Shizhao Sun",
            "Yibo Lin",
            "Runsheng Wang",
            "Jiang Bian"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19824",
        "abstract": "Analog circuits are crucial in modern electronic systems, and automating their design has attracted significant research interest. One of major challenges is topology synthesis, which determines circuit components and their connections. Recent studies explore large language models (LLM) for topology synthesis. However, the scenarios addressed by these studies do not align well with practical applications. Specifically, existing work uses vague design requirements as input and outputs an ideal model, but detailed structural requirements and device-level models are more practical. Moreover, current approaches either formulate topology synthesis as graph generation or Python code generation, whereas practical topology design is a complex process that demands extensive design knowledge. In this work, we propose AnalogXpert, a LLM-based agent aiming at solving practical topology synthesis problem by incorporating circuit design expertise into LLMs. First, we represent analog topology as SPICE code and introduce a subcircuit library to reduce the design space, in the same manner as experienced designers. Second, we decompose the problem into two sub-task (i.e., block selection and block connection) through the use of CoT and incontext learning techniques, to mimic the practical design process. Third, we introduce a proofreading strategy that allows LLMs to incrementally correct the errors in the initial design, akin to human designers who iteratively check and adjust the initial topology design to ensure accuracy. Finally, we construct a high-quality benchmark containing both real data (30) and synthetic data (2k). AnalogXpert achieves 40% and 23% success rates on the synthetic dataset and real dataset respectively, which is markedly better than those of GPT-4o (3% on both the synthetic dataset and the real dataset).",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "8",
        "title": "GFormer: Accelerating Large Language Models with Optimized Transformers on Gaudi Processors",
        "author": [
            "Chengming Zhang",
            "Xinheng Ding",
            "Baixi Sun",
            "Xiaodong Yu",
            "Weijian Zheng",
            "Zhen Xie",
            "Dingwen Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19829",
        "abstract": "Heterogeneous hardware like Gaudi processor has been developed to enhance computations, especially matrix operations for Transformer-based large language models (LLMs) for generative AI tasks. However, our analysis indicates that Transformers are not fully optimized on such emerging hardware, primarily due to inadequate optimizations in non-matrix computational kernels like Softmax and in heterogeneous resource utilization, particularly when processing long sequences. To address these issues, we propose an integrated approach (called GFormer) that merges sparse and linear attention mechanisms. GFormer aims to maximize the computational capabilities of the Gaudi processor's Matrix Multiplication Engine (MME) and Tensor Processing Cores (TPC) without compromising model quality. GFormer includes a windowed self-attention kernel and an efficient outer product kernel for causal linear attention, aiming to optimize LLM inference on Gaudi processors. Evaluation shows that GFormer significantly improves efficiency and model performance across various tasks on the Gaudi processor and outperforms state-of-the-art GPUs.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "9",
        "title": "Back To The Future: A Hybrid Transformer-XGBoost Model for Action-oriented Future-proofing Nowcasting",
        "author": [
            "Ziheng Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19832",
        "abstract": "Inspired by the iconic movie Back to the Future, this paper explores an innovative adaptive nowcasting approach that reimagines the relationship between present actions and future outcomes. In the movie, characters travel through time to manipulate past events, aiming to create a better future. Analogously, our framework employs predictive insights about the future to inform and adjust present conditions. This dual-stage model integrates the forecasting power of Transformers (future visionary) with the interpretability and efficiency of XGBoost (decision maker), enabling a seamless loop of future prediction and present adaptation. Through experimentation with meteorological datasets, we demonstrate the framework's advantage in achieving more accurate forecasting while guiding actionable interventions for real-time applications.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "10",
        "title": "ERPA: Efficient RPA Model Integrating OCR and LLMs for Intelligent Document Processing",
        "author": [
            "Osama Abdellaif",
            "Abdelrahman Nader",
            "Ali Hamdi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19840",
        "abstract": "This paper presents ERPA, an innovative Robotic Process Automation (RPA) model designed to enhance ID data extraction and optimize Optical Character Recognition (OCR) tasks within immigration workflows. Traditional RPA solutions often face performance limitations when processing large volumes of documents, leading to inefficiencies. ERPA addresses these challenges by incorporating Large Language Models (LLMs) to improve the accuracy and clarity of extracted text, effectively handling ambiguous characters and complex structures. Benchmark comparisons with leading platforms like UiPath and Automation Anywhere demonstrate that ERPA significantly reduces processing times by up to 94 percent, completing ID data extraction in just 9.94 seconds. These findings highlight ERPA's potential to revolutionize document automation, offering a faster and more reliable alternative to current RPA solutions.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "11",
        "title": "FlameGS: Reconstruct flame light field via Gaussian Splatting",
        "author": [
            "Yunhao Shui",
            "Fuhao Zhang",
            "Can Gao",
            "Hao Xue",
            "Zhiyin Ma",
            "Gang Xun",
            "Xuesong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19841",
        "abstract": "To address the time-consuming and computationally intensive issues of traditional ART algorithms for flame combustion diagnosis, inspired by flame simulation technology, we propose a novel representation method for flames. By modeling the luminous process of flames and utilizing 2D projection images for supervision, our experimental validation shows that this model achieves an average structural similarity index of 0.96 between actual images and predicted 2D projections, along with a Peak Signal-to-Noise Ratio of 39.05. Additionally, it saves approximately 34 times the computation time and about 10 times the memory compared to traditional algorithms.",
        "tags": [
            "Gaussian Splatting"
        ]
    },
    {
        "id": "12",
        "title": "Conditional Balance: Improving Multi-Conditioning Trade-Offs in Image Generation",
        "author": [
            "Nadav Z. Cohen",
            "Oron Nir",
            "Ariel Shamir"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19853",
        "abstract": "Balancing content fidelity and artistic style is a pivotal challenge in image generation. While traditional style transfer methods and modern Denoising Diffusion Probabilistic Models (DDPMs) strive to achieve this balance, they often struggle to do so without sacrificing either style, content, or sometimes both. This work addresses this challenge by analyzing the ability of DDPMs to maintain content and style equilibrium. We introduce a novel method to identify sensitivities within the DDPM attention layers, identifying specific layers that correspond to different stylistic aspects. By directing conditional inputs only to these sensitive layers, our approach enables fine-grained control over style and content, significantly reducing issues arising from over-constrained inputs. Our findings demonstrate that this method enhances recent stylization techniques by better aligning style and content, ultimately improving the quality of generated visual content.",
        "tags": [
            "DDPM",
            "Diffusion",
            "Style Transfer"
        ]
    },
    {
        "id": "13",
        "title": "UniAvatar: Taming Lifelike Audio-Driven Talking Head Generation with Comprehensive Motion and Lighting Control",
        "author": [
            "Wenzhang Sun",
            "Xiang Li",
            "Donglin Di",
            "Zhuding Liang",
            "Qiyuan Zhang",
            "Hao Li",
            "Wei Chen",
            "Jianxun Cui"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19860",
        "abstract": "Recently, animating portrait images using audio input is a popular task. Creating lifelike talking head videos requires flexible and natural movements, including facial and head dynamics, camera motion, realistic light and shadow effects. Existing methods struggle to offer comprehensive, multifaceted control over these aspects. In this work, we introduce UniAvatar, a designed method that provides extensive control over a wide range of motion and illumination conditions. Specifically, we use the FLAME model to render all motion information onto a single image, maintaining the integrity of 3D motion details while enabling fine-grained, pixel-level control. Beyond motion, this approach also allows for comprehensive global illumination control. We design independent modules to manage both 3D motion and illumination, permitting separate and combined control. Extensive experiments demonstrate that our method outperforms others in both broad-range motion control and lighting control. Additionally, to enhance the diversity of motion and environmental contexts in current datasets, we collect and plan to publicly release two datasets, DH-FaceDrasMvVid-100 and DH-FaceReliVid-200, which capture significant head movements during speech and various lighting scenarios.",
        "tags": [
            "3D",
            "Talking Head"
        ]
    },
    {
        "id": "14",
        "title": "Data-Free Group-Wise Fully Quantized Winograd Convolution via Learnable Scales",
        "author": [
            "Shuokai Pan",
            "Gerti Tuzi",
            "Sudarshan Sreeram",
            "Dibakar Gope"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19867",
        "abstract": "Despite the revolutionary breakthroughs of large-scale textto-image diffusion models for complex vision and downstream tasks, their extremely high computational and storage costs limit their usability. Quantization of diffusion models has been explored in recent works to reduce compute costs and memory bandwidth usage. To further improve inference time, fast convolution algorithms such as Winograd can be used for convolution layers, which account for a significant portion of computations in diffusion models. However, the significant quality loss of fully quantized Winograd using existing coarser-grained post-training quantization methods, combined with the complexity and cost of finetuning the Winograd transformation matrices for such large models to recover quality, makes them unsuitable for large-scale foundation models. Motivated by the presence of a large range of values in them, we investigate the impact of finer-grained group-wise quantization in quantizing diffusion models. While group-wise quantization can largely handle the fully quantized Winograd convolution, it struggles to deal with the large distribution imbalance in a sizable portion of the Winograd domain computation. To reduce range differences in the Winograd domain, we propose finetuning only the scale parameters of the Winograd transform matrices without using any domain-specific training data. Because our method does not depend on any training data, the generalization performance of quantized diffusion models is safely guaranteed. For text-to-image generation task, the 8-bit fully-quantized diffusion model with Winograd provides near-lossless quality (FID and CLIP scores) in comparison to the full-precision model. For image classification, our method outperforms the state-of-the-art Winograd PTQ method by 1.62% and 2.56% in top-1 ImageNet accuracy on ResNet18 and ResNet-34, respectively, with Winograd F(6, 3).",
        "tags": [
            "CLIP",
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "15",
        "title": "Evaluate Summarization in Fine-Granularity: Auto Evaluation with LLM",
        "author": [
            "Dong Yuan",
            "Eti Rastogi",
            "Fen Zhao",
            "Sagar Goyal",
            "Gautam Naik",
            "Sree Prasanna Rajagopal"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19906",
        "abstract": "Due to the exponential growth of information and the need for efficient information consumption the task of summarization has gained paramount importance. Evaluating summarization accurately and objectively presents significant challenges, particularly when dealing with long and unstructured texts rich in content. Existing methods, such as ROUGE (Lin, 2004) and embedding similarities, often yield scores that have low correlation with human judgements and are also not intuitively understandable, making it difficult to gauge the true quality of the summaries. LLMs can mimic human in giving subjective reviews but subjective scores are hard to interpret and justify. They can be easily manipulated by altering the models and the tones of the prompts. In this paper, we introduce a novel evaluation methodology and tooling designed to address these challenges, providing a more comprehensive, accurate and interpretable assessment of summarization outputs. Our method (SumAutoEval) proposes and evaluates metrics at varying granularity levels, giving objective scores on 4 key dimensions such as completeness, correctness, Alignment and readability. We empirically demonstrate, that SumAutoEval enhances the understanding of output quality with better human correlation.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "16",
        "title": "Char-SAM: Turning Segment Anything Model into Scene Text Segmentation Annotator with Character-level Visual Prompts",
        "author": [
            "Enze Xie",
            "Jiaho Lyu",
            "Daiqing Wu",
            "Huawen Shen",
            "Yu Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19917",
        "abstract": "The recent emergence of the Segment Anything Model (SAM) enables various domain-specific segmentation tasks to be tackled cost-effectively by using bounding boxes as prompts. However, in scene text segmentation, SAM can not achieve desirable performance. The word-level bounding box as prompts is too coarse for characters, while the character-level bounding box as prompts suffers from over-segmentation and under-segmentation issues. In this paper, we propose an automatic annotation pipeline named Char-SAM, that turns SAM into a low-cost segmentation annotator with a Character-level visual prompt. Specifically, leveraging some existing text detection datasets with word-level bounding box annotations, we first generate finer-grained character-level bounding box prompts using the Character Bounding-box Refinement CBR module. Next, we employ glyph information corresponding to text character categories as a new prompt in the Character Glyph Refinement (CGR) module to guide SAM in producing more accurate segmentation masks, addressing issues of over-segmentation and under-segmentation. These modules fully utilize the bbox-to-mask capability of SAM to generate high-quality text segmentation annotations automatically. Extensive experiments on TextSeg validate the effectiveness of Char-SAM. Its training-free nature also enables the generation of high-quality scene text segmentation datasets from real-world datasets like COCO-Text and MLT17.",
        "tags": [
            "Detection",
            "SAM",
            "Segment Anything",
            "Segmentation"
        ]
    },
    {
        "id": "17",
        "title": "HADES: Hardware Accelerated Decoding for Efficient Speculation in Large Language Models",
        "author": [
            "Ze Yang",
            "Yihong Jin",
            "Xinhe Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19925",
        "abstract": "Large Language Models (LLMs) have revolutionized natural language processing by understanding and generating human-like text. However, the increasing demand for more sophisticated LLMs presents significant computational challenges due to their scale and complexity. This paper introduces Hardware Accelerated Decoding (HADES), a novel approach to enhance the performance and energy efficiency of LLMs. We address the design of an LLM accelerator with hardware-level speculative decoding support, a concept not previously explored in existing literature. Our work demonstrates how speculative decoding can significantly improve the efficiency of LLM operations, paving the way for more advanced and practical applications of these models.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "18",
        "title": "Right vs. Right: Can LLMs Make Tough Choices?",
        "author": [
            "Jiaqing Yuan",
            "Pradeep K. Murukannaiah",
            "Munindar P. Singh"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19926",
        "abstract": "An ethical dilemma describes a choice between two \"right\" options involving conflicting moral values. We present a comprehensive evaluation of how LLMs navigate ethical dilemmas. Specifically, we investigate LLMs on their (1) sensitivity in comprehending ethical dilemmas, (2) consistency in moral value choice, (3) consideration of consequences, and (4) ability to align their responses to a moral value preference explicitly or implicitly specified in a prompt. Drawing inspiration from a leading ethical framework, we construct a dataset comprising 1,730 ethical dilemmas involving four pairs of conflicting values. We evaluate 20 well-known LLMs from six families. Our experiments reveal that: (1) LLMs exhibit pronounced preferences between major value pairs, and prioritize truth over loyalty, community over individual, and long-term over short-term considerations. (2) The larger LLMs tend to support a deontological perspective, maintaining their choices of actions even when negative consequences are specified. (3) Explicit guidelines are more effective in guiding LLMs' moral choice than in-context examples. Lastly, our experiments highlight the limitation of LLMs in comprehending different formulations of ethical dilemmas.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "19",
        "title": "Hidformer: Transformer-Style Neural Network in Stock Price Forecasting",
        "author": [
            "Kamil Ł. Szydłowski",
            "Jarosław A. Chudziak"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19932",
        "abstract": "This paper investigates the application of Transformer-based neural networks to stock price forecasting, with a special focus on the intersection of machine learning techniques and financial market analysis. The evolution of Transformer models, from their inception to their adaptation for time series analysis in financial contexts, is reviewed and discussed. Central to our study is the exploration of the Hidformer model, which is currently recognized for its promising performance in time series prediction. The primary aim of this paper is to determine whether Hidformer will also prove itself in the task of stock price prediction. This slightly modified model serves as the framework for our experiments, integrating the principles of technical analysis with advanced machine learning concepts to enhance stock price prediction accuracy. We conduct an evaluation of the Hidformer model's performance, using a set of criteria to determine its efficacy. Our findings offer additional insights into the practical application of Transformer architectures in financial time series forecasting, highlighting their potential to improve algorithmic trading strategies, including human decision making.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "20",
        "title": "Zero-shot Hazard Identification in Autonomous Driving: A Case Study on the COOOL Benchmark",
        "author": [
            "Lukas Picek",
            "Vojtěch Čermák",
            "Marek Hanzl"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19944",
        "abstract": "This paper presents our submission to the COOOL competition, a novel benchmark for detecting and classifying out-of-label hazards in autonomous driving. Our approach integrates diverse methods across three core tasks: (i) driver reaction detection, (ii) hazard object identification, and (iii) hazard captioning. We propose kernel-based change point detection on bounding boxes and optical flow dynamics for driver reaction detection to analyze motion patterns. For hazard identification, we combined a naive proximity-based strategy with object classification using a pre-trained ViT model. At last, for hazard captioning, we used the MOLMO vision-language model with tailored prompts to generate precise and context-aware descriptions of rare and low-resolution hazards. The proposed pipeline outperformed the baseline methods by a large margin, reducing the relative error by 33%, and scored 2nd on the final leaderboard consisting of 32 teams.",
        "tags": [
            "Detection",
            "ViT"
        ]
    },
    {
        "id": "21",
        "title": "Motion Planning Diffusion: Learning and Adapting Robot Motion Planning with Diffusion Models",
        "author": [
            "J. Carvalho",
            "A. Le",
            "P. Kicki",
            "D. Koert",
            "J. Peters"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19948",
        "abstract": "The performance of optimization-based robot motion planning algorithms is highly dependent on the initial solutions, commonly obtained by running a sampling-based planner to obtain a collision-free path. However, these methods can be slow in high-dimensional and complex scenes and produce non-smooth solutions. Given previously solved path-planning problems, it is highly desirable to learn their distribution and use it as a prior for new similar problems. Several works propose utilizing this prior to bootstrap the motion planning problem, either by sampling initial solutions from it, or using its distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we introduce Motion Planning Diffusion (MPD), an algorithm that learns trajectory distribution priors with diffusion models. These generative models have shown increasing success in encoding multimodal data and have desirable properties for gradient-based motion planning, such as cost guidance. Given a motion planning problem, we construct a cost function and sample from the posterior distribution using the learned prior combined with the cost function gradients during the denoising process. Instead of learning the prior on all trajectory waypoints, we propose learning a lower-dimensional representation of a trajectory using linear motion primitives, particularly B-spline curves. This parametrization guarantees that the generated trajectory is smooth, can be interpolated at higher frequencies, and needs fewer parameters than a dense waypoint representation. We demonstrate the results of our method ranging from simple 2D to more complex tasks using a 7-dof robot arm manipulator. In addition to learning from simulated data, we also use human demonstrations on a real-world pick-and-place task.",
        "tags": [
            "Diffusion",
            "Robot"
        ]
    },
    {
        "id": "22",
        "title": "DepthMamba with Adaptive Fusion",
        "author": [
            "Zelin Meng",
            "Zhichen Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19964",
        "abstract": "Multi-view depth estimation has achieved impressive performance over various benchmarks. However, almost all current multi-view systems rely on given ideal camera poses, which are unavailable in many real-world scenarios, such as autonomous driving. In this work, we propose a new robustness benchmark to evaluate the depth estimation system under various noisy pose settings. Surprisingly, we find current multi-view depth estimation methods or single-view and multi-view fusion methods will fail when given noisy pose settings. To tackle this challenge, we propose a two-branch network architecture which fuses the depth estimation results of single-view and multi-view branch. In specific, we introduced mamba to serve as feature extraction backbone and propose an attention-based fusion methods which adaptively select the most robust estimation results between the two branches. Thus, the proposed method can perform well on some challenging scenes including dynamic objects, texture-less regions, etc. Ablation studies prove the effectiveness of the backbone and fusion method, while evaluation experiments on challenging benchmarks (KITTI and DDAD) show that the proposed method achieves a competitive performance compared to the state-of-the-art methods.",
        "tags": [
            "Depth Estimation",
            "Mamba"
        ]
    },
    {
        "id": "23",
        "title": "Bridging Context Gaps: Enhancing Comprehension in Long-Form Social Conversations Through Contextualized Excerpts",
        "author": [
            "Shrestha Mohanty",
            "Sarah Xuan",
            "Jacob Jobraeel",
            "Anurag Kumar",
            "Deb Roy",
            "Jad Kabbara"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19966",
        "abstract": "We focus on enhancing comprehension in small-group recorded conversations, which serve as a medium to bring people together and provide a space for sharing personal stories and experiences on crucial social matters. One way to parse and convey information from these conversations is by sharing highlighted excerpts in subsequent conversations. This can help promote a collective understanding of relevant issues, by highlighting perspectives and experiences to other groups of people who might otherwise be unfamiliar with and thus unable to relate to these experiences. The primary challenge that arises then is that excerpts taken from one conversation and shared in another setting might be missing crucial context or key elements that were previously introduced in the original conversation. This problem is exacerbated when conversations become lengthier and richer in themes and shared experiences. To address this, we explore how Large Language Models (LLMs) can enrich these excerpts by providing socially relevant context. We present approaches for effective contextualization to improve comprehension, readability, and empathy. We show significant improvements in understanding, as assessed through subjective and objective evaluations. While LLMs can offer valuable context, they struggle with capturing key social aspects. We release the Human-annotated Salient Excerpts (HSE) dataset to support future work. Additionally, we show how context-enriched excerpts can provide more focused and comprehensive conversation summaries.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "24",
        "title": "MAKIMA: Tuning-free Multi-Attribute Open-domain Video Editing via Mask-Guided Attention Modulation",
        "author": [
            "Haoyu Zheng",
            "Wenqiao Zhang",
            "Zheqi Lv",
            "Yu Zhong",
            "Yang Dai",
            "Jianxiang An",
            "Yongliang Shen",
            "Juncheng Li",
            "Dongping Zhang",
            "Siliang Tang",
            "Yueting Zhuang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19978",
        "abstract": "Diffusion-based text-to-image (T2I) models have demonstrated remarkable results in global video editing tasks. However, their focus is primarily on global video modifications, and achieving desired attribute-specific changes remains a challenging task, specifically in multi-attribute editing (MAE) in video. Contemporary video editing approaches either require extensive fine-tuning or rely on additional networks (such as ControlNet) for modeling multi-object appearances, yet they remain in their infancy, offering only coarse-grained MAE solutions. In this paper, we present MAKIMA, a tuning-free MAE framework built upon pretrained T2I models for open-domain video editing. Our approach preserves video structure and appearance information by incorporating attention maps and features from the inversion process during denoising. To facilitate precise editing of multiple attributes, we introduce mask-guided attention modulation, enhancing correlations between spatially corresponding tokens and suppressing cross-attribute interference in both self-attention and cross-attention layers. To balance video frame generation quality and efficiency, we implement consistent feature propagation, which generates frame sequences by editing keyframes and propagating their features throughout the sequence. Extensive experiments demonstrate that MAKIMA outperforms existing baselines in open-domain multi-attribute video editing tasks, achieving superior results in both editing accuracy and temporal consistency while maintaining computational efficiency.",
        "tags": [
            "ControlNet",
            "Diffusion",
            "Text-to-Image",
            "Video Editing"
        ]
    },
    {
        "id": "25",
        "title": "An Ordinary Differential Equation Sampler with Stochastic Start for Diffusion Bridge Models",
        "author": [
            "Yuang Wang",
            "Pengfei Jin",
            "Li Zhang",
            "Quanzheng Li",
            "Zhiqiang Chen",
            "Dufan Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19992",
        "abstract": "Diffusion bridge models have demonstrated promising performance in conditional image generation tasks, such as image restoration and translation, by initializing the generative process from corrupted images instead of pure Gaussian noise. However, existing diffusion bridge models often rely on Stochastic Differential Equation (SDE) samplers, which result in slower inference speed compared to diffusion models that employ high-order Ordinary Differential Equation (ODE) solvers for acceleration. To mitigate this gap, we propose a high-order ODE sampler with a stochastic start for diffusion bridge models. To overcome the singular behavior of the probability flow ODE (PF-ODE) at the beginning of the reverse process, a posterior sampling approach was introduced at the first reverse step. The sampling was designed to ensure a smooth transition from corrupted images to the generative trajectory while reducing discretization errors. Following this stochastic start, Heun's second-order solver is applied to solve the PF-ODE, achieving high perceptual quality with significantly reduced neural function evaluations (NFEs). Our method is fully compatible with pretrained diffusion bridge models and requires no additional training. Extensive experiments on image restoration and translation tasks, including super-resolution, JPEG restoration, Edges-to-Handbags, and DIODE-Outdoor, demonstrated that our sampler outperforms state-of-the-art methods in both visual quality and Frechet Inception Distance (FID).",
        "tags": [
            "Diffusion",
            "ODE",
            "SDE",
            "Super Resolution"
        ]
    },
    {
        "id": "26",
        "title": "Learning Adaptive and View-Invariant Vision Transformer with Multi-Teacher Knowledge Distillation for Real-Time UAV Tracking",
        "author": [
            "You Wu",
            "Yongxin Li",
            "Mengyuan Liu",
            "Xucheng Wang",
            "Xiangyang Yang",
            "Hengzhou Ye",
            "Dan Zeng",
            "Qijun Zhao",
            "Shuiwang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20002",
        "abstract": "Visual tracking has made significant strides due to the adoption of transformer-based models. Most state-of-the-art trackers struggle to meet real-time processing demands on mobile platforms with constrained computing resources, particularly for real-time unmanned aerial vehicle (UAV) tracking. To achieve a better balance between performance and efficiency, we introduce AVTrack, an adaptive computation framework designed to selectively activate transformer blocks for real-time UAV tracking. The proposed Activation Module (AM) dynamically optimizes the ViT architecture by selectively engaging relevant components, thereby enhancing inference efficiency without significant compromise to tracking performance. Furthermore, to tackle the challenges posed by extreme changes in viewing angles often encountered in UAV tracking, the proposed method enhances ViTs' effectiveness by learning view-invariant representations through mutual information (MI) maximization. Two effective design principles are proposed in the AVTrack. Building on it, we propose an improved tracker, dubbed AVTrack-MD, which introduces the novel MI maximization-based multi-teacher knowledge distillation (MD) framework. It harnesses the benefits of multiple teachers, specifically the off-the-shelf tracking models from the AVTrack, by integrating and refining their outputs, thereby guiding the learning process of the compact student network. Specifically, we maximize the MI between the softened feature representations from the multi-teacher models and the student model, leading to improved generalization and performance of the student model, particularly in noisy conditions. Extensive experiments on multiple UAV tracking benchmarks demonstrate that AVTrack-MD not only achieves performance comparable to the AVTrack baseline but also reduces model complexity, resulting in a significant 17\\% increase in average tracking speed.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "27",
        "title": "Adaptive Parameter-Efficient Federated Fine-Tuning on Heterogeneous Devices",
        "author": [
            "Jun Liu",
            "Yunming Liao",
            "Hongli Xu",
            "Yang Xu",
            "Jianchun Liu",
            "Chen Qian"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20004",
        "abstract": "Federated fine-tuning (FedFT) has been proposed to fine-tune the pre-trained language models in a distributed manner. However, there are two critical challenges for efficient FedFT in practical applications, i.e., resource constraints and system heterogeneity. Existing works rely on parameter-efficient fine-tuning methods, e.g., low-rank adaptation (LoRA), but with major limitations. Herein, based on the inherent characteristics of FedFT, we observe that LoRA layers with higher ranks added close to the output help to save resource consumption while achieving comparable fine-tuning performance. Then we propose a novel LoRA-based FedFT framework, termed LEGEND, which faces the difficulty of determining the number of LoRA layers (called, LoRA depth) and the rank of each LoRA layer (called, rank distribution). We analyze the coupled relationship between LoRA depth and rank distribution, and design an efficient LoRA configuration algorithm for heterogeneous devices, thereby promoting fine-tuning efficiency. Extensive experiments are conducted on a physical platform with 80 commercial devices. The results show that LEGEND can achieve a speedup of 1.5-2.8$\\times$ and save communication costs by about 42.3% when achieving the target accuracy, compared to the advanced solutions.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "28",
        "title": "Adversarial Robustness for Deep Learning-based Wildfire Detection Models",
        "author": [
            "Ryo Ide",
            "Lei Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20006",
        "abstract": "Smoke detection using Deep Neural Networks (DNNs) is an effective approach for early wildfire detection. However, because smoke is temporally and spatially anomalous, there are limitations in collecting sufficient training data. This raises overfitting and bias concerns in existing DNN-based wildfire detection models. Thus, we introduce WARP (Wildfire Adversarial Robustness Procedure), the first model-agnostic framework for evaluating the adversarial robustness of DNN-based wildfire detection models. WARP addresses limitations in smoke image diversity using global and local adversarial attack methods. The global attack method uses image-contextualized Gaussian noise, while the local attack method uses patch noise injection, tailored to address critical aspects of wildfire detection. Leveraging WARP's model-agnostic capabilities, we assess the adversarial robustness of real-time Convolutional Neural Networks (CNNs) and Transformers. The analysis revealed valuable insights into the models' limitations. Specifically, the global attack method demonstrates that the Transformer model has more than 70\\% precision degradation than the CNN against global noise. In contrast, the local attack method shows that both models are susceptible to cloud image injections when detecting smoke-positive instances, suggesting a need for model improvements through data augmentation. WARP's comprehensive robustness analysis contributed to the development of wildfire-specific data augmentation strategies, marking a step toward practicality.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "29",
        "title": "BaiJia: A Large Scale Role-Playing Agent Corpus of Chinese Historical Charcaters",
        "author": [
            "Ting Bai",
            "Jiazheng Kang",
            "Jiayang Fan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20024",
        "abstract": "We introduce a comprehensive large-scale role-playing agent corpus, termed BaiJia, that comprises various Chinese historical characters. This corpus is noteworthy for being the pioneering compilation of low-resource data that can be utilized in large language models (LLMs) to engage in AI-driven historical role-playing agents. BaiJia addresses the challenges in terms of fragmented historical textual records in different forms and modalities, integrating various characters' information, including their biographical, literary, family relations, historical events, and so on. We conduct extensive experiments to demonstrate the effectiveness of our BaiJia agent corpus in bolstering the role-playing abilities of various foundational LLMs, and promoting the development and assessment of LLMs in the context of historical role-playing tasks. The agent corpus is available at http://baijia.online.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "30",
        "title": "STAYKATE: Hybrid In-Context Example Selection Combining Representativeness Sampling and Retrieval-based Approach -- A Case Study on Science Domains",
        "author": [
            "Chencheng Zhu",
            "Kazutaka Shimada",
            "Tomoki Taniguchi",
            "Tomoko Ohkuma"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20043",
        "abstract": "Large language models (LLMs) demonstrate the ability to learn in-context, offering a potential solution for scientific information extraction, which often contends with challenges such as insufficient training data and the high cost of annotation processes. Given that the selection of in-context examples can significantly impact performance, it is crucial to design a proper method to sample the efficient ones. In this paper, we propose STAYKATE, a static-dynamic hybrid selection method that combines the principles of representativeness sampling from active learning with the prevalent retrieval-based approach. The results across three domain-specific datasets indicate that STAYKATE outperforms both the traditional supervised methods and existing selection methods. The enhancement in performance is particularly pronounced for entity types that other methods pose challenges.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "31",
        "title": "Enhancing Diffusion Models for Inverse Problems with Covariance-Aware Posterior Sampling",
        "author": [
            "Shayan Mohajer Hamidi",
            "En-Hui Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20045",
        "abstract": "Inverse problems exist in many disciplines of science and engineering. In computer vision, for example, tasks such as inpainting, deblurring, and super resolution can be effectively modeled as inverse problems. Recently, denoising diffusion probabilistic models (DDPMs) are shown to provide a promising solution to noisy linear inverse problems without the need for additional task specific training. Specifically, with the prior provided by DDPMs, one can sample from the posterior by approximating the likelihood. In the literature, approximations of the likelihood are often based on the mean of conditional densities of the reverse process, which can be obtained using Tweedie formula. To obtain a better approximation to the likelihood, in this paper we first derive a closed form formula for the covariance of the reverse process. Then, we propose a method based on finite difference method to approximate this covariance such that it can be readily obtained from the existing pretrained DDPMs, thereby not increasing the complexity compared to existing approaches. Finally, based on the mean and approximated covariance of the reverse process, we present a new approximation to the likelihood. We refer to this method as covariance-aware diffusion posterior sampling (CA-DPS). Experimental results show that CA-DPS significantly improves reconstruction performance without requiring hyperparameter tuning. The code for the paper is put in the supplementary materials.",
        "tags": [
            "Deblurring",
            "Diffusion",
            "Inpainting",
            "Super Resolution"
        ]
    },
    {
        "id": "32",
        "title": "GSplatLoc: Ultra-Precise Camera Localization via 3D Gaussian Splatting",
        "author": [
            "Atticus J. Zeller"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20056",
        "abstract": "We present GSplatLoc, a camera localization method that leverages the differentiable rendering capabilities of 3D Gaussian splatting for ultra-precise pose estimation. By formulating pose estimation as a gradient-based optimization problem that minimizes discrepancies between rendered depth maps from a pre-existing 3D Gaussian scene and observed depth images, GSplatLoc achieves translational errors within 0.01 cm and near-zero rotational errors on the Replica dataset - significantly outperforming existing methods. Evaluations on the Replica and TUM RGB-D datasets demonstrate the method's robustness in challenging indoor environments with complex camera motions. GSplatLoc sets a new benchmark for localization in dense mapping, with important implications for applications requiring accurate real-time localization, such as robotics and augmented reality.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Pose Estimation",
            "Robotics"
        ]
    },
    {
        "id": "33",
        "title": "\"My life is miserable, have to sign 500 autographs everyday\": Exposing Humblebragging, the Brags in Disguise",
        "author": [
            "Sharath Naganna",
            "Saprativa Bhattacharjee",
            "Pushpak Bhattacharyya",
            "Biplab Banerjee"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20057",
        "abstract": "Humblebragging is a phenomenon where individuals present self-promotional statements under the guise of modesty or complaints. For example, a statement like, \"Ugh, I can't believe I got promoted to lead the entire team. So stressful!\", subtly highlights an achievement while pretending to be complaining. Detecting humblebragging is important for machines to better understand the nuances of human language, especially in tasks like sentiment analysis and intent recognition. However, this topic has not yet been studied in computational linguistics. For the first time, we introduce the task of automatically detecting humblebragging in text. We formalize the task by proposing a 4-tuple definition of humblebragging and evaluate machine learning, deep learning, and large language models (LLMs) on this task, comparing their performance with humans. We also create and release a dataset called HB24, containing 3,340 humblebrags generated using GPT-4o. Our experiments show that detecting humblebragging is non-trivial, even for humans. Our best model achieves an F1-score of 0.88. This work lays the foundation for further exploration of this nuanced linguistic phenomenon and its integration into broader natural language understanding systems.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "34",
        "title": "Comparative Analysis of Listwise Reranking with Large Language Models in Limited-Resource Language Contexts",
        "author": [
            "Yanxin Shen",
            "Lun Wang",
            "Chuanqi Shi",
            "Shaoshuai Du",
            "Yiyi Tao",
            "Yixian Shen",
            "Hang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20061",
        "abstract": "Large Language Models (LLMs) have demonstrated significant effectiveness across various NLP tasks, including text ranking. This study assesses the performance of large language models (LLMs) in listwise reranking for limited-resource African languages. We compare proprietary models RankGPT3.5, Rank4o-mini, RankGPTo1-mini and RankClaude-sonnet in cross-lingual contexts. Results indicate that these LLMs significantly outperform traditional baseline methods such as BM25-DT in most evaluation metrics, particularly in nDCG@10 and MRR@100. These findings highlight the potential of LLMs in enhancing reranking tasks for low-resource languages and offer insights into cost-effective solutions.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "35",
        "title": "MADiff: Text-Guided Fashion Image Editing with Mask Prediction and Attention-Enhanced Diffusion",
        "author": [
            "Zechao Zhan",
            "Dehong Gao",
            "Jinxia Zhang",
            "Jiale Huang",
            "Yang Hu",
            "Xin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20062",
        "abstract": "Text-guided image editing model has achieved great success in general domain. However, directly applying these models to the fashion domain may encounter two issues: (1) Inaccurate localization of editing region; (2) Weak editing magnitude. To address these issues, the MADiff model is proposed. Specifically, to more accurately identify editing region, the MaskNet is proposed, in which the foreground region, densepose and mask prompts from large language model are fed into a lightweight UNet to predict the mask for editing region. To strengthen the editing magnitude, the Attention-Enhanced Diffusion Model is proposed, where the noise map, attention map, and the mask from MaskNet are fed into the proposed Attention Processor to produce a refined noise map. By integrating the refined noise map into the diffusion model, the edited image can better align with the target prompt. Given the absence of benchmarks in fashion image editing, we constructed a dataset named Fashion-E, comprising 28390 image-text pairs in the training set, and 2639 image-text pairs for four types of fashion tasks in the evaluation set. Extensive experiments on Fashion-E demonstrate that our proposed method can accurately predict the mask of editing region and significantly enhance editing magnitude in fashion image editing compared to the state-of-the-art methods.",
        "tags": [
            "Diffusion",
            "Image Editing"
        ]
    },
    {
        "id": "36",
        "title": "VELoRA: A Low-Rank Adaptation Approach for Efficient RGB-Event based Recognition",
        "author": [
            "Lan Chen",
            "Haoxiang Yang",
            "Pengpeng Shao",
            "Haoyu Song",
            "Xiao Wang",
            "Zhicheng Zhao",
            "Yaowei Wang",
            "Yonghong Tian"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20064",
        "abstract": "Pattern recognition leveraging both RGB and Event cameras can significantly enhance performance by deploying deep neural networks that utilize a fine-tuning strategy. Inspired by the successful application of large models, the introduction of such large models can also be considered to further enhance the performance of multi-modal tasks. However, fully fine-tuning these models leads to inefficiency and lightweight fine-tuning methods such as LoRA and Adapter have been proposed to achieve a better balance between efficiency and performance. To our knowledge, there is currently no work that has conducted parameter-efficient fine-tuning (PEFT) for RGB-Event recognition based on pre-trained foundation models. To address this issue, this paper proposes a novel PEFT strategy to adapt the pre-trained foundation vision models for the RGB-Event-based classification. Specifically, given the RGB frames and event streams, we extract the RGB and event features based on the vision foundation model ViT with a modality-specific LoRA tuning strategy. The frame difference of the dual modalities is also considered to capture the motion cues via the frame difference backbone network. These features are concatenated and fed into high-level Transformer layers for efficient multi-modal feature learning via modality-shared LoRA tuning. Finally, we concatenate these features and feed them into a classification head to achieve efficient fine-tuning. The source code and pre-trained models will be released on \\url{https://github.com/Event-AHU/VELoRA}.",
        "tags": [
            "LoRA",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "37",
        "title": "MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration",
        "author": [
            "Boyun Li",
            "Haiyu Zhao",
            "Wenxin Wang",
            "Peng Hu",
            "Yuanbiao Gou",
            "Xi Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20066",
        "abstract": "Recent advancements in Mamba have shown promising results in image restoration. These methods typically flatten 2D images into multiple distinct 1D sequences along rows and columns, process each sequence independently using selective scan operation, and recombine them to form the outputs. However, such a paradigm overlooks two vital aspects: i) the local relationships and spatial continuity inherent in natural images, and ii) the discrepancies among sequences unfolded through totally different ways. To overcome the drawbacks, we explore two problems in Mamba-based restoration methods: i) how to design a scanning strategy preserving both locality and continuity while facilitating restoration, and ii) how to aggregate the distinct sequences unfolded in totally different ways. To address these problems, we propose a novel Mamba-based Image Restoration model (MaIR), which consists of Nested S-shaped Scanning strategy (NSS) and Sequence Shuffle Attention block (SSA). Specifically, NSS preserves locality and continuity of the input images through the stripe-based scanning region and the S-shaped scanning path, respectively. SSA aggregates sequences through calculating attention weights within the corresponding channels of different sequences. Thanks to NSS and SSA, MaIR surpasses 40 baselines across 14 challenging datasets, achieving state-of-the-art performance on the tasks of image super-resolution, denoising, deblurring and dehazing. Our codes will be available after acceptance.",
        "tags": [
            "Deblurring",
            "Mamba",
            "Super Resolution"
        ]
    },
    {
        "id": "38",
        "title": "MambaVO: Deep Visual Odometry Based on Sequential Matching Refinement and Training Smoothing",
        "author": [
            "Shuo Wang",
            "Wanting Li",
            "Yongcai Wang",
            "Zhaoxin Fan",
            "Zhe Huang",
            "Xudong Cai",
            "Jian Zhao",
            "Deying Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20082",
        "abstract": "Deep visual odometry has demonstrated great advancements by learning-to-optimize technology. This approach heavily relies on the visual matching across frames. However, ambiguous matching in challenging scenarios leads to significant errors in geometric modeling and bundle adjustment optimization, which undermines the accuracy and robustness of pose estimation. To address this challenge, this paper proposes MambaVO, which conducts robust initialization, Mamba-based sequential matching refinement, and smoothed training to enhance the matching quality and improve the pose estimation in deep visual odometry. Specifically, when a new frame is received, it is matched with the closest keyframe in the maintained Point-Frame Graph (PFG) via the semi-dense based Geometric Initialization Module (GIM). Then the initialized PFG is processed by a proposed Geometric Mamba Module (GMM), which exploits the matching features to refine the overall inter-frame pixel-to-pixel matching. The refined PFG is finally processed by deep BA to optimize the poses and the map. To deal with the gradient variance, a Trending-Aware Penalty (TAP) is proposed to smooth training by balancing the pose loss and the matching loss to enhance convergence and stability. A loop closure module is finally applied to enable MambaVO++. On public benchmarks, MambaVO and MambaVO++ demonstrate SOTA accuracy performance, while ensuring real-time running performance with low GPU memory requirement. Codes will be publicly available.",
        "tags": [
            "Mamba",
            "Pose Estimation"
        ]
    },
    {
        "id": "39",
        "title": "SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis",
        "author": [
            "Wenkun He",
            "Yun Liu",
            "Ruitao Liu",
            "Li Yi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20104",
        "abstract": "Synthesizing realistic human-object interaction motions is a critical problem in VR/AR and human animation. Unlike the commonly studied scenarios involving a single human or hand interacting with one object, we address a more generic multi-body setting with arbitrary numbers of humans, hands, and objects. This complexity introduces significant challenges in synchronizing motions due to the high correlations and mutual influences among bodies. To address these challenges, we introduce SyncDiff, a novel method for multi-body interaction synthesis using a synchronized motion diffusion strategy. SyncDiff employs a single diffusion model to capture the joint distribution of multi-body motions. To enhance motion fidelity, we propose a frequency-domain motion decomposition scheme. Additionally, we introduce a new set of alignment scores to emphasize the synchronization of different body motions. SyncDiff jointly optimizes both data sample likelihood and alignment likelihood through an explicit synchronization strategy. Extensive experiments across four datasets with various multi-body configurations demonstrate the superiority of SyncDiff over existing state-of-the-art motion synthesis methods.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "40",
        "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming",
        "author": [
            "Jiedong Zhuang",
            "Lu Lu",
            "Ming Dai",
            "Rui Hu",
            "Jian Chen",
            "Qiang Liu",
            "Haoji Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20105",
        "abstract": "Multimodal large language models (MLLMs) enhance their perceptual capabilities by integrating visual and textual information. However, processing the massive number of visual tokens incurs a significant computational cost. Existing analysis of the MLLM attention mechanisms remains shallow, leading to coarse-grain token pruning strategies that fail to effectively balance speed and accuracy. In this paper, we conduct a comprehensive investigation of MLLM attention mechanisms with LLaVA. We find that numerous visual tokens and partial attention computations are redundant during the decoding process. Based on this insight, we propose Spatial-Temporal Visual Token Trimming ($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without retraining. $\\textbf{ST}^{3}$ consists of two primary components: 1) Progressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive visual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}), which dynamically reduces the number of visual tokens in each layer as the generated tokens grow. Together, these techniques deliver around $\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache memory compared to the original LLaVA, while maintaining consistent performance across various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly integrated into existing pre-trained MLLMs, providing a plug-and-play solution for efficient inference.",
        "tags": [
            "LLaVA",
            "Large Language Models"
        ]
    },
    {
        "id": "41",
        "title": "Cross-Modal Mapping: Eliminating the Modality Gap for Few-Shot Image Classification",
        "author": [
            "Xi Yang",
            "Pai Peng",
            "Wulin Xie",
            "Xiaohuan Lu",
            "Jie Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20110",
        "abstract": "In few-shot image classification tasks, methods based on pretrained vision-language models (such as CLIP) have achieved significant progress. Many existing approaches directly utilize visual or textual features as class prototypes, however, these features fail to adequately represent their respective classes. We identify that this limitation arises from the modality gap inherent in pretrained vision-language models, which weakens the connection between the visual and textual modalities. To eliminate this modality gap and enable textual features to fully represent class prototypes, we propose a simple and efficient Cross-Modal Mapping (CMM) method. This method employs a linear transformation to map image features into the textual feature space, ensuring that both modalities are comparable within the same feature space. Nevertheless, the modality gap diminishes the effectiveness of this mapping. To address this, we further introduce a triplet loss to optimize the spatial relationships between image features and class textual features, allowing class textual features to naturally serve as class prototypes for image features. Experimental results on 11 benchmark demonstrate an average improvement of approximately 3.5% compared to conventional methods and exhibit competitive performance on 4 distribution shift benchmarks.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "42",
        "title": "M-MAD: Multidimensional Multi-Agent Debate Framework for Fine-grained Machine Translation Evaluation",
        "author": [
            "Zhaopeng Feng",
            "Jiayuan Su",
            "Jiamei Zheng",
            "Jiahan Ren",
            "Yan Zhang",
            "Jian Wu",
            "Hongwei Wang",
            "Zuozhu Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20127",
        "abstract": "Recent advancements in large language models (LLMs) have given rise to the LLM-as-a-judge paradigm, showcasing their potential to deliver human-like judgments. However, in the field of machine translation (MT) evaluation, current LLM-as-a-judge methods fall short of learned automatic metrics. In this paper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic LLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our findings demonstrate that M-MAD achieves significant advancements by (1) decoupling heuristic MQM criteria into distinct evaluation dimensions for fine-grained assessments; (2) employing multi-agent debates to harness the collaborative reasoning capabilities of LLMs; (3) synthesizing dimension-specific results into a final evaluation judgment to ensure robust and reliable outcomes. Comprehensive experiments show that M-MAD not only outperforms all existing LLM-as-a-judge methods but also competes with state-of-the-art reference-based automatic metrics, even when powered by a suboptimal model like GPT-4o mini. Detailed ablations and analysis highlight the superiority of our framework design, offering a fresh perspective for LLM-as-a-judge paradigm. Our code and data are publicly available at https://github.com/SU-JIAYUAN/M-MAD.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "43",
        "title": "ASE: Practical Acoustic Speed Estimation Beyond Doppler via Sound Diffusion Field",
        "author": [
            "Sheng Lyu",
            "Chenshu Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20142",
        "abstract": "Passive human speed estimation plays a critical role in acoustic sensing. Despite extensive study, existing systems, however, suffer from various limitations: First, previous acoustic speed estimation exploits Doppler Frequency Shifts (DFS) created by moving targets and relies on microphone arrays, making them only capable of sensing the radial speed within a constrained distance. Second, the channel measurement rate proves inadequate to estimate high moving speeds. To overcome these issues, we present ASE, an accurate and robust Acoustic Speed Estimation system on a single commodity microphone. We model the sound propagation from a unique perspective of the acoustic diffusion field, and infer the speed from the acoustic spatial distribution, a completely different way of thinking about speed estimation beyond prior DFS-based approaches. We then propose a novel Orthogonal Time-Delayed Multiplexing (OTDM) scheme for acoustic channel estimation at a high rate that was previously infeasible, making it possible to estimate high speeds. We further develop novel techniques for motion detection and signal enhancement to deliver a robust and practical system. We implement and evaluate ASE through extensive real-world experiments. Our results show that ASE reliably tracks walking speed, independently of target location and direction, with a mean error of 0.13 m/s, a reduction of 2.5x from DFS, and a detection rate of 97.4% for large coverage, e.g., free walking in a 4m $\\times$ 4m room. We believe ASE pushes acoustic speed estimation beyond the conventional DFS-based paradigm and will inspire exciting research in acoustic sensing.",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "44",
        "title": "DEGSTalk: Decomposed Per-Embedding Gaussian Fields for Hair-Preserving Talking Face Synthesis",
        "author": [
            "Kaijun Deng",
            "Dezhi Zheng",
            "Jindong Xie",
            "Jinbao Wang",
            "Weicheng Xie",
            "Linlin Shen",
            "Siyang Song"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20148",
        "abstract": "Accurately synthesizing talking face videos and capturing fine facial features for individuals with long hair presents a significant challenge. To tackle these challenges in existing methods, we propose a decomposed per-embedding Gaussian fields (DEGSTalk), a 3D Gaussian Splatting (3DGS)-based talking face synthesis method for generating realistic talking faces with long hairs. Our DEGSTalk employs Deformable Pre-Embedding Gaussian Fields, which dynamically adjust pre-embedding Gaussian primitives using implicit expression coefficients. This enables precise capture of dynamic facial regions and subtle expressions. Additionally, we propose a Dynamic Hair-Preserving Portrait Rendering technique to enhance the realism of long hair motions in the synthesized videos. Results show that DEGSTalk achieves improved realism and synthesis quality compared to existing approaches, particularly in handling complex facial dynamics and hair preservation. Our code will be publicly available at https://github.com/CVI-SZU/DEGSTalk.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Talking Face"
        ]
    },
    {
        "id": "45",
        "title": "Defending Against Network Attacks for Secure AI Agent Migration in Vehicular Metaverses",
        "author": [
            "Xinru Wen",
            "Jinbo Wen",
            "Ming Xiao",
            "Jiawen Kang",
            "Tao Zhang",
            "Xiaohuan Li",
            "Chuanxi Chen",
            "Dusit Niyato"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20154",
        "abstract": "Vehicular metaverses, blending traditional vehicular networks with metaverse technology, are expected to revolutionize fields such as autonomous driving. As virtual intelligent assistants in vehicular metaverses, Artificial Intelligence (AI) agents powered by large language models can create immersive 3D virtual spaces for passengers to enjoy on-broad vehicular applications and services. To provide users with seamless and engaging virtual interactions, resource-limited vehicles offload AI agents to RoadSide Units (RSUs) with adequate communication and computational capabilities. Due to the mobility of vehicles and the limited coverage of RSUs, AI agents need to migrate from one RSU to another RSU. However, potential network attacks pose significant challenges to ensuring reliable and efficient AI agent migration. In this paper, we first explore specific network attacks including traffic-based attacks (i.e., DDoS attacks) and infrastructure-based attacks (i.e., malicious RSU attacks). Then, we model the AI agent migration process as a Partially Observable Markov Decision Process (POMDP) and apply multi-agent proximal policy optimization algorithms to mitigate DDoS attacks. In addition, we propose a trust assessment mechanism to counter malicious RSU attacks. Numerical results validate that the proposed solutions effectively defend against these network attacks and reduce the total latency of AI agent migration by approximately 43.3%.",
        "tags": [
            "3D",
            "Large Language Models"
        ]
    },
    {
        "id": "46",
        "title": "Distilled Transformers with Locally Enhanced Global Representations for Face Forgery Detection",
        "author": [
            "Yaning Zhang",
            "Qiufu Li",
            "Zitong Yu",
            "Linlin Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20156",
        "abstract": "Face forgery detection (FFD) is devoted to detecting the authenticity of face images. Although current CNN-based works achieve outstanding performance in FFD, they are susceptible to capturing local forgery patterns generated by various manipulation methods. Though transformer-based detectors exhibit improvements in modeling global dependencies, they are not good at exploring local forgery artifacts. Hybrid transformer-based networks are designed to capture local and global manipulated traces, but they tend to suffer from the attention collapse issue as the transformer block goes deeper. Besides, soft labels are rarely available. In this paper, we propose a distilled transformer network (DTN) to capture both rich local and global forgery traces and learn general and common representations for different forgery faces. Specifically, we design a mixture of expert (MoE) module to mine various robust forgery embeddings. Moreover, a locally-enhanced vision transformer (LEVT) module is proposed to learn locally-enhanced global representations. We design a lightweight multi-attention scaling (MAS) module to avoid attention collapse, which can be plugged and played in any transformer-based models with only a slight increase in computational costs. In addition, we propose a deepfake self-distillation (DSD) scheme to provide the model with abundant soft label information. Extensive experiments show that the proposed method surpasses the state of the arts on five deepfake datasets.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "47",
        "title": "Multi-Modality Driven LoRA for Adverse Condition Depth Estimation",
        "author": [
            "Guanglei Yang",
            "Rui Tian",
            "Yongqiang Zhang",
            "Zhun Zhong",
            "Yongqiang Li",
            "Wangmeng Zuo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20162",
        "abstract": "The autonomous driving community is increasingly focused on addressing corner case problems, particularly those related to ensuring driving safety under adverse conditions (e.g., nighttime, fog, rain). To this end, the task of Adverse Condition Depth Estimation (ACDE) has gained significant attention. Previous approaches in ACDE have primarily relied on generative models, which necessitate additional target images to convert the sunny condition into adverse weather, or learnable parameters for feature augmentation to adapt domain gaps, resulting in increased model complexity and tuning efforts. Furthermore, unlike CLIP-based methods where textual and visual features have been pre-aligned, depth estimation models lack sufficient alignment between multimodal features, hindering coherent understanding under adverse conditions. To address these limitations, we propose Multi-Modality Driven LoRA (MMD-LoRA), which leverages low-rank adaptation matrices for efficient fine-tuning from source-domain to target-domain. It consists of two core components: Prompt Driven Domain Alignment (PDDA) and Visual-Text Consistent Contrastive Learning(VTCCL). During PDDA, the image encoder with MMD-LoRA generates target-domain visual representations, supervised by alignment loss that the source-target difference between language and image should be equal. Meanwhile, VTCCL bridges the gap between textual features from CLIP and visual features from diffusion model, pushing apart different weather representations (vision and text) and bringing together similar ones. Through extensive experiments, the proposed method achieves state-of-the-art performance on the nuScenes and Oxford RobotCar datasets, underscoring robustness and efficiency in adapting to varied adverse environments.",
        "tags": [
            "CLIP",
            "Depth Estimation",
            "Diffusion",
            "LoRA"
        ]
    },
    {
        "id": "48",
        "title": "StyleAutoEncoder for manipulating image attributes using pre-trained StyleGAN",
        "author": [
            "Andrzej Bedychaj",
            "Jacek Tabor",
            "Marek Śmieja"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20164",
        "abstract": "Deep conditional generative models are excellent tools for creating high-quality images and editing their attributes. However, training modern generative models from scratch is very expensive and requires large computational resources. In this paper, we introduce StyleAutoEncoder (StyleAE), a lightweight AutoEncoder module, which works as a plugin for pre-trained generative models and allows for manipulating the requested attributes of images. The proposed method offers a cost-effective solution for training deep generative models with limited computational resources, making it a promising technique for a wide range of applications. We evaluate StyleAutoEncoder by combining it with StyleGAN, which is currently one of the top generative models. Our experiments demonstrate that StyleAutoEncoder is at least as effective in manipulating image attributes as the state-of-the-art algorithms based on invertible normalizing flows. However, it is simpler, faster, and gives more freedom in designing neural",
        "tags": [
            "Normalizing Flows",
            "StyleGAN"
        ]
    },
    {
        "id": "49",
        "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
        "author": [
            "Hyucksung Kwon",
            "Kyungmo Koo",
            "Janghyeon Kim",
            "Woongkyu Lee",
            "Minjae Lee",
            "Hyungdeok Lee",
            "Yousub Jung",
            "Jaehan Park",
            "Yosub Song",
            "Byeongsu Yang",
            "Haerang Choi",
            "Guhyun Kim",
            "Jongsoon Won",
            "Woojae Shin",
            "Changhyun Kim",
            "Gyeongcheol Shin",
            "Yongkee Kwon",
            "Ilkon Kim",
            "Euicheol Lim",
            "John Kim",
            "Jungwook Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20166",
        "abstract": "The expansion of large language models (LLMs) with hundreds of billions of parameters presents significant challenges to computational resources, particularly data movement and memory bandwidth. Long-context LLMs, which process sequences of tens of thousands of tokens, further increase the demand on the memory system as the complexity in attention layers and key-value cache sizes is proportional to the context length. Processing-in-Memory (PIM) maximizes memory bandwidth by moving compute to the data and can address the memory bandwidth challenges; however, PIM is not necessarily scalable to accelerate long-context LLM because of limited per-module memory capacity and the inflexibility of fixed-functional unit PIM architecture and static memory management. In this work, we propose LoL-PIM which is a multi-node PIM architecture that accelerates long context LLM through hardware-software co-design. In particular, we propose how pipeline parallelism can be exploited across a multi-PIM module while a direct PIM access (DPA) controller (or DMA for PIM) is proposed that enables dynamic PIM memory management and results in efficient PIM utilization across a diverse range of context length. We developed an MLIR-based compiler for LoL-PIM extending a commercial PIM-based compiler where the software modifications were implemented and evaluated, while the hardware changes were modeled in the simulator. Our evaluations demonstrate that LoL-PIM significantly improves throughput and reduces latency for long-context LLM inference, outperforming both multi-GPU and GPU-PIM systems (up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient deployment of LLMs in real-world applications.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "50",
        "title": "Real-time Calibration Model for Low-cost Sensor in Fine-grained Time series",
        "author": [
            "Seokho Ahn",
            "Hyungjin Kim",
            "Sungbok Shin",
            "Young-Duk Seo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20170",
        "abstract": "Precise measurements from sensors are crucial, but data is usually collected from low-cost, low-tech systems, which are often inaccurate. Thus, they require further calibrations. To that end, we first identify three requirements for effective calibration under practical low-tech sensor conditions. Based on the requirements, we develop a model called TESLA, Transformer for effective sensor calibration utilizing logarithmic-binned attention. TESLA uses a high-performance deep learning model, Transformers, to calibrate and capture non-linear components. At its core, it employs logarithmic binning to minimize attention complexity. TESLA achieves consistent real-time calibration, even with longer sequences and finer-grained time series in hardware-constrained systems. Experiments show that TESLA outperforms existing novel deep learning and newly crafted linear models in accuracy, calibration speed, and energy efficiency.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "51",
        "title": "Pushing the Envelope of Low-Bit LLM via Dynamic Error Compensation",
        "author": [
            "Yeonhong Park",
            "Jake Hyun",
            "Hojoon Kim",
            "Jae W. Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20185",
        "abstract": "Quantization of Large Language Models (LLMs) has recently gained popularity, particularly for on-device settings with limited hardware resources. While efficient, quantization inevitably degrades model quality, especially in aggressive low-bit settings such as 3-bit and 4-bit precision. In this paper, we propose QDEC, an inference scheme that improves the quality of low-bit LLMs while preserving the key benefits of quantization: GPU memory savings and inference latency reduction. QDEC stores the residual matrix -- the difference between full-precision and quantized weights -- in CPU, and dynamically fetches the residuals for only a small portion of the weights. This portion corresponds to the salient channels, marked by activation outliers, with the fetched residuals helping to correct quantization errors in these channels. Salient channels are identified dynamically at each decoding step by analyzing the input activations -- this allows for the adaptation to the dynamic nature of activation distribution, and thus maximizes the effectiveness of error compensation. We demonstrate the effectiveness of QDEC by augmenting state-of-the-art quantization methods. For example, QDEC reduces the perplexity of a 3-bit Llama-3-8B-Instruct model from 10.15 to 9.12 -- outperforming its 3.5-bit counterpart -- while adding less than 0.0003\\% to GPU memory usage and incurring only a 1.7\\% inference slowdown on NVIDIA RTX 4050 Mobile GPU. The code will be publicly available soon.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "52",
        "title": "Towards Visual Grounding: A Survey",
        "author": [
            "Linhui Xiao",
            "Xiaoshan Yang",
            "Xiangyuan Lan",
            "Yaowei Wang",
            "Changsheng Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20206",
        "abstract": "Visual Grounding is also known as Referring Expression Comprehension and Phrase Grounding. It involves localizing a natural number of specific regions within an image based on a given textual description. The objective of this task is to emulate the prevalent referential relationships in social conversations, equipping machines with human-like multimodal comprehension capabilities. Consequently, it has extensive applications in various domains. However, since 2021, visual grounding has witnessed significant advancements, with emerging new concepts such as grounded pre-training, grounding multimodal LLMs, generalized visual grounding, and giga-pixel grounding, which have brought numerous new challenges. In this survey, we initially examine the developmental history of visual grounding and provide an overview of essential background knowledge. We systematically track and summarize the advancements and meticulously organize the various settings in visual grounding, thereby establishing precise definitions of these settings to standardize future research and ensure a fair comparison. Additionally, we delve into several advanced topics and highlight numerous applications of visual grounding. Finally, we outline the challenges confronting visual grounding and propose valuable directions for future research, which may serve as inspiration for subsequent researchers. By extracting common technical details, this survey encompasses the representative works in each subtopic over the past decade. To the best, this paper presents the most comprehensive overview currently available in the field of grounding. This survey is designed to be suitable for both beginners and experienced researchers, serving as an invaluable resource for understanding key concepts and tracking the latest research developments. We keep tracing related works at https://github.com/linhuixiao/Awesome-Visual-Grounding.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "53",
        "title": "Building a Rich Dataset to Empower the Persian Question Answering Systems",
        "author": [
            "Mohsen Yazdinejad",
            "Marjan Kaedi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20212",
        "abstract": "Question answering systems provide short, precise, and specific answers to questions. So far, many robust question answering systems have been developed for English, while some languages with fewer resources, like Persian, have few numbers of standard dataset. In this study, a comprehensive open-domain dataset is presented for Persian. This dataset is called NextQuAD and has 7,515 contexts, including 23,918 questions and answers. Then, a BERT-based question answering model has been applied to this dataset using two pre-trained language models, including ParsBERT and XLM-RoBERTa. The results of these two models have been ensembled using mean logits. Evaluation on the development set shows 0.95 Exact Match (EM) and 0.97 Fl_score. Also, to compare the NextQuAD with other Persian datasets, our trained model on the NextQuAD, is evaluated on two other datasets named PersianQA and ParSQuAD. Comparisons show that the proposed model increased EM by 0.39 and 0.14 respectively in PersianQA and ParSQuAD-manual, while a slight EM decline of 0.007 happened in ParSQuAD-automatic.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "54",
        "title": "IMSSA: Deploying modern state-space models on memristive in-memory compute hardware",
        "author": [
            "Sebastian Siegel",
            "Ming-Jay Yang",
            "John-Paul Strachan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20215",
        "abstract": "Processing long temporal sequences is a key challenge in deep learning. In recent years, Transformers have become state-of-the-art for this task, but suffer from excessive memory requirements due to the need to explicitly store the sequences. To address this issue, structured state-space sequential (S4) models recently emerged, offering a fixed memory state while still enabling the processing of very long sequence contexts. The recurrent linear update of the state in these models makes them highly efficient on modern graphics processing units (GPU) by unrolling the recurrence into a convolution. However, this approach demands significant memory and massively parallel computation, which is only available on the latest GPUs. In this work, we aim to bring the power of S4 models to edge hardware by significantly reducing the size and computational demand of an S4D model through quantization-aware training, even achieving ternary weights for a simple real-world task. To this end, we extend conventional quantization-aware training to tailor it for analog in-memory compute hardware. We then demonstrate the deployment of recurrent S4D kernels on memrisitve crossbar arrays, enabling their computation in an in-memory compute fashion. To our knowledge, this is the first implementation of S4 kernels on in-memory compute hardware.",
        "tags": [
            "State Space Models"
        ]
    },
    {
        "id": "55",
        "title": "YAD: Leveraging T5 for Improved Automatic Diacritization of Yor\\`ub\\'a Text",
        "author": [
            "Akindele Michael Olawole",
            "Jesujoba O. Alabi",
            "Aderonke Busayo Sakpere",
            "David I. Adelani"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20218",
        "abstract": "In this work, we present Yorùbá automatic diacritization (YAD) benchmark dataset for evaluating Yorùbá diacritization systems. In addition, we pre-train text-to-text transformer, T5 model for Yorùbá and showed that this model outperform several multilingually trained T5 models. Lastly, we showed that more data and larger models are better at diacritization for Yorùbá",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "56",
        "title": "Embodiment-Agnostic Navigation Policy Trained with Visual Demonstrations",
        "author": [
            "Nimrod Curtis",
            "Osher Azulay",
            "Avishai Sintov"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20226",
        "abstract": "Learning to navigate in unstructured environments is a challenging task for robots. While reinforcement learning can be effective, it often requires extensive data collection and can pose risk. Learning from expert demonstrations, on the other hand, offers a more efficient approach. However, many existing methods rely on specific robot embodiments, pre-specified target images and require large datasets. We propose the Visual Demonstration-based Embodiment-agnostic Navigation (ViDEN) framework, a novel framework that leverages visual demonstrations to train embodiment-agnostic navigation policies. ViDEN utilizes depth images to reduce input dimensionality and relies on relative target positions, making it more adaptable to diverse environments. By training a diffusion-based policy on task-centric and embodiment-agnostic demonstrations, ViDEN can generate collision-free and adaptive trajectories in real-time. Our experiments on human reaching and tracking demonstrate that ViDEN outperforms existing methods, requiring a small amount of data and achieving superior performance in various indoor and outdoor navigation scenarios. Project website: https://nimicurtis.github.io/ViDEN/.",
        "tags": [
            "Diffusion",
            "Robot"
        ]
    },
    {
        "id": "57",
        "title": "LLM Reasoning Engine: Specialized Training for Enhanced Mathematical Reasoning",
        "author": [
            "Shuguang Chen",
            "Guang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20227",
        "abstract": "Large Language Models (LLMs) have shown remarkable performance in various natural language processing tasks but face challenges in mathematical reasoning, where complex problem-solving requires both linguistic understanding and mathematical reasoning skills. Existing approaches to address this challenge often rely on ensemble methods and suffer from the problem of data scarcity in target domains. In this work, we present a novel method to enhance LLMs' capabilities in mathematical reasoning tasks. Motivated by the need to bridge this gap, our approach incorporates a question paraphrase strategy, which aims at diversifying the linguistic forms of mathematical questions to improve generalization. Additionally, specialized training objectives are employed to guide the model's learning process, focusing on enhancing its understanding of mathematical concepts and reasoning processes. We conduct experiments on four datasets using different LLMs, and demonstrate the effectiveness of our approach in improving LLMs' performance on mathematical reasoning tasks. Our findings underscore the significance of our methodology in the advancement of large language models and its potential implications for real-world applications that require mathematical reasoning abilities.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "58",
        "title": "Leveraging Large Language Models for Enhancing Autonomous Vehicle Perception",
        "author": [
            "Athanasios Karagounis"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20230",
        "abstract": "Autonomous vehicles (AVs) rely on sophisticated perception systems to interpret their surroundings, a cornerstone for safe navigation and decision-making. The integration of Large Language Models (LLMs) into AV perception frameworks offers an innovative approach to address challenges in dynamic environments, sensor fusion, and contextual reasoning. This paper presents a novel framework for incorporating LLMs into AV perception, enabling advanced contextual understanding, seamless sensor integration, and enhanced decision support. Experimental results demonstrate that LLMs significantly improve the accuracy and reliability of AV perception systems, paving the way for safer and more intelligent autonomous driving technologies. By expanding the scope of perception beyond traditional methods, LLMs contribute to creating a more adaptive and human-centric driving ecosystem, making autonomous vehicles more reliable and transparent in their operations. These advancements redefine the relationship between human drivers and autonomous systems, fostering trust through enhanced understanding and personalized decision-making. Furthermore, by integrating memory modules and adaptive learning mechanisms, LLMs introduce continuous improvement in AV perception, enabling vehicles to evolve with time and adapt to changing environments and user preferences.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "59",
        "title": "ComparisonQA: Evaluating Factuality Robustness of LLMs Through Knowledge Frequency Control and Uncertainty",
        "author": [
            "Qing Zong",
            "Zhaowei Wang",
            "Tianshi Zheng",
            "Xiyu Ren",
            "Yangqiu Song"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20251",
        "abstract": "The rapid development of LLMs has sparked extensive research into their factual knowledge. Current works claim that LLMs fall short on questions requiring less frequent knowledge. However, their proof is incomplete since they only study the influence of entity frequency, which can not fully represent knowledge frequency. So we introduce ComparisonQA benchmark, containing 283K abstract questions, each instantiated by a pair of high-frequency and low-frequency entities. It ensures a controllable comparison because the difference of knowledge frequency between such a pair is only related to entity frequency. In addition, to avoid possible semantic shortcuts, which is a severe problem of current LLMs study, we design a two-round method for knowledge robustness measurement utilizing both correctness and uncertainty. Experiments reveal that LLMs exhibit particularly low robustness regarding low-frequency knowledge, and GPT-4o is even the worst under this measurement. Besides, we introduce an automatic method to filter out questions with low-quality and shortcuts to form ComparisonQA-Hard. We find that uncertainty effectively identifies such questions while maintaining the data size.",
        "tags": [
            "GPT",
            "LLMs"
        ]
    },
    {
        "id": "60",
        "title": "Scoring with Large Language Models: A Study on Measuring Empathy of Responses in Dialogues",
        "author": [
            "Henry J. Xie",
            "Jinghan Zhang",
            "Xinhao Zhang",
            "Kunpeng Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20264",
        "abstract": "In recent years, Large Language Models (LLMs) have become increasingly more powerful in their ability to complete complex tasks. One such task in which LLMs are often employed is scoring, i.e., assigning a numerical value from a certain scale to a subject. In this paper, we strive to understand how LLMs score, specifically in the context of empathy scoring. We develop a novel and comprehensive framework for investigating how effective LLMs are at measuring and scoring empathy of responses in dialogues, and what methods can be employed to deepen our understanding of LLM scoring. Our strategy is to approximate the performance of state-of-the-art and fine-tuned LLMs with explicit and explainable features. We train classifiers using various features of dialogues including embeddings, the Motivational Interviewing Treatment Integrity (MITI) Code, a set of explicit subfactors of empathy as proposed by LLMs, and a combination of the MITI Code and the explicit subfactors. Our results show that when only using embeddings, it is possible to achieve performance close to that of generic LLMs, and when utilizing the MITI Code and explicit subfactors scored by an LLM, the trained classifiers can closely match the performance of fine-tuned LLMs. We employ feature selection methods to derive the most crucial features in the process of empathy scoring. Our work provides a new perspective toward understanding LLM empathy scoring and helps the LLM community explore the potential of LLM scoring in social science studies.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "61",
        "title": "Transformer-Based Contrastive Meta-Learning For Low-Resource Generalizable Activity Recognition",
        "author": [
            "Junyao Wang",
            "Mohammad Abdullah Al Faruque"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20290",
        "abstract": "Deep learning has been widely adopted for human activity recognition (HAR) while generalizing a trained model across diverse users and scenarios remains challenging due to distribution shifts. The inherent low-resource challenge in HAR, i.e., collecting and labeling adequate human-involved data can be prohibitively costly, further raising the difficulty of tackling DS. We propose TACO, a novel transformer-based contrastive meta-learning approach for generalizable HAR. TACO addresses DS by synthesizing virtual target domains in training with explicit consideration of model generalizability. Additionally, we extract expressive feature with the attention mechanism of Transformer and incorporate the supervised contrastive loss function within our meta-optimization to enhance representation learning. Our evaluation demonstrates that TACO achieves notably better performance across various low-resource DS scenarios.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "62",
        "title": "An analytic theory of creativity in convolutional diffusion models",
        "author": [
            "Mason Kamb",
            "Surya Ganguli"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20292",
        "abstract": "We obtain the first analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-based diffusion models can generate highly creative images that lie far from their training data. But optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in a fully analytic, completely mechanistically interpretable, equivariant local score (ELS) machine that, (3) without any training can quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median $r^2$ of $0.90, 0.91, 0.94$ on CIFAR10, FashionMNIST, and MNIST). Our ELS machine reveals a locally consistent patch mosaic model of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches in different image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median $r^2 \\sim 0.75$ on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics.",
        "tags": [
            "Diffusion",
            "Score Matching"
        ]
    },
    {
        "id": "63",
        "title": "FaGeL: Fabric LLMs Agent empowered Embodied Intelligence Evolution with Autonomous Human-Machine Collaboration",
        "author": [
            "Jia Liu",
            "Min Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20297",
        "abstract": "Recent advancements in Large Language Models (LLMs) have enhanced the reasoning capabilities of embodied agents, driving progress toward AGI-powered robotics. While LLMs have been applied to tasks like semantic reasoning and task generalization, their potential in open physical space exploration remains underexplored. This paper introduces FaGeL (Fabric aGent empowered by embodied intelligence with LLMs), an embodied agent integrating smart fabric technology for seamless, non-intrusive human-agent interaction. FaGeL autonomously generates tasks using multimodal data from wearable and ambient sensors, refining its behavior based on implicit human feedback in generated text, without explicit ratings or preferences. We also introduce a token-level saliency map to visualize LLM fine-tuning, enhancing the interpretability of token-level alignment. The system leverages dual feedback mechanisms to improve token-level alignment and addresses challenges in non-intrusive human-machine interaction and cognition evolution. Our contributions include FaGeL's development, the DualCUT algorithm for AI alignment, and experimental validation in cooperative tasks, demonstrating FaGeL's ability to adapt and evolve autonomously through implicit feedback. In the future, we plan to explore FaGeL's scalability in dynamic environments and its integration with other AI systems to develop AGI agents that adapt seamlessly to diverse human needs.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Robotics"
        ]
    },
    {
        "id": "64",
        "title": "GreenLLM: Disaggregating Large Language Model Serving on Heterogeneous GPUs for Lower Carbon Emissions",
        "author": [
            "Tianyao Shi",
            "Yanran Wu",
            "Sihang Liu",
            "Yi Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20322",
        "abstract": "LLMs have been widely adopted across many real-world applications. However, their widespread use comes with significant environmental costs due to their high computational intensity and resource demands. Specifically, this has driven the development of new generations of high-performing GPUs, exacerbating the problem of electronic waste and accelerating the premature disposal of devices. To address this problem, this paper focuses on reducing the carbon emissions of LLM serving by reusing older, low-performing GPUs. We present GreenLLM, an SLO-aware LLM serving framework designed to minimize carbon emissions by reusing older GPUs. GreenLLM builds on two identified use cases that disaggregate specific computations onto older GPUs, reducing carbon emissions while meeting performance goals. To deepen our understanding of the potential carbon savings from disaggregation, we also provide a theoretical analysis of its relationship with carbon intensity and GPU lifetime. Our evaluations show that GreenLLM reduces carbon emissions by up to 40.6% compared to running standard LLM serving on new GPU only, meeting latency SLOs for over 90% of requests across various applications, latency requirements, carbon intensities, and GPU lifetimes.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "65",
        "title": "Mind the Data Gap: Bridging LLMs to Enterprise Data Integration",
        "author": [
            "Moe Kayali",
            "Fabian Wenz",
            "Nesime Tatbul",
            "Çağatay Demiralp"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20331",
        "abstract": "Leading large language models (LLMs) are trained on public data. However, most of the world's data is dark data that is not publicly accessible, mainly in the form of private organizational or enterprise data. We show that the performance of methods based on LLMs seriously degrades when tested on real-world enterprise datasets. Current benchmarks, based on public data, overestimate the performance of LLMs. We release a new benchmark dataset, the GOBY Benchmark, to advance discovery in enterprise data integration. Based on our experience with this enterprise benchmark, we propose techniques to uplift the performance of LLMs on enterprise data, including (1) hierarchical annotation, (2) runtime class-learning, and (3) ontology synthesis. We show that, once these techniques are deployed, the performance on enterprise data becomes on par with that of public data. The Goby benchmark can be obtained at https://goby-benchmark.github.io/.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "66",
        "title": "Distilling Desired Comments for Enhanced Code Review with Large Language Models",
        "author": [
            "Yongda Yu",
            "Lei Zhang",
            "Guoping Rong",
            "Haifeng Shen",
            "Jiahao Zhang",
            "Haoxiang Yan",
            "Guohao Shi",
            "Dong Shao",
            "Ruiqi Pan",
            "Yuan Li",
            "Qiushi Wang",
            "Zhao Tian"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20340",
        "abstract": "There has been a growing interest in using Large Language Models (LLMs) for code review thanks to their proven proficiency in code comprehension. The primary objective of most review scenarios is to generate desired review comments (DRCs) that explicitly identify issues to trigger code fixes. However, existing LLM-based solutions are not so effective in generating DRCs for various reasons such as hallucination. To enhance their code review ability, they need to be fine-tuned with a customized dataset that is ideally full of DRCs. Nevertheless, such a dataset is not yet available, while manual annotation of DRCs is too laborious to be practical. In this paper, we propose a dataset distillation method, Desiview, which can automatically construct a distilled dataset by identifying DRCs from a code review dataset. Experiments on the CodeReviewer dataset comprising more than 150K review entries show that Desiview achieves an impressive performance of 88.93%, 80.37%, 86.67%, and 84.44% in terms of Precision, Recall, Accuracy, and F1, respectively, surpassing state-of-the-art methods. To validate the effect of such a distilled dataset on enhancing LLMs' code review ability, we first fine-tune the latest LLaMA series (i.e., LLaMA 3 and LLaMA 3.1) to build model Desiview4FT. We then enhance the model training effect through KTO alignment by feeding those review comments identified as non-DRCs to the LLMs, resulting in model Desiview4FA. Verification results indicate that Desiview4FA slightly outperforms Desiview4FT, while both models have significantly improved against the base models in terms of generating DRCs. Human evaluation confirms that both models identify issues more accurately and tend to generate review comments that better describe the issues contained in the code than the base LLMs do.",
        "tags": [
            "LLMs",
            "LLaMA",
            "Large Language Models"
        ]
    },
    {
        "id": "67",
        "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation",
        "author": [
            "Junqiao Wang",
            "Zeng Zhang",
            "Yangfan He",
            "Yuyang Song",
            "Tianyu Shi",
            "Yuchen Li",
            "Hengyuan Xu",
            "Kunyu Wu",
            "Guangwu Qian",
            "Qiuwu Chen",
            "Lewei He"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20367",
        "abstract": "With the rapid evolution of large language models (LLM), reinforcement learning (RL) has emerged as a pivotal technique for code generation and optimization in various domains. This paper presents a systematic survey of the application of RL in code optimization and generation, highlighting its role in enhancing compiler optimization, resource allocation, and the development of frameworks and tools. Subsequent sections first delve into the intricate processes of compiler optimization, where RL algorithms are leveraged to improve efficiency and resource utilization. The discussion then progresses to the function of RL in resource allocation, emphasizing register allocation and system optimization. We also explore the burgeoning role of frameworks and tools in code generation, examining how RL can be integrated to bolster their capabilities. This survey aims to serve as a comprehensive resource for researchers and practitioners interested in harnessing the power of RL to advance code generation and optimization techniques.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "RL"
        ]
    },
    {
        "id": "68",
        "title": "Differential Evolution Integrated Hybrid Deep Learning Model for Object Detection in Pre-made Dishes",
        "author": [
            "Lujia Lv",
            "Di Wu",
            "Yangyi Xia",
            "Jia Wu",
            "Xiaojing Liu",
            "Yi He"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20370",
        "abstract": "With the continuous improvement of people's living standards and fast-paced working conditions, pre-made dishes are becoming increasingly popular among families and restaurants due to their advantages of time-saving, convenience, variety, cost-effectiveness, standard quality, etc. Object detection is a key technology for selecting ingredients and evaluating the quality of dishes in the pre-made dishes industry. To date, many object detection approaches have been proposed. However, accurate object detection of pre-made dishes is extremely difficult because of overlapping occlusion of ingredients, similarity of ingredients, and insufficient light in the processing environment. As a result, the recognition scene is relatively complex and thus leads to poor object detection by a single model. To address this issue, this paper proposes a Differential Evolution Integrated Hybrid Deep Learning (DEIHDL) model. The main idea of DEIHDL is three-fold: 1) three YOLO-based and transformer-based base models are developed respectively to increase diversity for detecting objects of pre-made dishes, 2) the three base models are integrated by differential evolution optimized self-adjusting weights, and 3) weighted boxes fusion strategy is employed to score the confidence of the three base models during the integration. As such, DEIHDL possesses the multi-performance originating from the three base models to achieve accurate object detection in complex pre-made dish scenes. Extensive experiments on real datasets demonstrate that the proposed DEIHDL model significantly outperforms the base models in detecting objects of pre-made dishes.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "69",
        "title": "LLM2: Let Large Language Models Harness System 2 Reasoning",
        "author": [
            "Cheng Yang",
            "Chufan Shi",
            "Siheng Li",
            "Bo Shui",
            "Yujiu Yang",
            "Wai Lam"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20372",
        "abstract": "Large language models (LLMs) have exhibited impressive capabilities across a myriad of tasks, yet they occasionally yield undesirable outputs. We posit that these limitations are rooted in the foundational autoregressive architecture of LLMs, which inherently lacks mechanisms for differentiating between desirable and undesirable results. Drawing inspiration from the dual-process theory of human cognition, we introduce LLM2, a novel framework that combines an LLM (System 1) with a process-based verifier (System 2). Within LLM2, the LLM is responsible for generating plausible candidates, while the verifier provides timely process-based feedback to distinguish desirable and undesirable outputs. The verifier is trained with a pairwise comparison loss on synthetic process-supervision data generated through our token quality exploration strategy. Empirical results on mathematical reasoning benchmarks substantiate the efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8 (+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with self-consistency, LLM2 achieves additional improvements, boosting major@20 accuracy from 56.2 to 70.2 (+14.0).",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "70",
        "title": "Tri-Ergon: Fine-grained Video-to-Audio Generation with Multi-modal Conditions and LUFS Control",
        "author": [
            "Bingliang Li",
            "Fengyu Yang",
            "Yuxin Mao",
            "Qingwen Ye",
            "Hongkai Chen",
            "Yiran Zhong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20378",
        "abstract": "Video-to-audio (V2A) generation utilizes visual-only video features to produce realistic sounds that correspond to the scene. However, current V2A models often lack fine-grained control over the generated audio, especially in terms of loudness variation and the incorporation of multi-modal conditions. To overcome these limitations, we introduce Tri-Ergon, a diffusion-based V2A model that incorporates textual, auditory, and pixel-level visual prompts to enable detailed and semantically rich audio synthesis. Additionally, we introduce Loudness Units relative to Full Scale (LUFS) embedding, which allows for precise manual control of the loudness changes over time for individual audio channels, enabling our model to effectively address the intricate correlation of video and audio in real-world Foley workflows. Tri-Ergon is capable of creating 44.1 kHz high-fidelity stereo audio clips of varying lengths up to 60 seconds, which significantly outperforms existing state-of-the-art V2A methods that typically generate mono audio for a fixed duration.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "71",
        "title": "PTQ4VM: Post-Training Quantization for Visual Mamba",
        "author": [
            "Younghyun Cho",
            "Changhun Lee",
            "Seonggon Kim",
            "Eunhyeok Park"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20386",
        "abstract": "Visual Mamba is an approach that extends the selective space state model, Mamba, to vision tasks. It processes image tokens sequentially in a fixed order, accumulating information to generate outputs. Despite its growing popularity for delivering high-quality outputs at a low computational cost across various tasks, Visual Mamba is highly susceptible to quantization, which makes further performance improvements challenging. Our analysis reveals that the fixed token access order in Visual Mamba introduces unique quantization challenges, which we categorize into three main issues: 1) token-wise variance, 2) channel-wise outliers, and 3) a long tail of activations. To address these challenges, we propose Post-Training Quantization for Visual Mamba (PTQ4VM), which introduces two key strategies: Per-Token Static (PTS) quantization and Joint Learning of Smoothing Scale and Step Size (JLSS). To the our best knowledge, this is the first quantization study on Visual Mamba. PTQ4VM can be applied to various Visual Mamba backbones, converting the pretrained model to a quantized format in under 15 minutes without notable quality degradation. Extensive experiments on large-scale classification and regression tasks demonstrate its effectiveness, achieving up to 1.83x speedup on GPUs with negligible accuracy loss compared to FP16. Our code is available at https://github.com/YoungHyun197/ptq4vm.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "72",
        "title": "Design of an improved microstrip antenna operating at a frequency band of 28GHz",
        "author": [
            "S. O. Zakariyya",
            "B. O. Sadiq",
            "R. A. Alao",
            "J. A. Adesina",
            "E. Obi"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20400",
        "abstract": "The design of an improved microstrip antenna operating in the 28 GHz frequency spectrum is the main goal of this work. The design used a Roger RT 5880 LZ substrate with a thickness and permittivity of 0.762mm and 1.96, respectively. The antenna was simulated in CST Microwave Studio. As the antenna feed, a quarter-wave transformer was used to provide an impedance match of 50 ohms. To improve the antenna's performance, a Ushaped element was added to the ground plane. The antenna resonated at 28 GHz frequency, according to simulation data, with a return loss of -21.4 dB, VSWR of 1.18, bandwidth of 2.026 GHz, and gain of 8.19 dB. The proposed antenna exhibits a performance improvement in terms of gain and bandwidth due to the addition of U-shaped element when benchmarked with existing designs in the literature work",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "73",
        "title": "Open-Sora: Democratizing Efficient Video Production for All",
        "author": [
            "Zangwei Zheng",
            "Xiangyu Peng",
            "Tianji Yang",
            "Chenhui Shen",
            "Shenggui Li",
            "Hongxin Liu",
            "Yukun Zhou",
            "Tianyi Li",
            "Yang You"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20404",
        "abstract": "Vision and language are the two foundational senses for humans, and they build up our cognitive ability and intelligence. While significant breakthroughs have been made in AI language ability, artificial visual intelligence, especially the ability to generate and simulate the world we see, is far lagging behind. To facilitate the development and accessibility of artificial visual intelligence, we created Open-Sora, an open-source video generation model designed to produce high-fidelity video content. Open-Sora supports a wide spectrum of visual generation tasks, including text-to-image generation, text-to-video generation, and image-to-video generation. The model leverages advanced deep learning architectures and training/inference techniques to enable flexible video synthesis, which could generate video content of up to 15 seconds, up to 720p resolution, and arbitrary aspect ratios. Specifically, we introduce Spatial-Temporal Diffusion Transformer (STDiT), an efficient diffusion framework for videos that decouples spatial and temporal attention. We also introduce a highly compressive 3D autoencoder to make representations compact and further accelerate training with an ad hoc training strategy. Through this initiative, we aim to foster innovation, creativity, and inclusivity within the community of AI content creation. By embracing the open-source principle, Open-Sora democratizes full access to all the training/inference/data preparation codes as well as model weights. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora.",
        "tags": [
            "3D",
            "Diffusion",
            "Diffusion Transformer",
            "Sora",
            "Text-to-Image",
            "Text-to-Video",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "74",
        "title": "Multi-Objective Large Language Model Unlearning",
        "author": [
            "Zibin Pan",
            "Shuwen Zhang",
            "Yuesheng Zheng",
            "Chi Li",
            "Yuheng Cheng",
            "Junhua Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20412",
        "abstract": "Machine unlearning in the domain of large language models (LLMs) has attracted great attention recently, which aims to effectively eliminate undesirable behaviors from LLMs without full retraining from scratch. In this paper, we explore the Gradient Ascent (GA) approach in LLM unlearning, which is a proactive way to decrease the prediction probability of the model on the target data in order to remove their influence. We analyze two challenges that render the process impractical: gradient explosion and catastrophic forgetting. To address these issues, we propose Multi-Objective Large Language Model Unlearning (MOLLM) algorithm. We first formulate LLM unlearning as a multi-objective optimization problem, in which the cross-entropy loss is modified to the unlearning version to overcome the gradient explosion issue. A common descent update direction is then calculated, which enables the model to forget the target data while preserving the utility of the LLM. Our empirical results verify that MoLLM outperforms the SOTA GA-based LLM unlearning methods in terms of unlearning effect and model utility preservation.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "75",
        "title": "EraseAnything: Enabling Concept Erasure in Rectified Flow Transformers",
        "author": [
            "Daiheng Gao",
            "Shilin Lu",
            "Shaw Walters",
            "Wenbo Zhou",
            "Jiaming Chu",
            "Jie Zhang",
            "Bang Zhang",
            "Mengxi Jia",
            "Jian Zhao",
            "Zhaoxin Fan",
            "Weiming Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20413",
        "abstract": "Removing unwanted concepts from large-scale text-to-image (T2I) diffusion models while maintaining their overall generative quality remains an open challenge. This difficulty is especially pronounced in emerging paradigms, such as Stable Diffusion (SD) v3 and Flux, which incorporate flow matching and transformer-based architectures. These advancements limit the transferability of existing concept-erasure techniques that were originally designed for the previous T2I paradigm (\\textit{e.g.}, SD v1.4). In this work, we introduce \\logopic \\textbf{EraseAnything}, the first method specifically developed to address concept erasure within the latest flow-based T2I framework. We formulate concept erasure as a bi-level optimization problem, employing LoRA-based parameter tuning and an attention map regularizer to selectively suppress undesirable activations. Furthermore, we propose a self-contrastive learning strategy to ensure that removing unwanted concepts does not inadvertently harm performance on unrelated ones. Experimental results demonstrate that EraseAnything successfully fills the research gap left by earlier methods in this new T2I paradigm, achieving state-of-the-art performance across a wide range of concept erasure tasks.",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "LoRA",
            "Rectified Flow",
            "Text-to-Image",
            "Transformer"
        ]
    },
    {
        "id": "76",
        "title": "Comparative Performance of Advanced NLP Models and LLMs in Multilingual Geo-Entity Detection",
        "author": [
            "Kalin Kopanov"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20414",
        "abstract": "The integration of advanced Natural Language Processing (NLP) methodologies and Large Language Models (LLMs) has significantly enhanced the extraction and analysis of geospatial data from multilingual texts, impacting sectors such as national and international security. This paper presents a comprehensive evaluation of leading NLP models -- SpaCy, XLM-RoBERTa, mLUKE, GeoLM -- and LLMs, specifically OpenAI's GPT 3.5 and GPT 4, within the context of multilingual geo-entity detection. Utilizing datasets from Telegram channels in English, Russian, and Arabic, we examine the performance of these models through metrics such as accuracy, precision, recall, and F1 scores, to assess their effectiveness in accurately identifying geospatial references. The analysis exposes each model's distinct advantages and challenges, underscoring the complexities involved in achieving precise geo-entity identification across varied linguistic landscapes. The conclusions drawn from this experiment aim to direct the enhancement and creation of more advanced and inclusive NLP tools, thus advancing the field of geospatial analysis and its application to global security.",
        "tags": [
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "77",
        "title": "Bringing Objects to Life: 4D generation from 3D objects",
        "author": [
            "Ohad Rahamim",
            "Ori Malca",
            "Dvir Samuel",
            "Gal Chechik"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20422",
        "abstract": "Recent advancements in generative modeling now enable the creation of 4D content (moving 3D objects) controlled with text prompts. 4D generation has large potential in applications like virtual worlds, media, and gaming, but existing methods provide limited control over the appearance and geometry of generated content. In this work, we introduce a method for animating user-provided 3D objects by conditioning on textual prompts to guide 4D generation, enabling custom animations while maintaining the identity of the original object. We first convert a 3D mesh into a ``static\" 4D Neural Radiance Field (NeRF) that preserves the visual attributes of the input object. Then, we animate the object using an Image-to-Video diffusion model driven by text. To improve motion realism, we introduce an incremental viewpoint selection protocol for sampling perspectives to promote lifelike movement and a masked Score Distillation Sampling (SDS) loss, which leverages attention maps to focus optimization on relevant regions. We evaluate our model in terms of temporal coherence, prompt adherence, and visual fidelity and find that our method outperforms baselines that are based on other approaches, achieving up to threefold improvements in identity preservation measured using LPIPS scores, and effectively balancing visual quality with dynamic content.",
        "tags": [
            "3D",
            "Diffusion",
            "NeRF"
        ]
    },
    {
        "id": "78",
        "title": "AmalREC: A Dataset for Relation Extraction and Classification Leveraging Amalgamation of Large Language Models",
        "author": [
            "Mansi",
            "Pranshu Pandya",
            "Mahek Bhavesh Vora",
            "Soumya Bharadwaj",
            "Ashish Anand"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20427",
        "abstract": "Existing datasets for relation classification and extraction often exhibit limitations such as restricted relation types and domain-specific biases. This work presents a generic framework to generate well-structured sentences from given tuples with the help of Large Language Models (LLMs). This study has focused on the following major questions: (i) how to generate sentences from relation tuples, (ii) how to compare and rank them, (iii) can we combine strengths of individual methods and amalgamate them to generate an even bette quality of sentences, and (iv) how to evaluate the final dataset? For the first question, we employ a multifaceted 5-stage pipeline approach, leveraging LLMs in conjunction with template-guided generation. We introduce Sentence Evaluation Index(SEI) that prioritizes factors like grammatical correctness, fluency, human-aligned sentiment, accuracy, and complexity to answer the first part of the second question. To answer the second part of the second question, this work introduces a SEI-Ranker module that leverages SEI to select top candidate generations. The top sentences are then strategically amalgamated to produce the final, high-quality sentence. Finally, we evaluate our dataset on LLM-based and SOTA baselines for relation classification. The proposed dataset features 255 relation types, with 15K sentences in the test set and around 150k in the train set organized in, significantly enhancing relational diversity and complexity. This work not only presents a new comprehensive benchmark dataset for RE/RC task, but also compare different LLMs for generation of quality sentences from relational tuples.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "79",
        "title": "Image Augmentation Agent for Weakly Supervised Semantic Segmentation",
        "author": [
            "Wangyu Wu",
            "Xianglin Qiu",
            "Siqi Song",
            "Zhenhong Chen",
            "Xiaowei Huang",
            "Fei Ma",
            "Jimin Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20439",
        "abstract": "Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets.",
        "tags": [
            "Diffusion",
            "LLMs",
            "Large Language Models",
            "Segmentation"
        ]
    },
    {
        "id": "80",
        "title": "Enhancing Entertainment Translation for Indian Languages using Adaptive Context, Style and LLMs",
        "author": [
            "Pratik Rakesh Singh",
            "Mohammadi Zaki",
            "Pankaj Wasnik"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20440",
        "abstract": "We address the challenging task of neural machine translation (NMT) in the entertainment domain, where the objective is to automatically translate a given dialogue from a source language content to a target language. This task has various applications, particularly in automatic dubbing, subtitling, and other content localization tasks, enabling source content to reach a wider audience. Traditional NMT systems typically translate individual sentences in isolation, without facilitating knowledge transfer of crucial elements such as the context and style from previously encountered sentences. In this work, we emphasize the significance of these fundamental aspects in producing pertinent and captivating translations. We demonstrate their significance through several examples and propose a novel framework for entertainment translation, which, to our knowledge, is the first of its kind. Furthermore, we introduce an algorithm to estimate the context and style of the current session and use these estimations to generate a prompt that guides a Large Language Model (LLM) to generate high-quality translations. Our method is both language and LLM-agnostic, making it a general-purpose tool. We demonstrate the effectiveness of our algorithm through various numerical studies and observe significant improvement in the COMET scores over various state-of-the-art LLMs. Moreover, our proposed method consistently outperforms baseline LLMs in terms of win-ratio.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "81",
        "title": "Single-image reflection removal via self-supervised diffusion models",
        "author": [
            "Zhengyang Lu",
            "Weifan Wang",
            "Tianhao Guo",
            "Feng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20466",
        "abstract": "Reflections often degrade the visual quality of images captured through transparent surfaces, and reflection removal methods suffers from the shortage of paired real-world http://samples.This paper proposes a hybrid approach that combines cycle-consistency with denoising diffusion probabilistic models (DDPM) to effectively remove reflections from single images without requiring paired training data. The method introduces a Reflective Removal Network (RRN) that leverages DDPMs to model the decomposition process and recover the transmission image, and a Reflective Synthesis Network (RSN) that re-synthesizes the input image using the separated components through a nonlinear attention-based mechanism. Experimental results demonstrate the effectiveness of the proposed method on the SIR$^2$, Flash-Based Reflection Removal (FRR) Dataset, and a newly introduced Museum Reflection Removal (MRR) dataset, showing superior performance compared to state-of-the-art methods.",
        "tags": [
            "DDPM",
            "Diffusion"
        ]
    },
    {
        "id": "82",
        "title": "JADE: Joint-aware Latent Diffusion for 3D Human Generative Modeling",
        "author": [
            "Haorui Ji",
            "Rong Wang",
            "Taojun Lin",
            "Hongdong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20470",
        "abstract": "Generative modeling of 3D human bodies have been studied extensively in computer vision. The core is to design a compact latent representation that is both expressive and semantically interpretable, yet existing approaches struggle to achieve both requirements. In this work, we introduce JADE, a generative framework that learns the variations of human shapes with fined-grained control. Our key insight is a joint-aware latent representation that decomposes human bodies into skeleton structures, modeled by joint positions, and local surface geometries, characterized by features attached to each joint. This disentangled latent space design enables geometric and semantic interpretation, facilitating users with flexible controllability. To generate coherent and plausible human shapes under our proposed decomposition, we also present a cascaded pipeline where two diffusions are employed to model the distribution of skeleton structures and local surface geometries respectively. Extensive experiments are conducted on public datasets, where we demonstrate the effectiveness of JADE framework in multiple tasks in terms of autoencoding reconstruction accuracy, editing controllability and generation quality compared with existing methods.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "83",
        "title": "Toward Scene Graph and Layout Guided Complex 3D Scene Generation",
        "author": [
            "Yu-Hsiang Huang",
            "Wei Wang",
            "Sheng-Yu Huang",
            "Yu-Chiang Frank Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20473",
        "abstract": "Recent advancements in object-centric text-to-3D generation have shown impressive results. However, generating complex 3D scenes remains an open challenge due to the intricate relations between objects. Moreover, existing methods are largely based on score distillation sampling (SDS), which constrains the ability to manipulate multiobjects with specific interactions. Addressing these critical yet underexplored issues, we present a novel framework of Scene Graph and Layout Guided 3D Scene Generation (GraLa3D). Given a text prompt describing a complex 3D scene, GraLa3D utilizes LLM to model the scene using a scene graph representation with layout bounding box information. GraLa3D uniquely constructs the scene graph with single-object nodes and composite super-nodes. In addition to constraining 3D generation within the desirable layout, a major contribution lies in the modeling of interactions between objects in a super-node, while alleviating appearance leakage across objects within such nodes. Our experiments confirm that GraLa3D overcomes the above limitations and generates complex 3D scenes closely aligned with text prompts.",
        "tags": [
            "3D",
            "Text-to-3D"
        ]
    },
    {
        "id": "84",
        "title": "Multimodal Variational Autoencoder: a Barycentric View",
        "author": [
            "Peijie Qiu",
            "Wenhui Zhu",
            "Sayantan Kumar",
            "Xiwen Chen",
            "Xiaotong Sun",
            "Jin Yang",
            "Abolfazl Razi",
            "Yalin Wang",
            "Aristeidis Sotiras"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20487",
        "abstract": "Multiple signal modalities, such as vision and sounds, are naturally present in real-world phenomena. Recently, there has been growing interest in learning generative models, in particular variational autoencoder (VAE), to for multimodal representation learning especially in the case of missing modalities. The primary goal of these models is to learn a modality-invariant and modality-specific representation that characterizes information across multiple modalities. Previous attempts at multimodal VAEs approach this mainly through the lens of experts, aggregating unimodal inference distributions with a product of experts (PoE), a mixture of experts (MoE), or a combination of both. In this paper, we provide an alternative generic and theoretical formulation of multimodal VAE through the lens of barycenter. We first show that PoE and MoE are specific instances of barycenters, derived by minimizing the asymmetric weighted KL divergence to unimodal inference distributions. Our novel formulation extends these two barycenters to a more flexible choice by considering different types of divergences. In particular, we explore the Wasserstein barycenter defined by the 2-Wasserstein distance, which better preserves the geometry of unimodal distributions by capturing both modality-specific and modality-invariant representations compared to KL divergence. Empirical studies on three multimodal benchmarks demonstrated the effectiveness of the proposed method.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "85",
        "title": "TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs via Bidirectional Communication",
        "author": [
            "Zongwu Wang",
            "Fangxin Liu",
            "Mingshuai Li",
            "Li Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20501",
        "abstract": "Efficient parallelization of Large Language Models (LLMs) with long sequences is essential but challenging due to their significant computational and memory demands, particularly stemming from communication bottlenecks in attention mechanisms. While sequence parallelism (SP) has been introduced as a potential solution, existing methods often suffer from limited scalability or inefficiency, rendering their effectiveness.\nRing-Attention demonstrates the potential for scaling sequence processing but faces significant limitations due to its reliance on peer-to-peer (P2P) communication and inefficient utilization of network resources. As the degree of SP increases, the quadratic decrease in computation time per step contrasts sharply with the linear reduction in communication volume, exacerbating communication bottlenecks. To address these challenges, we propose TokenRing, a fine-grained parallel framework that leverages bidirectional P2P communication to effectively overlap computation and data transmission. By partitioning the attention block and concurrently transmitting Query and block outputs (i.e., $block\\_out$ and $block\\_lse$) within a fully connected mesh topology, TokenRing achieves significant reductions in communication overhead and better load balancing. These innovations improve the scalability and efficiency of distributed Transformer models, particularly for long-context sequences. Experimental results demonstrate that TokenRing enhances throughput and reduces communication latency. Moreover, its design adapts seamlessly to various multi-GPU interconnect solutions, such as Huawei Ascend, ensuring broad compatibility and cost-effectiveness for distributed LLM inference and training. The code is available at: \\url{https://github.com/ACA-Lab-SJTU/token-ring}.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Transformer"
        ]
    },
    {
        "id": "86",
        "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video Understanding",
        "author": [
            "Xiao Wang",
            "Qingyi Si",
            "Jianlong Wu",
            "Shiyu Zhu",
            "Li Cao",
            "Liqiang Nie"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20504",
        "abstract": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in video understanding. However, existing VideoLLMs often inherit the limitations of their backbone LLMs in handling long sequences, leading to challenges for long video understanding. Common solutions either simply uniformly sample videos' frames or compress visual tokens, which focus primarily on low-level temporal visual redundancy, overlooking high-level knowledge redundancy. This limits the achievable compression rate with minimal loss. To this end. we introduce a training-free method, $\\textbf{ReTaKe}$, containing two novel modules DPSelect and PivotKV, to jointly model and reduce both temporal visual redundancy and knowledge redundancy for long video understanding. Specifically, DPSelect identifies keyframes with local maximum peak distance based on their visual features, which are closely aligned with human video perception. PivotKV employs the obtained keyframes as pivots and conducts KV-Cache compression for the non-pivot tokens with low attention scores, which are derived from the learned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and LVBench, show that ReTaKe can support 4x longer video sequences with minimal performance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%, even surpassing or on par with much larger ones. Our code is available at https://github.com/SCZwangxiao/video-ReTaKe",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "87",
        "title": "Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical Urban Planning",
        "author": [
            "Hang Ni",
            "Yuzhi Wang",
            "Hao Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20505",
        "abstract": "Urban regeneration presents significant challenges within the context of urbanization, requiring adaptive approaches to tackle evolving needs. Leveraging advancements in large language models (LLMs), we propose Cyclical Urban Planning (CUP), a new paradigm that continuously generates, evaluates, and refines urban plans in a closed-loop. Specifically, our multi-agent LLM-based framework consists of three key components: (1) Planning, where LLM agents generate and refine urban plans based on contextual data; (2) Living, where agents simulate the behaviors and interactions of residents, modeling life in the urban environment; and (3) Judging, which involves evaluating plan effectiveness and providing iterative feedback for improvement. The cyclical process enables a dynamic and responsive planning approach. Experiments on the real-world dataset demonstrate the effectiveness of our framework as a continuous and adaptive planning process.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "88",
        "title": "DPBridge: Latent Diffusion Bridge for Dense Prediction",
        "author": [
            "Haorui Ji",
            "Taojun Lin",
            "Hongdong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20506",
        "abstract": "Diffusion models have demonstrated remarkable success in dense prediction problems, which aims to model per-pixel relationship between RGB images and dense signal maps, thanks to their ability to effectively capture complex data distributions. However, initiating the reverse sampling trajectory from uninformative noise prior introduces limitations such as degraded performance and slow inference speed. In this work, we propose DPBridge, a generative framework that formulates dense prediction tasks as image-conditioned generation problems and establishes a direct mapping between input image and its corresponding dense map based on fully-tractable diffusion bridge process. This approach addresses aforementioned limitations in conventional diffusion-based solutions. In addition, we introduce finetuning strategies to adapt our model from pretrained image diffusion backbone, leveraging its rich visual prior knowledge to facilitate both efficient training and robust generalization ability. Experimental results shows that our DPBridge can achieve competitive performance compared to both feed-forward and diffusion-based approaches across various benchmarks, highlighting its effectiveness and adaptability.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "89",
        "title": "Goal-Conditioned Data Augmentation for Offline Reinforcement Learning",
        "author": [
            "Xingshuai Huang",
            "Di Wu Member",
            "Benoit Boulet"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20519",
        "abstract": "Offline reinforcement learning (RL) enables policy learning from pre-collected offline datasets, relaxing the need to interact directly with the environment. However, limited by the quality of offline datasets, it generally fails to learn well-qualified policies in suboptimal datasets. To address datasets with insufficient optimal demonstrations, we introduce Goal-cOnditioned Data Augmentation (GODA), a novel goal-conditioned diffusion-based method for augmenting samples with higher quality. Leveraging recent advancements in generative modeling, GODA incorporates a novel return-oriented goal condition with various selection mechanisms. Specifically, we introduce a controllable scaling technique to provide enhanced return-based guidance during data sampling. GODA learns a comprehensive distribution representation of the original offline datasets while generating new data with selectively higher-return goals, thereby maximizing the utility of limited optimal demonstrations. Furthermore, we propose a novel adaptive gated conditioning method for processing noised inputs and conditions, enhancing the capture of goal-oriented guidance. We conduct experiments on the D4RL benchmark and real-world challenges, specifically traffic signal control (TSC) tasks, to demonstrate GODA's effectiveness in enhancing data quality and superior performance compared to state-of-the-art data augmentation methods across various offline RL algorithms.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "90",
        "title": "MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks",
        "author": [
            "Yifei Liu",
            "Zhihang Zhong",
            "Yifan Zhan",
            "Sheng Xu",
            "Xiao Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20522",
        "abstract": "While 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in novel view synthesis and real-time rendering, the high memory consumption due to the use of millions of Gaussians limits its practicality. To mitigate this issue, improvements have been made by pruning unnecessary Gaussians, either through a hand-crafted criterion or by using learned masks. However, these methods deterministically remove Gaussians based on a snapshot of the pruning moment, leading to sub-optimized reconstruction performance from a long-term perspective. To address this issue, we introduce MaskGaussian, which models Gaussians as probabilistic entities rather than permanently removing them, and utilize them according to their probability of existence. To achieve this, we propose a masked-rasterization technique that enables unused yet probabilistically existing Gaussians to receive gradients, allowing for dynamic assessment of their contribution to the evolving scene and adjustment of their probability of existence. Hence, the importance of Gaussians iteratively changes and the pruned Gaussians are selected diversely. Extensive experiments demonstrate the superiority of the proposed method in achieving better rendering quality with fewer Gaussians than previous pruning methods, pruning over 60% of Gaussians on average with only a 0.02 PSNR decline. Our code can be found at: https://github.com/kaikai23/MaskGaussian",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "91",
        "title": "The Impact of Prompt Programming on Function-Level Code Generation",
        "author": [
            "Ranim Khojah",
            "Francisco Gomes de Oliveira Neto",
            "Mazen Mohamad",
            "Philipp Leitner"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20545",
        "abstract": "Large Language Models (LLMs) are increasingly used by software engineers for code generation. However, limitations of LLMs such as irrelevant or incorrect code have highlighted the need for prompt programming (or prompt engineering) where engineers apply specific prompt techniques (e.g., chain-of-thought or input-output examples) to improve the generated code. Despite this, the impact of different prompt techniques -- and their combinations -- on code generation remains underexplored. In this study, we introduce CodePromptEval, a dataset of 7072 prompts designed to evaluate five prompt techniques (few-shot, persona, chain-of-thought, function signature, list of packages) and their effect on the correctness, similarity, and quality of complete functions generated by three LLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt techniques significantly influence the generated code, combining multiple techniques does not necessarily improve the outcome. Additionally, we observed a trade-off between correctness and quality when using prompt techniques. Our dataset and replication package enable future research on improving LLM-generated code and evaluating new prompt techniques.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "92",
        "title": "Counterfactual Samples Constructing and Training for Commonsense Statements Estimation",
        "author": [
            "Chong Liu",
            "Zaiwen Feng",
            "Lin Liu",
            "Zhenyun Deng",
            "Jiuyong Li",
            "Ruifang Zhai",
            "Debo Cheng",
            "Li Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20563",
        "abstract": "Plausibility Estimation (PE) plays a crucial role for enabling language models to objectively comprehend the real world. While large language models (LLMs) demonstrate remarkable capabilities in PE tasks but sometimes produce trivial commonsense errors due to the complexity of commonsense knowledge. They lack two key traits of an ideal PE model: a) Language-explainable: relying on critical word segments for decisions, and b) Commonsense-sensitive: detecting subtle linguistic variations in commonsense. To address these issues, we propose a novel model-agnostic method, referred to as Commonsense Counterfactual Samples Generating (CCSG). By training PE models with CCSG, we encourage them to focus on critical words, thereby enhancing both their language-explainable and commonsense-sensitive capabilities. Specifically, CCSG generates counterfactual samples by strategically replacing key words and introducing low-level dropout within sentences. These counterfactual samples are then incorporated into a sentence-level contrastive training framework to further enhance the model's learning process. Experimental results across nine diverse datasets demonstrate the effectiveness of CCSG in addressing commonsense reasoning challenges, with our CCSG method showing 3.07% improvement against the SOTA methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "93",
        "title": "Controlling Out-of-Domain Gaps in LLMs for Genre Classification and Generated Text Detection",
        "author": [
            "Dmitri Roussinov",
            "Serge Sharoff",
            "Nadezhda Puchnina"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20595",
        "abstract": "This study demonstrates that the modern generation of Large Language Models (LLMs, such as GPT-4) suffers from the same out-of-domain (OOD) performance gap observed in prior research on pre-trained Language Models (PLMs, such as BERT). We demonstrate this across two non-topical classification tasks: 1) genre classification and 2) generated text detection. Our results show that when demonstration examples for In-Context Learning (ICL) come from one domain (e.g., travel) and the system is tested on another domain (e.g., history), classification performance declines significantly.\nTo address this, we introduce a method that controls which predictive indicators are used and which are excluded during classification. For the two tasks studied here, this ensures that topical features are omitted, while the model is guided to focus on stylistic rather than content-based attributes. This approach reduces the OOD gap by up to 20 percentage points in a few-shot setup. Straightforward Chain-of-Thought (CoT) methods, used as the baseline, prove insufficient, while our approach consistently enhances domain transfer performance.",
        "tags": [
            "BERT",
            "Detection",
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "94",
        "title": "Zero-Shot Image Restoration Using Few-Step Guidance of Consistency Models (and Beyond)",
        "author": [
            "Tomer Garber",
            "Tom Tirer"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20596",
        "abstract": "In recent years, it has become popular to tackle image restoration tasks with a single pretrained diffusion model (DM) and data-fidelity guidance, instead of training a dedicated deep neural network per task. However, such \"zero-shot\" restoration schemes currently require many Neural Function Evaluations (NFEs) for performing well, which may be attributed to the many NFEs needed in the original generative functionality of the DMs. Recently, faster variants of DMs have been explored for image generation. These include Consistency Models (CMs), which can generate samples via a couple of NFEs. However, existing works that use guided CMs for restoration still require tens of NFEs or fine-tuning of the model per task that leads to performance drop if the assumptions during the fine-tuning are not accurate. In this paper, we propose a zero-shot restoration scheme that uses CMs and operates well with as little as 4 NFEs. It is based on a wise combination of several ingredients: better initialization, back-projection guidance, and above all a novel noise injection mechanism. We demonstrate the advantages of our approach for image super-resolution, deblurring and inpainting. Interestingly, we show that the usefulness of our noise injection technique goes beyond CMs: it can also mitigate the performance degradation of existing guided DM methods when reducing their NFE count.",
        "tags": [
            "Consistency Models",
            "Deblurring",
            "Diffusion",
            "Inpainting",
            "Super Resolution"
        ]
    },
    {
        "id": "95",
        "title": "MATEY: multiscale adaptive foundation models for spatiotemporal physical systems",
        "author": [
            "Pei Zhang",
            "M. Paul Laiu",
            "Matthew Norman",
            "Doug Stefanski",
            "John Gounley"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20601",
        "abstract": "Accurate representation of the multiscale features in spatiotemporal physical systems using vision transformer (ViT) architectures requires extremely long, computationally prohibitive token sequences. To address this issue, we propose two adaptive tokenization schemes that dynamically adjust patch sizes based on local features: one ensures convergent behavior to uniform patch refinement, while the other offers better computational efficiency. Moreover, we present a set of spatiotemporal attention schemes, where the temporal or axial spatial dimensions are decoupled, and evaluate their computational and data efficiencies. We assess the performance of the proposed multiscale adaptive model, MATEY, in a sequence of experiments. The results show that adaptive tokenization schemes achieve improved accuracy without significantly increasing the length of the token sequence. Compared to a full spatiotemporal attention scheme or a scheme that decouples only the temporal dimension, we find that fully decoupled axial attention is less efficient and expressive, requiring more training time and model weights to achieve the same accuracy. Finally, we demonstrate in two fine-tuning tasks featuring different physics that models pretrained on PDEBench data outperform the ones trained from scratch, especially in the low data regime with frozen attention.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "96",
        "title": "NLP-based Regulatory Compliance -- Using GPT 4.0 to Decode Regulatory Documents",
        "author": [
            "Bimal Kumar",
            "Dmitri Roussinov"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20602",
        "abstract": "Large Language Models (LLMs) such as GPT-4.0 have shown significant promise in addressing the semantic complexities of regulatory documents, particularly in detecting inconsistencies and contradictions. This study evaluates GPT-4.0's ability to identify conflicts within regulatory requirements by analyzing a curated corpus with artificially injected ambiguities and contradictions, designed in collaboration with architects and compliance engineers. Using metrics such as precision, recall, and F1 score, the experiment demonstrates GPT-4.0's effectiveness in detecting inconsistencies, with findings validated by human experts. The results highlight the potential of LLMs to enhance regulatory compliance processes, though further testing with larger datasets and domain-specific fine-tuning is needed to maximize accuracy and practical applicability. Future work will explore automated conflict resolution and real-world implementation through pilot projects with industry partners.",
        "tags": [
            "GPT",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "97",
        "title": "Do Current Video LLMs Have Strong OCR Abilities? A Preliminary Study",
        "author": [
            "Yulin Fei",
            "Yuhui Gao",
            "Xingyuan Xian",
            "Xiaojin Zhang",
            "Tao Wu",
            "Wei Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20613",
        "abstract": "With the rise of multimodal large language models, accurately extracting and understanding textual information from video content, referred to as video based optical character recognition (Video OCR), has become a crucial capability. This paper introduces a novel benchmark designed to evaluate the video OCR performance of multi-modal models in videos. Comprising 1,028 videos and 2,961 question-answer pairs, this benchmark proposes several key challenges through 6 distinct subtasks: (1) Recognition of text content itself and its basic visual attributes, (2)Semantic and Spatial Comprehension of OCR objects in videos (3) Dynamic Motion detection and Temporal Localization. We developed this benchmark using a semi-automated approach that integrates the OCR ability of image LLMs with manual refinement, balancing efficiency, cost, and data quality. Our resource aims to help advance research in video LLMs and underscores the need for improving OCR ability for video LLMs. The benchmark will be released on https://github.com/YuHuiGao/FG-Bench.git.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "98",
        "title": "FreqMixFormerV2: Lightweight Frequency-aware Mixed Transformer for Human Skeleton Action Recognition",
        "author": [
            "Wenhan Wu",
            "Pengfei Wang",
            "Chen Chen",
            "Aidong Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20621",
        "abstract": "Transformer-based human skeleton action recognition has been developed for years. However, the complexity and high parameter count demands of these models hinder their practical applications, especially in resource-constrained environments. In this work, we propose FreqMixForemrV2, which was built upon the Frequency-aware Mixed Transformer (FreqMixFormer) for identifying subtle and discriminative actions with pioneered frequency-domain analysis. We design a lightweight architecture that maintains robust performance while significantly reducing the model complexity. This is achieved through a redesigned frequency operator that optimizes high-frequency and low-frequency parameter adjustments, and a simplified frequency-aware attention module. These improvements result in a substantial reduction in model parameters, enabling efficient deployment with only a minimal sacrifice in accuracy. Comprehensive evaluations of standard datasets (NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets) demonstrate that the proposed model achieves a superior balance between efficiency and accuracy, outperforming state-of-the-art methods with only 60% of the parameters.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "99",
        "title": "EVOLVE: Emotion and Visual Output Learning via LLM Evaluation",
        "author": [
            "Jordan Sinclair",
            "Christopher Reardon"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20632",
        "abstract": "Human acceptance of social robots is greatly effected by empathy and perceived understanding. This necessitates accurate and flexible responses to various input data from the user. While systems such as this can become increasingly complex as more states or response types are included, new research in the application of large language models towards human-robot interaction has allowed for more streamlined perception and reaction pipelines. LLM-selected actions and emotional expressions can help reinforce the realism of displayed empathy and allow for improved communication between the robot and user. Beyond portraying empathy in spoken or written responses, this shows the possibilities of using LLMs in actuated, real world scenarios. In this work we extend research in LLM-driven nonverbal behavior for social robots by considering more open-ended emotional response selection leveraging new advances in vision-language models, along with emotionally aligned motion and color pattern selections that strengthen conveyance of meaning and empathy.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "Robot"
        ]
    },
    {
        "id": "100",
        "title": "Knowledge Editing for Large Language Model with Knowledge Neuronal Ensemble",
        "author": [
            "Yongchang Li",
            "Yujin Zhu",
            "Tao Yan",
            "Shijian Fan",
            "Gang Wu",
            "Liang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20637",
        "abstract": "As real-world knowledge is constantly evolving, ensuring the timeliness and accuracy of a model's knowledge is crucial. This has made knowledge editing in large language models increasingly important. However, existing knowledge editing methods face several challenges, including parameter localization coupling, imprecise localization, and a lack of dynamic interaction across layers. In this paper, we propose a novel knowledge editing method called Knowledge Neuronal Ensemble (KNE). A knowledge neuronal ensemble represents a group of neurons encoding specific knowledge, thus mitigating the issue of frequent parameter modification caused by coupling in parameter localization. The KNE method enhances the precision and accuracy of parameter localization by computing gradient attribution scores for each parameter at each layer. During the editing process, only the gradients and losses associated with the knowledge neuronal ensemble are computed, with error backpropagation performed accordingly, ensuring dynamic interaction and collaborative updates among parameters. Experimental results on three widely used knowledge editing datasets show that the KNE method significantly improves the accuracy of knowledge editing and achieves, or even exceeds, the performance of the best baseline methods in portability and locality metrics.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "101",
        "title": "SafeSynthDP: Leveraging Large Language Models for Privacy-Preserving Synthetic Data Generation Using Differential Privacy",
        "author": [
            "Md Mahadi Hasan Nahid",
            "Sadid Bin Hasan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20641",
        "abstract": "Machine learning (ML) models frequently rely on training data that may include sensitive or personal information, raising substantial privacy concerns. Legislative frameworks such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have necessitated the development of strategies that preserve privacy while maintaining the utility of data. In this paper, we investigate the capability of Large Language Models (LLMs) to generate synthetic datasets integrated with Differential Privacy (DP) mechanisms, thereby enabling data-driven research and model training without direct exposure of sensitive information. Our approach incorporates DP-based noise injection methods, including Laplace and Gaussian distributions, into the data generation process. We then evaluate the utility of these DP-enhanced synthetic datasets by comparing the performance of ML models trained on them against models trained on the original data. To substantiate privacy guarantees, we assess the resilience of the generated synthetic data to membership inference attacks and related threats. The experimental results demonstrate that integrating DP within LLM-driven synthetic data generation offers a viable balance between privacy protection and data utility. This study provides a foundational methodology and insight into the privacy-preserving capabilities of LLMs, paving the way for compliant and effective ML research and applications.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "102",
        "title": "YOLO-UniOW: Efficient Universal Open-World Object Detection",
        "author": [
            "Lihao Liu",
            "Juexiao Feng",
            "Hui Chen",
            "Ao Wang",
            "Lin Song",
            "Jungong Han",
            "Guiguang Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20645",
        "abstract": "Traditional object detection models are constrained by the limitations of closed-set datasets, detecting only categories encountered during training. While multimodal models have extended category recognition by aligning text and image modalities, they introduce significant inference overhead due to cross-modality fusion and still remain restricted by predefined vocabulary, leaving them ineffective at handling unknown objects in open-world scenarios. In this work, we introduce Universal Open-World Object Detection (Uni-OWD), a new paradigm that unifies open-vocabulary and open-world object detection tasks. To address the challenges of this setting, we propose YOLO-UniOW, a novel model that advances the boundaries of efficiency, versatility, and performance. YOLO-UniOW incorporates Adaptive Decision Learning to replace computationally expensive cross-modality fusion with lightweight alignment in the CLIP latent space, achieving efficient detection without compromising generalization. Additionally, we design a Wildcard Learning strategy that detects out-of-distribution objects as \"unknown\" while enabling dynamic vocabulary expansion without the need for incremental learning. This design empowers YOLO-UniOW to seamlessly adapt to new categories in open-world environments. Extensive experiments validate the superiority of YOLO-UniOW, achieving achieving 34.6 AP and 30.0 APr on LVIS with an inference speed of 69.6 FPS. The model also sets benchmarks on M-OWODB, S-OWODB, and nuScenes datasets, showcasing its unmatched performance in open-world object detection. Code and models are available at https://github.com/THU-MIG/YOLO-UniOW.",
        "tags": [
            "CLIP",
            "Detection"
        ]
    },
    {
        "id": "103",
        "title": "Enhancing Visual Representation for Text-based Person Searching",
        "author": [
            "Wei Shen",
            "Ming Fang",
            "Yuxia Wang",
            "Jiafeng Xiao",
            "Diping Li",
            "Huangqun Chen",
            "Ling Xu",
            "Weifeng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20646",
        "abstract": "Text-based person search aims to retrieve the matched pedestrians from a large-scale image database according to the text description. The core difficulty of this task is how to extract effective details from pedestrian images and texts, and achieve cross-modal alignment in a common latent space. Prior works adopt image and text encoders pre-trained on unimodal data to extract global and local features from image and text respectively, and then global-local alignment is achieved explicitly. However, these approaches still lack the ability of understanding visual details, and the retrieval accuracy is still limited by identity confusion. In order to alleviate the above problems, we rethink the importance of visual features for text-based person search, and propose VFE-TPS, a Visual Feature Enhanced Text-based Person Search model. It introduces a pre-trained multimodal backbone CLIP to learn basic multimodal features and constructs Text Guided Masked Image Modeling task to enhance the model's ability of learning local visual details without explicit annotation. In addition, we design Identity Supervised Global Visual Feature Calibration task to guide the model learn identity-aware global visual features. The key finding of our study is that, with the help of our proposed auxiliary tasks, the knowledge embedded in the pre-trained CLIP model can be successfully adapted to text-based person search task, and the model's visual understanding ability is significantly enhanced. Experimental results on three benchmarks demonstrate that our proposed model exceeds the existing approaches, and the Rank-1 accuracy is significantly improved with a notable margin of about $1\\%\\sim9\\%$. Our code can be found at https://github.com/zhangweifeng1218/VFE_TPS.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "104",
        "title": "Overcoming Class Imbalance: Unified GNN Learning with Structural and Semantic Connectivity Representations",
        "author": [
            "Abdullah Alchihabi",
            "Hao Yan",
            "Yuhong Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20656",
        "abstract": "Class imbalance is pervasive in real-world graph datasets, where the majority of annotated nodes belong to a small set of classes (majority classes), leaving many other classes (minority classes) with only a handful of labeled nodes. Graph Neural Networks (GNNs) suffer from significant performance degradation in the presence of class imbalance, exhibiting bias towards majority classes and struggling to generalize effectively on minority classes. This limitation stems, in part, from the message passing process, leading GNNs to overfit to the limited neighborhood of annotated nodes from minority classes and impeding the propagation of discriminative information throughout the entire graph. In this paper, we introduce a novel Unified Graph Neural Network Learning (Uni-GNN) framework to tackle class-imbalanced node classification. The proposed framework seamlessly integrates both structural and semantic connectivity representations through semantic and structural node encoders. By combining these connectivity types, Uni-GNN extends the propagation of node embeddings beyond immediate neighbors, encompassing non-adjacent structural nodes and semantically similar nodes, enabling efficient diffusion of discriminative information throughout the graph. Moreover, to harness the potential of unlabeled nodes within the graph, we employ a balanced pseudo-label generation mechanism that augments the pool of available labeled nodes from minority classes in the training set. Experimental results underscore the superior performance of our proposed Uni-GNN framework compared to state-of-the-art class-imbalanced graph learning baselines across multiple benchmark datasets.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "105",
        "title": "Diffgrasp: Whole-Body Grasping Synthesis Guided by Object Motion Using a Diffusion Model",
        "author": [
            "Yonghao Zhang",
            "Qiang He",
            "Yanguang Wan",
            "Yinda Zhang",
            "Xiaoming Deng",
            "Cuixia Ma",
            "Hongan Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20657",
        "abstract": "Generating high-quality whole-body human object interaction motion sequences is becoming increasingly important in various fields such as animation, VR/AR, and robotics. The main challenge of this task lies in determining the level of involvement of each hand given the complex shapes of objects in different sizes and their different motion trajectories, while ensuring strong grasping realism and guaranteeing the coordination of movement in all body parts. Contrasting with existing work, which either generates human interaction motion sequences without detailed hand grasping poses or only models a static grasping pose, we propose a simple yet effective framework that jointly models the relationship between the body, hands, and the given object motion sequences within a single diffusion model. To guide our network in perceiving the object's spatial position and learning more natural grasping poses, we introduce novel contact-aware losses and incorporate a data-driven, carefully designed guidance. Experimental results demonstrate that our approach outperforms the state-of-the-art method and generates plausible whole-body motion sequences.",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "106",
        "title": "Enhancing Table Recognition with Vision LLMs: A Benchmark and Neighbor-Guided Toolchain Reasoner",
        "author": [
            "Yitong Zhou",
            "Mingyue Cheng",
            "Qingyang Mao",
            "Qi Liu",
            "Feiyang Xu",
            "Xin Li",
            "Enhong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20662",
        "abstract": "Pre-trained foundation models have recently significantly progressed in structured table understanding and reasoning. However, despite advancements in areas such as table semantic understanding and table question answering, recognizing the structure and content of unstructured tables using Vision Large Language Models (VLLMs) remains under-explored. In this work, we address this research gap by employing VLLMs in a training-free reasoning paradigm. First, we design a benchmark with various hierarchical dimensions relevant to table recognition. Subsequently, we conduct in-depth evaluations using pre-trained VLLMs, finding that low-quality image input is a significant bottleneck in the recognition process. Drawing inspiration from these findings, we propose the Neighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by integrating multiple lightweight models for low-level visual processing operations aimed at mitigating issues with low-quality input images. Specifically, we utilize a neighbor retrieval mechanism to guide the generation of multiple tool invocation plans, transferring tool selection experiences from similar neighbors to the given input, thereby facilitating suitable tool selection. Additionally, we introduce a reflection module to supervise the tool invocation process. Extensive experiments on public table recognition datasets demonstrate that our approach significantly enhances the recognition capabilities of the vanilla VLLMs. We believe that the designed benchmark and the proposed NGTR framework could provide an alternative solution in table recognition.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "107",
        "title": "Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA",
        "author": [
            "Qingyun Jin",
            "Xiaohui Song",
            "Feng Zhou",
            "Zengchang Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20677",
        "abstract": "Large language models have been shown to perform well on a variety of natural language processing problems. However, as the model size and the input sequence's length increase, the rapid increase of KV Cache significantly slows down inference speed. Therefore GQA model, as an alternative to MHA model, has been widely introduced into LLMs. In this work, we propose a low-cost method for pruning MHA models into GQA models with any compression ratio of key-value heads. Our method is based on $\\mathit{L_0}$ masks to gradually remove redundant parameters. In addition, we apply orthogonal transformations to attention heads without changing the model to increase similarity between attention heads before pruning training, in order to further improve performance of the model. Our method can be compatible with rotary position embedding (RoPE), which means the model after training can be fully adapted to the mainstream standard GQA framework. Experiments demonstrate that our strategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model without too much performance degradation, just achieved through supervised fine-tuning.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "108",
        "title": "Learning to Rank Pre-trained Vision-Language Models for Downstream Tasks",
        "author": [
            "Yuhe Ding",
            "Bo Jiang",
            "Aihua Zheng",
            "Qin Xu",
            "Jian Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20682",
        "abstract": "Vision language models (VLMs) like CLIP show stellar zero-shot capability on classification benchmarks. However, selecting the VLM with the highest performance on the unlabeled downstream task is non-trivial. Existing VLM selection methods focus on the class-name-only setting, relying on a supervised large-scale dataset and large language models, which may not be accessible or feasible during deployment. This paper introduces the problem of \\textbf{unsupervised vision-language model selection}, where only unsupervised downstream datasets are available, with no additional information provided. To solve this problem, we propose a method termed Visual-tExtual Graph Alignment (VEGA), to select VLMs without any annotations by measuring the alignment of the VLM between the two modalities on the downstream task. VEGA is motivated by the pretraining paradigm of VLMs, which aligns features with the same semantics from the visual and textual modalities, thereby mapping both modalities into a shared representation space. Specifically, we first construct two graphs on the vision and textual features, respectively. VEGA is then defined as the overall similarity between the visual and textual graphs at both node and edge levels. Extensive experiments across three different benchmarks, covering a variety of application scenarios and downstream datasets, demonstrate that VEGA consistently provides reliable and accurate estimates of VLMs' performance on unlabeled downstream tasks.",
        "tags": [
            "CLIP",
            "Large Language Models"
        ]
    },
    {
        "id": "109",
        "title": "UBER: Uncertainty-Based Evolution with Large Language Models for Automatic Heuristic Design",
        "author": [
            "Zijie Chen",
            "Zhanchao Zhou",
            "Yu Lu",
            "Renjun Xu",
            "Lili Pan",
            "Zhenzhong Lan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20694",
        "abstract": "NP-hard problem-solving traditionally relies on heuristics, but manually crafting effective heuristics for complex problems remains challenging. While recent work like FunSearch has demonstrated that large language models (LLMs) can be leveraged for heuristic design in evolutionary algorithm (EA) frameworks, their potential is not fully realized due to its deficiency in exploitation and exploration. We present UBER (Uncertainty-Based Evolution for Refinement), a method that enhances LLM+EA methods for automatic heuristic design by integrating uncertainty on top of the FunSearch framework. UBER introduces two key innovations: an Uncertainty-Inclusive Evolution Process (UIEP) for adaptive exploration-exploitation balance, and a principled Uncertainty-Inclusive Island Reset (UIIS) strategy for maintaining population diversity. Through extensive experiments on challenging NP-complete problems, UBER demonstrates significant improvements over FunSearch. Our work provides a new direction for the synergy of LLMs and EA, advancing the field of automatic heuristic design.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "110",
        "title": "HFI: A unified framework for training-free detection and implicit watermarking of latent diffusion model generated images",
        "author": [
            "Sungik Choi",
            "Sungwoo Park",
            "Jaehoon Lee",
            "Seunghyun Kim",
            "Stanley Jungkyu Choi",
            "Moontae Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20704",
        "abstract": "Dramatic advances in the quality of the latent diffusion models (LDMs) also led to the malicious use of AI-generated images. While current AI-generated image detection methods assume the availability of real/AI-generated images for training, this is practically limited given the vast expressibility of LDMs. This motivates the training-free detection setup where no related data are available in advance. The existing LDM-generated image detection method assumes that images generated by LDM are easier to reconstruct using an autoencoder than real images. However, we observe that this reconstruction distance is overfitted to background information, leading the current method to underperform in detecting images with simple backgrounds. To address this, we propose a novel method called HFI. Specifically, by viewing the autoencoder of LDM as a downsampling-upsampling kernel, HFI measures the extent of aliasing, a distortion of high-frequency information that appears in the reconstructed image. HFI is training-free, efficient, and consistently outperforms other training-free methods in detecting challenging images generated by various generative models. We also show that HFI can successfully detect the images generated from the specified LDM as a means of implicit watermarking. HFI outperforms the best baseline method while achieving magnitudes of",
        "tags": [
            "Detection",
            "Diffusion",
            "LDMs"
        ]
    },
    {
        "id": "111",
        "title": "ChartAdapter: Large Vision-Language Model for Chart Summarization",
        "author": [
            "Peixin Xu",
            "Yujuan Ding",
            "Wenqi Fan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20715",
        "abstract": "Chart summarization, which focuses on extracting key information from charts and interpreting it in natural language, is crucial for generating and delivering insights through effective and accessible data analysis. Traditional methods for chart understanding and summarization often rely on multi-stage pipelines, which may produce suboptimal semantic alignment between visual and textual information. In comparison, recently developed LLM-based methods are more dependent on the capability of foundation images or languages, while ignoring the characteristics of chart data and its relevant challenges. To address these limitations, we propose ChartAdapter, a novel lightweight transformer module designed to bridge the gap between charts and textual summaries. ChartAdapter employs learnable query vectors to extract implicit semantics from chart data and incorporates a cross-modal alignment projector to enhance vision-to-language generative learning. By integrating ChartAdapter with an LLM, we enable end-to-end training and efficient chart summarization. To further enhance the training, we introduce a three-stage hierarchical training procedure and develop a large-scale dataset specifically curated for chart summarization, comprising 190,618 samples. Experimental results on the standard Chart-to-Text testing set demonstrate that our approach significantly outperforms existing methods, including state-of-the-art models, in generating high-quality chart summaries. Ablation studies further validate the effectiveness of key components in ChartAdapter. This work highlights the potential of tailored LLM-based approaches to advance chart understanding and sets a strong foundation for future research in this area.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "112",
        "title": "M$^3$oralBench: A MultiModal Moral Benchmark for LVLMs",
        "author": [
            "Bei Yan",
            "Jie Zhang",
            "Zhiyuan Chen",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20718",
        "abstract": "Recently, large foundation models, including large language models (LLMs) and large vision-language models (LVLMs), have become essential tools in critical fields such as law, finance, and healthcare. As these models increasingly integrate into our daily life, it is necessary to conduct moral evaluation to ensure that their outputs align with human values and remain within moral boundaries. Previous works primarily focus on LLMs, proposing moral datasets and benchmarks limited to text modality. However, given the rapid development of LVLMs, there is still a lack of multimodal moral evaluation methods. To bridge this gap, we introduce M$^3$oralBench, the first MultiModal Moral Benchmark for LVLMs. M$^3$oralBench expands the everyday moral scenarios in Moral Foundations Vignettes (MFVs) and employs the text-to-image diffusion model, SD3.0, to create corresponding scenario images. It conducts moral evaluation across six moral foundations of Moral Foundations Theory (MFT) and encompasses tasks in moral judgement, moral classification, and moral response, providing a comprehensive assessment of model performance in multimodal moral understanding and reasoning. Extensive experiments on 10 popular open-source and closed-source LVLMs demonstrate that M$^3$oralBench is a challenging benchmark, exposing notable moral limitations in current models. Our benchmark is publicly available.",
        "tags": [
            "Diffusion",
            "LLMs",
            "Large Language Models",
            "Text-to-Image"
        ]
    },
    {
        "id": "113",
        "title": "4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives",
        "author": [
            "Zeyu Yang",
            "Zijie Pan",
            "Xiatian Zhu",
            "Li Zhang",
            "Yu-Gang Jiang",
            "Philip H.S. Torr"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20720",
        "abstract": "Dynamic 3D scene representation and novel view synthesis from captured videos are crucial for enabling immersive experiences required by AR/VR and metaverse applications. However, this task is challenging due to the complexity of unconstrained real-world scenes and their temporal dynamics. In this paper, we frame dynamic scenes as a spatio-temporal 4D volume learning problem, offering a native explicit reformulation with minimal assumptions about motion, which serves as a versatile dynamic scene learning framework. Specifically, we represent a target dynamic scene using a collection of 4D Gaussian primitives with explicit geometry and appearance features, dubbed as 4D Gaussian splatting (4DGS). This approach can capture relevant information in space and time by fitting the underlying spatio-temporal volume. Modeling the spacetime as a whole with 4D Gaussians parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, our model can naturally learn view-dependent and time-evolved appearance with 4D spherindrical harmonics. Notably, our 4DGS model is the first solution that supports real-time rendering of high-resolution, photorealistic novel views for complex dynamic scenes. To enhance efficiency, we derive several compact variants that effectively reduce memory footprint and mitigate the risk of overfitting. Extensive experiments validate the superiority of 4DGS in terms of visual quality and efficiency across a range of dynamic scene-related tasks (e.g., novel view synthesis, 4D generation, scene understanding) and scenarios (e.g., single object, indoor scenes, driving environments, synthetic and real data).",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "114",
        "title": "Dialogue Director: Bridging the Gap in Dialogue Visualization for Multimodal Storytelling",
        "author": [
            "Min Zhang",
            "Zilin Wang",
            "Liyan Chen",
            "Kunhong Liu",
            "Juncong Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20725",
        "abstract": "Recent advances in AI-driven storytelling have enhanced video generation and story visualization. However, translating dialogue-centric scripts into coherent storyboards remains a significant challenge due to limited script detail, inadequate physical context understanding, and the complexity of integrating cinematic principles. To address these challenges, we propose Dialogue Visualization, a novel task that transforms dialogue scripts into dynamic, multi-view storyboards. We introduce Dialogue Director, a training-free multimodal framework comprising a Script Director, Cinematographer, and Storyboard Maker. This framework leverages large multimodal models and diffusion-based architectures, employing techniques such as Chain-of-Thought reasoning, Retrieval-Augmented Generation, and multi-view synthesis to improve script understanding, physical context comprehension, and cinematic knowledge integration. Experimental results demonstrate that Dialogue Director outperforms state-of-the-art methods in script interpretation, physical world understanding, and cinematic principle application, significantly advancing the quality and controllability of dialogue-based story visualization.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "115",
        "title": "AverageLinear: Enhance Long-Term Time series forcasting with simple averaging",
        "author": [
            "Gaoxiang Zhao",
            "Li Zhou",
            "Xiaoqiang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20727",
        "abstract": "Long-term time series analysis aims to forecast long-term trends by examining changes over past and future periods. The intricacy of time series data poses significant challenges for modeling. Models based on the Transformer architecture, through the application of attention mechanisms to channels and sequences, have demonstrated notable performance advantages. In contrast, methods based on convolutional neural networks or linear models often struggle to effectively handle scenarios with large number of channels. However, our research reveals that the attention mechanism is not the core component responsible for performance enhancement. We have designed an exceedingly simple linear structure AverageLinear. By employing straightforward channel embedding and averaging operations, this model can effectively capture correlations between channels while maintaining a lightweight architecture. Experimentss on real-world datasets shows that AverageLinear matches or even surpasses state-of-the-art Transformer-based structures in performance. This indicates that using purely linear structures can also endow models with robust predictive power.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "116",
        "title": "Attributing Culture-Conditioned Generations to Pretraining Corpora",
        "author": [
            "Huihan Li",
            "Arnav Goel",
            "Keyu He",
            "Xiang Ren"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20760",
        "abstract": "In open-ended generative tasks like narrative writing or dialogue, large language models often exhibit cultural biases, showing limited knowledge and generating templated outputs for less prevalent cultures. Recent works show that these biases may stem from uneven cultural representation in pretraining corpora. This work investigates how pretraining leads to biased culture-conditioned generations by analyzing how models associate entities with cultures based on pretraining data patterns. We propose the MEMOed framework (MEMOrization from pretraining document) to determine whether a generation for a culture arises from memorization. Using MEMOed on culture-conditioned generations about food and clothing for 110 cultures, we find that high-frequency cultures in pretraining data yield more generations with memorized symbols, while some low-frequency cultures produce none. Additionally, the model favors generating entities with extraordinarily high frequency regardless of the conditioned culture, reflecting biases toward frequent pretraining terms irrespective of relevance. We hope that the MEMOed framework and our insights will inspire more works on attributing model performance on pretraining data.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "117",
        "title": "KeyGS: A Keyframe-Centric Gaussian Splatting Method for Monocular Image Sequences",
        "author": [
            "Keng-Wei Chang",
            "Zi-Ming Wang",
            "Shang-Hong Lai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20767",
        "abstract": "Reconstructing high-quality 3D models from sparse 2D images has garnered significant attention in computer vision. Recently, 3D Gaussian Splatting (3DGS) has gained prominence due to its explicit representation with efficient training speed and real-time rendering capabilities. However, existing methods still heavily depend on accurate camera poses for reconstruction. Although some recent approaches attempt to train 3DGS models without the Structure-from-Motion (SfM) preprocessing from monocular video datasets, these methods suffer from prolonged training times, making them impractical for many applications.\nIn this paper, we present an efficient framework that operates without any depth or matching model. Our approach initially uses SfM to quickly obtain rough camera poses within seconds, and then refines these poses by leveraging the dense representation in 3DGS. This framework effectively addresses the issue of long training times. Additionally, we integrate the densification process with joint refinement and propose a coarse-to-fine frequency-aware densification to reconstruct different levels of details. This approach prevents camera pose estimation from being trapped in local minima or drifting due to high-frequency signals. Our method significantly reduces training time from hours to minutes while achieving more accurate novel view synthesis and camera pose estimation compared to previous methods.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Pose Estimation"
        ]
    },
    {
        "id": "118",
        "title": "Large Language Model Enabled Multi-Task Physical Layer Network",
        "author": [
            "Tianyue Zheng",
            "Linglong Dai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20772",
        "abstract": "The recent advance of Artificial Intelligence (AI) is continuously reshaping the future 6G wireless communications. Recently, the development of Large Language Models (LLMs) offers a promising approach to effectively improve the performance and generalization for different physical layer tasks. However, most existing works finetune dedicated LLM networks for a single wireless communication task separately. Thus performing diverse physical layer tasks introduces extremely high training resources, memory usage, and deployment costs. To solve the problem, we propose a LLM-enabled multi-task physical layer network to unify multiple tasks with a single LLM. Specifically, we first propose a multi-task LLM framework, which finetunes LLM to perform multi-user precoding, signal detection and channel prediction simultaneously. Besides, multi-task instruction module, input encoders, as well as output decoders, are elaborately designed to distinguish multiple tasks and adapted the features of different formats of wireless data for the features of LLM. Numerical simulations are also displayed to verify the effectiveness of the proposed method.",
        "tags": [
            "Detection",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "119",
        "title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for LLMs in Cybersecurity",
        "author": [
            "Pengfei Jing",
            "Mengyun Tang",
            "Xiaorong Shi",
            "Xing Zheng",
            "Sen Nie",
            "Shi Wu",
            "Yong Yang",
            "Xiapu Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20787",
        "abstract": "Evaluating Large Language Models (LLMs) is crucial for understanding their capabilities and limitations across various applications, including natural language processing and code generation. Existing benchmarks like MMLU, C-Eval, and HumanEval assess general LLM performance but lack focus on specific expert domains such as cybersecurity. Previous attempts to create cybersecurity datasets have faced limitations, including insufficient data volume and a reliance on multiple-choice questions (MCQs). To address these gaps, we propose SecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in the cybersecurity domain. SecBench includes questions in various formats (MCQs and short-answer questions (SAQs)), at different capability levels (Knowledge Retention and Logical Reasoning), in multiple languages (Chinese and English), and across various sub-domains. The dataset was constructed by collecting high-quality data from open sources and organizing a Cybersecurity Question Design Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used the powerful while cost-effective LLMs to (1). label the data and (2). constructing a grading agent for automatic evaluation of http://SAQs.Benchmarking results on 13 SOTA LLMs demonstrate the usability of SecBench, which is arguably the largest and most comprehensive benchmark dataset for LLMs in cybersecurity. More information about SecBench can be found at our website, and the dataset can be accessed via the artifact link.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "120",
        "title": "VMix: Improving Text-to-Image Diffusion Model with Cross-Attention Mixing Control",
        "author": [
            "Shaojin Wu",
            "Fei Ding",
            "Mengqi Huang",
            "Wei Liu",
            "Qian He"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20800",
        "abstract": "While diffusion models show extraordinary talents in text-to-image generation, they may still fail to generate highly aesthetic images. More specifically, there is still a gap between the generated images and the real-world aesthetic images in finer-grained dimensions including color, lighting, composition, etc. In this paper, we propose Cross-Attention Value Mixing Control (VMix) Adapter, a plug-and-play aesthetics adapter, to upgrade the quality of generated images while maintaining generality across visual concepts by (1) disentangling the input text prompt into the content description and aesthetic description by the initialization of aesthetic embedding, and (2) integrating aesthetic conditions into the denoising process through value-mixed cross-attention, with the network connected by zero-initialized linear layers. Our key insight is to enhance the aesthetic presentation of existing diffusion models by designing a superior condition control method, all while preserving the image-text alignment. Through our meticulous design, VMix is flexible enough to be applied to community models for better visual performance without retraining. To validate the effectiveness of our method, we conducted extensive experiments, showing that VMix outperforms other state-of-the-art methods and is compatible with other community modules (e.g., LoRA, ControlNet, and IPAdapter) for image generation. The project page is https://vmix-diffusion.github.io/VMix/.",
        "tags": [
            "ControlNet",
            "Diffusion",
            "LoRA",
            "Text-to-Image"
        ]
    },
    {
        "id": "121",
        "title": "ReFlow6D: Refraction-Guided Transparent Object 6D Pose Estimation via Intermediate Representation Learning",
        "author": [
            "Hrishikesh Gupta",
            "Stefan Thalhammer",
            "Jean-Baptiste Weibel",
            "Alexander Haberl",
            "Markus Vincze"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20830",
        "abstract": "Transparent objects are ubiquitous in daily life, making their perception and robotics manipulation important. However, they present a major challenge due to their distinct refractive and reflective properties when it comes to accurately estimating the 6D pose. To solve this, we present ReFlow6D, a novel method for transparent object 6D pose estimation that harnesses the refractive-intermediate representation. Unlike conventional approaches, our method leverages a feature space impervious to changes in RGB image space and independent of depth information. Drawing inspiration from image matting, we model the deformation of the light path through transparent objects, yielding a unique object-specific intermediate representation guided by light refraction that is independent of the environment in which objects are observed. By integrating these intermediate features into the pose estimation network, we show that ReFlow6D achieves precise 6D pose estimation of transparent objects, using only RGB images as input. Our method further introduces a novel transparent object compositing loss, fostering the generation of superior refractive-intermediate features. Empirical evaluations show that our approach significantly outperforms state-of-the-art methods on TOD and Trans32K-6D datasets. Robot grasping experiments further demonstrate that ReFlow6D's pose estimation accuracy effectively translates to real-world robotics task. The source code is available at: https://github.com/StoicGilgamesh/ReFlow6D and https://github.com/StoicGilgamesh/matting_rendering.",
        "tags": [
            "Matting",
            "Pose Estimation",
            "Robot",
            "Robotics"
        ]
    },
    {
        "id": "122",
        "title": "Disentangling Preference Representation and Text Generation for Efficient Individual Preference Alignment",
        "author": [
            "Jianfei Zhang",
            "Jun Bai",
            "Bei Li",
            "Yanmeng Wang",
            "Rumei Li",
            "Chenghua Lin",
            "Wenge Rong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20834",
        "abstract": "Aligning Large Language Models (LLMs) with general human preferences has been proved crucial in improving the interaction quality between LLMs and human. However, human values are inherently diverse among different individuals, making it insufficient to align LLMs solely with general preferences. To address this, personalizing LLMs according to individual feedback emerges as a promising solution. Nonetheless, this approach presents challenges in terms of the efficiency of alignment algorithms. In this work, we introduce a flexible paradigm for individual preference alignment. Our method fundamentally improves efficiency by disentangling preference representation from text generation in LLMs. We validate our approach across multiple text generation tasks and demonstrate that it can produce aligned quality as well as or better than PEFT-based methods, while reducing additional training time for each new individual preference by $80\\%$ to $90\\%$ in comparison with them.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "123",
        "title": "Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation",
        "author": [
            "Shubh Singhal",
            "Raül Pérez-Gonzalo",
            "Andreas Espersen",
            "Antonio Agudo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20838",
        "abstract": "Accurate segmentation of wind turbine blade (WTB) images is critical for effective assessments, as it directly influences the performance of automated damage detection systems. Despite advancements in large universal vision models, these models often underperform in domain-specific tasks like WTB segmentation. To address this, we extend Intrinsic LoRA for image segmentation, and propose a novel dual-space augmentation strategy that integrates both image-level and latent-space augmentations. The image-space augmentation is achieved through linear interpolation between image pairs, while the latent-space augmentation is accomplished by introducing a noise-based latent probabilistic model. Our approach significantly boosts segmentation accuracy, surpassing current state-of-the-art methods in WTB image segmentation.",
        "tags": [
            "Detection",
            "LoRA",
            "Segmentation"
        ]
    },
    {
        "id": "124",
        "title": "Are LLMs Really Not Knowledgable? Mining the Submerged Knowledge in LLMs' Memory",
        "author": [
            "Xingjian Tao",
            "Yiwei Wang",
            "Yujun Cai",
            "Zhicheng Yang",
            "Jing Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20846",
        "abstract": "Large language models (LLMs) have shown promise as potential knowledge bases, yet they often struggle with question-answering tasks and are prone to hallucinations. While previous research attributes these issues to knowledge gaps in the model's parameters, our investigation reveals a different phenomenon: LLMs often retain correct knowledge even when generating incorrect answers. Through analysis of model's internal representations, we find that correct answers frequently appear among high-probability tokens despite not being selected as final outputs. Based on this observation, we introduce Hits@k, a new metric to assess knowledge retention independent of expression accuracy. Our extensive experiments demonstrate that LLMs store significantly more knowledge than their QA performance suggests. Building on these findings, we develop SkipUnsure, a method to improve answer accuracy by leveraging detected but unexpressed knowledge. Experiments on both open-domain and specific-domain datasets show consistent improvements, with accuracy gains of up to 11.8% on DBPedia and 6.3% on IMDB, without requiring model retraining.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "125",
        "title": "Enhancing Annotated Bibliography Generation with LLM Ensembles",
        "author": [
            "Sergio Bermejo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20864",
        "abstract": "This work proposes a novel approach to enhancing annotated bibliography generation through Large Language Model (LLM) ensembles. In particular, multiple LLMs in different roles -- controllable text generation, evaluation, and summarization -- are introduced and validated using a systematic methodology to enhance model performance in scholarly tasks. Output diversity among the ensemble that generates text is obtained using different LLM parameters, followed by an LLM acting as a judge to assess relevance, accuracy, and coherence. Responses selected by several combining strategies are then merged and refined through summarization and redundancy removal techniques. The preliminary experimental validation demonstrates that the combined outputs from the LLM ensemble improve coherence and relevance compared to individual responses, leading to a 38% improvement in annotation quality and a 51% reduction in content redundancy, thus highlighting the potential for automating complex scholarly tasks while maintaining high-quality standards.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "126",
        "title": "Attention Is All You Need For Mixture-of-Depths Routing",
        "author": [
            "Advait Gadhikar",
            "Souptik Kumar Majumdar",
            "Niclas Popp",
            "Piyapat Saranrittichai",
            "Martin Rapp",
            "Lukas Schott"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20875",
        "abstract": "Advancements in deep learning are driven by training models with increasingly larger numbers of parameters, which in turn heightens the computational demands. To address this issue, Mixture-of-Depths (MoD) models have been proposed to dynamically assign computations only to the most relevant parts of the inputs, thereby enabling the deployment of large-parameter models with high efficiency during inference and training. These MoD models utilize a routing mechanism to determine which tokens should be processed by a layer, or skipped. However, conventional MoD models employ additional network layers specifically for the routing which are difficult to train, and add complexity and deployment overhead to the model. In this paper, we introduce a novel attention-based routing mechanism A-MoD that leverages the existing attention map of the preceding layer for routing decisions within the current layer. Compared to standard routing, A-MoD allows for more efficient training as it introduces no additional trainable parameters and can be easily adapted from pretrained transformer models. Furthermore, it can increase the performance of the MoD model. For instance, we observe up to 2% higher accuracy on ImageNet compared to standard routing and isoFLOP ViT baselines. Furthermore, A-MoD improves the MoD training convergence, leading to up to 2x faster transfer learning.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "127",
        "title": "DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models",
        "author": [
            "Xiaolin Hu",
            "Xiang Cheng",
            "Peiyu Liu",
            "Wei Liu",
            "Jian Luan",
            "Bin Wang",
            "Yong Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20891",
        "abstract": "Low-rank adaptation (LoRA) reduces the computational and memory demands of fine-tuning large language models (LLMs) by approximating updates with low-rank matrices. However, low-rank approximation in two-dimensional space fails to capture high-dimensional structures within the target matrix. Recently, tensor decomposition methods have been explored for fine-tuning LLMs, leveraging their ability to extract structured information. Yet, these approaches primarily rely on random initialization, and the impact of initialization on tensor adaptation remains underexplored. In this paper, we reveal that random initialization significantly diverges from the validation loss achieved by full fine-tuning. To address this, we propose Weight-Decomposed Tensor Adaptation (DoTA), which leverages the Matrix Product Operator (MPO) decomposition of pre-trained weights for effective initialization in fine-tuning LLMs. Additionally, we introduce QDoTA, a quantized version of DoTA designed for 4-bit quantization. Experiments on commonsense and arithmetic reasoning tasks show that DoTA outperforms random initialization methods with fewer parameters. QDoTA further reduces memory consumption and achieves comparable performance to DoTA on commonsense reasoning tasks. We will release our code to support future research.",
        "tags": [
            "LLMs",
            "Large Language Models",
            "LoRA"
        ]
    },
    {
        "id": "128",
        "title": "Towards Compatible Fine-tuning for Vision-Language Model Updates",
        "author": [
            "Zhengbo Wang",
            "Jian Liang",
            "Lijun Sheng",
            "Ran He",
            "Zilei Wang",
            "Tieniu Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20895",
        "abstract": "So far, efficient fine-tuning has become a popular strategy for enhancing the capabilities of foundation models on downstream tasks by learning plug-and-play modules. However, existing methods overlook a crucial issue: if the underlying foundation model is updated, are these plug-and-play modules still effective? In this paper, we first conduct a detailed analysis of various fine-tuning methods on the CLIP in terms of their compatibility with model updates. The study reveals that many high-performing fine-tuning methods fail to be compatible with the upgraded models. To address this, we propose a novel approach, Class-conditioned Context Optimization (ContCoOp), which integrates learnable prompts with class embeddings using an attention layer before inputting them into the text encoder. Consequently, the prompts can dynamically adapt to the changes in embedding space (due to model updates), ensuring continued effectiveness. Extensive experiments over 15 datasets show that our ContCoOp achieves the highest compatibility over the baseline methods, and exhibits robust out-of-distribution generalization.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "129",
        "title": "DDIM sampling for Generative AIBIM, a faster intelligent structural design framework",
        "author": [
            "Zhili He",
            "Yu-Hsing Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20899",
        "abstract": "Generative AIBIM, a successful structural design pipeline, has proven its ability to intelligently generate high-quality, diverse, and creative shear wall designs that are tailored to specific physical conditions. However, the current module of Generative AIBIM that generates designs, known as the physics-based conditional diffusion model (PCDM), necessitates 1000 iterations for each generation due to its reliance on the denoising diffusion probabilistic model (DDPM) sampling process. This leads to a time-consuming and computationally demanding generation process. To address this issue, this study introduces the denoising diffusion implicit model (DDIM), an accelerated generation method that replaces the DDPM sampling process in PCDM. While the original DDIM was designed for DDPM and the optimization process of PCDM differs from that of DDPM, this paper designs \"DDIM sampling for PCDM,\" which modifies the original DDIM formulations to adapt to the optimization process of PCDM. Experimental results demonstrate that DDIM sampling for PCDM can accelerate the generation process of the original PCDM by a factor of 100 while maintaining the same visual quality in the generated results. This study effectively showcases the effectiveness of DDIM sampling for PCDM in expediting intelligent structural design. Furthermore, this paper reorganizes the contents of DDIM, focusing on the practical usage of DDIM. This change is particularly meaningful for researchers who may not possess a strong background in machine learning theory but are interested in utilizing the tool effectively.",
        "tags": [
            "DDIM",
            "DDPM",
            "Diffusion"
        ]
    },
    {
        "id": "130",
        "title": "ILDiff: Generate Transparent Animated Stickers by Implicit Layout Distillation",
        "author": [
            "Ting Zhang",
            "Zhiqiang Yuan",
            "Yeshuang Zhu",
            "Jinchao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20901",
        "abstract": "High-quality animated stickers usually contain transparent channels, which are often ignored by current video generation models. To generate fine-grained animated transparency channels, existing methods can be roughly divided into video matting algorithms and diffusion-based algorithms. The methods based on video matting have poor performance in dealing with semi-open areas in stickers, while diffusion-based methods are often used to model a single image, which will lead to local flicker when modeling animated stickers. In this paper, we firstly propose an ILDiff method to generate animated transparent channels through implicit layout distillation, which solves the problems of semi-open area collapse and no consideration of temporal information in existing methods. Secondly, we create the Transparent Animated Sticker Dataset (TASD), which contains 0.32M high-quality samples with transparent channel, to provide data support for related fields. Extensive experiments demonstrate that ILDiff can produce finer and smoother transparent channels compared to other methods such as Matting Anything and Layer Diffusion. Our code and dataset will be released at link https://xiaoyuan1996.github.io.",
        "tags": [
            "Diffusion",
            "Matting",
            "Video Generation"
        ]
    },
    {
        "id": "131",
        "title": "Low-Light Image Enhancement via Generative Perceptual Priors",
        "author": [
            "Han Zhou",
            "Wei Dong",
            "Xiaohong Liu",
            "Yulun Zhang",
            "Guangtao Zhai",
            "Jun Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20916",
        "abstract": "Although significant progress has been made in enhancing visibility, retrieving texture details, and mitigating noise in Low-Light (LL) images, the challenge persists in applying current Low-Light Image Enhancement (LLIE) methods to real-world scenarios, primarily due to the diverse illumination conditions encountered. Furthermore, the quest for generating enhancements that are visually realistic and attractive remains an underexplored realm. In response to these challenges, we introduce a novel \\textbf{LLIE} framework with the guidance of \\textbf{G}enerative \\textbf{P}erceptual \\textbf{P}riors (\\textbf{GPP-LLIE}) derived from vision-language models (VLMs). Specifically, we first propose a pipeline that guides VLMs to assess multiple visual attributes of the LL image and quantify the assessment to output the global and local perceptual priors. Subsequently, to incorporate these generative perceptual priors to benefit LLIE, we introduce a transformer-based backbone in the diffusion process, and develop a new layer normalization (\\textit{\\textbf{GPP-LN}}) and an attention mechanism (\\textit{\\textbf{LPP-Attn}}) guided by global and local perceptual priors. Extensive experiments demonstrate that our model outperforms current SOTA methods on paired LL datasets and exhibits superior generalization on real-world data. The code is released at \\url{https://github.com/LowLevelAI/GPP-LLIE}.",
        "tags": [
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "132",
        "title": "Enhanced Multimodal RAG-LLM for Accurate Visual Question Answering",
        "author": [
            "Junxiao Xue",
            "Quan Deng",
            "Fei Yu",
            "Yanhao Wang",
            "Jun Wang",
            "Yuehua Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20927",
        "abstract": "Multimodal large language models (MLLMs), such as GPT-4o, Gemini, LLaVA, and Flamingo, have made significant progress in integrating visual and textual modalities, excelling in tasks like visual question answering (VQA), image captioning, and content retrieval. They can generate coherent and contextually relevant descriptions of images. However, they still face challenges in accurately identifying and counting objects and determining their spatial locations, particularly in complex scenes with overlapping or small objects. To address these limitations, we propose a novel framework based on multimodal retrieval-augmented generation (RAG), which introduces structured scene graphs to enhance object recognition, relationship identification, and spatial understanding within images. Our framework improves the MLLM's capacity to handle tasks requiring precise visual descriptions, especially in scenarios with challenging perspectives, such as aerial views or scenes with dense object arrangements. Finally, we conduct extensive experiments on the VG-150 dataset that focuses on first-person visual understanding and the AUG dataset that involves aerial imagery. The results show that our approach consistently outperforms existing MLLMs in VQA tasks, which stands out in recognizing, localizing, and quantifying objects in different spatial contexts and provides more accurate visual descriptions.",
        "tags": [
            "GPT",
            "LLaVA",
            "Large Language Models",
            "RAG"
        ]
    },
    {
        "id": "133",
        "title": "Influence Maximization in Temporal Networks with Persistent and Reactive Behaviors",
        "author": [
            "Aaqib Zahoor",
            "Iqra Altaf Gillani",
            "Janib ul Bashir"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20936",
        "abstract": "Influence maximization in temporal social networks presents unique challenges due to the dynamic interactions that evolve over time. Traditional diffusion models often fall short in capturing the real-world complexities of active-inactive transitions among nodes, obscuring the true behavior of influence spread. In dynamic networks, nodes do not simply transition to an active state once; rather, they can oscillate between active and inactive states, with the potential for reactivation and reinforcement over time. This reactivation allows previously influenced nodes to regain influence potency, enhancing their ability to spread influence to others and amplifying the overall diffusion process. Ignoring these transitions can thus conceal the cumulative impact of influence, making it essential to account for them in any effective diffusion model. To address these challenges, we introduce the Continuous Persistent Susceptible-Infected Model with Reinforcement and Re-activation (cpSI-R), which explicitly incorporates active-inactive transitions, capturing the progressive reinforcement that makes nodes more potent spreaders upon reactivation. This model naturally leads to a submodular and monotone objective function, which supports efficient optimization for seed selection in influence maximization tasks. Alongside cpSI-R, we propose an efficient temporal snapshot sampling method, simplifying the analysis of evolving networks. We then adapt the prior algorithms of seed selection to our model and sampling strategy, resulting in reduced computational costs and enhanced seed selection efficiency. Experimental evaluations on diverse datasets demonstrate substantial improvements in performance over baseline methods, underscoring the effectiveness of cpSI-R for real-world temporal networks",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "134",
        "title": "Ontology-grounded Automatic Knowledge Graph Construction by LLM under Wikidata schema",
        "author": [
            "Xiaohan Feng",
            "Xixin Wu",
            "Helen Meng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20942",
        "abstract": "We propose an ontology-grounded approach to Knowledge Graph (KG) construction using Large Language Models (LLMs) on a knowledge base. An ontology is authored by generating Competency Questions (CQ) on knowledge base to discover knowledge scope, extracting relations from CQs, and attempt to replace equivalent relations by their counterpart in Wikidata. To ensure consistency and interpretability in the resulting KG, we ground generation of KG with the authored ontology based on extracted relations. Evaluation on benchmark datasets demonstrates competitive performance in knowledge graph construction task. Our work presents a promising direction for scalable KG construction pipeline with minimal human intervention, that yields high quality and human-interpretable KGs, which are interoperable with Wikidata semantics for potential knowledge base expansion.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "135",
        "title": "AGON: Automated Design Framework for Customizing Processors from ISA Documents",
        "author": [
            "Chongxiao Li",
            "Di Huang",
            "Pengwei Jin",
            "Tianyun Ma",
            "Husheng Han",
            "Shuyao Cheng",
            "Yifan Hao",
            "Yongwei Zhao",
            "Guanglin Xu",
            "Zidong Du",
            "Rui Zhang",
            "Xiaqing Li",
            "Yuanbo Wen",
            "Yanjun Wu",
            "Chen Zhao",
            "Xing Hu",
            "Qi Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20954",
        "abstract": "Customized processors are attractive solutions for vast domain-specific applications due to their high energy efficiency. However, designing a processor in traditional flows is time-consuming and expensive. To address this, researchers have explored methods including the use of agile development tools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming languages like C or SystemC, and more recently, leveraging large language models (LLMs) to generate hardware description language (HDL) code from natural language descriptions. However, each method has limitations in terms of expressiveness, correctness, and performance, leading to a persistent contradiction between the level of automation and the effectiveness of the design. Overall, how to automatically design highly efficient and practical processors with minimal human effort remains a challenge.\nIn this paper, we propose AGON, a novel framework designed to leverage LLMs for the efficient design of out-of-order (OoO) customized processors with minimal human effort. Central to AGON is the nano-operator function (nOP function) based Intermediate Representation (IR), which bridges high-level descriptions and hardware implementations while decoupling functionality from performance optimization, thereby providing an automatic design framework that is expressive and efficient, has correctness guarantees, and enables PPA (Power, Performance, and Area) optimization.\nExperimental results show that superior to previous LLM-assisted automatic design flows, AGON facilitates designing a series of customized OoO processors that achieve on average 2.35 $\\times$ speedup compared with BOOM, a general-purpose CPU designed by experts, with minimal design effort.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "136",
        "title": "Rise of Generative Artificial Intelligence in Science",
        "author": [
            "Liangping Ding",
            "Cornelia Lawson",
            "Philip Shapira"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20960",
        "abstract": "Generative Artificial Intelligence (GenAI, generative AI) has rapidly become available as a tool in scientific research. To explore the use of generative AI in science, we conduct an empirical analysis using OpenAlex. Analyzing GenAI publications and other AI publications from 2017 to 2023, we profile growth patterns, the diffusion of GenAI publications across fields of study, and the geographical spread of scientific research on generative AI. We also investigate team size and international collaborations to explore whether GenAI, as an emerging scientific research area, shows different collaboration patterns compared to other AI technologies. The results indicate that generative AI has experienced rapid growth and increasing presence in scientific publications. The use of GenAI now extends beyond computer science to other scientific research domains. Over the study period, U.S. researchers contributed nearly two-fifths of global GenAI publications. The U.S. is followed by China, with several small and medium-sized advanced economies demonstrating relatively high levels of GenAI deployment in their research publications. Although scientific research overall is becoming increasingly specialized and collaborative, our results suggest that GenAI research groups tend to have slightly smaller team sizes than found in other AI fields. Furthermore, notwithstanding recent geopolitical tensions, GenAI research continues to exhibit levels of international collaboration comparable to other AI technologies.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "137",
        "title": "Hierarchical Pose Estimation and Mapping with Multi-Scale Neural Feature Fields",
        "author": [
            "Evgenii Kruzhkov",
            "Alena Savinykh",
            "Sven Behnke"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20976",
        "abstract": "Robotic applications require a comprehensive understanding of the scene. In recent years, neural fields-based approaches that parameterize the entire environment have become popular. These approaches are promising due to their continuous nature and their ability to learn scene priors. However, the use of neural fields in robotics becomes challenging when dealing with unknown sensor poses and sequential measurements. This paper focuses on the problem of sensor pose estimation for large-scale neural implicit SLAM. We investigate implicit mapping from a probabilistic perspective and propose hierarchical pose estimation with a corresponding neural network architecture. Our method is well-suited for large-scale implicit map representations. The proposed approach operates on consecutive outdoor LiDAR scans and achieves accurate pose estimation, while maintaining stable mapping quality for both short and long trajectories. We built our method on a structured and sparse implicit representation suitable for large-scale reconstruction and evaluated it using the KITTI and MaiCity datasets. Our approach outperforms the baseline in terms of mapping with unknown poses and achieves state-of-the-art localization accuracy.",
        "tags": [
            "Pose Estimation",
            "Robotics",
            "SLAM"
        ]
    },
    {
        "id": "138",
        "title": "AlignAb: Pareto-Optimal Energy Alignment for Designing Nature-Like Antibodies",
        "author": [
            "Yibo Wen",
            "Chenwei Xu",
            "Jerry Yao-Chieh Hu",
            "Han Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20984",
        "abstract": "We present a three-stage framework for training deep learning models specializing in antibody sequence-structure co-design. We first pre-train a language model using millions of antibody sequence data. Then, we employ the learned representations to guide the training of a diffusion model for joint optimization over both sequence and structure of antibodies. During the final alignment stage, we optimize the model to favor antibodies with low repulsion and high attraction to the antigen binding site, enhancing the rationality and functionality of the designs. To mitigate conflicting energy preferences, we extend AbDPO (Antibody Direct Preference Optimization) to guide the model towards Pareto optimality under multiple energy-based alignment objectives. Furthermore, we adopt an iterative learning paradigm with temperature scaling, enabling the model to benefit from diverse online datasets without requiring additional data. In practice, our proposed methods achieve high stability and efficiency in producing a better Pareto front of antibody designs compared to top samples generated by baselines and previous alignment techniques. Through extensive experiments, we showcase the superior performance of our methods in generating nature-like antibodies with high binding affinity consistently.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "139",
        "title": "Efficiently Serving LLM Reasoning Programs with Certaindex",
        "author": [
            "Yichao Fu",
            "Junda Chen",
            "Siqi Zhu",
            "Zheyu Fu",
            "Zhongdongming Dai",
            "Aurick Qiao",
            "Hao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20993",
        "abstract": "The rapid evolution of large language models (LLMs) has unlocked their capabilities in advanced reasoning tasks like mathematical problem-solving, code generation, and legal analysis. Central to this progress are inference-time reasoning algorithms, which refine outputs by exploring multiple solution paths, at the cost of increasing compute demands and response latencies. Existing serving systems fail to adapt to the scaling behaviors of these algorithms or the varying difficulty of queries, leading to inefficient resource use and unmet latency targets.\nWe present Dynasor, a system that optimizes inference-time compute for LLM reasoning queries. Unlike traditional engines, Dynasor tracks and schedules requests within reasoning queries and uses Certaindex, a proxy that measures statistical reasoning progress based on model certainty, to guide compute allocation dynamically. Dynasor co-adapts scheduling with reasoning progress: it allocates more compute to hard queries, reduces compute for simpler ones, and terminates unpromising queries early, balancing accuracy, latency, and cost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50% in batch processing and sustaining 3.3x higher query rates or 4.7x tighter latency SLOs in online serving.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "140",
        "title": "KARPA: A Training-free Method of Adapting Knowledge Graph as References for Large Language Model's Reasoning Path Aggregation",
        "author": [
            "Siyuan Fang",
            "Kaijing Ma",
            "Tianyu Zheng",
            "Xinrun Du",
            "Ningxuan Lu",
            "Ge Zhang",
            "Qingkun Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20995",
        "abstract": "Large language models (LLMs) demonstrate exceptional performance across a variety of tasks, yet they are often affected by hallucinations and the timeliness of knowledge. Leveraging knowledge graphs (KGs) as external knowledge sources has emerged as a viable solution, but existing methods for LLM-based knowledge graph question answering (KGQA) are often limited by step-by-step decision-making on KGs, restricting the global planning and reasoning capabilities of LLMs, or they require fine-tuning or pre-training on specific KGs. To address these challenges, we propose Knowledge graph Assisted Reasoning Path Aggregation (KARPA), a novel framework that harnesses the global planning abilities of LLMs for efficient and accurate KG reasoning. KARPA operates in three steps: pre-planning relation paths using the LLM's global planning capabilities, matching semantically relevant paths via an embedding model, and reasoning over these paths to generate answers. Unlike existing KGQA methods, KARPA avoids stepwise traversal, requires no additional training, and is adaptable to various LLM architectures. Extensive experimental results show that KARPA achieves state-of-the-art performance in KGQA tasks, delivering both high efficiency and accuracy. Our code will be available on Github.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "141",
        "title": "Plug-and-Play Training Framework for Preference Optimization",
        "author": [
            "Jingyuan Ma",
            "Rui Li",
            "Zheng Li",
            "Lei Sha",
            "Zhifang Sui"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20996",
        "abstract": "Recently, preference optimization methods such as DPO have significantly enhanced large language models (LLMs) in wide tasks including dialogue and question-answering. However, current methods fail to account for the varying difficulty levels of training samples during preference optimization, leading to mediocre performance in tasks with high accuracy requirements, particularly in mathematical reasoning. To address this limitation, we propose a novel training framework, which employs multiple sampling to analyze output distributions, assign different weights to samples, and incorporate these weights into the preference optimization process. This plug-and-play approach enables LLMs to prioritize challenging examples during training, improving learning efficiency. Experimental results demonstrate that our framework integrates seamlessly with various preference optimization methods and achieves consistent improvements in mathematical reasoning tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "142",
        "title": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria",
        "author": [
            "Joonwon Jang",
            "Jaehee Kim",
            "Wonbin Kweon",
            "Hwanjo Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21006",
        "abstract": "Large Language Models (LLMs) rely on generating extensive intermediate reasoning units (e.g., tokens, sentences) to enhance final answer quality across a wide range of complex tasks. While generating multiple reasoning paths or iteratively refining rationales proves effective for improving performance, these approaches inevitably result in significantly higher inference costs. In this work, we propose a novel sentence-level rationale reduction training framework that leverages likelihood-based criteria, verbosity, to identify and remove redundant reasoning sentences. Unlike previous approaches that utilize token-level reduction, our sentence-level reduction framework maintains model performance while reducing generation length. This preserves the original reasoning abilities of LLMs and achieves an average 17.15% reduction in generation costs across various models and tasks.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "143",
        "title": "Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline",
        "author": [
            "Nicola Messina",
            "Lucia Vadicamo",
            "Leo Maltese",
            "Claudio Gennaro"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21009",
        "abstract": "Recent advancements in deep learning have significantly enhanced content-based retrieval methods, notably through models like CLIP that map images and texts into a shared embedding space. However, these methods often struggle with domain-specific entities and long-tail concepts absent from their training data, particularly in identifying specific individuals. In this paper, we explore the task of identity-aware cross-modal retrieval, which aims to retrieve images of persons in specific contexts based on natural language queries. This task is critical in various scenarios, such as for searching and browsing personalized video collections or large audio-visual archives maintained by national broadcasters. We introduce a novel dataset, COCO Person FaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched with deepfake-generated faces from VGGFace2. This dataset addresses the lack of large-scale datasets needed for training and evaluating models for this task. Our experiments assess the performance of different CLIP variations repurposed for this task, including our architecture, Identity-aware CLIP (Id-CLIP), which achieves competitive retrieval performance through targeted fine-tuning. Our contributions lay the groundwork for more robust cross-modal retrieval systems capable of recognizing long-tail identities and contextual nuances. Data and code are available at https://github.com/mesnico/IdCLIP.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "144",
        "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
        "author": [
            "Mahir Labib Dihan",
            "Mohammed Eunus Ali",
            "Md Rizwan Parvez"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21015",
        "abstract": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet Maps, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, a web application that streamlines the creation of reproducible, traceable map-based QA datasets. With its plug-and-play architecture, MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "145",
        "title": "Automated Robustness Testing for LLM-based NLP Software",
        "author": [
            "Mingxuan Xiao",
            "Yan Xiao",
            "Shunhui Ji",
            "Hanbo Cai",
            "Lei Xue",
            "Pengcheng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21016",
        "abstract": "Benefiting from the advancements in LLMs, NLP software has undergone rapid development. Such software is widely employed in various safety-critical tasks, such as financial sentiment analysis, toxic content moderation, and log generation. To our knowledge, there are no known automated robustness testing methods specifically designed for LLM-based NLP software. Given the complexity of LLMs and the unpredictability of real-world inputs (including prompts and examples), it is essential to examine the robustness of overall inputs to ensure the safety of such software.\nTo this end, this paper introduces the first AutOmated Robustness Testing frAmework, AORTA, which reconceptualizes the testing process into a combinatorial optimization problem. Existing testing methods designed for DNN-based software can be applied to LLM-based software by AORTA, but their effectiveness is limited. To address this, we propose a novel testing method for LLM-based software within AORTA called Adaptive Beam Search. ABS is tailored for the expansive feature space of LLMs and improves testing effectiveness through an adaptive beam width and the capability for backtracking.\nWe successfully embed 18 test methods in the designed framework AORTA and compared the test validity of ABS with three datasets and five threat models. ABS facilitates a more comprehensive and accurate robustness assessment before software deployment, with an average test success rate of 86.138%. Compared to the currently best-performing baseline PWWS, ABS significantly reduces the computational overhead by up to 3441.895 seconds per successful test case and decreases the number of queries by 218.762 times on average. Furthermore, test cases generated by ABS exhibit greater naturalness and transferability.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "146",
        "title": "Text Classification: Neural Networks VS Machine Learning Models VS Pre-trained Models",
        "author": [
            "Christos Petridis"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21022",
        "abstract": "Text classification is a very common task nowadays and there are many efficient methods and algorithms that we can employ to accomplish it. Transformers have revolutionized the field of deep learning, particularly in Natural Language Processing (NLP) and have rapidly expanded to other domains such as computer vision, time-series analysis and more. The transformer model was firstly introduced in the context of machine translation and its architecture relies on self-attention mechanisms to capture complex relationships within data sequences. It is able to handle long-range dependencies more effectively than traditional neural networks (such as Recurrent Neural Networks and Multilayer Perceptrons). In this work, we present a comparison between different techniques to perform text classification. We take into consideration seven pre-trained models, three standard neural networks and three machine learning models. For standard neural networks and machine learning models we also compare two embedding techniques: TF-IDF and GloVe, with the latter consistently outperforming the former. Finally, we demonstrate the results from our experiments where pre-trained models such as BERT and DistilBERT always perform better than standard models/algorithms.",
        "tags": [
            "BERT",
            "Transformer"
        ]
    },
    {
        "id": "147",
        "title": "Plancraft: an evaluation dataset for planning with LLM agents",
        "author": [
            "Gautier Dagan",
            "Frank Keller",
            "Alex Lascarides"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21033",
        "abstract": "We present Plancraft, a multi-modal evaluation dataset for LLM agents. Plancraft has both a text-only and multi-modal interface, based on the Minecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and Retrieval Augmented Generation (RAG), as well as an oracle planner and oracle RAG information extractor, to ablate the different components of a modern agent architecture. To evaluate decision-making, Plancraft also includes a subset of examples that are intentionally unsolvable, providing a realistic challenge that requires the agent not only to complete tasks but also to decide whether they are solvable at all. We benchmark both open-source and closed-source LLMs and strategies on our task and compare their performance to a handcrafted planner. We find that LLMs and VLMs struggle with the planning problems that Plancraft introduces, and we offer suggestions on how to improve their capabilities.",
        "tags": [
            "LLMs",
            "RAG"
        ]
    },
    {
        "id": "148",
        "title": "GePBench: Evaluating Fundamental Geometric Perception for Multimodal Large Language Models",
        "author": [
            "Shangyu Xing",
            "Changhao Xiang",
            "Yuteng Han",
            "Yifan Yue",
            "Zhen Wu",
            "Xinyu Liu",
            "Zhangtai Wu",
            "Fei Zhao",
            "Xinyu Dai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21036",
        "abstract": "Multimodal large language models (MLLMs) have achieved significant advancements in integrating visual and linguistic understanding. While existing benchmarks evaluate these models in context-rich, real-life scenarios, they often overlook fundamental perceptual skills essential for environments deviating from everyday realism. In particular, geometric perception, the ability to interpret spatial relationships and abstract visual patterns, remains underexplored. To address this limitation, we introduce GePBench, a novel benchmark designed to assess the geometric perception capabilities of MLLMs. Results from extensive evaluations reveal that current state-of-the-art MLLMs exhibit significant deficiencies in such tasks. Additionally, we demonstrate that models trained with data sourced from GePBench show notable improvements on a wide range of downstream tasks, underscoring the importance of geometric perception as a foundation for advanced multimodal applications. Our code and datasets will be publicly available.",
        "tags": [
            "Large Language Models"
        ]
    },
    {
        "id": "149",
        "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization",
        "author": [
            "Chia-Yu Hung",
            "Navonil Majumder",
            "Zhifeng Kong",
            "Ambuj Mehrish",
            "Rafael Valle",
            "Bryan Catanzaro",
            "Soujanya Poria"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21037",
        "abstract": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models lies in the difficulty of creating preference pairs, as TTA lacks structured mechanisms like verifiable rewards or gold-standard answers available for Large Language Models (LLMs). To address this, we propose CLAP-Ranked Preference Optimization (CRPO), a novel framework that iteratively generates and optimizes preference data to enhance TTA alignment. We demonstrate that the audio preference dataset generated using CRPO outperforms existing alternatives. With this framework, TangoFlux achieves state-of-the-art performance across both objective and subjective benchmarks. We open source all code and models to support further research in TTA generation.",
        "tags": [
            "Flow Matching",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "150",
        "title": "A locally-conservative proximal Galerkin method for pointwise bound constraints",
        "author": [
            "Guosheng Fu",
            "Brendan Keith",
            "Rami Masri"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21039",
        "abstract": "We introduce the first-order system proximal Galerkin (FOSPG) method, a locally mass-conserving, hybridizable finite element method for solving heterogeneous anisotropic diffusion and obstacle problems. Like other proximal Galerkin methods, FOSPG finds solutions by solving a recursive sequence of smooth, discretized, nonlinear subproblems. We establish the well-posedness and convergence of these nonlinear subproblems along with stability and error estimates under low regularity assumptions for the linearized equations obtained by solving each subproblem using Newton's method. The FOSPG method exhibits several advantages, including high-order accuracy, discrete maximum principle or bound-preserving discrete solutions, and local mass conservation. It also achieves prescribed solution accuracy within asymptotically mesh-independent numbers of subproblems and linear solves per subproblem iteration. Numerical experiments on benchmarks for anisotropic diffusion and obstacle problems confirm these attributes. Furthermore, an open-source implementation of the method is provided to facilitate broader adoption and reproducibility.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "151",
        "title": "Visual Style Prompt Learning Using Diffusion Models for Blind Face Restoration",
        "author": [
            "Wanglong Lu",
            "Jikai Wang",
            "Tao Wang",
            "Kaihao Zhang",
            "Xianta Jiang",
            "Hanli Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21042",
        "abstract": "Blind face restoration aims to recover high-quality facial images from various unidentified sources of degradation, posing significant challenges due to the minimal information retrievable from the degraded images. Prior knowledge-based methods, leveraging geometric priors and facial features, have led to advancements in face restoration but often fall short of capturing fine details. To address this, we introduce a visual style prompt learning framework that utilizes diffusion probabilistic models to explicitly generate visual prompts within the latent space of pre-trained generative models. These prompts are designed to guide the restoration process. To fully utilize the visual prompts and enhance the extraction of informative and rich patterns, we introduce a style-modulated aggregation transformation layer. Extensive experiments and applications demonstrate the superiority of our method in achieving high-quality blind face restoration. The source code is available at \\href{https://github.com/LonglongaaaGo/VSPBFR}{https://github.com/LonglongaaaGo/VSPBFR}.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "152",
        "title": "E2EDiff: Direct Mapping from Noise to Data for Enhanced Diffusion Models",
        "author": [
            "Zhiyu Tan",
            "WenXu Qian",
            "Hesen Chen",
            "Mengping Yang",
            "Lei Chen",
            "Hao Li"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21044",
        "abstract": "Diffusion models have emerged as a powerful framework for generative modeling, achieving state-of-the-art performance across various tasks. However, they face several inherent limitations, including a training-sampling gap, information leakage in the progressive noising process, and the inability to incorporate advanced loss functions like perceptual and adversarial losses during training. To address these challenges, we propose an innovative end-to-end training framework that aligns the training and sampling processes by directly optimizing the final reconstruction output. Our method eliminates the training-sampling gap, mitigates information leakage by treating the training process as a direct mapping from pure noise to the target data distribution, and enables the integration of perceptual and adversarial losses into the objective. Extensive experiments on benchmarks such as COCO30K and HW30K demonstrate that our approach consistently outperforms traditional diffusion models, achieving superior results in terms of FID and CLIP score, even with reduced sampling steps. These findings highlight the potential of end-to-end training to advance diffusion-based generative models toward more robust and efficient solutions.",
        "tags": [
            "CLIP",
            "Diffusion"
        ]
    },
    {
        "id": "153",
        "title": "Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense",
        "author": [
            "Yuyang Zhou",
            "Guang Cheng",
            "Kang Du",
            "Zihan Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21051",
        "abstract": "The rapid evolution of cloud computing technologies and the increasing number of cloud applications have provided a large number of benefits in daily lives. However, the diversity and complexity of different components pose a significant challenge to cloud security, especially when dealing with sophisticated and advanced cyberattacks. Recent advancements in generative foundation models (GFMs), particularly in the large language models (LLMs), offer promising solutions for security intelligence. By exploiting the powerful abilities in language understanding, data analysis, task inference, action planning, and code generation, we present LLM-PD, a novel proactive defense architecture that defeats various threats in a proactive manner. LLM-PD can efficiently make a decision through comprehensive data analysis and sequential reasoning, as well as dynamically creating and deploying actionable defense mechanisms on the target cloud. Furthermore, it can flexibly self-evolve based on experience learned from previous interactions and adapt to new attack scenarios without additional training. The experimental results demonstrate its remarkable ability in terms of defense effectiveness and efficiency, particularly highlighting an outstanding success rate when compared with other existing methods.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "154",
        "title": "VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation",
        "author": [
            "Jiazheng Xu",
            "Yu Huang",
            "Jiale Cheng",
            "Yuanming Yang",
            "Jiajun Xu",
            "Yuan Wang",
            "Wenbo Duan",
            "Shen Yang",
            "Qunlin Jin",
            "Shurun Li",
            "Jiayan Teng",
            "Zhuoyi Yang",
            "Wendi Zheng",
            "Xiao Liu",
            "Ming Ding",
            "Xiaohan Zhang",
            "Xiaotao Gu",
            "Shiyu Huang",
            "Minlie Huang",
            "Jie Tang",
            "Yuxiao Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21059",
        "abstract": "We present a general strategy to aligning visual generation models -- both image and video generation -- with human preference. To start with, we build VisionReward -- a fine-grained and multi-dimensional reward model. We decompose human preferences in images and videos into multiple dimensions, each represented by a series of judgment questions, linearly weighted and summed to an interpretable and accurate score. To address the challenges of video quality assessment, we systematically analyze various dynamic features of videos, which helps VisionReward surpass VideoScore by 17.2% and achieve top performance for video preference prediction. Based on VisionReward, we develop a multi-objective preference learning algorithm that effectively addresses the issue of confounding factors within preference data. Our approach significantly outperforms existing image and video scoring methods on both machine metrics and human evaluation. All code and datasets are provided at https://github.com/THUDM/VisionReward.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "155",
        "title": "BridgePure: Revealing the Fragility of Black-box Data Protection",
        "author": [
            "Yihan Wang",
            "Yiwei Lu",
            "Xiao-Shan Gao",
            "Gautam Kamath",
            "Yaoliang Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21061",
        "abstract": "Availability attacks, or unlearnable examples, are defensive techniques that allow data owners to modify their datasets in ways that prevent unauthorized machine learning models from learning effectively while maintaining the data's intended functionality. It has led to the release of popular black-box tools for users to upload personal data and receive protected counterparts. In this work, we show such black-box protections can be substantially bypassed if a small set of unprotected in-distribution data is available. Specifically, an adversary can (1) easily acquire (unprotected, protected) pairs by querying the black-box protections with the unprotected dataset; and (2) train a diffusion bridge model to build a mapping. This mapping, termed BridgePure, can effectively remove the protection from any previously unseen data within the same distribution. Under this threat model, our method demonstrates superior purification performance on classification and style mimicry tasks, exposing critical vulnerabilities in black-box data protection.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "156",
        "title": "Varformer: Adapting VAR's Generative Prior for Image Restoration",
        "author": [
            "Siyang Wang",
            "Feng Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21063",
        "abstract": "Generative models trained on extensive high-quality datasets effectively capture the structural and statistical properties of clean images, rendering them powerful priors for transforming degraded features into clean ones in image restoration. VAR, a novel image generative paradigm, surpasses diffusion models in generation quality by applying a next-scale prediction approach. It progressively captures both global structures and fine-grained details through the autoregressive process, consistent with the multi-scale restoration principle widely acknowledged in the restoration community. Furthermore, we observe that during the image reconstruction process utilizing VAR, scale predictions automatically modulate the input, facilitating the alignment of representations at subsequent scales with the distribution of clean images. To harness VAR's adaptive distribution alignment capability in image restoration tasks, we formulate the multi-scale latent representations within VAR as the restoration prior, thus advancing our delicately designed VarFormer framework. The strategic application of these priors enables our VarFormer to achieve remarkable generalization on unseen tasks while also reducing training computational costs. Extensive experiments underscores that our VarFormer outperforms existing multi-task image restoration methods across various restoration tasks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "157",
        "title": "Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight Task-Specific Adapters for Automatic Scoring",
        "author": [
            "Ehsan Latif",
            "Xiaoming Zhai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21065",
        "abstract": "The integration of Artificial Intelligence (AI) in education requires scalable and efficient frameworks that balance performance, adaptability, and cost. This paper addresses these needs by proposing a shared backbone model architecture enhanced with lightweight LoRA adapters for task-specific fine-tuning, targeting the automated scoring of student responses across 27 mutually exclusive tasks. By achieving competitive performance (average QWK of 0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memory consumption by 60% and inference latency by 40%, the framework demonstrates significant efficiency gains. This approach aligns with the workshops' focus on improving language models for educational tasks, creating responsible innovations for cost-sensitive deployment, and supporting educators by streamlining assessment workflows. The findings underscore the potential of scalable AI to enhance learning outcomes while maintaining fairness and transparency in automated scoring systems.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "158",
        "title": "Numerical analysis of a stabilized optimal control problem governed by a parabolic convection--diffusion equation",
        "author": [
            "Christos Pervolianakis"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21070",
        "abstract": "We consider an optimal control problem on a bounded domain $\\Omega\\subset{\\mathbb{R}}^2,$ governed by a parabolic convection--diffusion equation with pointwise control constraints. We follow the optimize-then-discretize-approach, where for the state and the co-state variable, we consider the piecewise finite element method alongside with the algebraic flux correction method for its stabilization and the for temporal discretization, we use the backward Euler for the state variable and explicit Euler for the co-state variable. The discrete control variable is obtained by the projection of the discretized adjoint state on the set of admissible controls. The resulting stabilized fully--discrete scheme is nonlinear and a fixed point argument is used in order to prove its existence and uniqueness under a mild condition between the time step $k$ and the mesh step $h,$ e.g., $k = \\mathcal{O}(h^{1+\\epsilon}),\\,0<\\epsilon<1.$ Further, for sufficiently regular solution, we derive error estimates in $L^2$ and $H^1$ norm with respect on space and $\\ell^\\infty$ norm in time for the state and the co-state variable. For the control variable we also derive an $L^2$ estimate for its error with respect to spatial variable and $\\ell^\\infty$ in time. Finally, we present numerical experiments that validate the the order of convergence of the stabilized fully--discrete scheme via algebraic flux correction method.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "159",
        "title": "Edicho: Consistent Image Editing in the Wild",
        "author": [
            "Qingyan Bai",
            "Hao Ouyang",
            "Yinghao Xu",
            "Qiuyu Wang",
            "Ceyuan Yang",
            "Ka Leong Cheng",
            "Yujun Shen",
            "Qifeng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21079",
        "abstract": "As a verified need, consistent editing across in-the-wild images remains a technical challenge arising from various unmanageable factors, like object poses, lighting conditions, and photography environments. Edicho steps in with a training-free solution based on diffusion models, featuring a fundamental design principle of using explicit image correspondence to direct editing. Specifically, the key components include an attention manipulation module and a carefully refined classifier-free guidance (CFG) denoising strategy, both of which take into account the pre-estimated correspondence. Such an inference-time algorithm enjoys a plug-and-play nature and is compatible to most diffusion-based editing methods, such as ControlNet and BrushNet. Extensive results demonstrate the efficacy of Edicho in consistent cross-image editing under diverse settings. We will release the code to facilitate future studies.",
        "tags": [
            "ControlNet",
            "Diffusion",
            "Image Editing"
        ]
    },
    {
        "id": "160",
        "title": "Vinci: A Real-time Embodied Smart Assistant based on Egocentric Vision-Language Model",
        "author": [
            "Yifei Huang",
            "Jilan Xu",
            "Baoqi Pei",
            "Yuping He",
            "Guo Chen",
            "Lijin Yang",
            "Xinyuan Chen",
            "Yaohui Wang",
            "Zheng Nie",
            "Jinyao Liu",
            "Guoshun Fan",
            "Dechen Lin",
            "Fang Fang",
            "Kunpeng Li",
            "Chang Yuan",
            "Yali Wang",
            "Yu Qiao",
            "Limin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21080",
        "abstract": "We introduce Vinci, a real-time embodied smart assistant built upon an egocentric vision-language model. Designed for deployment on portable devices such as smartphones and wearable cameras, Vinci operates in an \"always on\" mode, continuously observing the environment to deliver seamless interaction and assistance. Users can wake up the system and engage in natural conversations to ask questions or seek assistance, with responses delivered through audio for hands-free convenience. With its ability to process long video streams in real-time, Vinci can answer user queries about current observations and historical context while also providing task planning based on past interactions. To further enhance usability, Vinci integrates a video generation module that creates step-by-step visual demonstrations for tasks that require detailed guidance. We hope that Vinci can establish a robust framework for portable, real-time egocentric AI systems, empowering users with contextual and actionable insights. We release the complete implementation for the development of the device in conjunction with a demo web platform to test uploaded videos at https://github.com/OpenGVLab/vinci.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "161",
        "title": "Lyapunov-Based Deep Neural Networks for Adaptive Control of Stochastic Nonlinear Systems",
        "author": [
            "Saiedeh Akbari",
            "Cristian F. Nino",
            "Omkar Sudhir Patil",
            "Warren E. Dixon"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21095",
        "abstract": "Controlling nonlinear stochastic dynamical systems involves substantial challenges when the dynamics contain unknown and unstructured nonlinear state-dependent terms. For such complex systems, deep neural networks can serve as powerful black box approximators for the unknown drift and diffusion processes. Recent developments construct Lyapunov-based deep neural network (Lb-DNN) controllers to compensate for deterministic uncertainties using adaptive weight update laws derived from a Lyapunov-based analysis based on insights from the compositional structure of the DNN architecture. However, these Lb-DNN controllers do not account for non-deterministic uncertainties. This paper develops Lb-DNNs to adaptively compensate for both the drift and diffusion uncertainties of nonlinear stochastic dynamic systems. Through a Lyapunov-based stability analysis, a DNN-based approximation and corresponding DNN weight adaptation laws are constructed to eliminate the unknown state-dependent terms resulting from the nonlinear diffusion and drift processes. The tracking error is shown to be uniformly ultimately bounded in probability. Simulations are performed on a nonlinear stochastic dynamical system to show efficacy of the proposed method.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "162",
        "title": "Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D Scene Generation",
        "author": [
            "Yuanbo Yang",
            "Jiahao Shao",
            "Xinyang Li",
            "Yujun Shen",
            "Andreas Geiger",
            "Yiyi Liao"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21117",
        "abstract": "In this work, we introduce Prometheus, a 3D-aware latent diffusion model for text-to-3D generation at both object and scene levels in seconds. We formulate 3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussian generation within the latent diffusion paradigm. To ensure generalizability, we build our model upon pre-trained text-to-image generation model with only minimal adjustments, and further train it using a large number of images from both single-view and multi-view datasets. Furthermore, we introduce an RGB-D latent space into 3D Gaussian generation to disentangle appearance and geometry information, enabling efficient feed-forward generation of 3D Gaussians with better fidelity and geometry. Extensive experimental results demonstrate the effectiveness of our method in both feed-forward 3D Gaussian reconstruction and text-to-3D generation. Project page: https://freemty.github.io/project-prometheus/",
        "tags": [
            "3D",
            "Diffusion",
            "Text-to-3D",
            "Text-to-Image"
        ]
    },
    {
        "id": "163",
        "title": "ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language Modeling Exploitation",
        "author": [
            "Ruixuan Liu",
            "Toan Tran",
            "Tianhao Wang",
            "Hongsheng Hu",
            "Shuo Wang",
            "Li Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21123",
        "abstract": "As large language models (LLMs) increasingly depend on web-scraped datasets, concerns over unauthorized use of copyrighted or personal content for training have intensified. Despite regulations such as the General Data Protection Regulation (GDPR), data owners still have limited control over the use of their content in model training. To address this, we propose ExpShield, a proactive self-guard mechanism that empowers content owners to embed invisible perturbations into their text, limiting data misuse in LLMs training without affecting readability. This preemptive approach enables data owners to protect sensitive content directly, without relying on a third-party to perform defense. Starting from the random perturbation, we demonstrate the rationale for using perturbation to conceal protected content. We further enhance the efficiency by identifying memorization triggers and creating pitfalls to diverge the model memorization in a more focused way. To validate our defense's effectiveness, we propose a novel metric of instance exploitation which captures the individual risk raised by model training. The experimental results validate the effectiveness of our approach as the MIA AUC decreases from 0.95 to 0.55, and instance exploitation approaches zero. This suggests that the individual risk does not increase after training, underscoring the significance of proactive defenses in protecting copyrighted data.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "164",
        "title": "Adaptive Batch Size Schedules for Distributed Training of Language Models with Data and Model Parallelism",
        "author": [
            "Tim Tsz-Kit Lau",
            "Weijian Li",
            "Chenwei Xu",
            "Han Liu",
            "Mladen Kolar"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21124",
        "abstract": "An appropriate choice of batch sizes in large-scale model training is crucial, yet it involves an intrinsic yet inevitable dilemma: large-batch training improves training efficiency in terms of memory utilization, while generalization performance often deteriorates due to small amounts of gradient noise. Despite this dilemma, the common practice of choosing batch sizes in language model training often prioritizes training efficiency -- employing either constant large sizes with data parallelism or implementing batch size warmup schedules. However, such batch size schedule designs remain heuristic and often fail to adapt to training dynamics, presenting the challenge of designing adaptive batch size schedules. Given the abundance of available datasets and the data-hungry nature of language models, data parallelism has become an indispensable distributed training paradigm, enabling the use of larger batch sizes for gradient computation. However, vanilla data parallelism requires replicas of model parameters, gradients, and optimizer states at each worker, which prohibits training larger models with billions of parameters. To optimize memory usage, more advanced parallelism strategies must be employed. In this work, we propose general-purpose and theoretically principled adaptive batch size schedules compatible with data parallelism and model parallelism. We develop a practical implementation with PyTorch Fully Sharded Data Parallel, facilitating the pretraining of language models of different sizes. We empirically demonstrate that our proposed approaches outperform constant batch sizes and heuristic batch size warmup schedules in the pretraining of models in the Llama family, with particular focus on smaller models with up to 3 billion parameters. We also establish theoretical convergence guarantees for such adaptive batch size schedules with Adam for general smooth nonconvex objectives.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "165",
        "title": "Facilitating large language model Russian adaptation with Learned Embedding Propagation",
        "author": [
            "Mikhail Tikhomirov",
            "Daniil Chernyshev"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21140",
        "abstract": "Rapid advancements of large language model (LLM) technologies led to the introduction of powerful open-source instruction-tuned LLMs that have the same text generation quality as the state-of-the-art counterparts such as GPT-4. While the emergence of such models accelerates the adoption of LLM technologies in sensitive-information environments the authors of such models don not disclose the training data necessary for replication of the results thus making the achievements model-exclusive. Since those open-source models are also multilingual this in turn reduces the benefits of training a language specific LLMs as improved inference computation efficiency becomes the only guaranteed advantage of such costly procedure. More cost-efficient options such as vocabulary extension and subsequent continued pre-training are also inhibited by the lack of access to high-quality instruction-tuning data since it is the major factor behind the resulting LLM task-solving capabilities. To address the limitations and cut the costs of the language adaptation pipeline we propose Learned Embedding Propagation (LEP). Unlike existing approaches our method has lower training data size requirements due to minimal impact on existing LLM knowledge which we reinforce using novel ad-hoc embedding propagation procedure that allows to skip the instruction-tuning step and instead implant the new language knowledge directly into any existing instruct-tuned variant. We evaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B, showing that LEP is competitive with traditional instruction-tuning methods, achieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with further improvements via self-calibration and continued tuning enhancing task-solving capabilities.",
        "tags": [
            "GPT",
            "LLMs",
            "LLaMA"
        ]
    },
    {
        "id": "166",
        "title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
        "author": [
            "Xingyu Chen",
            "Jiahao Xu",
            "Tian Liang",
            "Zhiwei He",
            "Jianhui Pang",
            "Dian Yu",
            "Linfeng Song",
            "Qiuzhi Liu",
            "Mengfei Zhou",
            "Zhuosheng Zhang",
            "Rui Wang",
            "Zhaopeng Tu",
            "Haitao Mi",
            "Dong Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21187",
        "abstract": "The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.",
        "tags": [
            "LLMs"
        ]
    },
    {
        "id": "167",
        "title": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation",
        "author": [
            "Zhaojian Yu",
            "Yilun Zhao",
            "Arman Cohan",
            "Xiao-Ping Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21199",
        "abstract": "We introduce self-invoking code generation, a new task designed to evaluate the progressive reasoning and problem-solving capabilities of LLMs. In this task, models are presented with a base problem and a related, more complex problem. They must solve the base problem and then utilize its solution to address the more complex one. This work features three key contributions. First, we propose a general recipe for generating more challenging versions of existing benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on self-invoking code generation. Second, from the analysis of experimental results over twenty LLMs on our benchmarks, we have two important observations: (i) Most LLMs excel in traditional code generation benchmarks like HumanEval and MBPP, but their performance declines on self-invoking tasks. For example, o1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro. (ii) On self-invoking code generation task, the instruction-tuned models demonstrate only marginal improvements compared to the base models. Third, we disclose the types of failure modes that exist in our evaluation results. All these results underscore the need for further advancements in self-invoking code generation tasks and provide a new direction for future research on enhancing LLMs' code reasoning capabilities.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "168",
        "title": "Distributed Mixture-of-Agents for Edge Inference with Large Language Models",
        "author": [
            "Purbesh Mitra",
            "Priyanka Kaswan",
            "Sennur Ulukus"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21200",
        "abstract": "Mixture-of-Agents (MoA) has recently been proposed as a method to enhance performance of large language models (LLMs), enabling multiple individual LLMs to work together for collaborative inference. This collaborative approach results in improved responses to user prompts compared to relying on a single LLM. In this paper, we consider such an MoA architecture in a distributed setting, where LLMs operate on individual edge devices, each uniquely associated with a user and equipped with its own distributed computing power. These devices exchange information using decentralized gossip algorithms, allowing different device nodes to talk without the supervision of a centralized server. In the considered setup, different users have their own LLM models to address user prompts. Additionally, the devices gossip either their own user-specific prompts or augmented prompts to generate more refined answers to certain queries. User prompts are temporarily stored in the device queues when their corresponding LLMs are busy. Given the memory limitations of edge devices, it is crucial to ensure that the average queue sizes in the system remain bounded. In this paper, we address this by theoretically calculating the queuing stability conditions for the device queues under reasonable assumptions, which we validate experimentally as well. Further, we demonstrate through experiments, leveraging open-source LLMs for the implementation of distributed MoA, that certain MoA configurations produce higher-quality responses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The implementation is available at: https://github.com/purbeshmitra/distributed_moa.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "169",
        "title": "PERSE: Personalized 3D Generative Avatars from A Single Portrait",
        "author": [
            "Hyunsoo Cha",
            "Inhee Lee",
            "Hanbyul Joo"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21206",
        "abstract": "We present PERSE, a method for building an animatable personalized generative avatar from a reference portrait. Our avatar model enables facial attribute editing in a continuous and disentangled latent space to control each facial attribute, while preserving the individual's identity. To achieve this, our method begins by synthesizing large-scale synthetic 2D video datasets, where each video contains consistent changes in the facial expression and viewpoint, combined with a variation in a specific facial attribute from the original input. We propose a novel pipeline to produce high-quality, photorealistic 2D videos with facial attribute editing. Leveraging this synthetic attribute dataset, we present a personalized avatar creation method based on the 3D Gaussian Splatting, learning a continuous and disentangled latent space for intuitive facial attribute manipulation. To enforce smooth transitions in this latent space, we introduce a latent space regularization technique by using interpolated 2D faces as supervision. Compared to previous approaches, we demonstrate that PERSE generates high-quality avatars with interpolated attributes while preserving identity of reference person.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "170",
        "title": "From Generalist to Specialist: A Survey of Large Language Models for Chemistry",
        "author": [
            "Yang Han",
            "Ziping Wan",
            "Lu Chen",
            "Kai Yu",
            "Xin Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2412.19994",
        "abstract": "Large Language Models (LLMs) have significantly transformed our daily life and established a new paradigm in natural language processing (NLP). However, the predominant pretraining of LLMs on extensive web-based texts remains insufficient for advanced scientific discovery, particularly in chemistry. The scarcity of specialized chemistry data, coupled with the complexity of multi-modal data such as 2D graph, 3D structure and spectrum, present distinct challenges. Although several studies have reviewed Pretrained Language Models (PLMs) in chemistry, there is a conspicuous absence of a systematic survey specifically focused on chemistry-oriented LLMs. In this paper, we outline methodologies for incorporating domain-specific chemistry knowledge and multi-modal information into LLMs, we also conceptualize chemistry LLMs as agents using chemistry tools and investigate their potential to accelerate scientific research. Additionally, we conclude the existing benchmarks to evaluate chemistry ability of LLMs. Finally, we critically examine the current challenges and identify promising directions for future research. Through this comprehensive survey, we aim to assist researchers in staying at the forefront of developments in chemistry LLMs and to inspire innovative applications in the field.",
        "tags": [
            "3D",
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "171",
        "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
        "author": [
            "Yijia Xiao",
            "Edward Sun",
            "Di Luo",
            "Wei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20138",
        "abstract": "Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs). In finance, efforts have largely focused on single-agent systems handling specific tasks or multi-agent frameworks independently gathering data. However, multi-agent systems' potential to replicate real-world trading firms' collaborative dynamics remains underexplored. TradingAgents proposes a novel stock trading framework inspired by trading firms, featuring LLM-powered agents in specialized roles such as fundamental analysts, sentiment analysts, technical analysts, and traders with varied risk profiles. The framework includes Bull and Bear researcher agents assessing market conditions, a risk management team monitoring exposure, and traders synthesizing insights from debates and historical data to make informed decisions. By simulating a dynamic, collaborative trading environment, this framework aims to improve trading performance. Detailed architecture and extensive experiments reveal its superiority over baseline models, with notable improvements in cumulative returns, Sharpe ratio, and maximum drawdown, highlighting the potential of multi-agent LLM frameworks in financial trading.",
        "tags": [
            "LLMs",
            "Large Language Models"
        ]
    },
    {
        "id": "172",
        "title": "Bird Vocalization Embedding Extraction Using Self-Supervised Disentangled Representation Learning",
        "author": [
            "Runwu Shi",
            "Katsutoshi Itoyama",
            "Kazuhiro Nakadai"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20146",
        "abstract": "This paper addresses the extraction of the bird vocalization embedding from the whole song level using disentangled representation learning (DRL). Bird vocalization embeddings are necessary for large-scale bioacoustic tasks, and self-supervised methods such as Variational Autoencoder (VAE) have shown their performance in extracting such low-dimensional embeddings from vocalization segments on the note or syllable level. To extend the processing level to the entire song instead of cutting into segments, this paper regards each vocalization as the generalized and discriminative part and uses two encoders to learn these two parts. The proposed method is evaluated on the Great Tits dataset according to the clustering performance, and the results outperform the compared pre-trained models and vanilla VAE. Finally, this paper analyzes the informative part of the embedding, further compresses its dimension, and explains the disentangled performance of bird vocalizations.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "173",
        "title": "EmoReg: Directional Latent Vector Modeling for Emotional Intensity Regularization in Diffusion-based Voice Conversion",
        "author": [
            "Ashishkumar Gudmalwar",
            "Ishan D. Biyani",
            "Nirmesh Shah",
            "Pankaj Wasnik",
            "Rajiv Ratn Shah"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20359",
        "abstract": "The Emotional Voice Conversion (EVC) aims to convert the discrete emotional state from the source emotion to the target for a given speech utterance while preserving linguistic content. In this paper, we propose regularizing emotion intensity in the diffusion-based EVC framework to generate precise speech of the target emotion. Traditional approaches control the intensity of an emotional state in the utterance via emotion class probabilities or intensity labels that often lead to inept style manipulations and degradations in quality. On the contrary, we aim to regulate emotion intensity using self-supervised learning-based feature representations and unsupervised directional latent vector modeling (DVM) in the emotional embedding space within a diffusion-based framework. These emotion embeddings can be modified based on the given target emotion intensity and the corresponding direction vector. Furthermore, the updated embeddings can be fused in the reverse diffusion process to generate the speech with the desired emotion and intensity. In summary, this paper aims to achieve high-quality emotional intensity regularization in the diffusion-based EVC framework, which is the first of its kind work. The effectiveness of the proposed method has been shown across state-of-the-art (SOTA) baselines in terms of subjective and objective evaluations for the English and Hindi languages \\footnote{Demo samples are available at the following URL: \\url{https://nirmesh-sony.github.io/EmoReg/}}.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "174",
        "title": "Iterative structural coarse-graining for contagion dynamics in complex networks",
        "author": [
            "Leyang Xue",
            "Zengru Di",
            "An Zeng"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20503",
        "abstract": "Contagion dynamics in complex networks drive critical phenomena such as epidemic spread and information diffusion,but their analysis remains computationally prohibitive in large-scale, high-complexity systems. Here, we introduce the Iterative Structural Coarse-Graining (ISCG) framework, a scalable methodology that reduces network complexity while preserving key contagion dynamics with high fidelity. Importantly, we derive theoretical conditions ensuring the precise preservation of both macroscopic outbreak sizes and microscopic node-level infection probabilities during network reduction. Under these conditions, extensive experiments on diverse empirical networks demonstrate that ISCG achieves significant complexity reduction without sacrificing prediction accuracy. Beyond simplification, ISCG reveals multiscale structural patterns that govern contagion processes, enabling practical solutions to longstanding challenges in contagion dynamics. Specifically, ISCG outperforms traditional adaptive centrality-based approaches in identifying influential spreaders, immunizing critical edges, and optimizing sentinel placement for early outbreak detection, offering superior accuracy and computational efficiency. By bridging computational efficiency with dynamical fidelity, ISCG provides a transformative framework for analyzing large-scale contagion processes, with broad applications for epidemiology, information dissemination, and network resilience.",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "175",
        "title": "Testing and Improving the Robustness of Amortized Bayesian Inference for Cognitive Models",
        "author": [
            "Yufei Wu",
            "Stefan Radev",
            "Francis Tuerlinckx"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20586",
        "abstract": "Contaminant observations and outliers often cause problems when estimating the parameters of cognitive models, which are statistical models representing cognitive processes. In this study, we test and improve the robustness of parameter estimation using amortized Bayesian inference (ABI) with neural networks. To this end, we conduct systematic analyses on a toy example and analyze both synthetic and real data using a popular cognitive model, the Drift Diffusion Models (DDM). First, we study the sensitivity of ABI to contaminants with tools from robust statistics: the empirical influence function and the breakdown point. Next, we propose a data augmentation or noise injection approach that incorporates a contamination distribution into the data-generating process during training. We examine several candidate distributions and evaluate their performance and cost in terms of accuracy and efficiency loss relative to a standard estimator. Introducing contaminants from a Cauchy distribution during training considerably increases the robustness of the neural density estimator as measured by bounded influence functions and a much higher breakdown point. Overall, the proposed method is straightforward and practical to implement and has a broad applicability in fields where outlier detection or removal is challenging.",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "176",
        "title": "Metadata-Enhanced Speech Emotion Recognition: Augmented Residual Integration and Co-Attention in Two-Stage Fine-Tuning",
        "author": [
            "Zixiang Wan",
            "Ziyue Qiu",
            "Yiyang Liu",
            "Wei-Qiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20707",
        "abstract": "Speech Emotion Recognition (SER) involves analyzing vocal expressions to determine the emotional state of speakers, where the comprehensive and thorough utilization of audio information is paramount. Therefore, we propose a novel approach on self-supervised learning (SSL) models that employs all available auxiliary information -- specifically metadata -- to enhance performance. Through a two-stage fine-tuning method in multi-task learning, we introduce the Augmented Residual Integration (ARI) module, which enhances transformer layers in encoder of SSL models. The module efficiently preserves acoustic features across all different levels, thereby significantly improving the performance of metadata-related auxiliary tasks that require various levels of features. Moreover, the Co-attention module is incorporated due to its complementary nature with ARI, enabling the model to effectively utilize multidimensional information and contextual relationships from metadata-related auxiliary tasks. Under pre-trained base models and speaker-independent setup, our approach consistently surpasses state-of-the-art (SOTA) models on multiple SSL encoders for the IEMOCAP dataset.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "177",
        "title": "Optimal Diffusion Processes",
        "author": [
            "Saber Jafarizadeh"
        ],
        "pdf": "https://arxiv.org/pdf/2412.20934",
        "abstract": "Of stochastic differential equations, diffusion processes have been adopted in numerous applications, as more relevant and flexible models. This paper studies diffusion processes in a different setting, where for a given stationary distribution and average variance, it seeks the diffusion process with optimal convergence rate. It is shown that the optimal drift function is a linear function and the convergence rate of the stochastic process is bounded by the ratio of the average variance to the variance of the stationary distribution. Furthermore, the concavity of the optimal relaxation time as a function of the stationary distribution has been proven, and it is shown that all Pearson diffusion processes of the Hypergeometric type with polynomial functions of at most degree two as the variance functions are optimal.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "178",
        "title": "Quantum Diffusion Model for Quark and Gluon Jet Generation",
        "author": [
            "Mariia Baidachna",
            "Rey Guadarrama",
            "Gopal Ramesh Dahale",
            "Tom Magorsch",
            "Isabel Pedraza",
            "Konstantin T. Matchev",
            "Katia Matcheva",
            "Kyoungchul Kong",
            "Sergei Gleyzer"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21082",
        "abstract": "Diffusion models have demonstrated remarkable success in image generation, but they are computationally intensive and time-consuming to train. In this paper, we introduce a novel diffusion model that benefits from quantum computing techniques in order to mitigate computational challenges and enhance generative performance within high energy physics data. The fully quantum diffusion model replaces Gaussian noise with random unitary matrices in the forward process and incorporates a variational quantum circuit within the U-Net in the denoising architecture. We run evaluations on the structurally complex quark and gluon jets dataset from the Large Hadron Collider. The results demonstrate that the fully quantum and hybrid models are competitive with a similar classical model for jet generation, highlighting the potential of using quantum techniques for machine learning problems.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "179",
        "title": "Sparse chaos in cortical circuits",
        "author": [
            "Rainer Engelken",
            "Michael Monteforte",
            "Fred Wolf"
        ],
        "pdf": "https://arxiv.org/pdf/2412.21188",
        "abstract": "Nerve impulses, the currency of information flow in the brain, are generated by an instability of the neuronal membrane potential dynamics. Neuronal circuits exhibit collective chaos that appears essential for learning, memory, sensory processing, and motor control. However, the factors controlling the nature and intensity of collective chaos in neuronal circuits are not well understood. Here we use computational ergodic theory to demonstrate that basic features of nerve impulse generation profoundly affect collective chaos in neuronal circuits. Numerically exact calculations of Lyapunov spectra, Kolmogorov-Sinai-entropy, and upper and lower bounds on attractor dimension show that changes in nerve impulse generation in individual neurons moderately impact information encoding rates but qualitatively transform phase space structure. Specifically, we find a drastic reduction in the number of unstable manifolds, Kolmogorov-Sinai entropy, and attractor dimension. Beyond a critical point, marked by the simultaneous breakdown of the diffusion approximation, a peak in the largest Lyapunov exponent, and a localization transition of the leading covariant Lyapunov vector, networks exhibit sparse chaos: prolonged periods of near stable dynamics interrupted by short bursts of intense chaos. Analysis of large, more realistically structured networks supports the generality of these findings. In cortical circuits, biophysical properties appear tuned to this regime of sparse chaos. Our results reveal a close link between fundamental aspects of single-neuron biophysics and the collective dynamics of cortical circuits, suggesting that nerve impulse generation mechanisms are adapted to enhance circuit controllability and information flow.",
        "tags": [
            "Diffusion"
        ]
    }
]